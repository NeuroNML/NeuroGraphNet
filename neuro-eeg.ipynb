{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroGraphNet\n",
    "\n",
    "*A graph-based deep learning framework for EEG seizure detection, designed to improve accuracy and interpretability by leveraging Graph Neural Networks (GNNs) to capture spatial and temporal brain dynamics.*\n",
    "\n",
    "<hr />\n",
    "\n",
    "This notebook presents **NeuroGraphNet**, a model that applies Graph Neural Networks to EEG data for seizure detection. The primary goal is to **compare the performance and interpretability of graph-based methods versus traditional deep learning approaches**. Through this comparison, we aim to demonstrate the advantages of incorporating brain connectivity information into the learning process.\n",
    "\n",
    "**Authors**: Luca Di Bello, Guillaume Andr√© B√©lissent, Abdessalem Ben Ali, Beatriz Izquierdo Gonz√°lez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run feature extraction on same node\n",
    "import subprocess\n",
    "subprocess.run('python3 ./feature_extraction.py', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from src.utils.seeder import seed_everything\n",
    "\n",
    "# set seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# create useful constants\n",
    "RANDOM_SEED = 42\n",
    "IS_SCITAS = True # set to True if running on SCITAS cluster\n",
    "LOCAL_DATA_ROOT = Path(\"./data\")\n",
    "DATA_ROOT = Path(\"/home/ogut/data\") if IS_SCITAS else LOCAL_DATA_ROOT\n",
    "CHECKPOINT_ROOT = Path(\"./.checkpoints\")\n",
    "SUBMISSION_ROOT = Path(\"./.submissions\")\n",
    "\n",
    "# create directories if they do not exist\n",
    "CHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# set dataset root\n",
    "seed_everything(RANDOM_SEED)\n",
    "\n",
    "# setup torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading EEG segment tables‚Ä¶\n",
      "‚úîÔ∏è Loaded: 12993 train rows, 3614 test rows (took 0.2s)\n"
     ]
    }
   ],
   "source": [
    "from src.utils.signal import time_filtering, normalize\n",
    "%aimport src.utils.signal\n",
    "from src.utils.index import ensure_eeg_multiindex\n",
    "\n",
    "start = time.time()\n",
    "print(\"‚è≥ Loading EEG segment tables‚Ä¶\")\n",
    "clips_tr = pd.read_parquet(DATA_ROOT / \"train\" / \"segments.parquet\").dropna()\n",
    "clips_te = pd.read_parquet(DATA_ROOT / \"test\" / \"segments.parquet\").dropna()\n",
    "\n",
    "# load clips with label\n",
    "clips_tr = ensure_eeg_multiindex(clips_tr, id_col_name='id')\n",
    "clips_te = ensure_eeg_multiindex(clips_te, id_col_name='id')\n",
    "\n",
    "print(f\"‚úîÔ∏è Loaded: {len(clips_tr)} train rows, {len(clips_te)} test rows \"\n",
    "      f\"(took {time.time()-start:.1f}s)\")\n",
    "# NOTE: Merge clips for sanity checks\n",
    "clips = pd.concat([clips_tr, clips_te]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train extracted features shape: (12993, 228)\n",
      "Test extracted features shape: (3614, 228)\n"
     ]
    }
   ],
   "source": [
    "# print feature shapes\n",
    "X_train = np.load(LOCAL_DATA_ROOT / \"extracted_features\" / \"X_train.npy\", allow_pickle=True)\n",
    "X_test = np.load(LOCAL_DATA_ROOT / \"extracted_features\" / \"X_test.npy\", allow_pickle=True)\n",
    "y_train = np.load(LOCAL_DATA_ROOT / \"labels\" / \"y_train.npy\", allow_pickle=True)\n",
    "sample_subject_array = np.load(LOCAL_DATA_ROOT / \"extracted_features\" / \"sample_subject_array_train.npy\",allow_pickle=True)\n",
    "\n",
    "# sanity checks to ensure validity of the data\n",
    "assert X_train.shape[0]  == y_train.shape[0], \"Mismatch in number of training samples and labels\"\n",
    "assert X_train.shape[1] == X_test.shape[1], \"Mismatch in number of features between train and test sets\"\n",
    "assert clips_tr.shape[0] == y_train.shape[0], \"Mismatch in number of training samples and segments\"\n",
    "assert X_train.shape[0] == sample_subject_array.shape[0], \"Mismatch in number of training samples and subjects\"\n",
    "assert clips_tr.shape[0] == sample_subject_array.shape[0], \"Mismatch in number of training segments and subjects\"\n",
    "\n",
    "print(\"Train extracted features shape:\", X_train.shape)\n",
    "print(\"Test extracted features shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- Extracted features -----------------------------------------------#\n",
    "channels = ['FP1', 'FP2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2', 'F7', 'F8', 'T3', 'T4', 'T5', 'T6', 'FZ', 'CZ', 'PZ']\n",
    "features = [\n",
    "    \"rms\", \"linelen\", \"hj_mob\", \"hj_cmp\", \"spec_ent\",\n",
    "    \"alpha_pow\", \"beta_pow\", \"theta_pow\", \"gamma_pow\",\n",
    "    \"rel_alpha\", \"rel_theta\", \"theta_alpha_ratio\"\n",
    "]\n",
    "n_features = len(features)\n",
    "n_channels = len(channels)\n",
    "\n",
    "feature_names = [f\"{ch} - {ft}\" for ch in channels for ft in features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings (WORK IN PROGRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating timeseries dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing TimeseriesEEGDataset in SIGNAL mode.\n",
      "   - Sampling rate: 250 Hz\n",
      "   - Derived segment length: 3000 timesteps.\n",
      "   - Segment length: 3000 timesteps.\n",
      "   ‚úÖ Using existing cached data from data/timeseries_dataset_train_signal/processed\n",
      "üèÅ TimeseriesEEGDataset initialization complete. Loaded 12993 samples.\n",
      "üöÄ Initializing TimeseriesEEGDataset in FEATURE mode.\n",
      "   ‚úÖ Using existing cached data from data/timeseries_dataset_train_features/processed\n",
      "üèÅ TimeseriesEEGDataset initialization complete. Loaded 12993 samples.\n",
      "üöÄ Initializing TimeseriesEEGDataset in SIGNAL mode.\n",
      "   - Sampling rate: 250 Hz\n",
      "   - Derived segment length: 3000 timesteps.\n",
      "   - Segment length: 3000 timesteps.\n",
      "   ‚ö†Ô∏è Info: Column 'label' not found in clips_df. Processing without labels (e.g., test set).\n",
      "   ‚úÖ Using existing cached data from data/timeseries_dataset_test_signal/processed\n",
      "üèÅ TimeseriesEEGDataset initialization complete. Loaded 3614 samples.\n",
      "üöÄ Initializing TimeseriesEEGDataset in FEATURE mode.\n",
      "   ‚ö†Ô∏è Info: Column 'label' not found in clips_df. Processing without labels (e.g., test set).\n",
      "   ‚úÖ Using existing cached data from data/timeseries_dataset_test_features/processed\n",
      "üèÅ TimeseriesEEGDataset initialization complete. Loaded 3614 samples.\n"
     ]
    }
   ],
   "source": [
    "%aimport src.utils.timeseries_eeg_dataset\n",
    "from src.utils.timeseries_eeg_dataset import TimeseriesEEGDataset\n",
    "\n",
    "LOCAL_DATA_ROOT = Path(\"./data\")\n",
    "\n",
    "timeseries_datasets_tr = {\n",
    "    \"signal\": TimeseriesEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_signal\"),\n",
    "        signal_folder=str(DATA_ROOT / 'train'),\n",
    "        clips_df=clips_tr,\n",
    "        mode='signal',\n",
    "    ),\n",
    "    \"feature\": TimeseriesEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_features\"),\n",
    "        signal_folder=str(DATA_ROOT / 'train'),\n",
    "        clips_df=clips_tr,\n",
    "        mode='feature',\n",
    "        feature_file_path=str(LOCAL_DATA_ROOT / \"extracted_features\" / \"X_train.npy\"),\n",
    "    ),\n",
    "    # FIXME: Uncomment the embedding dataset as soon as the embedding file is available\n",
    "    # \"embedding\": TimeseriesEEGDataset(\n",
    "    #     root=str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_embedding\"),\n",
    "    #     signal_folder=str(DATA_ROOT / 'train'),\n",
    "    #     clips_df=clips_tr,\n",
    "    #     mode='embedding',\n",
    "    #     embedding_file_path=str(LOCAL_DATA_ROOT / \"embeddings\" / \"X_train_embedding.npy\"),\n",
    "    #     labels_for_embedding_file_path=str(LOCAL_DATA_ROOT / \"labels\" / \"y_train.npy\")\n",
    "    # ),\n",
    "}\n",
    "\n",
    "timeseries_datasets_te = {\n",
    "    \"signal\": TimeseriesEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_signal\"),\n",
    "        signal_folder=str(DATA_ROOT / 'test'),\n",
    "        clips_df=clips_te,\n",
    "        mode='signal',\n",
    "    ),\n",
    "    \"feature\": TimeseriesEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_features\"),\n",
    "        signal_folder=str(DATA_ROOT / 'test'),\n",
    "        clips_df=clips_te,\n",
    "        mode='feature',\n",
    "        feature_file_path=str(LOCAL_DATA_ROOT / \"extracted_features\" / \"X_test.npy\"),\n",
    "    ),\n",
    "    # FIXME: Uncomment the embedding dataset as soon as the embedding file is available\n",
    "    # \"embedding\": TimeseriesEEGDataset(\n",
    "    #     root=str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_embedding\"),\n",
    "    #     signal_folder=str(DATA_ROOT / 'test'),\n",
    "    #     clips_df=clips_te,\n",
    "    #     mode='embedding',\n",
    "    #     embedding_file_path=str(LOCAL_DATA_ROOT / \"embeddings\" / \"X_test_embedding.npy\"),\n",
    "    #     labels_for_embedding_file_path=str(LOCAL_DATA_ROOT / \"labels\" / \"y_test.npy\")\n",
    "    # ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing GraphEEGDataset in FEATURE mode.\n",
      "   - Root: data/graph_dataset_train\n",
      "   - Mode: FEATURE\n",
      "   - Edge strategy: full\n",
      "   - Node Feature Normalization: True\n",
      "‚öôÔ∏è process() called for FEATURE mode. Target processed directory: data/graph_dataset_train/processed\n",
      "   - Starting processing from pre-extracted features...\n",
      "   - Total feature sets to process: 12993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Processing feature item 1299/12993...\n",
      "     - Processing feature item 2598/12993...\n",
      "     - Processing feature item 3897/12993...\n",
      "     - Processing feature item 5196/12993...\n",
      "     - Processing feature item 6495/12993...\n",
      "     - Processing feature item 7794/12993...\n",
      "     - Processing feature item 9093/12993...\n",
      "     - Processing feature item 10392/12993...\n",
      "     - Processing feature item 11691/12993...\n",
      "     - Processing feature item 12990/12993...\n",
      "     - Processing feature item 12993/12993...\n",
      "   ‚úÖ Processed and saved 12993 items from features.\n",
      "üèÅ process() finished. Total items processed and saved in this run for feature mode: 12993\n",
      "   - Found 12993 existing processed files for feature mode.\n",
      "üèÅ GraphEEGDataset initialization complete. Current mode: FEATURE. Known processed items: 12993\n",
      "üöÄ Initializing GraphEEGDataset in FEATURE mode.\n",
      "   - Root: data/graph_dataset_train\n",
      "   - Mode: FEATURE\n",
      "   - Edge strategy: spatial\n",
      "   - Node Feature Normalization: True\n",
      "   - Loading spatial distances...\n",
      "     - Loaded 180 unique spatial distances relevant to defined channels.\n",
      "‚öôÔ∏è process() called for FEATURE mode. Target processed directory: data/graph_dataset_train/processed\n",
      "   - Starting processing from pre-extracted features...\n",
      "   - Total feature sets to process: 12993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Processing feature item 1299/12993...\n",
      "     - Processing feature item 2598/12993...\n",
      "     - Processing feature item 3897/12993...\n",
      "     - Processing feature item 5196/12993...\n",
      "     - Processing feature item 6495/12993...\n",
      "     - Processing feature item 7794/12993...\n",
      "     - Processing feature item 9093/12993...\n",
      "     - Processing feature item 10392/12993...\n",
      "     - Processing feature item 11691/12993...\n",
      "     - Processing feature item 12990/12993...\n",
      "     - Processing feature item 12993/12993...\n",
      "   ‚úÖ Processed and saved 12993 items from features.\n",
      "üèÅ process() finished. Total items processed and saved in this run for feature mode: 12993\n",
      "   - Found 12993 existing processed files for feature mode.\n",
      "üèÅ GraphEEGDataset initialization complete. Current mode: FEATURE. Known processed items: 12993\n",
      "üöÄ Initializing GraphEEGDataset in FEATURE mode.\n",
      "   - Root: data/graph_dataset_train\n",
      "   - Mode: FEATURE\n",
      "   - Edge strategy: spatial\n",
      "   - Node Feature Normalization: True\n",
      "   - Loading spatial distances...\n",
      "     - Loaded 180 unique spatial distances relevant to defined channels.\n",
      "‚öôÔ∏è process() called for FEATURE mode. Target processed directory: data/graph_dataset_train/processed\n",
      "   - Starting processing from pre-extracted features...\n",
      "   - Total feature sets to process: 12993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Processing feature item 1299/12993...\n",
      "     - Processing feature item 2598/12993...\n",
      "     - Processing feature item 3897/12993...\n",
      "     - Processing feature item 5196/12993...\n",
      "     - Processing feature item 6495/12993...\n",
      "     - Processing feature item 7794/12993...\n",
      "     - Processing feature item 9093/12993...\n",
      "     - Processing feature item 10392/12993...\n",
      "     - Processing feature item 11691/12993...\n",
      "     - Processing feature item 12990/12993...\n",
      "     - Processing feature item 12993/12993...\n",
      "   ‚úÖ Processed and saved 12993 items from features.\n",
      "üèÅ process() finished. Total items processed and saved in this run for feature mode: 12993\n",
      "   - Found 12993 existing processed files for feature mode.\n",
      "üèÅ GraphEEGDataset initialization complete. Current mode: FEATURE. Known processed items: 12993\n",
      "üöÄ Initializing GraphEEGDataset in FEATURE mode.\n",
      "   - Root: data/graph_dataset_test\n",
      "   - Mode: FEATURE\n",
      "   - Edge strategy: full\n",
      "   - Node Feature Normalization: True\n",
      "‚öôÔ∏è process() called for FEATURE mode. Target processed directory: data/graph_dataset_test/processed\n",
      "   - Starting processing from pre-extracted features...\n",
      "   - Total feature sets to process: 3614\n",
      "   ‚ö†Ô∏è Warning: Labels for features will be None or use a default. 'label' column not in clips_df or length mismatch (3614 vs 3614).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Processing feature item 361/3614...\n",
      "     - Processing feature item 722/3614...\n",
      "     - Processing feature item 1083/3614...\n",
      "     - Processing feature item 1444/3614...\n",
      "     - Processing feature item 1805/3614...\n",
      "     - Processing feature item 2166/3614...\n",
      "     - Processing feature item 2527/3614...\n",
      "     - Processing feature item 2888/3614...\n",
      "     - Processing feature item 3249/3614...\n",
      "     - Processing feature item 3610/3614...\n",
      "     - Processing feature item 3614/3614...\n",
      "   ‚úÖ Processed and saved 3614 items from features.\n",
      "üèÅ process() finished. Total items processed and saved in this run for feature mode: 3614\n",
      "   - Found 3614 existing processed files for feature mode.\n",
      "üèÅ GraphEEGDataset initialization complete. Current mode: FEATURE. Known processed items: 3614\n",
      "üöÄ Initializing GraphEEGDataset in FEATURE mode.\n",
      "   - Root: data/graph_dataset_test\n",
      "   - Mode: FEATURE\n",
      "   - Edge strategy: spatial\n",
      "   - Node Feature Normalization: True\n",
      "   - Loading spatial distances...\n",
      "     - Loaded 180 unique spatial distances relevant to defined channels.\n",
      "‚öôÔ∏è process() called for FEATURE mode. Target processed directory: data/graph_dataset_test/processed\n",
      "   - Starting processing from pre-extracted features...\n",
      "   - Total feature sets to process: 3614\n",
      "   ‚ö†Ô∏è Warning: Labels for features will be None or use a default. 'label' column not in clips_df or length mismatch (3614 vs 3614).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Processing feature item 361/3614...\n",
      "     - Processing feature item 722/3614...\n",
      "     - Processing feature item 1083/3614...\n",
      "     - Processing feature item 1444/3614...\n",
      "     - Processing feature item 1805/3614...\n",
      "     - Processing feature item 2166/3614...\n",
      "     - Processing feature item 2527/3614...\n",
      "     - Processing feature item 2888/3614...\n",
      "     - Processing feature item 3249/3614...\n",
      "     - Processing feature item 3610/3614...\n",
      "     - Processing feature item 3614/3614...\n",
      "   ‚úÖ Processed and saved 3614 items from features.\n",
      "üèÅ process() finished. Total items processed and saved in this run for feature mode: 3614\n",
      "   - Found 3614 existing processed files for feature mode.\n",
      "üèÅ GraphEEGDataset initialization complete. Current mode: FEATURE. Known processed items: 3614\n",
      "üöÄ Initializing GraphEEGDataset in FEATURE mode.\n",
      "   - Root: data/graph_dataset_test\n",
      "   - Mode: FEATURE\n",
      "   - Edge strategy: correlation\n",
      "   - Node Feature Normalization: True\n",
      "‚öôÔ∏è process() called for FEATURE mode. Target processed directory: data/graph_dataset_test/processed\n",
      "   - Starting processing from pre-extracted features...\n",
      "   - Total feature sets to process: 3614\n",
      "   ‚ö†Ô∏è Warning: Labels for features will be None or use a default. 'label' column not in clips_df or length mismatch (3614 vs 3614).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n",
      "/home/ldibello/venvs/neuro/lib/python3.10/site-packages/numpy/lib/function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Processing feature item 361/3614...\n",
      "     - Processing feature item 722/3614...\n",
      "     - Processing feature item 1083/3614...\n",
      "     - Processing feature item 1444/3614...\n",
      "     - Processing feature item 1805/3614...\n",
      "     - Processing feature item 2166/3614...\n",
      "     - Processing feature item 2527/3614...\n",
      "     - Processing feature item 2888/3614...\n",
      "     - Processing feature item 3249/3614...\n",
      "     - Processing feature item 3610/3614...\n",
      "     - Processing feature item 3614/3614...\n",
      "   ‚úÖ Processed and saved 3614 items from features.\n",
      "üèÅ process() finished. Total items processed and saved in this run for feature mode: 3614\n",
      "   - Found 3614 existing processed files for feature mode.\n",
      "üèÅ GraphEEGDataset initialization complete. Current mode: FEATURE. Known processed items: 3614\n",
      "Graph training datasets created:\n",
      "  full: GraphEEGDataset(12993) (length: 12993)\n",
      "     INFO: Attempting torch.load with weights_only=False for data/graph_dataset_train/processed/data_feat_0.pt\n",
      "    First training sample: Data(x=[19, 12], edge_index=[2, 342], y=[1], original_idx=[1])\n",
      "  spatial: GraphEEGDataset(12993) (length: 12993)\n",
      "     INFO: Attempting torch.load with weights_only=False for data/graph_dataset_train/processed/data_feat_0.pt\n",
      "    First training sample: Data(x=[19, 12], edge_index=[2, 342], y=[1], original_idx=[1])\n",
      "  correlation: GraphEEGDataset(12993) (length: 12993)\n",
      "     INFO: Attempting torch.load with weights_only=False for data/graph_dataset_train/processed/data_feat_0.pt\n",
      "    First training sample: Data(x=[19, 12], edge_index=[2, 342], y=[1], original_idx=[1])\n",
      "\n",
      "Graph test datasets created:\n",
      "  full: GraphEEGDataset(3614) (length: 3614)\n",
      "     INFO: Attempting torch.load with weights_only=False for data/graph_dataset_test/processed/data_feat_0.pt\n",
      "    First test sample: Data(x=[19, 12], edge_index=[2, 0], original_idx=[1]), Label (y): None\n",
      "  spatial: GraphEEGDataset(3614) (length: 3614)\n",
      "     INFO: Attempting torch.load with weights_only=False for data/graph_dataset_test/processed/data_feat_0.pt\n",
      "    First test sample: Data(x=[19, 12], edge_index=[2, 0], original_idx=[1]), Label (y): None\n",
      "  correlation: GraphEEGDataset(3614) (length: 3614)\n",
      "     INFO: Attempting torch.load with weights_only=False for data/graph_dataset_test/processed/data_feat_0.pt\n",
      "    First test sample: Data(x=[19, 12], edge_index=[2, 0], original_idx=[1]), Label (y): None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from src.utils.graph_eeg_dataset import GraphEEGDataset\n",
    "\n",
    "LOCAL_DATA_ROOT = Path(\"./data\")\n",
    "\n",
    "graph_datasets_tr = {\n",
    "    'full': GraphEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / 'graph_dataset_train'),\n",
    "        clips_df=clips_tr,\n",
    "        signal_folder=str(DATA_ROOT / 'train'),\n",
    "        extracted_features_array=X_train,\n",
    "        use_extracted_features=True,\n",
    "        edge_strategy='full',\n",
    "        spatial_distance_file=str(LOCAL_DATA_ROOT / 'distances_3d.csv'),\n",
    "        force_reprocess=False,\n",
    "        prefetch_data=False,\n",
    "    ),\n",
    "    'spatial': GraphEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / 'graph_dataset_train'),\n",
    "        clips_df=clips_tr,\n",
    "        signal_folder=str(DATA_ROOT / 'train'),\n",
    "        extracted_features_array=X_train,\n",
    "        use_extracted_features=True,\n",
    "        edge_strategy='spatial',\n",
    "        spatial_distance_file=str(LOCAL_DATA_ROOT / 'distances_3d.csv'),\n",
    "        force_reprocess=False,\n",
    "        prefetch_data=False,\n",
    "    ),\n",
    "    'correlation': GraphEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / 'graph_dataset_train'),\n",
    "        clips_df=clips_tr,\n",
    "        signal_folder=str(DATA_ROOT / 'train'),\n",
    "        extracted_features_array=X_train,\n",
    "        use_extracted_features=True,\n",
    "        edge_strategy='spatial',\n",
    "        spatial_distance_file=str(LOCAL_DATA_ROOT / 'distances_3d.csv'),\n",
    "        force_reprocess=False,\n",
    "        prefetch_data=False,\n",
    "    ),\n",
    "}\n",
    "\n",
    "graph_datasets_te = {\n",
    "    'full': GraphEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / 'graph_dataset_test'),\n",
    "        clips_df=clips_te,\n",
    "        signal_folder=str(DATA_ROOT / 'test'),\n",
    "        extracted_features_array=X_test,\n",
    "        use_extracted_features=True,\n",
    "        edge_strategy='full',\n",
    "        spatial_distance_file=str(LOCAL_DATA_ROOT / 'distances_3d.csv'),\n",
    "        force_reprocess=False, # Force reprocess for test dataset\n",
    "        prefetch_data=False,\n",
    "    ),\n",
    "    'spatial': GraphEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / 'graph_dataset_test'),\n",
    "        clips_df=clips_te,\n",
    "        signal_folder=str(DATA_ROOT / 'test'),\n",
    "        extracted_features_array=X_test,\n",
    "        use_extracted_features=True,\n",
    "        edge_strategy='spatial',\n",
    "        spatial_distance_file=str(LOCAL_DATA_ROOT / 'distances_3d.csv'),\n",
    "        force_reprocess=False, # Force reprocess for test dataset\n",
    "        prefetch_data=False,\n",
    "    ),\n",
    "    'correlation': GraphEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / 'graph_dataset_test'),\n",
    "        clips_df=clips_te,\n",
    "        signal_folder=str(DATA_ROOT / 'test'),\n",
    "        extracted_features_array=X_test,\n",
    "        use_extracted_features=True,\n",
    "        edge_strategy='correlation',\n",
    "        spatial_distance_file=str(LOCAL_DATA_ROOT / 'distances_3d.csv'),\n",
    "        force_reprocess=False, # Force reprocess for test dataset\n",
    "        prefetch_data=False,\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Graph training datasets created:\")\n",
    "for key, ds in graph_datasets_tr.items():\n",
    "    print(f\"  {key}: {ds} (length: {len(ds)})\")\n",
    "    if len(ds) > 0:\n",
    "        sample = ds[0]\n",
    "        print(f\"    First training sample: {sample}\")\n",
    "\n",
    "print(\"\\nGraph test datasets created:\")\n",
    "for key, ds in graph_datasets_te.items():\n",
    "    print(f\"  {key}: {ds} (length: {len(ds)})\")\n",
    "    if len(ds) > 0:\n",
    "        sample = ds[0]\n",
    "        print(f\"    First test sample: {sample}, Label (y): {getattr(sample, 'y', None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries datasets train-validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal: Train size = 10394, Val size = 2599\n",
      "feature: Train size = 10394, Val size = 2599\n",
      "{'signal': <torch.utils.data.dataset.Subset object at 0x7f8422b2a470>, 'feature': <torch.utils.data.dataset.Subset object at 0x7f8422b2a1d0>}\n",
      "{'signal': <torch.utils.data.dataset.Subset object at 0x7f8422b28ee0>, 'feature': <torch.utils.data.dataset.Subset object at 0x7f8422b2b370>}\n",
      "{'signal': <src.utils.timeseries_eeg_dataset.TimeseriesEEGDataset object at 0x7f8665b85060>, 'feature': <src.utils.timeseries_eeg_dataset.TimeseriesEEGDataset object at 0x7f8665b85ab0>}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# split training dataset into train/val sets if not already done\n",
    "TRAIN_RATIO = 0.8\n",
    "TIMESERIES_TRAIN_SIZE = int(len(timeseries_datasets_tr[\"signal\"]) * TRAIN_RATIO)\n",
    "TIMESERIES_VAL_SIZE = len(timeseries_datasets_tr[\"signal\"]) - TIMESERIES_TRAIN_SIZE\n",
    "\n",
    "# Store original datasets before reassigning\n",
    "original_timeseries_datasets_tr = timeseries_datasets_tr.copy()\n",
    "\n",
    "# Split each training (both timeseries and graph) dataset into train/val\n",
    "timeseries_datasets_tr = {}\n",
    "timeseries_datasets_val = {}\n",
    "for key, ds in original_timeseries_datasets_tr.items():\n",
    "    tr, val = random_split(ds, [TIMESERIES_TRAIN_SIZE, TIMESERIES_VAL_SIZE], generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
    "    timeseries_datasets_tr[key] = tr\n",
    "    timeseries_datasets_val[key] = val\n",
    "    print(f\"{key}: Train size = {len(tr)}, Val size = {len(val)}\")\n",
    "print(timeseries_datasets_tr)\n",
    "print(timeseries_datasets_val)\n",
    "print(timeseries_datasets_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph datasets train-validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full: Train size = 10394, Val size = 2599\n",
      "spatial: Train size = 10394, Val size = 2599\n",
      "correlation: Train size = 10394, Val size = 2599\n",
      "{'full': <torch.utils.data.dataset.Subset object at 0x7f84281d0f70>, 'spatial': <torch.utils.data.dataset.Subset object at 0x7f8422b2a8c0>, 'correlation': <torch.utils.data.dataset.Subset object at 0x7f8422b2b520>}\n",
      "{'full': <torch.utils.data.dataset.Subset object at 0x7f8422b28f40>, 'spatial': <torch.utils.data.dataset.Subset object at 0x7f8422b28f70>, 'correlation': <torch.utils.data.dataset.Subset object at 0x7f8422b2ae90>}\n",
      "{'full': GraphEEGDataset(3614), 'spatial': GraphEEGDataset(3614), 'correlation': GraphEEGDataset(3614)}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# split training dataset into train/val sets if not already done\n",
    "TRAIN_RATIO = 0.8\n",
    "GRAPH_TRAIN_SIZE = int(len(graph_datasets_tr[\"full\"]) * TRAIN_RATIO)\n",
    "GRAPH_VAL_SIZE = len(graph_datasets_tr[\"full\"]) - GRAPH_TRAIN_SIZE\n",
    "\n",
    "# Store original datasets before reassigning\n",
    "original_graph_datasets_tr = graph_datasets_tr\n",
    "\n",
    "# Split each training (both timeseries and graph) dataset into train/val\n",
    "graph_datasets_tr = {}\n",
    "graph_datasets_val = {}\n",
    "for key, ds in original_graph_datasets_tr.items():\n",
    "    tr, val = random_split(ds, [GRAPH_TRAIN_SIZE, GRAPH_VAL_SIZE], generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
    "    graph_datasets_tr[key] = tr\n",
    "    graph_datasets_val[key] = val\n",
    "    print(f\"{key}: Train size = {len(tr)}, Val size = {len(val)}\")\n",
    "print(graph_datasets_tr)\n",
    "print(graph_datasets_val)\n",
    "print(graph_datasets_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Creating Timeseries DataLoaders‚Ä¶\n",
      "\t  signal: Train loader size = 21, Val loader size = 6, Test loader size = 8\n",
      "\t  feature: Train loader size = 21, Val loader size = 6, Test loader size = 8\n",
      "‚úîÔ∏è DataLoaders created in 0.0s\n",
      "‚è≥ Creating Graph DataLoaders‚Ä¶\n",
      "\t  full: Train loader size = 21, Val loader size = 6, Test loader size = 8\n",
      "\t  spatial: Train loader size = 21, Val loader size = 6, Test loader size = 8\n",
      "\t  correlation: Train loader size = 21, Val loader size = 6, Test loader size = 8\n",
      "‚úîÔ∏è GeoDataLoaders created in 0.0s\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders for both timeseries and graph datasets\n",
    "common_loader_kwargs = dict(\n",
    "    batch_size=512,\n",
    "    num_workers=16,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "dl_start = time.time()\n",
    "timeseries_loader_tr= {}\n",
    "timeseries_loader_val = {}\n",
    "timeseries_loader_te = {}\n",
    "print(\"‚è≥ Creating Timeseries DataLoaders‚Ä¶\")\n",
    "for kind in timeseries_datasets_tr.keys():\n",
    "    timeseries_loader_tr[kind] = DataLoader(timeseries_datasets_tr[kind], shuffle=True,  **common_loader_kwargs) # type: ignore\n",
    "    timeseries_loader_val[kind]   = DataLoader(timeseries_datasets_val[kind], shuffle=False, **common_loader_kwargs) # type: ignore\n",
    "    timeseries_loader_te[kind]  = DataLoader(timeseries_datasets_te[kind], shuffle=False, **common_loader_kwargs) # type: ignore\n",
    "    print(f\"\\t  {kind}: Train loader size = {len(timeseries_loader_tr[kind])}, \"\n",
    "            f\"Val loader size = {len(timeseries_loader_val[kind])}, \"\n",
    "            f\"Test loader size = {len(timeseries_loader_te[kind])}\")\n",
    "print(f\"‚úîÔ∏è DataLoaders created in {time.time() - dl_start:.1f}s\")\n",
    "\n",
    "dl_start = time.time()\n",
    "graph_loader_tr= {}\n",
    "graph_loader_val = {}\n",
    "graph_loader_te = {}\n",
    "print(\"‚è≥ Creating Graph DataLoaders‚Ä¶\")\n",
    "for kind in graph_datasets_tr.keys():\n",
    "    graph_loader_tr[kind] = GeoDataLoader(graph_datasets_tr[kind], shuffle=True,  **common_loader_kwargs) # type: ignore\n",
    "    graph_loader_val[kind]   = GeoDataLoader(graph_datasets_val[kind], shuffle=False, **common_loader_kwargs) # type: ignore\n",
    "    graph_loader_te[kind]  = GeoDataLoader(graph_datasets_te[kind], shuffle=False, **common_loader_kwargs) # type: ignore\n",
    "    print(f\"\\t  {kind}: Train loader size = {len(graph_loader_tr[kind])}, \"\n",
    "            f\"Val loader size = {len(graph_loader_val[kind])}, \"\n",
    "            f\"Test loader size = {len(graph_loader_te[kind])}\")\n",
    "print(f\"‚úîÔ∏è GeoDataLoaders created in {time.time() - dl_start:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch type: <class 'list'>\n",
      "Batch length: 2\n",
      "torch.Size([512, 19, 3000])\n",
      "torch.Size([512, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch in timeseries_loader_tr['signal']:\n",
    "    # Print batch shape and type\n",
    "    print(f\"Batch type: {type(batch)}\")\n",
    "    print(f\"Batch length: {len(batch)}\")\n",
    "    print(batch[0].shape)    \n",
    "    print(batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional approaches (no additional features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.train import train_model, evaluate_model\n",
    "from src.utils.plot import plot_training_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "src.utils.signal src.utils.timeseries_eeg_dataset\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.layers.simple_mlp import SimpleMLP\n",
    "\n",
    "# build model with current parameters\n",
    "model = SimpleMLP(\n",
    "    input_dim=228,\n",
    "    hidden_dims=[256, 128],\n",
    "    dropout=0.3,\n",
    "    num_classes=1\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "src.utils.signal src.utils.timeseries_eeg_dataset\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "üîç Applying oversampling to the training data...\n",
      "  Attribute-based label extraction not applicable or failed. Iterating through 10394 dataset samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Finished iterating. Collected 10394 labels.\n",
      "  Class counts before oversampling: [8375, 2019]\n",
      "  Successfully created an oversampled DataLoader.\n",
      "üí™ Starting training from epoch 1 to 300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|‚ñé                                                                                  | 1/300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (512x665 and 228x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m SIMPLE_MLP_SUBMISSION_PATH \u001b[38;5;241m=\u001b[39m SUBMISSION_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple_mlp_submission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# train model on training set (or load existing)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeseries_loader_tr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeseries_loader_val\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSIMPLE_MLP_SAVE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_oversampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:269\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, use_oversampling)\u001b[0m\n\u001b[1;32m    267\u001b[0m     y_targets \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y_batch, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(y_batch, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    268\u001b[0m     y_targets \u001b[38;5;241m=\u001b[39m y_targets\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m y_targets\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m y_targets\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m--> 269\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data_batch_item, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data_batch_item, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m to_dense_batch: \u001b[38;5;66;03m# Should be caught earlier if PyG not installed but good check\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/layers/simple_mlp.py:85\u001b[0m, in \u001b[0;36mSimpleMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    Forward pass\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m        Model predictions of shape (batch_size, num_classes)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(features)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (512x665 and 228x256)"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "SIMPLE_MLP_SAVE_PATH = CHECKPOINT_ROOT / \"simple_mlp_best_model.pt\"\n",
    "SIMPLE_MLP_SUBMISSION_PATH = SUBMISSION_ROOT / \"simple_mlp_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        model, timeseries_loader_tr[\"feature\"], timeseries_loader_val[\"feature\"],\n",
    "        criterion, optimizer, device,\n",
    "        save_path=SIMPLE_MLP_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        scheduler=scheduler,\n",
    "        overwrite=True,\n",
    "        use_gnn=False,\n",
    "        use_oversampling=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Feature Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "src.layers.lstm src.utils.signal src.utils.timeseries_eeg_dataset src.utils.train\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.layers.feature_net import FeatureNet\n",
    "\n",
    "# build model with current parameters\n",
    "model = FeatureNet(\n",
    "    input_dim=665,\n",
    "    hidden_dims=[512, 256, 128],\n",
    "    dropout=0.3,\n",
    "    num_classes=1\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "src.layers.lstm src.utils.signal src.utils.timeseries_eeg_dataset src.utils.train\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "   Using WeightedRandomSampler for oversampling. BCEWithLogitsLoss will not use explicit pos_weight.\n",
      "üîç Applying oversampling to the training data...\n",
      "Original train dataset: <torch.utils.data.dataset.Subset object at 0x7f06582f5570>\n",
      "Index 0 (tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0.]))\n",
      "Index 1 (tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0.]))\n",
      "  dataset attributes:\n",
      "    _is_protocol: bool = False\n",
      "    dataset: TimeseriesEEGDataset = <src.utils.timeseries_eeg_dataset.TimeseriesEEGDataset object at 0x7f06719fe2c0>\n",
      "    indices: list = [198, 7668, 7722, 2917, 6492, 12336, 418, 2545, 4033, 8738, 3673, 9375, 11345, 5187, 1517, 3223, 8158, 12200, 3210, 10997, 11747, 7853, 3648, 2744, 7034, 2310, 7490, 12534, 11470, 3132, 6744, 11787, 8409, 12442, 8405, 1963, 6515, 3793, 1465, 10635, 9492, 6471, 8359, 412, 6600, 5324, 11491, 3437, 7176, 3310, 6470, 3462, 5293, 6432, 10474, 7230, 3859, 713, 6531, 8795, 10313, 3815, 9624, 8563, 11773, 11858, 1734, 8819, 9929, 9274, 4914, 4394, 3322, 8186, 5773, 5378, 11889, 5016, 7511, 4296, 11624, 6412, 4243, 9176, 5797, 10503, 11066, 5775, 8518, 6002, 124, 6032, 583, 9882, 7001, 6526, 10552, 7716, 11704, 5243, 11407, 210, 5034, 9758, 9647, 1146, 2962, 12540, 6223, 12587, 10117, 5823, 5717, 4734, 7301, 6142, 291, 2434, 10979, 8761, 11465, 5125, 4902, 67, 2521, 8084, 3030, 6135, 9182, 6908, 9299, 7093, 686, 1013, 8057, 3211, 1984, 11097, 6663, 1693, 10970, 2013, 4034, 1512, 11074, 9839, 5482, 6839, 721, 1120, 2887, 10051, 1119, 331, 10295, 12960, 5964, 6438, 2503, 2578, 9414, 4393, 11047, 12800, 12566, 5861, 9818, 9547, 12270, 11374, 9214, 1109, 2936, 1284, 9252, 11566, 4530, 12147, 7481, 7309, 5984, 1279, 9694, 11023, 9591, 12701, 5801, 4021, 8474, 5678, 6379, 2015, 4398, 11663, 2366, 7896, 3302, 12376, 6634, 6295, 8583, 4808, 2142, 6491, 1344, 3749, 6574, 274, 1047, 5205, 10887, 5573, 4854, 1315, 6725, 4721, 1757, 6189, 9425, 11813, 12621, 3175, 9522, 694, 1689, 6762, 1443, 5356, 10163, 10909, 4812, 9316, 9888, 3012, 9056, 8344, 7931, 12918, 9223, 8606, 6031, 1767, 10502, 7319, 1627, 4314, 10894, 3897, 5380, 9595, 8441, 5401, 3436, 8285, 3114, 5223, 3902, 12614, 1254, 6835, 11392, 10554, 12196, 7775, 6430, 2218, 8371, 3528, 9918, 5263, 3392, 2144, 4121, 9720, 3982, 8026, 2770, 1066, 10225, 2608, 9858, 4268, 8690, 4903, 7846, 7743, 11726, 4561, 3725, 600, 9417, 6715, 2026, 11674, 7859, 608, 7008, 10847, 7397, 7444, 7480, 3085, 650, 9640, 12398, 8869, 11436, 4062, 9792, 10298, 6970, 6225, 8900, 8223, 368, 868, 7867, 3525, 8074, 2068, 3443, 9349, 8427, 10642, 5230, 9310, 9835, 4447, 2292, 12536, 4707, 7825, 8194, 12199, 4381, 3088, 180, 6451, 7203, 7892, 3052, 1660, 6739, 9186, 2489, 10111, 10354, 7976, 9046, 8655, 2987, 6097, 4732, 8936, 11292, 6159, 8597, 1136, 12584, 12245, 9672, 7473, 10938, 1014, 11814, 4995, 26, 12050, 7468, 10855, 2028, 524, 5026, 7464, 7756, 8862, 12441, 2825, 3697, 6811, 3768, 10294, 7276, 10543, 10514, 9440, 9665, 1093, 5257, 10813, 11454, 5175, 2642, 9696, 8218, 12496, 7146, 8482, 867, 9667, 3516, 8196, 4489, 8145, 7789, 7840, 11521, 12544, 1787, 7526, 3399, 9588, 11529, 2465, 9229, 9381, 6716, 12466, 9036, 9712, 10397, 790, 8985, 4603, 5737, 3333, 6929, 12600, 987, 564, 5119, 782, 6965, 6672, 7180, 2394, 12213, 8452, 6, 7379, 1312, 5160, 11565, 9402, 9563, 5695, 11808, 7604, 12683, 2518, 10505, 4990, 11507, 2378, 3589, 6105, 4029, 1800, 1408, 11619, 597, 11481, 7260, 2037, 11160, 11295, 1056, 5357, 1437, 6636, 9842, 2878, 11076, 2804, 3234, 5868, 5215, 12884, 10823, 8590, 5416, 11006, 2773, 5702, 11050, 1434, 5178, 12454, 411, 3405, 4196, 9995, 2793, 7930, 8496, 8771, 10282, 2876, 1755, 3120, 7478, 12541, 11883, 1132, 11941, 11745, 12247, 11171, 252, 122, 526, 7012, 12985, 4022, 6399, 9897, 7939, 4790, 6797, 7334, 4299, 11319, 12569, 7517, 2593, 1827, 2192, 10486, 7484, 9306, 10403, 11612, 4778, 5889, 12953, 117, 7949, 6447, 11271, 5668, 12463, 8058, 4024, 7726, 4144, 3512, 2881, 2872, 5413, 7039, 1351, 7890, 12172, 12708, 1559, 8210, 10000, 6846, 409, 7100, 6096, 8467, 11410, 6637, 6877, 9814, 8524, 10168, 11912, 12039, 3273, 5429, 9957, 954, 5059, 8570, 12180, 7254, 8937, 10780, 3780, 4865, 10085, 4080, 2241, 11071, 3599, 5225, 591, 2557, 10154, 5068, 11453, 11356, 7988, 8002, 9811, 4047, 7810, 4518, 885, 373, 1775, 12524, 2437, 7731, 3625, 4248, 8540, 3248, 4114, 1238, 7103, 12654, 8802, 10127, 2317, 10704, 3421, 5111, 7898, 1350, 6930, 11408, 9298, 7306, 8662, 11168, 8674, 7195, 11478, 8252, 3877, 8657, 10369, 5760, 5443, 5731, 11890, 662, 7055, 1718, 5024, 5873, 11128, 12317, 6106, 9065, 8039, 2205, 5708, 2506, 8746, 909, 2659, 2705, 4936, 1891, 4719, 6576, 433, 35, 9500, 7106, 2634, 11581, 12664, 2916, 10762, 2131, 4565, 6840, 4401, 6686, 4868, 2145, 7410, 8116, 7399, 11106, 4596, 9901, 3157, 10664, 2621, 5022, 1083, 2715, 1535, 12602, 10123, 12224, 9582, 11760, 9747, 10310, 10174, 86, 5008, 112, 4894, 2472, 10861, 5656, 5313, 8877, 4927, 8390, 7928, 6312, 3353, 1758, 119, 6380, 6542, 4994, 9798, 9644, 10868, 6384, 1264, 7971, 1058, 923, 1165, 8681, 5173, 9849, 11546, 11964, 10207, 747, 4048, 6700, 7666, 3461, 1260, 5309, 10722, 6150, 1753, 1430, 3499, 11147, 4535, 2306, 5323, 9011, 8255, 4378, 11188, 1331, 2786, 7178, 9297, 4910, 10714, 7823, 2420, 12177, 4340, 6682, 3188, 11414, 1436, 8663, 10638, 1642, 12826, 7577, 5488, 221, 8966, 750, 10996, 10905, 7965, 6072, 678, 10899, 9248, 12111, 10002, 11836, 2266, 9777, 9616, 1086, 7398, 4591, 4508, 10322, 6108, 3100, 3544, 3441, 7482, 184, 1897, 6547, 3715, 5534, 492, 6047, 11185, 9880, 10790, 4053, 1354, 1037, 9797, 4955, 10678, 3616, 12194, 10515, 7950, 8435, 2584, 12637, 7900, 3892, 12089, 10459, 6257, 7436, 4996, 1110, 1462, 10068, 12159, 12931, 11296, 11468, 5209, 394, 3215, 3069, 5904, 3241, 5192, 12840, 4409, 3251, 7849, 3229, 5617, 519, 2339, 11838, 10975, 3298, 8542, 3019, 7755, 5929, 8823, 502, 10396, 5379, 9386, 3205, 931, 7836, 48, 11025, 3076, 835, 8842, 4754, 2427, 6268, 11979, 7193, 10285, 11599, 3204, 7386, 6733, 8353, 4931, 9062, 7259, 6264, 11949, 9925, 7863, 2752, 11719, 1601, 3308, 1573, 828, 824, 3191, 8266, 9457, 4220, 4915, 5683, 8388, 488, 6495, 11981, 2765, 10170, 12056, 64, 3335, 95, 3818, 2803, 5528, 1266, 2256, 3970, 3874, 10772, 6335, 10682, 12251, 9951, 9348, 12373, 82, 1135, 7086, 11744, 8838, 6137, 170, 12520, 2689, 5245, 9431, 5661, 9748, 4133, 11488, 12810, 2448, 2625, 1933, 3611, 9556, 10831, 3218, 6086, 862, 9549, 12838, 12642, 11578, 11556, 962, 8755, 6843, 4829, 7338, 727, 10012, 8433, 7175, 8633, 5444, 3557, 7505, 11918, 6362, 9579, 985, 162, 6707, 3699, 348, 1298, 4619, 9600, 1209, 4632, 6022, 4744, 850, 5625, 3259, 5122, 12184, 5134, 1010, 5316, 1957, 11752, 4032, 11708, 8557, 1355, 9336, 9408, 9534, 571, 8938, 554, 5550, 10255, 11715, 12293, 6593, 1411, 7734, 155, 6315, 6071, 516, 9487, 8743, 9862, 968, 10195, 635, 11337, 304, 7401, 8836, 1490, 3082, 700, 2530, 12279, 9829, 1057, 4574, 825, 10876, 7206, 12248, 1282, 8813, 5991, 12380, 1099, 10729, 12666, 2197, 12067, 7634, 1427, 3667, 11843, 2133, 5193, 6307, 1857, 4223, 4427, 3202, 10631, 1875, 9175, 10132, 6012, 196, 11861, 3638, 5639, 4804, 8195, 7027, 9337, 4998, 3757, 398, 9932, 11910, 817, 12620, 4793, 2866, 12081, 9892, 5487, 11657, 582, 6849, 2754, 10226, 11569, 10334, 9016, 2885, 4194, 2161, 6035, 12362, 12419, 8920, 10744, 8852, 7390, 7942, 4811, 10078, 3334, 7288, 2009, 5986, 4844, 1738, 1380, 7895, 12335, 458, 5907, 834, 8871, 8894, 148, 2901, 1051, 6604, 8492, 6938, 4747, 2799, 8085, 12594, 7300, 2097, 2912, 11658, 1322, 2469, 10608, 11215, 4827, 1680, 12449, 11525, 5506, 1631, 12377, 7192, 6237, 10091, 4759, 11579, 12926, 12968, 11140, 12020, 7661, 12261, 11021, 6945, 5130, 8525, 10483, 9805, 8833, 11690, 826, 4789, 6469, 11316, 1224, 321, 4526, 12191, 2732, 720, 11835, 3879, 12017, 9463, 5947, 4328, 3791, 2507, 279, 5811, 10901, 9983, 4240, 7325, 6376, 4833, 1668, 6886, 6075, 1679, 7420, 11116, 12795, 4287, 12744, 5329, 10113, 3682, 327, 9165, 10125, 7439, 6914, 815, 11294, 6440, 1946, 8063, 10528, 12572, 810, 5398, 472, 778, 1714, 6234, 6053, 10726, 8090, 8605, 4309, 8987, 11258, 6844, 10739, 5359, 10750, 2778, 3776, 4958, 8619, 9313, 755, 11756, 10065, 5764, 5936, 895, 8992, 5664, 4073, 8190, 4908, 10220, 7271, 11489, 9280, 7091, 11372, 5203, 10204, 2025, 6138, 7272, 12330, 6256, 531, 8670, 5531, 440, 6693, 465, 5759, 3166, 12451, 11590, 7899, 6628, 4304, 1486, 5074, 12053, 2908, 10874, 7835, 6952, 6172, 6267, 4831, 5618, 11126, 10442, 7335, 637, 9557, 1813, 10086, 7368, 4356, 11211, 6417, 12382, 12125, 12488, 3693, 1225, 12502, 2854, 8601, 12793, 9596, 10372, 4076, 11586, 6460, 12383, 7111, 9181, 12476, 11155, 7452, 10161, 5396, 2323, 11621, 12762, 10890, 8120, 8604, 5663, 9219, 704, 2731, 12214, 10302, 5751, 2269, 267, 11369, 7858, 12933, 71, 4848, 11748, 1384, 3025, 593, 10087, 7926, 1467, 8274, 479, 2124, 3579, 3133, 1960, 7747, 9535, 9583, 8503, 11777, 1869, 7984, 1020, 5342, 9977, 6050, 10189, 771, 1836, 356, 6946, 2358, 7285, 11554, 388, 27, 2882, 1674, 11098, 9922, 10256, 6679, 6711, 5946, 5630, 38, 4799, 7973, 8275, 5830, 395, 4272, 2303, 12674, 12745, 12084, 432, 12122, 9286, 10441, 3222, 10794, 12150, 9446, 1261, 4200, 6894, 10139, 9898, 2209, 5977, 6203, 11404, 325, 12790, 7536, 1000, 12211, 269, 4490, 426, 417, 3655, 5484, 854, 434, 7083, 4255, 5201, 12757, 6873, 12297, 6367, 10561, 996, 7430, 361, 10665, 6284, 1463, 11062, 1663, 2810, 2700, 3587, 11966, 512, 12481, 10478, 2781, 6049, 7170, 79, 6363, 5489, 11681, 273, 11542, 372, 9580, 335, 5427, 2563, 10251, 12222, 11615, 5432, 8508, 1623, 5545, 9495, 873, 10187, 1921, 11817, 5462, 7217, 11024, 9080, 386, 12064, 7844, 6555, 3410, 10309, 11944, 1542, 10928, 2519, 2920, 6174, 1218, 9144, 10777, 5875, 6563, 7143, 6743, 1992, 12897, 5707, 7487, 429, 9458, 4788, 12163, 5966, 2808, 1007, 9613, 3061, 5454, 2742, 9461, 12773, 3187, 8013, 4576, 712, 8360, 11995, 10701, 3567, 11769, 2532, 3736, 12259, 6291, 5296, 1607, 3209, 12121, 354, 6403, 6437, 11359, 4325, 4885, 858, 4604, 10549, 7510, 8037, 11661, 1774, 10972, 282, 4214, 5371, 4589, 10639, 7767, 11510, 9197, 12758, 477, 1865, 5819, 3635, 5048, 1547, 209, 4897, 10244, 1803, 8049, 4444, 2171, 1594, 10223, 7231, 10116, 10050, 5694, 12582, 4615, 1397, 11818, 7327, 10494, 12909, 553, 8480, 10922, 2663, 3551, 3964, 1128, 6820, 4667, 8006, 7702, 2396, 5018, 12754, 11730, 12378, 7305, 2126, 9120, 8004, 7137, 3593, 10436, 6842, 6036, 5478, 2329, 7818, 5783, 12303, 12095, 9813, 1082, 1301, 1336, 10444, 4131, 6219, 10387, 11332, 5170, 11572, 5985, 11691, 10879, 10721, 10289, 5827, 3001, 4843, 8962, 10, 11736, 12071, 2815, 10265, 12919, 2514, 10850, 7565, 454, 8671, 8015, 6953, 6561, 8955, 8613, 7800, 5012, 6955, 1761, 9078, 12992, 12394, 8995, 5463, 7114, 3314, 10490, 1033, 11288, 4092, 7903, 5609, 9405, 10243, 10723, 10672, 10862, 7078, 4979, 4724, 5457, 9370, 1502, 5595, 8357, 4654, 11860, 2787, 12525, 10157, 5105, 7239, 6029, 10577, 9683, 12706, 4712, 5040, 3378, 9993, 12651, 2279, 11709, 11239, 7611, 403, 10636, 11085, 12470, 5198, 5035, 2657, 2734, 10529, 6375, 6751, 3380, 8546, 11686, 6088, 1705, 11111, 10624, 9006, 9618, 4681, 7642, 1402, 5865, 311, 1672, 12046, 7318, 11274, 11819, 4933, 1513, 11605, 10691, 11339, 3518, 7156, 1701, 4515, 342, 7764, 11166, 1801, 6857, 12250, 11016, 2494, 8574, 7449, 12103, 8978, 4741, 3121, 7674, 5385, 6001, 9601, 3666, 2403, 11583, 3786, 11347, 4613, 6650, 2330, 11983, 10916, 7914, 7768, 4821, 7788, 5389, 2455, 2199, 9837, 12011, 2457, 11544, 7002, 3419, 1890, 11005, 11307, 7107, 2974, 8592, 6943, 5340, 12006, 1923, 154, 8400, 4536, 1271, 1162, 979, 673, 7592, 7317, 8468, 6790, 9002, 3684, 3838, 2120, 6903, 11389, 11786, 5179, 6706, 6149, 6169, 12045, 12821, 431, 6978, 3159, 8343, 3397, 3689, 139, 11970, 6684, 966, 5265, 8053, 1625, 2611, 4185, 1823, 993, 8719, 11538, 9275, 9604, 4782, 6936, 5285, 982, 2412, 4703, 9605, 10746, 11177, 2767, 6691, 7125, 5718, 598, 1468, 4580, 7831, 11759, 6590, 2998, 5921, 1016, 6627, 4695, 8958, 4519, 8714, 8416, 11193, 10956, 1006, 569, 9128, 12129, 10329, 11447, 12311, 10428, 1072, 143, 10422, 3161, 6090, 11052, 9949, 4901, 4014, 2851, 1046, 7868, 4134, 10355, 5274, 1287, 5494, 10493, 7085, 4472, 6042, 2084, 3221, 1102, 6838, 12635, 5306, 956, 482, 7560, 3816, 1034, 4399, 3921, 11551, 3968, 4459, 8648, 9476, 2016, 11213, 3084, 10869, 6562, 3594, 12197, 8621, 10954, 10945, 9853, 964, 3886, 630, 3872, 10612, 5045, 1912, 12118, 11547, 345, 11189, 6124, 3239, 10618, 7281, 11490, 1268, 1474, 6507, 3451, 4694, 5128, 5291, 9844, 6192, 7573, 1861, 11640, 10815, 11810, 1980, 173, 12412, 6647, 4640, 10232, 6236, 11854, 9587, 9371, 12543, 5252, 2900, 5812, 7953, 4944, 11471, 3017, 11775, 9726, 4265, 12808, 2802, 2136, 10393, 11123, 10136, 9939, 12977, 9567, 12001, 8967, 10480, 641, 1548, 8651, 4787, 4884, 2030, 10108, 6632, 5871, 8257, 8929, 11558, 12232, 3868, 10688, 2753, 8741, 3103, 12661, 12825, 11967, 3415, 11707, 6533, 11322, 10245, 338, 11068, 189, 9800, 1620, 11871, 9001, 9869, 3051, 6905, 3581, 9531, 3809, 77, 12802, 2272, 7745, 2834, 2534, 8016, 3098, 1760, 11834, 10303, 1816, 625, 8424, 12244, 1194, 9524, 12139, 1676, 738, 8410, 696, 1272, 9157, 1376, 9775, 736, 445, 5439, 11637, 12707, 3746, 11232, 11713, 5333, 2599, 7581, 3549, 9195, 12673, 405, 7326, 11805, 448, 3931, 4274, 5927, 4682, 12278, 3304, 8398, 10172, 9132, 2379, 7757, 10425, 2475, 7869, 1305, 11, 4826, 8549, 2050, 78, 4479, 5196, 9200, 7724, 2372, 7095, 7182, 8705, 10857, 2231, 12979, 2318, 8709, 2911, 2002, 7148, 1441, 8775, 7013, 9307, 474, 9871, 9111, 6675, 581, 5213, 9070, 3337, 11384, 5879, 8103, 7997, 2926, 3641, 8041, 11924, 4529, 2550, 6008, 2117, 4124, 9816, 2828, 3118, 1872, 7124, 10641, 2476, 2334, 2968, 2389, 399, 6756, 4188, 4349, 3129, 6881, 11678, 7011, 7607, 7224, 2610, 9724, 2174, 4690, 12939, 5735, 3296, 9840, 5863, 4267, 9716, 12555, 3325, 9421, 11035, 8841, 3096, 2154, 5784, 11235, 1673, 4279, 261, 7765, 190, 1914, 9762, 3519, 11042, 9451, 6426, 2728, 4670, 5256, 5959, 2726, 10439, 5145, 8677, 9796, 12127, 8022, 1747, 8105, 2326, 6860, 11280, 4339, 2297, 2260, 11043, 6719, 8157, 9789, 5598, 647, 2568, 10986, 12726, 2208, 2830, 11187, 12107, 3270, 4956, 12271, 4540, 3893, 4039, 8064, 6282, 7025, 2109, 12345, 3181, 2875, 6296, 1860, 9860, 6769, 9272, 8731, 6712, 4971, 1023, 3745, 7543, 2685, 9191, 2661, 989, 1509, 12741, 11611, 648, 2157, 5418, 8258, 8904, 3627, 5246, 690, 3232, 8579, 9643, 7072, 7404, 12950, 1920, 1793, 623, 7833, 7218, 1220, 3355, 5754, 4638, 621, 670, 3590, 12109, 5881, 187, 5312, 12227, 5440, 12138, 9261, 11075, 3950, 12521, 9308, 5355, 6875, 4260, 10770, 12231, 5987, 11723, 4498, 11868, 2877, 5153, 11315, 11131, 11078, 3935, 3728, 9523, 723, 1569, 7441, 4189, 4094, 12411, 9948, 7515, 10084, 4954, 12699, 8913, 8018, 4370, 3165, 12898, 7861, 5788, 5813, 806, 5880, 2137, 4775, 6199, 480, 649, 10941, 10671, 11153, 11095, 3721, 2222, 10323, 6918, 9626, 10743, 3368, 4063, 3853, 8849, 6009, 8754, 3698, 746, 6216, 8422, 6041, 7354, 6398, 422, 6658, 2214, 167, 1248, 7631, 11528, 11593, 1045, 9472, 2043, 2676, 2555, 11004, 5763, 12381, 9094, 12048, 3563, 6397, 12429, 6319, 9322, 1530, 7820, 575, 10325, 1898, 12086, 3619, 11735, 9571, 10175, 4566, 5302, 8630, 1444, 10560, 6808, 10578, 11324, 4488, 9305, 1822, 3081, 7829, 8201, 7316, 12357, 11270, 237, 1962, 5301, 11442, 4953, 4216, 7681, 2099, 6522, 12784, 2010, 8383, 12393, 2832, 1922, 8332, 10138, 11725, 7221, 7341, 4586, 11293, 10383, 9552, 6887, 12369, 8735, 12143, 2447, 7062, 9527, 4882, 8909, 2772, 7709, 2939, 5227, 11321, 7426, 4319, 3523, 3063, 4355, 2579, 2431, 1498, 11683, 2314, 4046, 1549, 9867, 889, 11608, 3513, 3752, 2510, 10167, 2331, 10201, 10985, 11734, 1661, 7605, 12693, 5348, 12564, 4828, 6391, 4001, 7501, 5917, 5091, 879, 8720, 8163, 10706, 10660, 6202, 4779, 11299, 2397, 10257, 8497, 11420, 744, 8960, 5431, 3244, 5479, 3737, 2889, 7612, 12322, 12225, 4334, 10525, 4952, 4284, 5455, 59, 10058, 9372, 1012, 8007, 11034, 681, 11792, 10828, 12167, 852, 6708, 12858, 8713, 1835, 8365, 7547, 2975, 6260, 8707, 7807, 2896, 2806, 11904, 6572, 9824, 11705, 4921, 2688, 2761, 11954, 2449, 12962, 6167, 109, 12347, 10802, 9845, 2497, 10603, 6612, 6322, 12033, 4774, 5845, 10845, 2169, 11694, 5294, 12100, 10949, 6266, 10430, 1811, 3937, 722, 3297, 11951, 4290, 10063, 6649, 710, 7237, 9734, 8451, 6588, 4247, 860, 5828, 5397, 1252, 11477, 5231, 8831, 5442, 2696, 4636, 4205, 3407, 9850, 10128, 2191, 9632, 12010, 7185, 8118, 6163, 10611, 1341, 2783, 5790, 85, 2313, 94, 5782, 10786, 8827, 1199, 12702, 2755, 8322, 3477, 8044, 7944, 4835, 1553, 8752, 12047, 4620, 980, 7090, 5631, 10973, 2607, 2598, 2391, 8129, 8688, 12803, 9889, 9291, 3184, 208, 3472, 7996, 2567, 11466, 9608, 6760, 1449, 12072, 3109, 9823, 2189, 3559, 10467, 9319, 8685, 1385, 7744, 10054, 2365, 3094, 3657, 6579, 4068, 52, 11038, 9004, 12024, 7194, 5919, 12574, 1929, 5988, 6567, 1529, 5622, 9513, 5995, 7883, 4649, 3013, 4229, 3766, 4366, 6755, 10224, 9689, 1886, 2699, 7837, 12352, 9585, 1572, 952, 11782, 12768, 4347, 4962, 121, 4621, 2996, 12837, 12235, 2175, 3092, 6092, 1043, 4168, 1968, 981, 905, 3851, 5705, 2245, 12983, 2835, 12205, 6419, 4553, 532, 3162, 10768, 2374, 8626, 359, 7115, 8892, 6866, 4170, 535, 8860, 1873, 6454, 1998, 5311, 4968, 3409, 7999, 1558, 5611, 4798, 9357, 1181, 6271, 6383, 4462, 10596, 7418, 3631, 12622, 3115, 7191, 4970, 222, 8701, 2348, 11649, 5067, 5570, 12331, 12503, 377, 3969, 2316, 5884, 5675, 3008, 10306, 7754, 9391, 11290, 4364, 8244, 3149, 10270, 10769, 11107, 1578, 527, 2631, 965, 1855, 10926, 2897, 11603, 6058, 2766, 2674, 9895, 8403, 3928, 3866, 12969, 10940, 402, 10763, 203, 2533, 6568, 414, 1739, 6993, 8421, 6989, 10259, 4898, 5544, 1817, 6592, 4156, 5279, 487, 12815, 7698, 12878, 1881, 1068, 11284, 4016, 11300, 3059, 1053, 5044, 9206, 7355, 5582, 7413, 716, 3714, 12130, 9396, 6182, 10753, 8046, 739, 9239, 5516, 3826, 7299, 6358, 7739, 5777, 2693, 12474, 1839, 9194, 12328, 9256, 3300, 5472, 5615, 1851, 9230, 9687, 10615, 12494, 3710, 4230, 2800, 11798, 4221, 7322, 4961, 1877, 9542, 4630, 6641, 12720, 5923, 1918, 351, 4544, 7815, 10599, 2181, 2573, 3947, 5548, 4872, 906, 1127, 779, 6034, 6136, 1144, 4946, 1098, 2819, 1246, 3843, 8437, 10284, 10531, 10605, 4012, 2640, 9695, 3309, 1972, 4389, 12936, 5580, 3915, 4294, 7802, 10475, 12340, 11027, 3000, 10873, 4280, 9152, 6798, 1904, 6165, 5905, 848, 8602, 5165, 152, 9232, 11607, 8559, 8628, 4365, 1342, 6673, 3183, 10839, 4422, 9179, 1692, 2938, 6530, 8358, 709, 12514, 10364, 355, 146, 3927, 4358, 2750, 3609, 4963, 12452, 9028, 11484, 929, 12559, 1996, 8303, 12774, 8259, 5960, 1850, 2398, 9294, 3834, 12730, 1938, 11513, 7200, 5704, 2087, 8048, 3881, 8634, 10499, 4157, 9117, 12395, 11655, 10149, 4043, 2574, 3775, 10060, 12450, 12506, 4148, 7315, 9448, 8156, 404, 7279, 8066, 2795, 3047, 4869, 11375, 4025, 7265, 10837, 6273, 11398, 3110, 3613, 3487, 9656, 1979, 11986, 8345, 11533, 11054, 5956, 4491, 7344, 7459, 5106, 8125, 11073, 6959, 12669, 11272, 5748, 1343, 10904, 7166, 7186, 11879, 9713, 6832, 4372, 3898, 7711, 12034, 3840, 2226, 9981, 1067, 7703, 5698, 3979, 12266, 1212, 9506, 6107, 11765, 9102, 9565, 3833, 3653, 1657, 3632, 7824, 12087, 11399, 8654, 4495, 4035, 574, 10437, 3401, 6854, 349, 2003, 7828, 3371, 10606, 2905, 1289, 7623, 5944, 12526, 3669, 8933, 3718, 10233, 7658, 2450, 4198, 4411, 11800, 7620, 7559, 11393, 12495, 12591, 1804, 10464, 7242, 2462, 12221, 11281, 4449, 6070, 2257, 8585, 9251, 8140, 2965, 4182, 8128, 2032, 9714, 12880, 11386, 9058, 4714, 10421, 4911, 4174, 11997, 3180, 8948, 8327, 10990, 3582, 1186, 5407, 9207, 7257, 10417, 12951, 1590, 2504, 7053, 10181, 5942, 3734, 8815, 8100, 8184, 12280, 8282, 655, 9917, 653, 8928, 4533, 3865, 4175, 1269, 12421, 11029, 5292, 4937, 6069, 1828, 8138, 4637, 761, 10300, 6015, 9787, 6464, 9300, 1833, 10651, 7054, 4231, 8846, 11998, 9233, 9699, 11749, 3153, 7004, 5382, 7248, 4289, 12508, 789, 2149, 12490, 2679, 12313, 6859, 10507, 668, 9145, 10129, 5952, 4125, 3373, 7929, 2110, 312, 6241, 6095, 9376, 1420, 10994, 6532, 1874, 8245, 374, 6817, 2871, 11227, 47, 2275, 7457, 3169, 3274, 2864, 9765, 7569, 11498, 12149, 5890, 9184, 6540, 11665, 12833, 2429, 9622, 6004, 9379, 10517, 4871, 5297, 7162, 6131, 10290, 3071, 10097, 8110, 10366, 6770, 5893, 8350, 3048, 2824, 12329, 4749, 3601, 6112, 5619, 7805, 11987, 763, 5710, 616, 10340, 4825, 8160, 4050, 3562, 11585, 7811, 8341, 7769, 1203, 8832, 7972, 3941, 6352, 3934, 4816, 12717, 2590, 10895, 1274, 7360, 5539, 6497, 3884, 7261, 11460, 3203, 9974, 9525, 877, 1059, 9324, 8656, 6258, 9032, 350, 8056, 11083, 7557, 7244, 8635, 10416, 7909, 12014, 11413, 10315, 1501, 4653, 3900, 84, 12738, 5667, 10720, 4710, 10570, 6214, 6668, 8785, 12069, 10820, 8081, 3788, 4527, 7496, 10900, 5405, 8683, 3286, 4119, 11411, 5200, 5347, 1919, 11508, 4729, 5588, 11656, 5690, 6193, 2223, 6103, 8188, 1177, 5916, 5635, 3938, 5191, 8552, 2629, 12462, 2101, 10029, 3753, 5898, 3148, 1915, 12190, 10319, 9715, 2066, 1080, 5115, 6056, 4005, 5662, 3044, 928, 10602, 9834, 10461, 857, 4113, 9153, 12649, 6298, 6778, 3896, 10218, 8386, 9042, 6209, 5136, 9841, 10118, 10292, 10865, 1609, 11452, 8939, 12410, 3448, 7516, 4466, 9641, 4823, 1725, 7864, 9607, 2703, 4582, 3258, 10182, 10984, 3374, 5807, 199, 765, 223, 6252, 6028, 1370, 11391, 11222, 4705, 318, 12427, 2289, 6573, 29, 4206, 9944, 1695, 7117, 4250, 1990, 10016, 8512, 853, 12863, 7879, 7918, 2322, 7132, 9057, 8300, 10784, 988, 10400, 10089, 11172, 12170, 2407, 10713, 5046, 12032, 7787, 2063, 9806, 1324, 10273, 11527, 9988, 805, 7183, 749, 11305, 820, 9072, 11335, 4320, 11550, 7538, 5384, 9403, 4302, 6052, 12260, 2055, 1709, 2636, 1140, 4120, 10535, 8851, 2491, 6802, 8811, 5183, 9160, 7463, 8434, 449, 7692, 2014, 585, 7871, 5908, 685, 10741, 6896, 3744, 5943, 5592, 12781, 9254, 4259, 6571, 10565, 7264, 2036, 6435, 2612, 1015, 12554, 4386, 5627, 2788, 5832, 6688, 7099, 12695, 12060, 2719, 7080, 2840, 7269, 80, 6104, 8485, 4905, 813, 9578, 5836, 11614, 463, 7978, 14, 8801, 6144, 6087, 12458, 997, 8867, 12787, 9911, 4743, 9738, 8910, 4625, 3264, 11122, 12963, 12567, 5109, 974, 9701, 3572, 7806, 11751, 5689, 2903, 4718, 7198, 1699, 12917, 9623, 5319, 9064, 4845, 7749, 2502, 5721, 5174, 4698, 3936, 1069, 3764, 12577, 3649, 10835, 10533, 5568, 6677, 5867, 6730, 467, 4784, 3292, 6596, 4181, 5805, 11090, 8146, 10241, 4895, 11114, 4691, 8562, 8349, 2836, 9697, 2125, 8094, 2390, 8032, 3108, 5885, 1060, 8080, 3354, 8017, 1231, 3131, 7321, 1028, 12972, 4612, 617, 10822, 4147, 5093, 916, 1606, 642, 10083, 6401, 9041, 6966, 3201, 3600, 3628, 5278, 11231, 9592, 10694, 6371, 5614, 9832, 1178, 10796, 5522, 9069, 7435, 1163, 2204, 7383, 5138, 5139, 5195, 12841, 4273, 1650, 5224, 6487, 8850, 8362, 10194, 5728, 4336, 672, 8445, 3539, 9785, 9285, 3574, 4767, 1638, 4673, 4546, 2797, 197, 6292, 10184, 9467, 3228, 11588, 10892, 12956, 6589, 10229, 521, 3585, 7684, 7507, 818, 3093, 12827, 1771, 2211, 3403, 1031, 5470, 7776, 6578, 10250, 12891, 8242, 12732, 11626, 4909, 3376, 5336, 8767, 3506, 8689, 8885, 3919, 8455, 230, 2237, 8043, 7440, 7138, 295, 1107, 12165, 1216, 2020, 6782, 12929, 4236, 41, 12670, 7750, 3856, 12108, 6850, 4225, 3837, 2235, 5171, 1977, 11864, 12576, 7214, 12562, 272, 11000, 12274, 10536, 2299, 11474, 10141, 12703, 9754, 2035, 2356, 297, 12133, 12424, 12295, 3946, 10880, 4570, 9067, 7784, 3624, 5156, 1694, 8796, 6222, 4964, 8411, 2747, 10854, 9025, 9764, 2006, 5556, 6274, 115, 1838, 2971, 1568, 887, 12752, 6728, 4999, 7407, 10031, 1398, 8318, 1055, 1002, 9084, 6622, 9566, 2440, 8212, 11727, 3238, 8676, 12920, 11224, 4514, 9836, 7628, 3535, 12292, 9154, 9778, 12957, 8923, 5590, 5190, 1720, 7945, 1978, 9459, 12366, 4215, 8807, 6683, 5399, 8715, 9430, 3861, 7874, 10034, 2978, 9923, 10185, 528, 11015, 7226, 7346, 795, 1616, 10312, 2764, 9496, 3962, 8971, 9053, 8198, 8983, 3864, 9928, 3500, 3494, 8297, 11416, 6269, 11844, 613, 11459, 4669, 11852, 1189, 6551, 8220, 2220, 8530, 8930, 7904, 11019, 8092, 6745, 9083, 9688, 4004, 4443, 538, 9148, 7181, 5060, 10009, 4769, 8293, 10004, 11202, 4286, 8008, 8582, 3411, 8914, 2060, 207, 11900, 12855, 2031, 7205, 3954, 7514, 9986, 3470, 12316, 4207, 8541, 4162, 8618, 4237, 9075, 1214, 5351, 11859, 4709, 6132, 11724, 5896, 10107, 9606, 2741, 12204, 442, 1940, 12890, 732, 1510, 12647, 3283, 12710, 5049, 5874, 8310, 1597, 3125, 1198, 11087, 1423, 1091, 10343, 5391, 136, 471, 859, 12660, 2305, 7783, 8254, 268, 5424, 2011, 9992, 3580, 7032, 11517, 10370, 7247, 12387, 6415, 12668, 11915, 9553, 7907, 2428, 10140, 1961, 3454, 5328, 8247, 5436, 3113, 6290, 9105, 11143, 10269, 8132, 6714, 10453, 953, 11137, 8174, 558, 9985, 8393, 9886, 5723, 3431, 12415, 6475, 11219, 1830, 4892, 12154, 9967, 73, 10913, 12162, 11058, 11888, 8505, 5560, 4556, 6339, 352, 3172, 9991, 11753, 10767, 5098, 9211, 4585, 4548, 10082, 8021, 5793, 10674, 959, 7707, 11450, 9364, 9705, 4840, 1416, 9731, 9559, 5493, 5491, 8806, 1614, 7876, 11340, 5062, 7546, 3558, 5344, 10395, 3823, 10801, 3249, 4419, 8874, 12786, 12097, 11437, 12091, 5706, 160, 5612, 8379, 5971, 4000, 2838, 7074, 11710, 10019, 6554, 8419, 375, 3530, 3755, 7376, 10099, 777, 7832, 9661, 6272, 4579, 1901, 7228, 9802, 11796, 1528, 6556, 5461, 4555, 1303, 1721, 1488, 9044, 12853, 10350, 8428, 9875, 12830, 10978, 2540, 1192, 10311, 11680, 9707, 1179, 12610, 7145, 4036, 5131, 7720, 743, 220, 2453, 3729, 3230, 8368, 3660, 2343, 1413, 10568, 6758, 3346, 5, 3623, 12202, 605, 3128, 3798, 12959, 7366, 6878, 10165, 4982, 772, 1531, 10047, 8757, 7616, 290, 3130, 6304, 3278, 3685, 6792, 12290, 2535, 5469, 12388, 11263, 1292, 2019, 2213, 1659, 6853, 3560, 8415, 12126, 3460, 7131, 878, 12281, 7369, 8059, 1722, 10077, 1749, 5758, 3315, 11803, 1255, 1947, 12321, 11214, 1077, 6724, 2736, 6569, 11679, 2227, 6253, 4818, 5542, 2628, 5063, 737, 2207, 10371, 8321, 9964, 6804, 9597, 6355, 10925, 2470, 11816, 3208, 8027, 10466, 6188, 11230, 4832, 6318, 11159, 2649, 9226, 6061, 5558, 5778, 3844, 11829, 4742, 3173, 2560, 4496, 4429, 1896, 5075, 3639, 6671, 11086, 6232, 3068, 3457, 11248, 819, 10595, 3965, 1453, 6155, 1111, 280, 842, 12550, 1810, 4143, 7141, 12806, 6874, 5973, 10458, 2021, 4836, 5360, 5603, 9126, 3760, 9255, 3743, 9205, 8072, 6657, 12148, 8179, 702, 5552, 8708, 9692, 9244, 5838, 8011, 1150, 6146, 7098, 4866, 10237, 11195, 10413, 284, 1768, 12570, 1935, 10787, 3156, 1348, 2473, 5511, 10471, 10747, 8988, 2362, 5848, 8513, 5151, 3078, 9138, 6228, 9338, 7794, 6429, 7164, 5897, 12873, 10933, 298, 926, 911, 2383, 6865, 12876, 5557, 2074, 9742, 2743, 7250, 1469, 3614, 5259, 1880, 10716, 4674, 7323, 5370, 12675, 6385, 6226, 6229, 1, 1796, 8294, 12684, 9961, 6996, 10048, 4987, 8264, 8776, 12852, 5250, 5163, 11480, 5502, 391, 11909, 745, 3981, 7352, 11950, 6643, 384, 9177, 3395, 5787, 5361, 3824, 1071, 563, 2123, 1184, 5052, 7165, 147, 1789, 1399, 3006, 4755, 7172, 7392, 2616, 3662, 9659, 1870, 5320, 3050, 8171, 2386, 8035, 1138, 2816, 1892, 11652, 11113, 8408, 2883, 28, 5604, 10921, 4684, 660, 3830, 6499, 7664, 4167, 12169, 12116, 382, 2623, 11594, 2044, 12341, 6786, 3789, 11512, 7894, 11033, 5610, 227, 5961, 6998, 3233, 7016, 12023, 652, 8509, 2307, 5080, 5900, 8799, 9890, 12598, 5925, 1648, 11763, 7234, 3803, 7667, 8769, 4918, 2812, 10776, 8213, 3532, 4983, 10778, 8528, 9173, 3435, 11032, 688, 3140, 6582, 8575, 12457, 8069, 12353, 3246, 1426, 9730, 4658, 12948, 5377, 8925, 11940, 11127, 12935, 2605, 12254, 12406, 2831, 6541, 2818, 12763, 2418, 728, 10304, 7167, 1579, 12772, 9158, 9759, 12425, 11456, 3014, 7136, 3391, 3790, 5965, 6586, 9581, 10056, 3227, 2588, 9980, 9263, 9969, 10456, 7209, 8185, 9196, 4899, 5369, 12195, 9399, 11785, 10216, 8347, 3039, 3671, 10585, 2708, 5354, 6977, 1554, 4426, 8758, 5039, 12851, 566, 2796, 126, 3678, 12444, 2433, 8104, 25, 6170, 5037, 305, 9418, 10491, 12134, 5724, 7680, 3713, 3150, 1514, 8439, 8313, 8366, 6118, 1538, 9700, 3438, 8404, 11182, 6754, 5077, 12477, 2243, 4348, 5289, 11622, 3835, 12420, 3545, 676, 200, 8442, 5996, 4939, 2531, 2294, 7069, 6162, 1448, 7433, 11089, 8517, 7375, 9861, 4545, 10192, 9538, 2606, 12902, 1560, 6439, 8134, 1445, 10896, 6224, 12978, 6033, 10522, 6201, 11427, 6204, 4934, 5108, 10115, 12005, 3147, 10630, 9536, 6984, 10260, 5505, 10476, 3878, 1678, 10818, 9281, 258, 1129, 12618, 5699, 2572, 4572, 4656, 10625, 9397, 8516, 9501, 3502, 2121, 7695, 1270, 11878, 3961, 1438, 4975, 3708, 9245, 3998, 12747, 9081, 13, 7938, 4407, 1062, 9984, 12875, 12530, 7534, 10039, 253, 6374, 9159, 951, 3016, 9786, 12375, 827, 6893, 11397, 15, 1494, 11174, 9086, 3564, 9109, 6184, 11552, 12371, 9781, 11532, 7235, 357, 5727, 7358, 8704, 9611, 5716, 7499, 2949, 11536, 4611, 7813, 4926, 5535, 5352, 5687, 4609, 4657, 2225, 7974, 12218, 2474, 4838, 9188, 6330, 11758, 11922, 58, 3347, 7310, 5574, 10276, 12905, 10137, 11403, 4562, 6346, 7561, 1630, 2267, 11365, 8828, 10435, 4266, 12924, 323, 9628, 6413, 870, 4070, 7913, 9384, 4503, 4323, 10317, 23, 12965, 627, 4019, 5206, 2849, 4345, 1359, 9235, 12751, 2438, 12589, 8071, 1340, 9740, 10574, 570, 9690, 6126, 610, 1117, 12326, 1526, 9289, 7402, 6655, 4817, 1910, 1520, 9203, 108, 12363, 8329, 12151, 3680, 4123, 5073, 5334, 4352, 8319, 4507, 10349, 3255, 11869, 9020, 10124, 781, 478, 7102, 2955, 11218, 3765, 999, 3260, 2221, 2656, 8121, 5935, 1754, 3417, 5536, 5386, 7548, 4190, 5475, 7311, 8956, 12718, 545, 1883, 5157, 3576, 7927, 6151, 10579, 10756, 1239, 2461, 1864, 8028, 10221, 9045, 811, 4141, 226, 2967, 2012, 3387, 5745, 12440, 6206, 534, 4646, 3747, 8732, 9199, 7683, 1893, 9629, 9330, 9438, 6535, 1766, 12186, 4313, 3384, 1125, 3813, 10520, 1008, 11974, 8935, 3906, 8947, 443, 1704, 8847, 12843, 10238, 6334, 12272, 12209, 7277, 4797, 4371, 3849, 3802, 8650, 7422, 4756, 976, 1459, 5345, 8034, 3070, 2350, 9586, 3271, 5674, 12879, 1085, 11433, 8906, 2528, 8699, 1455, 2338, 8473, 7371, 11012, 12680, 102, 4258, 7977, 12337, 11899, 3763, 4627, 2083, 2597, 11037, 10363, 9723, 1580, 756, 9134, 10798, 9876, 9374, 10150, 12092, 9347, 490, 2575, 1101, 10853, 5447, 4343, 4676, 12715, 5514, 2254, 1288, 5795, 2580, 567, 2576, 11564, 12937, 4700, 8647, 5218, 2381, 9470, 12319, 5207, 4739, 10927, 2248, 7357, 6654, 6467, 9071, 328, 9432, 1586, 8009, 1546, 4839, 3002, 12018, 7855, 7042, 1019, 2711, 4117, 807, 7015, 3458, 3450, 5646, 4226, 3831, 11772, 5029, 12560, 10166, 6262, 2614, 2442, 11435, 8465, 9893, 8702, 10155, 11766, 4172, 5446, 7838, 4380, 3621, 6799, 5899, 5850, 11659, 7362, 7994, 9943, 4665, 333, 8805, 10731, 4820, 410, 11421, 3538, 12597, 1994, 4115, 7706, 8556, 6640, 10693, 6869, 7799, 12434, 9851, 8203, 12914, 1726, 2727, 1296, 4618, 758, 786, 6621, 3652, 804, 5979, 324, 6884, 5127, 7670, 3974, 8610, 5148, 7123, 7109, 6741, 915, 843, 4441, 3819, 2382, 4373, 10773, 4876, 751, 1476, 530, 10541, 3795, 9010, 4007, 10130, 11580, 1403, 11061, 3787, 1759, 2048, 5041, 12498, 5891, 2692, 2177, 6425, 1982, 11764, 6239, 7252, 10797, 9966, 8989, 8231, 5002, 1716, 832, 1196, 11069, 973, 46, 11606, 10324, 9129, 9962, 7489, 6390, 141, 9043, 1702, 444, 10180, 12652, 2321, 12471, 1941, 7533, 8876, 9772, 1113, 3901, 4671, 11125, 7693, 2976, 7173, 4397, 2664, 12475, 4880, 2541, 1770, 7902, 1433, 99, 6247, 9905, 7189, 4037, 3536, 1021, 11289, 5980, 1843, 3732, 9169, 5185, 6987, 11154, 11170, 5606, 12300, 11972, 5572, 9768, 735, 4403, 785, 6836, 1041, 12735, 3032, 4149, 4593, 9271, 12436, 10405, 8773, 8560, 6190, 9693, 9652, 1431, 1188, 1195, 1564, 8783, 12627, 6856, 1723, 10407, 5123, 8749, 1333, 12035, 7635, 4715, 192, 6303, 10571, 9434, 11353, 1489, 2547, 4568, 5104, 1440, 6690, 5872, 6765, 5001, 6211, 12359, 11105, 10341, 2468, 5341, 2004, 9373, 11382, 3517, 6230, 1953, 3916, 864, 2247, 1985, 6543, 12777, 11048, 2210, 5846, 7830, 5481, 3396, 4547, 2486, 10277, 2843, 4084, 2186, 2104, 10066, 896, 5602, 1797, 5219, 518, 1815, 5054, 4628, 10472, 6336, 3360, 9246, 129, 9937, 10183, 12448, 6698, 1460, 12656, 7423, 9946, 7537, 6261, 1094, 2738, 10524, 8169, 865, 9908, 1472, 5199, 263, 11587, 12835, 12731, 10583, 8152, 3526, 5239, 9234, 10188, 4912, 757, 10959, 5765, 1030, 7540, 1325, 3199, 11146, 7460, 5709, 10649, 2073, 11855, 12721, 6642, 10619, 8762, 6054, 12834, 6082, 275, 7245, 11472, 1493, 5392, 2809, 11132, 6338, 11326, 4726, 9503, 1871, 9097, 7088, 5567, 1814, 4737, 5922, 1244, 11925, 9270, 4173, 9427, 4420, 5158, 2076, 1643, 9710, 6898, 3850, 3784, 1556, 12759, 6729, 1176, 10338, 2053, 2683, 4203, 1697, 2857, 3852, 1300, 4171, 4253, 9751, 7921, 6171, 8564, 1952, 3651, 8470, 5657, 7140, 9426, 11150, 1197, 4425, 11589, 5982, 11287, 1809, 1666, 7746, 1265, 8029, 4502, 11327, 4445, 11742, 12119, 4859, 8042, 9279, 5238, 9436, 11781, 11044, 504, 10661, 4551, 1574, 1293, 11064, 5480, 9007, 11275, 5081, 1447, 8273, 1432, 10707, 7644, 2259, 7384, 5877, 3842, 11494, 4353, 8774, 1781, 961, 557, 3095, 5749, 1428, 7081, 7564, 4368, 11684, 8504, 9780, 4980, 9924, 6089, 4563, 2446, 3144, 4454, 1154, 2352, 6310, 6079, 4541, 2233, 2490, 2250, 9651, 4261, 8475, 5722, 6670, 10135, 3867, 12857, 5585, 11700, 68, 4795, 12867, 7555, 10509, 8625, 7304, 5031, 11645, 2445, 10573, 4363, 12975, 1646, 12889, 8003, 6975, 1247, 6553, 5840, 2105, 2046, 4647, 7519, 5412, 10950, 8019, 3608, 10569, 8725, 706, 11568, 20, 7901, 6603, 6341, 7785, 51, 419, 217, 2107, 4453, 9444, 4118, 1035, 6969, 4781, 7491, 6812, 5940, 7051, 9047, 2444, 8281, 247, 10297, 3366, 2320, 6717, 8820, 821, 12986, 4396, 12974, 11325, 3709, 6270, 5930, 6566, 11613, 246, 4191, 11976, 7941, 7905, 2001, 3197, 1139, 2143, 7037, 7580, 9698, 2782, 8099, 3508, 56, 10452, 11431, 8448, 1335, 9854, 10545, 4297, 1654, 5161, 1424, 546, 6906, 1533, 4450, 1907, 11007, 11601, 7476, 10506, 7066, 101, 6353, 4077, 9952, 11866, 1366, 7121, 2603, 8280, 6205, 4723, 10064, 9479, 1149, 4013, 1764, 8187, 9795, 1495, 6249, 5700, 12990, 3398, 5042, 8380, 3444, 8717, 5078, 2789, 10814, 5554, 9664, 3782, 3583, 10145, 12734, 11053, 8199, 1405, 9722, 6479, 5298, 12068, 3038, 9088, 1658, 7222, 5277, 12076, 6424, 1894, 7119, 10581, 10263, 11209, 12750, 2252, 1536, 4740, 11093, 2860, 7847, 8787, 3920, 1786, 2484, 7881, 6819, 12002, 9283, 901, 7878, 8207, 11956, 2977, 1969, 7558, 5387, 12739, 9815, 6474, 12954, 4473, 12616, 9213, 7329, 2921, 9532, 4598, 3507, 10011, 1899, 10648, 8646, 8529, 10379, 7101, 7986, 6066, 4107, 8501, 3170, 6176, 6484, 11876, 1211, 910, 4164, 7762, 4209, 8115, 2670, 8078, 2439, 7700, 9979, 1491, 1309, 8464, 11022, 8652, 7199, 9217, 595, 5736, 5284, 4929, 3060, 7363, 8246, 10809, 3948, 10877, 1347, 10783, 573, 2928, 5186, 5616, 7295, 5388, 133, 4841, 11689, 2454, 8490, 317, 11511, 6833, 9137, 12882, 12407, 6956, 902, 12401, 3416, 165, 5792, 8054, 326, 5244, 10557, 9090, 1278, 1241, 10915, 5665, 10690, 847, 10519, 10217, 9063, 11360, 10365, 3054, 9968, 7770, 177, 10266, 7020, 9934, 3869, 7454, 5133, 3284, 3247, 381, 6749, 5914, 8881, 5116, 9657, 9493, 6882, 3871, 12895, 7964, 7614, 12000, 12308, 4746, 6235, 4244, 8502, 1942, 4138, 2645, 8457, 1571, 6557, 3939, 10053, 8062, 1537, 8777, 6772, 6900, 3028, 11268, 6123, 3467, 7075, 7343, 5143, 3036, 1421, 10492, 1478, 5909, 9915, 6122, 11119, 2969, 4283, 12062, 9040, 3723, 9662, 8354, 7578, 2570, 9704, 9725, 4537, 1497, 11001, 11469, 401, 10697, 8742, 10328, 5376, 12456, 6019, 10936, 7851, 1100, 7647, 4708, 1256, 4238, 7608, 10542, 9474, 4351, 2271, 7233, 1042, 309, 10653, 2817, 692, 5825, 1703, 4850, 428, 1228, 10017, 4431, 5746, 486, 5887, 8572, 12507, 2923, 2311, 4204, 1763, 10614, 3066, 10952, 559, 229, 11720, 5079, 6078, 5613, 808, 9499, 7050, 9679, 7639, 5555, 6314, 3033, 3889, 262, 301, 2103, 3811, 1845, 2697, 3245, 12856, 3043, 1357, 3005, 5932, 11139, 5742, 497, 12775, 10564, 4415, 9864, 8346, 8951, 5236, 1395, 10348, 7970, 9505, 11405, 2458, 6373, 8030, 2135, 8233, 2522, 11846, 8568, 11563, 110, 4318, 9143, 6771, 9978, 2586, 6372, 10800, 380, 5951, 3294, 7060, 9737, 3801, 9125, 10891, 9029, 7860, 8260, 12791, 11432, 775, 9627, 6351, 12928, 4717, 11914, 7586, 9848, 3741, 9825, 7177, 10268, 11395, 2203, 12027, 9413, 2626, 2844, 683, 10955, 7880, 6646, 10967, 6917, 11169, 2639, 11609, 5915, 1157, 499, 9909, 11634, 823, 8217, 2129, 6452, 1557, 10912, 10559, 3243, 2296, 9332, 12958, 3524, 12910, 11091, 4249, 9258, 11628, 12663, 2328, 764, 5993, 12623, 5126, 6827, 5547, 4875, 1374, 9100, 6623, 11179, 2089, 7232, 6114, 3716, 7427, 5918, 4950, 10703, 11349, 6613, 4575, 7416, 1425, 11493, 3139, 4470, 11184, 8109, 12804, 2762, 12646, 11957, 10658, 3626, 7057, 11014, 1349, 12465, 93, 2509, 10059, 8927, 225, 8789, 9482, 329, 7704, 8304, 5241, 10546, 8982, 10884, 8678, 7682, 3810, 618, 2650, 10408, 10446, 4087, 12593, 6581, 5523, 6490, 11804, 3459, 6692, 12565, 5970, 4531, 4772, 8315, 3073, 2543, 4066, 12504, 10589, 8075, 7778, 12604, 1931, 2061, 7437, 3053, 11780, 8020, 10202, 2309, 2730, 11136, 11728, 2952, 5255, 5515, 8137, 9460, 1373, 1321, 240, 8830, 7550, 11867, 11712, 10788, 1281, 6076, 6359, 8537, 3146, 8778, 8586, 10627, 8397, 2284, 2694, 10725, 7819, 2959, 1617, 6687, 12789, 4988, 10042, 11065, 7701, 2829, 7710, 10918, 10643, 5490, 2863, 2094, 5437, 2345, 6732, 6059, 10134, 11394, 2114, 1167, 10667, 9857, 572, 6102, 4683, 1534, 1257, 12713, 11216, 10988, 9112, 3754, 5814, 3122, 10335, 9881, 6809, 4316, 6051, 6148, 11030, 9721, 4919, 5486, 914, 10429, 3510, 7610, 9621, 4322, 8763, 5118, 12289, 6828, 671, 9257, 238, 3475, 7948, 9827, 2178, 3072, 2940, 9577, 144, 8964, 9034, 4662, 7421, 1236, 5835, 12725, 863, 7562, 485, 11537, 9719, 2587, 113, 11825, 8697, 9635, 3578, 10008, 11266, 11988, 4306, 6000, 3119, 8730, 7587, 6227, 6602, 7732, 7936, 7991, 339, 171, 2492, 7479, 10803, 3482, 10196, 8770, 2527, 3046, 8489, 8025, 9564, 1159, 2160, 6546, 8316, 7699, 10637, 542, 2526, 12171, 74, 1263, 9060, 11206, 360, 10120, 11051, 6323, 1575, 464, 4791, 8153, 2538, 6421, 1882, 12298, 5166, 6822, 156, 6197, 9530, 3342, 2479, 7127, 9323, 4341, 8944, 3841, 5434, 5978, 7388, 1499, 3533, 12262, 7990, 12609, 6255, 11806, 6333, 4392, 12812, 5886, 5579, 5533, 12531, 5415, 9135, 9387, 12135, 4735, 6259, 10604, 12453, 5466, 5659, 265, 10197, 9352, 5465, 8667, 2979, 6233, 10381, 96, 4152, 12370, 11247, 1577, 4180, 8706, 5994, 6652, 1352, 1496, 424, 3735, 10114, 11570, 10728, 7470, 6971, 759, 8600, 8887, 4329, 539, 5924, 7253, 7036, 12698, 6891, 2723, 7197, 7122, 12075, 12486, 1819, 4460, 11762, 4402, 12482, 8087, 1937, 9377, 3281, 9355, 9614, 6570, 876, 7297, 11636, 5809, 9784, 9039, 5968, 1450, 10993, 5146, 5234, 6023, 3489, 8122, 4881, 638, 12865, 12547, 6276, 2859, 11308, 11896, 5258, 10878, 8682, 11323, 908, 11771, 3372, 2201, 2115, 7992, 3634, 1276, 5882, 8969, 4993, 10253, 12915, 10527, 8855, 9123, 3778, 45, 12113, 2873, 3701, 3024, 5928, 7542, 8561, 10443, 5581, 1667, 2792, 5715, 8994, 3466, 2229, 9883, 11056, 5189, 10645, 11882, 10885, 1587, 4974, 1049, 7324, 5210, 2261, 6926, 2236, 6694, 9296, 1337, 8240, 12595, 2387, 836, 11135, 2369, 10367, 3029, 10346, 6175, 8243, 4595, 11928, 9412, 8724, 6781, 7885, 8711, 2995, 2151, 536, 10500, 3922, 4485, 5744, 3383, 12749, 10532, 5459, 12237, 9822, 11629, 3434, 4763, 9365, 2899, 9449, 7796, 9846, 2856, 1191, 7373, 2041, 11934, 7609, 5851, 11849, 10228, 5135, 11875, 11467, 2376, 10689, 10942, 12877, 11301, 6980, 6021, 1746, 11002, 10702, 3142, 4558, 7190, 393, 4468, 4202, 10468, 5483, 740, 7462, 6444, 9510, 11207, 2108, 10326, 11096, 10871, 11913, 11757, 2288, 7816, 11242, 9400, 4581, 11793, 3929, 10143, 3413, 1108, 5888, 12455, 1805, 9868, 11297, 2190, 12391, 8829, 1185, 7432, 10339, 236, 1259, 626, 7348, 3606, 12801, 9151, 7791, 5826, 9005, 3650, 12182, 362, 8581, 10075, 7040, 2561, 371, 6044, 9555, 11343, 7786, 3352, 9617, 1795, 8902, 1927, 10316, 9394, 9826, 3903, 3154, 10131, 4354, 7498, 4179, 2943, 5162, 6885, 8548, 6851, 2179, 6748, 9794, 9009, 6789, 5030, 1417, 12509, 2122, 9483, 7438, 12265, 6337, 7906, 3045, 2690, 12831, 10247, 2706, 3476, 4257, 4814, 5367, 7268, 11011, 4108, 10710, 693, 9170, 1712, 4736, 11978, 11358, 2704, 6164, 6753, 1063, 8426, 92, 11072, 10911, 2128, 5372, 9746, 2982, 245, 9709, 11893, 6360, 12548, 8809, 6445, 4085, 6406, 4020, 11520, 1656, 3953, 4079, 10373, 8522, 12400, 5441, 11373, 12057, 10715, 9782, 9333, 6113, 5010, 4303, 12479, 7625, 1234, 2983, 3212, 6449, 2187, 12776, 10479, 3379, 1551, 12230, 11103, 2070, 11675, 1375, 3804, 11737, 10662, 6331, 5831, 6964, 1906, 10826, 886, 9633, 7339, 12657, 1382, 11906, 11070, 10504, 10510, 1076, 12932, 10576, 3253, 6109, 12210, 9419, 513, 6013, 1856, 9779, 2134, 10680, 10275, 3285, 5738, 9554, 8206, 3351, 8238, 4659, 294, 164, 5680, 10013, 6134, 5076, 1142, 10035, 6954, 7656, 2141, 9343, 3023, 6544, 4805, 7424, 10445, 689, 6121, 6285, 6168, 4451, 7643, 4847, 1121, 9018, 5242, 5671, 6329, 3275, 11318, 6342, 1955, 5569, 5693, 2839, 840, 185, 7679, 5121, 10496, 498, 4697, 8736, 12677, 9171, 10267, 8821, 11152, 4210, 2392, 5317, 5703, 396, 12578, 12240, 12233, 7022, 10906, 4193, 12203, 8469, 11333, 11874, 4104, 6858, 12523, 8150, 7455, 2361, 8119, 1484, 2551, 12886, 7659, 8148, 12864, 4096, 2554, 1235, 11226, 2228, 134, 9277, 2537, 8729, 3913, 6509, 8154, 4559, 12553, 2556, 12930, 8355, 12782, 9682, 9185, 11381, 10333, 1561, 2341, 493, 3504, 2423, 3570, 9997, 9051, 8593, 8954, 3158, 10937, 12037, 2401, 6457, 10640, 2230, 960, 7381, 708, 4326, 6538, 3124, 3860, 2790, 9838, 1464, 6127, 1290, 5740, 4482, 10737, 7097, 10142, 3267, 3429, 10222, 6370, 4078, 6620, 7461, 11409, 8438, 11714, 2017, 3603, 1123, 8531, 11991, 9282, 2739, 3854, 11560, 12755, 10286, 4976, 4517, 10162, 12672, 5000, 783, 7201, 2188, 11328, 4624, 12026, 4406, 4607, 4071, 9480, 7593, 3808, 5422, 715, 4773, 12106, 2424, 125, 10742, 2155, 1943, 11746, 10388, 5903, 450, 5057, 2106, 2902, 1096, 4463, 1523, 8848, 10173, 9104, 9166, 2651, 4027, 7539, 7549, 11088, 2880, 10431, 510, 3873, 2622, 9429, 8014, 11482, 10920, 9318, 5508, 801, 5906, 6941, 10321, 5330, 11362, 2934, 10433, 3252, 9362, 216, 950, 7842, 2067, 3189, 5033, 2721, 4969, 2167, 9146, 2422, 3237, 8312, 9744, 9761, 10540, 9155, 1690, 1588, 1958, 2215, 7442, 3319, 7067, 12073, 11464, 2065, 4423, 8052, 11312, 332, 2865, 8759, 5651, 11703, 11237, 7110, 5287, 10033, 3870, 3781, 4758, 4883, 3207, 12823, 12212, 9803, 2172, 3726, 6813, 2671, 11894, 11582, 4584, 1313, 2082, 7872, 5476, 11279, 12004, 10765, 12433, 496, 7056, 8055, 2933, 11283, 8991, 10634, 549, 4183, 8596, 12052, 5682, 9574, 6179, 11400, 10875, 1634, 6913, 7752, 10851, 12445, 2720, 1926, 4506, 9533, 1206, 977, 4481, 6213, 8822, 5504, 2023, 3971, 11197, 10626, 2238, 10041, 12432, 4432, 3846, 3091, 3414, 3679, 11259, 794, 7982, 9227, 9774, 2635, 5061, 2293, 2302, 3918, 8864, 4467, 2869, 10870, 11257, 792, 9284, 5670, 5789, 10231, 9311, 1950, 12255, 1061, 88, 4435, 3301, 1308, 9174, 10687, 10929, 2615, 4940, 1508, 7545, 3942, 6305, 10427, 12588, 7713, 556, 12478, 446, 5575, 3905, 2914, 3542, 12487, 4870, 10806, 5769, 4738, 8001, 12512, 5017, 12807, 12061, 9140, 9099, 7495, 4382, 8640, 11178, 9648, 8636, 7535, 9877, 8323, 12361, 3468, 6477, 1737, 5852, 12088, 3375, 3426, 9167, 10211, 3537, 560, 8853, 3164, 6933, 11255, 11383, 1338, 2989, 6927, 8573, 4242, 12306, 135, 7897, 4477, 8311, 5253, 11939, 10965, 4310, 5338, 4856, 3195, 12310, 188, 9783, 9238, 10368, 9620, 7188, 128, 11540, 10448, 2033, 7877, 1318, 9225, 2805, 103, 7525, 4768, 7981, 7591, 8278, 12964, 8972, 5660, 6761, 10353, 8040, 1602, 10455, 6662, 4055, 1729, 5739, 244, 8523, 3675, 1187, 7955, 1681, 10888, 9190, 11702, 10508, 9314, 1079, 9243, 12323, 11102, 11439, 5762, 1970, 2163, 6265, 11881, 7215, 3605, 12396, 8061, 4344, 5417, 5226, 3219, 6972, 12558, 11371, 5456, 5247, 7174, 2596, 4433, 215, 5806, 3686, 7671, 2564, 2924, 7940, 2388, 3863, 12131, 12911, 12464, 5998, 8135, 12696, 7328, 12229, 3193, 9392, 1852, 69, 9409, 3250, 7152, 2239, 2961, 1778, 4511, 8631, 5089, 788, 4608, 8486, 12123, 7708, 6806, 10235, 10007, 9464, 776, 8453, 991, 9655, 7047, 11937, 7691, 3501, 8917, 9576, 10897, 11138, 4692, 12283, 5288, 1360, 769, 7584, 11269, 211, 9382, 4567, 6644, 8797, 4335, 31, 7804, 3707, 2460, 7353, 11338, 6110, 10171, 4571, 1388, 5211, 5270, 2785, 10191, 11916, 9478, 11191, 12667, 1993, 3332, 9237, 12192, 7850, 3999, 105, 6997, 4486, 3356, 12894, 3636, 8968, 12497, 2333, 62, 6101, 9544, 2111, 4212, 1928, 8339, 8963, 12176, 3719, 8031, 3361, 11968, 10659, 2477, 12054, 4493, 3825, 734, 4631, 5895, 8494, 9674, 1222, 12714, 9000, 9940, 3727, 9870, 3311, 5177, 10758, 8376, 8290, 11698, 6974, 793, 9095, 11653, 2604, 12515, 2654, 1837, 8974, 7650, 8381, 11156, 53, 7370, 3622, 12832, 7150, 12356, 3004, 9776, 2206, 11596, 10991, 12355, 9435, 3179, 11354, 3022, 7092, 9290, 631, 9017, 2079, 7741, 4951, 7740, 3495, 508, 10040, 9989, 6394, 1863, 4095, 4285, 7793, 4246, 1806, 12568, 11627, 5803, 9003, 6221, 6740, 9485, 231, 10992, 6277, 3213, 9920, 6062, 4851, 2972, 8879, 3738, 3198, 4610, 3592, 11228, 1825, 12446, 9215, 541, 8510, 4597, 6738, 5140, 7160, 7760, 4298, 9423, 12792, 3975, 5685, 8272, 8804, 3367, 5393, 4008, 8840, 6780, 10593, 10580, 699, 8141, 12785, 6759, 578, 4471, 3343, 1361, 7275, 6651, 3740, 1452, 584, 9161, 663, 12849, 7429, 6863, 1543, 12686, 5688, 7023, 666, 4293, 7714, 5449, 3674, 8642, 2777, 4383, 4521, 8580, 8765, 10406, 7888, 1166, 5561, 2166, 7920, 11112, 11342, 11496, 8810, 1930, 5099, 6763, 3425, 8514, 12152, 4177, 2769, 2419, 7243, 10281, 11425, 587, 7500, 10151, 6920, 8205, 10999, 12529, 9872, 10914, 6669, 10495, 10095, 944, 6208, 2081, 1233, 6617, 1304, 1297, 9910, 9570, 9675, 10389, 9180, 6010, 7696, 8193, 1807, 6388, 3235, 12164, 1599, 8364, 5362, 387, 9466, 2888, 213, 9228, 7267, 6624, 12414, 8863, 2270, 7028, 9855, 8161, 7617, 8888, 194, 5020, 5373, 4333, 846, 7450, 439, 7677, 2775, 6186, 5172, 5308, 12813, 5562, 7993, 4716, 4438, 9594, 9033, 9447, 4573, 1924, 9023, 1471, 5878, 2052, 1169, 8149, 10272, 4651, 12036, 8784, 1570, 7035, 6210, 2262, 760, 5641, 6120, 4455, 7133, 3359, 7995, 11199, 8532, 7727, 11479, 4522, 8723, 4159, 6483, 6063, 10488, 10236, 1765, 3862, 8780, 4771, 12137, 897, 1204, 12676, 2056, 4661, 3963, 717, 2336, 5892, 6348, 12638, 2426, 1744, 2647, 11559, 1074, 3455, 5471, 9221, 3168, 10020, 6405, 3430, 7293, 10886, 7725, 1458, 11870, 5859, 9441, 1519, 3529, 8721, 4418, 1418, 7220, 590, 12607, 1917, 3488, 2613, 11850, 1475, 7208, 6450, 435, 6889, 1103, 7213, 8265, 2870, 11223, 2542, 8086, 7280, 5414, 6217, 193, 2784, 7729, 11264, 10278, 3800, 2481, 54, 3565, 724, 7273, 6525, 7403, 8932, 10749, 2942, 12966, 1752, 2406, 7443, 10948, 8164, 3276, 4130, 5623, 8903, 7395, 12334, 2637, 6301, 5232, 8270, 11055, 5450, 8893, 1651, 3216, 2409, 7645, 11971, 2837, 9292, 6286, 4722, 1641, 1621, 12949, 5962, 1706, 9539, 3313, 6904, 874, 6396, 3021, 3618, 9956, 1277, 12043, 10717, 7925, 8023, 6453, 1900, 12581, 5164, 219, 10264, 9453, 4802, 455, 1700, 12748, 568, 10676, 436, 7954, 3945, 1237, 6248, 9938, 8143, 9304, 11549, 6386, 9187, 5009, 4819, 1605, 11320, 11926, 4256, 7494, 3882, 6520, 1727, 7812, 12940, 2415, 481, 10679, 11776, 6824, 3074, 2113, 9262, 2990, 4783, 5855, 5006, 1516, 10655, 7336, 72, 11847, 9468, 1126, 4109, 6446, 2698, 8873, 83, 11401, 7865, 153, 2702, 7975, 4642, 10465, 6648, 10681, 2850, 9936, 9320, 475, 10586, 3041, 3845, 12041, 5757, 4713, 3717, 1844, 8587, 3820, 2707, 2842, 8167, 4748, 12003, 3079, 11428, 5295, 4400, 1273, 9902, 12417, 5526, 4997, 408, 1148, 5955, 2745, 4706, 11109, 12256, 2951, 9766, 5343, 1743, 1095, 11841, 1686, 2935, 179, 341, 7472, 7294, 12740, 3287, 8418, 520, 4688, 12358, 2827, 1334, 3261, 3770, 10724, 10860, 10361, 12513, 2483, 1356, 5566, 9410, 3691, 1748, 11722, 6119, 12896, 4083, 1731, 232, 7600, 10735, 2416, 10810, 8173, 4639, 322, 12546, 420, 10463, 11943, 1018, 12409, 1527, 2000, 6480, 4549, 3993, 11164, 4617, 5818, 7937, 6215, 10178, 8865, 5055, 9945, 748, 7676, 12970, 12728, 2658, 2822, 7130, 4041, 7637, 11040, 8377, 2620, 5808, 9636, 457, 4099, 543, 5701, 2451, 12746, 4853, 12402, 10987, 9116, 986, 4288, 6198, 2298, 6580, 6251, 12413, 12941, 7795, 2595, 10331, 5644, 1205, 6608, 1675, 3924, 10071, 9241, 12770, 6057, 10049, 3607, 4307, 8454, 6925, 7425, 181, 2405, 6897, 12682, 1004, 609, 8620, 5468, 6506, 10805, 5642, 10358, 8750, 4592, 11485, 5019, 8716, 1504, 7662, 4855, 4629, 1550, 10751, 1130, 8479, 9049, 6660, 6667, 7251, 12437, 2643, 1311, 4907, 7618, 6369, 9521, 3377, 2054, 4446, 8392, 9407, 548, 6746, 12193, 4889, 6737, 2127, 8571, 6992, 8420, 12590, 9649, 10061, 8413, 1156, 741, 1885, 5087, 4760, 495, 2740, 11750, 7599, 6133, 3420, 1088, 6387, 12282, 243, 11942, 1791, 12727, 9247, 9515, 594, 7411, 9887, 5621, 2340, 3777, 10859, 4252, 7638, 9603, 9471, 8608, 12814, 6976, 8447, 4155, 4127, 10668, 8976, 3520, 9224, 1087, 7380, 0, 8330, 11830, 9645, 6981, 9087, 3548, 336, 7419, 9770, 10590, 6727, 12980, 7910, 2410, 4049, 11506, 6158, 5591, 2064, 343, 12617, 9941, 6685, 547, 9172, 12015, 9999, 1090, 11190, 1064, 904, 4224, 6831, 12946, 3217, 3306, 9498, 3908, 2393, 10934, 12392, 9894, 10811, 7287, 9030, 9118, 11515, 10708, 10675, 3647, 6011, 4668, 2680, 11495, 2682, 9843, 6610, 6200, 6026, 12634, 2373, 8886, 8623, 1515, 6731, 5350, 11346, 10044, 7845, 11031, 4992, 3105, 9068, 675, 4137, 10023, 3279, 6494, 6901, 6294, 2456, 11660, 11317, 8891, 9755, 9830, 11872, 1636, 515, 12307, 11225, 3481, 5112, 5271, 10392, 8649, 9711, 11492, 11306, 11840, 3917, 9637, 6077, 7660, 6443, 3083, 9439, 3348, 5375, 10144, 6020, 7694, 8798, 5047, 8882, 7043, 3887, 1227, 3427, 8980, 204, 264, 7530, 6537, 9393, 7048, 10646, 131, 11142, 10376, 11721, 10629, 1208, 3620, 6287, 6005, 2198, 11388, 6512, 7956, 10841, 5304, 145, 9076, 8458, 8973, 5839, 10088, 7453, 8779, 12155, 70, 3479, 4098, 9276, 10481, 3350, 10032, 11990, 8506, 8374, 8668, 11555, 5869, 8487, 1105, 9820, 8328, 3986, 11693, 9639, 4150, 50, 12273, 2984, 5586, 11101, 2673, 1824, 4199, 8139, 8123, 3595, 9958, 2285, 1215, 358, 11200, 7475, 12028, 11120, 3305, 6098, 11980, 1916, 11251, 1966, 9484, 5767, 1913, 8638, 1532, 5799, 6074, 10206, 8033, 3369, 9609, 4510, 12916, 5931, 66, 5834, 11945, 10100, 6194, 12819, 3571, 7007, 11509, 1555, 12866, 11767, 11124, 8511, 7158, 2511, 12608, 2072, 3858, 11522, 4026, 3075, 4660, 1383, 11898, 11992, 9650, 11462, 3550, 9561, 425, 1442, 11181, 12021, 2480, 7163, 992, 657, 6024, 3266, 12893, 6354, 12881, 2265, 9757, 4945, 8166, 6473, 1401, 5426, 218, 1381, 9728, 2813, 7021, 12128, 9050, 6606, 306, 9865, 6278, 5264, 6584, 7155, 11265, 9504, 7886, 3362, 10584, 5599, 4643, 12493, 4583, 8521, 7572, 872, 8095, 9884, 1741, 2665, 4028, 30, 3541, 1029, 6585, 10957, 3370, 9551, 10096, 4822, 3731, 12408, 2737, 11157, 12489, 1732, 957, 2718, 256, 5043, 2646, 9678, 3064, 4989, 12423, 8481, 3773, 10779, 4388, 2672, 9486, 6368, 1756, 12575, 8576, 3345, 6703, 1026, 9799, 10572, 2371, 2276, 8155, 7063, 12822, 4972, 9727, 3428, 4332, 8645, 10755, 8351, 2402, 7943, 9703, 4369, 7229, 5747, 6504, 3136, 11276, 1466, 7058, 12467, 2242, 8299, 5920, 12343, 11210, 2180, 2140, 7129, 9931, 2085, 7641, 9654, 3089, 7082, 12907, 12124, 7626, 7687, 3404, 7780, 4852, 9354, 6666, 4251, 5051, 8957, 1685, 12040, 1250, 3659, 1644, 5779, 10930, 9990, 12201, 5318, 5963, 4785, 233, 8834, 34, 12655, 8675, 11948, 10411, 6912, 4023, 7009, 3785, 544, 5509, 5520, 9965, 7688, 7447, 10186, 2758, 9913, 10781, 9344, 3040, 6316, 4128, 7602, 2801, 5713, 3555, 9024, 7738, 10462, 10977, 127, 5519, 3226, 1624, 4524, 6017, 4978, 5275, 7527, 7774, 9098, 2549, 4616, 11079, 12301, 10121, 2308, 796, 2807, 365, 40, 1040, 4241, 1600, 6722, 10046, 5452, 3540, 9462, 12120, 3534, 3642, 11638, 3767, 3471, 7135, 11361, 1122, 11920, 12816, 8664, 4002, 8391, 10014, 1973, 12136, 8288, 9671, 8919, 6281, 1151, 10817, 5858, 943, 1483, 11545, 11205, 10081, 8375, 8183, 4494, 5358, 3987, 11234, 2300, 1772, 5261, 8483, 9491, 8295, 1089, 3949, 3704, 4269, 6462, 9390, 5743, 5796, 5820, 6734, 4476, 10055, 8262, 9916, 12443, 17, 12435, 6275, 385, 8493, 6921, 5684, 7387, 8890, 4803, 1719, 12099, 5524, 2930, 186, 205, 7808, 5658, 2759, 8248, 12276, 3821, 378, 2776, 1316, 11625, 1454, 4390, 8456, 4161, 11630, 11779, 4650, 470, 11444, 9509, 3018, 10669, 9312, 11376, 6982, 11597, 2165, 11020, 6498, 1295, 1762, 6160, 2173, 7733, 4140, 459, 12601, 5629, 3584, 10632, 7566, 5110, 9706, 7553, 4379, 6246, 5729, 12549, 5538, 9879, 5527, 7029, 3268, 1470, 4913, 3117, 8107, 3654, 9642, 3751, 10759, 10246, 7409, 8215, 2290, 6801, 10179, 12044, 11856, 451, 2086, 10511, 11267, 10613, 10447, 12243, 9852, 8825, 11994, 5331, 12971, 7372, 12599, 12385, 4666, 1193, 10454, 2954, 11697, 4685, 6942, 8182, 11067, 10234, 5860, 2496, 11670, 106, 6459, 8931, 7772, 6125, 11897, 11557, 9702, 665, 12505, 11672, 1975, 7240, 9156, 6231, 8176, 1481, 4264, 10903, 1902, 11175, 6485, 10982, 10080, 12557, 1669, 2964, 3629, 2988, 1073, 4693, 3890, 12374, 11013, 3643, 6837, 441, 9873, 6083, 8333, 1147, 11440, 12788, 8617, 11839, 316, 8659, 12760, 4065, 2713, 3985, 4550, 12982, 9560, 2548, 5954, 941, 8800, 2414, 91, 3832, 7969, 7532, 3336, 6365, 10931, 6007, 150, 6039, 10426, 1391, 8159, 11892, 1750, 7556, 6423, 9790, 8108, 6327, 8124, 12198, 5990, 10812, 11273, 6456, 6161, 416, 5402, 7258, 6472, 7351, 5100, 3522, 3057, 9833, 8342, 11959, 924, 12585, 3645, 3712, 1152, 11487, 7857, 9236, 5222, 6468, 10539, 11380, 2709, 10212, 176, 3412, 6948, 7161, 9658, 12737, 7646, 9878, 11794, 9061, 10550, 3742, 12063, 12580, 8760, 7719, 12416, 10332, 6605, 10449, 8219, 4675, 1173, 4504, 1115, 7952, 10523, 7596, 1112, 6702, 5643, 7721, 6280, 1363, 11081, 2848, 839, 12277, 942, 6153, 1889, 4733, 4136, 1412, 5938, 8740, 555, 8271, 1327, 5069, 4930, 9119, 11246, 3543, 2525, 9477, 8168, 6870, 2980, 483, 7520, 7983, 9793, 1967, 12082, 10177, 2482, 11832, 10006, 7017, 5781, 773, 9052, 10199, 12403, 7961, 1888, 11463, 580, 921, 1392, 6018, 10440, 8975, 107, 182, 4896, 10380, 8279, 1652, 5712, 6879, 9519, 3988, 460, 1649, 1168, 4557, 10771, 11204, 2932, 11598, 7337, 7917, 7575, 8147, 12114, 6496, 533, 10074, 2565, 589, 6999, 1393, 3956, 8684, 364, 2517, 11367, 932, 5571, 5934, 6404, 1114, 3552, 3646, 1908, 9278, 9303, 7503, 7415, 523, 6508, 9124, 9342, 6297, 4648, 6279, 10307, 2633, 5272, 2182, 4317, 11415, 937, 8839, 11633, 12522, 10330, 9691, 2354, 12952, 3658, 9942, 11129, 2992, 12689, 10038, 7467, 10534, 10538, 6183, 6618, 2112, 1092, 4761, 7303, 330, 6320, 6726, 2096, 255, 11821, 2264, 11151, 2619, 4796, 10320, 11254, 3711, 4663, 1632, 12712, 619, 10450, 8803, 3930, 7314, 18, 6436, 3940, 3307, 9677, 8050, 8772, 8703, 4387, 7951, 6665, 7292, 2630, 10344, 3256, 6332, 11982, 6156, 260, 3327, 12624, 1240, 8739, 11531, 234, 1945, 10018, 114, 8728, 7465, 5496, 2185, 3598, 6038, 8114, 12625, 4414, 10345, 4110, 10548, 8495, 8117, 8324, 7365, 8672, 42, 5403, 6583, 6519, 4500, 12733, 9903, 10342, 1724, 11519, 1155, 10709, 869, 10733, 12156, 11357, 10119, 7000, 2571, 12241, 2648, 9014, 8335, 11501, 6534, 4900, 6465, 1307, 6664, 4680, 81, 726, 1780, 6486, 423, 7801, 6014, 3847, 6895, 7006, 6361, 742, 6916, 11457, 812, 11355, 7105, 10939, 9380, 12368, 7781, 8624, 10526, 3911, 11448, 3408, 5411, 6433, 11446, 10647, 2005, 10262, 1581, 3220, 3677, 6549, 9163, 12350, 1369, 11008, 9327, 12596, 10359, 2069, 2910, 6514, 8314, 7089, 271, 8338, 4312, 11221, 2833, 1619, 9422, 10102, 8113, 8639, 6139, 884, 11304, 6924, 12606, 2092, 5847, 9456, 11692, 11738, 8235, 12846, 2562, 8637, 12665, 3341, 4413, 4245, 8287, 7947, 12870, 8883, 768, 714, 6245, 2325, 8814, 7524, 12384, 3799, 1905, 10791, 8498, 3163, 4792, 11863, 43, 8000, 11851, 9326, 5451, 2377, 12174, 4543, 10357, 9130, 7026, 5423, 10807, 10203, 12219, 11908, 3944, 11567, 4122, 6990, 9475, 12685, 8337, 6947, 11330, 1180, 4762, 8144, 11911, 7826, 3498, 2918, 5273, 12908, 6111, 8812, 9395, 2520, 8325, 3463, 3496, 5014, 9808, 4184, 6048, 11677, 3423, 10022, 11802, 10026, 5669, 8875, 1131, 6400, 7168, 4560, 4501, 628, 2291, 12253, 12991, 3836, 3269, 5589, 5212, 3323, 2501, 12252, 6638, 622, 2102, 822, 10069, 5833, 10607, 1362, 913, 10274, 2184, 6757, 12499, 875, 8076, 643, 7742, 6985, 3891, 7552, 12291, 123, 11935, 10377, 2202, 11530, 7856, 4837, 12519, 4564, 9267, 1728, 9201, 8907, 4129, 10434, 9059, 7238, 7270, 6523, 12275, 5913, 11201, 11010, 2040, 5725, 7486, 10752, 2820, 604, 2791, 292, 11402, 12178, 3994, 2351, 12988, 552, 12612, 6482, 4097, 1751, 4100, 7434, 12405, 3702, 10347, 6871, 12309, 703, 4800, 1847, 1422, 4158, 10610, 2158, 12249, 6892, 4227, 7246, 4873, 5025, 11475, 718, 4538, 4478, 308, 4606, 6949, 7598, 228, 10898, 9121, 7065, 11985, 6458, 10677, 32, 7378, 12619, 8843, 7934, 4416, 8747, 12327, 5400, 9231, 11631, 2355, 168, 5626, 12976, 8520, 562, 2399, 7005, 10621, 1024, 2505, 8065, 2991, 7405, 7798, 2335, 5290, 10036, 11973, 1285, 10261, 10544, 8858, 12461, 809, 4061, 9739, 10848, 10919, 11351, 6639, 7070, 5097, 7758, 5911, 3814, 4410, 9027, 10685, 11233, 10385, 9452, 2589, 9919, 7493, 9769, 6410, 6565, 3393, 1249, 2466, 3722, 6500, 8859, 9193, 6611, 8616, 624, 5719, 4350, 468, 9388, 12183, 12153, 12439, 5981, 656, 10872, 10287, 2277, 5132, 6890, 1862, 12817, 9351, 10193, 11331, 12090, 5876, 2196, 242, 1841, 7589, 12690, 9625, 1576, 5822, 10391, 8901, 120, 7862, 9528, 10623, 235, 9428, 9568, 8106, 6195, 10299, 5949, 3042, 2164, 11260, 7841, 7889, 9192, 2493, 11891, 2049, 752, 6466, 4634, 4877, 11236, 9537, 9896, 3152, 8402, 11503, 4430, 7377, 5065, 3596, 6045, 3977, 11789, 10881, 2360, 10684, 6084, 12516, 9753, 11141, 4292, 8306, 6631, 11999, 9602, 2774, 12828, 6767, 3958, 2725, 6185, 2749, 12236, 10960, 4067, 6601, 7225, 8835, 9242, 4879, 2861, 5634, 6766, 501, 6528, 10620, 12836, 9317, 4275, 12650, 3484, 5202, 9562, 5266, 7839, 861, 3983, 6868, 12771, 565, 3857, 5095, 9204, 9361, 1959, 2385, 719, 430, 6803, 12947, 8395, 11539, 6907, 8102, 11180, 4655, 1306, 7142, 4874, 4809, 3196, 11438, 12145, 8922, 698, 4263, 837, 2712, 8526, 2895, 3231, 9708, 3151, 1161, 7340, 10663, 2553, 12934, 972, 7570, 5576, 10219, 8089, 8554, 11117, 9162, 11701, 7038, 12157, 3389, 10384, 138, 466, 11791, 5237, 1640, 1223, 302, 12753, 3705, 5584, 9189, 8226, 10487, 10764, 10789, 3663, 5594, 12662, 4614, 7485, 9589, 11422, 9208, 9168, 8175, 3566, 3899, 11253, 4867, 1541, 10832, 1974, 12447, 12080, 9008, 8946, 12885, 7985, 4925, 10882, 6116, 2729, 3027, 285, 5176, 9481, 8911, 1604, 5337, 6558, 10924, 606, 8844, 9653, 2771, 11958, 7236, 5800, 5353, 1003, 8786, 4860, 948, 174, 10995, 2485, 8519, 2411, 10902, 6340, 7531, 1645, 5027, 8459, 3492, 4601, 4965, 4160, 10423, 7139, 12611, 10889, 762, 5221, 2162, 12022, 6317, 392, 12302, 4262, 4017, 1671, 550, 3174, 5815, 1934, 3876, 1707, 3972, 4300, 6016, 4505, 7630, 3724, 8745, 2255, 9619, 12641, 10964, 4727, 6718, 8598, 5096, 9385, 797, 12850, 10109, 5028, 4428, 9465, 11901, 5655, 1944, 7282, 7852, 5315, 3995, 9788, 2609, 7108, 8227, 1172, 7059, 3758, 12820, 8130, 11217, 278, 8126, 3505, 5058, 4745, 2945, 2395, 10030, 2042, 11387, 855, 11314, 8614, 5503, 2768, 4888, 11822, 11635, 7159, 8466, 1229, 7128, 33, 8440, 1779, 9669, 12208, 1368, 4058, 8793, 11969, 12085, 12724, 6775, 7149, 7169, 9804, 5103, 6065, 1378, 7031, 1742, 8088, 12922, 212, 10210, 707, 11801, 149, 10351, 2464, 10969, 8396, 2080, 8693, 3668, 5720, 5816, 7412, 3690, 4086, 7627, 5518, 6349, 12742, 6548, 8191, 4090, 2075, 6681, 7717, 7891, 3955, 12104, 7946, 5950, 10738, 3703, 3055, 24, 12386, 10700, 1682, 6575, 12132, 3446, 5772, 12058, 11877, 5912, 11483, 10005, 3914, 2714, 1078, 3035, 8534, 2999, 8589, 5082, 845, 6937, 7915, 9684, 4018, 8515, 5776, 733, 10695, 537, 10782, 12533, 12912, 12538, 8234, 2855, 4704, 7567, 2986, 3978, 10757, 2618, 5182, 3812, 11706, 11162, 1567, 8766, 4044, 9127, 3604, 6377, 7216, 3358, 10836, 7104, 11173, 917, 8237, 8249, 5458, 10808, 4891, 5537, 11334, 4687, 12906, 158, 2675, 9615, 6293, 3320, 5419, 3086, 462, 11642, 5750, 7052, 5229, 10983, 9164, 2811, 6308, 10760, 2666, 9947, 11543, 7202, 11298, 2287, 5686, 10652, 10824, 2644, 55, 5766, 12925, 7506, 4512, 12007, 9488, 4235, 2929, 1820, 5972, 2319, 11604, 5015, 10418, 1065, 12711, 4277, 9541, 9906, 5365, 5303, 5428, 2960, 3997, 1243, 9773, 11441, 3432, 4417, 320, 12501, 12903, 10498, 9975, 1017, 11644, 3586, 8462, 2193, 7227, 5254, 945, 1610, 4069, 6742, 11788, 9681, 5498, 5654, 11977, 12263, 5473, 3177, 8861, 2139, 10122, 10386, 4126, 9096, 6928, 4338, 12887, 12704, 6607, 6653, 12042, 5408, 5084, 11774, 1784, 8794, 2582, 3382, 11245, 4009, 8949, 1407, 6478, 803, 1769, 1736, 6152, 22, 4135, 4060, 3794, 4072, 1145, 12579, 6524, 6815, 998, 7603, 7843, 1525, 10951, 1925, 7483, 592, 3442, 2304, 1790, 8384, 12586, 9831, 11534, 12141, 6944, 7349, 1299, 11514, 5650, 7356, 3912, 9998, 10198, 12332, 10699, 6254, 3020, 118, 2907, 3923, 2400, 11845, 7513, 8768, 8790, 4305, 10477, 2413, 11718, 947, 5072, 10775, 6709, 5364, 2681, 3695, 12984, 2058, 2380, 871, 6091, 1319, 4731, 3440, 1976, 9437, 5780, 3449, 1540, 12859, 1981, 1353, 10057, 5513, 9856, 8268, 6834, 12365, 2132, 6357, 2958, 4928, 7751, 6300, 1615, 816, 11654, 5842, 9302, 11028, 4949, 8430, 2364, 8162, 1275, 5578, 2594, 7112, 5005, 1802, 7113, 11241, 11575, 9507, 10521, 4211, 452, 6841, 10148, 11084, 4750, 2253, 3474, 1849, 1939, 3160, 2, 5600, 7579, 11729, 6710, 11208, 11286, 658, 7574, 1848, 10482, 7908, 7651, 7933, 3547, 10160, 2915, 11784, 11499, 8363, 1983, 11930, 12379, 10401, 8269, 7184, 9994, 8609, 6960, 12304, 5857, 1684, 10673, 407, 4308, 4337, 6364, 11524, 7728, 2088, 6594, 10656, 9136, 5652, 5260, 705, 2884, 577, 4395, 970, 4360, 640, 6434, 994, 473, 11947, 11996, 1485, 11449, 6550, 137, 3127, 3254, 10293, 5120, 8660, 9212, 10512, 11699, 2093, 4916, 8666, 10093, 11220, 9914, 3447, 7763, 2273, 603, 1310, 11364, 11820, 8083, 87, 11144, 3167, 3143, 5159, 12144, 3138, 10867, 4578, 11390, 1404, 4169, 7528, 9079, 12629, 7474, 6598, 2748, 2641, 11895, 2601, 5282, 11080, 7120, 7087, 4111, 11632, 10998, 5208, 389, 2823, 8068, 1999, 12868, 9771, 7144, 8334, 10907, 3681, 36, 4777, 10792, 11049, 5155, 9122, 1048, 10164, 10705, 1812, 6983, 8845, 6695, 6659, 9178, 774, 8070, 7730, 3326, 1326, 5007, 6552, 3796, 5050, 8878, 104, 10628, 3338, 4321, 614, 2686, 12764, 8331, 12101, 11486, 2212, 11842, 12769, 257, 4346, 3111, 65, 8905, 8535, 12583, 8555, 6747, 12346, 10537, 4208, 12296, 12944, 5314, 11423, 963, 9021, 3531, 3895, 11933, 9368, 2529, 1302, 3007, 7400, 5894, 3363, 5666, 10563, 7669, 1733, 4385, 9288, 6934, 10062, 891, 6505, 6393, 6140, 4132, 8691, 9035, 8443, 1995, 9863, 5608, 9266, 1840, 11244, 2757, 6441, 4786, 8653, 2251, 7154, 9729, 5168, 10227, 8673, 680, 1400, 6115, 2098, 1487, 12257, 9511, 6909, 787, 3303, 1711, 3104, 11430, 9253, 11571, 9183, 4938, 8298, 10696, 753, 12472, 12055, 5696, 799, 12066, 2194, 3381, 9497, 8251, 2435, 12640, 6427, 10153, 10382, 10944, 907, 7041, 12643, 5071, 4887, 7923, 9424, 2794, 5541, 1633, 11561, 2059, 1251, 3952, 10834, 2045, 12633, 9369, 5810, 814, 3206, 10209, 8934, 3656, 10711, 2258, 6476, 2947, 172, 7848, 7256, 7010, 6915, 1054, 11148, 10730, 11099, 11149, 11158, 1582, 9328, 9885, 7522, 5404, 11809, 9108, 1174, 11716, 5117, 8942, 5730, 766, 9558, 12339, 4088, 10308, 2138, 7283, 6678, 7821, 3692, 10963, 5849, 4452, 4219, 8632, 12314, 4702, 11833, 8302, 7777, 4539, 3556, 2695, 1776, 3990, 3960, 7134, 10712, 1986, 12860, 11502, 3288, 6888, 8622, 10015, 2846, 12927, 12422, 9971, 10989, 12679, 1158, 8680, 3058, 4480, 10470, 5233, 5901, 10021, 6378, 9335, 12074, 8679, 7712, 4730, 2195, 1075, 8940, 8038, 3031, 214, 8993, 2953, 11641, 5038, 3145, 1783, 3242, 9150, 5937, 1242, 5124, 12175, 2357, 191, 8289, 7678, 9960, 4890, 4696, 1821, 9147, 7873, 11406, 12942, 1846, 1473, 12535, 3730, 4434, 1903, 7962, 8098, 3135, 2944, 2274, 9767, 9638, 9473, 3011, 4810, 4254, 5217, 6407, 12779, 1794, 596, 11616, 11731, 413, 6931, 11919, 838, 4424, 2183, 9733, 11623, 4218, 9240, 2283, 3439, 1592, 936, 3720, 1628, 8926, 2970, 286, 4091, 8477, 2894, 5734, 9598, 6463, 1832, 5066, 12264, 1622, 7307, 7882, 4664, 7715, 3733, 4942, 8222, 8965, 7071, 3422, 6510, 4082, 11419, 9756, 3772, 6345, 4145, 10838, 3331, 10968, 7779, 453, 2495, 6299, 1406, 800, 2948, 4766, 4492, 8857, 1608, 8751, 12031, 5090, 2931, 9085, 9139, 1379, 2478, 8536, 7347, 11797, 7884, 1372, 4806, 7595, 7523, 7332, 7935, 8460, 8897, 1032, 140, 7249, 1283, 12338, 6689, 5975, 3503, 7345, 8385, 10883, 11664, 8578, 5094, 307, 60, 1219, 7502, 5438, 10205, 7302, 11865, 8276, 11108, 259, 6389, 5753, 10958, 5989, 4192, 9341, 7619, 12188, 7615, 12630, 8908, 2617, 3390, 8367, 10567, 11261, 7640, 934, 12206, 16, 11778, 3612, 11952, 2973, 251, 7046, 7286, 9415, 3192, 3400, 11711, 12094, 6037, 511, 5280, 841, 7736, 6597, 7568, 9540, 8984, 9209, 8369, 4405, 8221, 5102, 11646, 9398, 7298, 12299, 11458, 654, 984, 5883, 5756, 4753, 10415, 4569, 8133, 6306, 2544, 11984, 2150, 1954, 6418, 2890, 7014, 890, 12242, 7049, 5394, 1566, 2985, 3037, 5240, 1792, 3883, 3190, 5276, 10981, 6040, 6064, 8051, 6736, 5681, 6635, 2488, 4484]\n",
      "  Label attribute access failed or not applicable. Iterating through 10394 dataset items...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m FEATURE_NET_SAVE_PATH \u001b[38;5;241m=\u001b[39m CHECKPOINT_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_net_best_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m FEATURE_NET_SUBMISSION_PATH \u001b[38;5;241m=\u001b[39m SUBMISSION_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_net_submission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeseries_loader_tr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeseries_loader_val\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBCEWithLogitsLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Base criterion, will be adjusted internally\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_oversampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Enable oversampling\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Not needed if use_oversampling=True, or provide if you want to use when oversampling fails\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:188\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, patience, min_delta, grad_clip, class_weights, use_oversampling)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(original_train_dataset)):\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_train_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    189\u001b[0m         label_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sample, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sample) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:412\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/timeseries_eeg_dataset.py:411\u001b[0m, in \u001b[0;36mTimeseriesEEGDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processed_items_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 411\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cached_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[idx]\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/timeseries_eeg_dataset.py:439\u001b[0m, in \u001b[0;36mTimeseriesEEGDataset._get_cached_item\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    437\u001b[0m file_path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_file_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:1462\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1469\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1470\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:1964\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   1963\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1964\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1965\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1967\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/_weights_only_unpickler.py:458\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m TUPLE1[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],)\n\u001b[0;32m--> 458\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m TUPLE2[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])]\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m TUPLE3[\u001b[38;5;241m0\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "FEATURE_NET_SAVE_PATH = CHECKPOINT_ROOT / \"feature_net_best_model.pt\"\n",
    "FEATURE_NET_SUBMISSION_PATH = SUBMISSION_ROOT / \"feature_net_submission.csv\"\n",
    "\n",
    "train_history, val_history = train_model(\n",
    "        model, timeseries_loader_tr[\"feature\"], timeseries_loader_val[\"feature\"],\n",
    "        criterion, optimizer, device,\n",
    "        save_path=FEATURE_NET_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        scheduler=scheduler,\n",
    "        overwrite=True,\n",
    "        use_gnn=False,\n",
    "        use_oversampling=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot train and val loss\n",
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Applying oversampling to the training data...\n",
      "  Attribute-based label extraction not applicable or failed. Iterating through 10394 dataset samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Finished iterating. Collected 10394 labels.\n",
      "  Class counts before oversampling: [8375, 2019]\n",
      "  Successfully created an oversampled DataLoader.\n",
      "üí™ Starting training from epoch 1 to 300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|‚ñé                                                                                  | 1/300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m FEATURE_NET_SUBMISSION_PATH \u001b[38;5;241m=\u001b[39m SUBMISSION_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_net_submission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# train model on training set (or load existing)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeseries_loader_tr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeseries_loader_val\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFEATURE_NET_SAVE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_oversampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:295\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, use_oversampling)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch data must be a tuple (x, y) or have \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m attributes for non-GNN mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 295\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_targets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grad_clip \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/loss.py:821\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 821\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/functional.py:3638\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3636\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m-> 3638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()):\n\u001b[1;32m   3639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3640\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3641\u001b[0m     )\n\u001b[1;32m   3643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\n\u001b[1;32m   3644\u001b[0m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[1;32m   3645\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "FEATURE_NET_SAVE_PATH = CHECKPOINT_ROOT / \"feature_net_best_model.pt\"\n",
    "FEATURE_NET_SUBMISSION_PATH = SUBMISSION_ROOT / \"feature_net_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        model, timeseries_loader_tr[\"feature\"], timeseries_loader_val[\"feature\"],\n",
    "        criterion, optimizer, device,\n",
    "        save_path=FEATURE_NET_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        scheduler=scheduler,\n",
    "        overwrite=True,\n",
    "        use_gnn=False,\n",
    "        use_oversampling=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LSTM classifier (baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Signal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.lstm\n",
    "from src.layers.lstm import LSTM\n",
    "\n",
    "# build model with current parameters\n",
    "lstm_model = LSTM(input_dim=19,\n",
    "                hidden_dim=64,\n",
    "                num_layers=4,\n",
    "                dropout=0.3, input_type=\"signal\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    lstm_model = nn.DataParallel(lstm_model)\n",
    "lstm_model = lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Using WeightedRandomSampler for oversampling. BCEWithLogitsLoss will not use explicit pos_weight.\n",
      "üîç Applying oversampling to the training data...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Subset' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m LSTM_SUBMISSION_PATH \u001b[38;5;241m=\u001b[39m SUBMISSION_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlstm_signal_submission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# train model on training set (or load existing)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlstm_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeseries_loader_tr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msignal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeseries_loader_val\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msignal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBCEWithLogitsLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_oversampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:156\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, patience, min_delta, grad_clip, class_weights, use_oversampling)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîç Applying oversampling to the training data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    155\u001b[0m original_train_dataset \u001b[38;5;241m=\u001b[39m train_loader\u001b[38;5;241m.\u001b[39mdataset\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal train dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_train_dataset\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m all_labels_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Attempt to get labels directly from dataset attributes\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Subset' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"lstm_signal_best_model.pt\"\n",
    "LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"lstm_signal_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        model=lstm_model,\n",
    "        train_loader=timeseries_loader_tr[\"signal\"],\n",
    "        val_loader=timeseries_loader_val[\"signal\"],\n",
    "        criterion=nn.BCEWithLogitsLoss(),\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=100,\n",
    "        use_oversampling=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bidirectional LSTM with early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.lstm\n",
    "from src.layers.lstm import LSTM\n",
    "\n",
    "# build model with current parameters\n",
    "lstm_model = LSTM(input_dim=665,\n",
    "                hidden_dim=64,\n",
    "                num_layers=4,\n",
    "                dropout=0.1,\n",
    "                bidirectional=True)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    lstm_model = nn.DataParallel(lstm_model)\n",
    "lstm_model = lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Attempting to load checkpoint from .checkpoints/bi_lstm_feature_best_model.pt...\n",
      "   - Loading checkpoint from: .checkpoints/bi_lstm_feature_best_model.pt\n",
      "   - Detected full checkpoint dictionary.\n",
      "   - Optimizer state loaded from checkpoint.\n",
      "   - Model state successfully loaded.\n",
      " ‚úÖ Checkpoint loaded. Resuming from epoch 2. Best 'val_loss' score: 0.0000\n",
      "üí™ Starting training from epoch 2 to 300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  34%|‚ñé| 101/300 [00:52<01:46,  1.87it/s, train_loss=0.4942, val_loss=0.4450, best_val_loss=0.0000, lr=1.83e-08, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõë Early stopping: no 'val_loss' improvement in 100 epochs.\n",
      "\n",
      "‚úÖ Training complete.\n",
      "‚Ü©Ô∏è Loading best model state from .checkpoints/bi_lstm_feature_best_model.pt for return.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BI_LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"bi_lstm_feature_best_model.pt\"\n",
    "BI_LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"bi_lstm_feature_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        lstm_model, timeseries_loader_tr[\"feature\"], timeseries_loader_val[\"feature\"],\n",
    "        criterion, optimizer, device,\n",
    "        save_path=BI_LSTM_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        scheduler=scheduler,\n",
    "        overwrite=False,\n",
    "        use_gnn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"], \"BiLSTM [Features]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.utils.train\n",
    "from src.utils.train import evaluate_model\n",
    "# evaluate model on test set and generate submission file\n",
    "evaluate_model(\n",
    "    lstm_model, timeseries_loader_te[\"feature\"], device,\n",
    "    checkpoint_path=BI_LSTM_SAVE_PATH,\n",
    "    submission_path=BI_LSTM_SUBMISSION_PATH,\n",
    "    use_gnn=False,\n",
    "    input_type=\"feature\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features + Oversampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BI_LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"bi_lstm_feature_best_model.pt\"\n",
    "BI_LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"bi_lstm_feature_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "    lstm_model, timeseries_loader_tr[\"feature\"], timeseries_loader_val[\"feature\"],\n",
    "    criterion, optimizer, device,\n",
    "    save_path=BI_LSTM_SAVE_PATH,\n",
    "    num_epochs=300,\n",
    "    patience=10,\n",
    "    monitor=\"val_loss\",\n",
    "    scheduler=scheduler,\n",
    "    overwrite=False,\n",
    "    use_gnn=False,\n",
    "    use_oversampling=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LSTM with Attention and early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.lstm\n",
    "from src.layers.lstm import LSTMAttention\n",
    "\n",
    "# Create model and fit it\n",
    "attention_lstm_model = LSTMAttention(input_dim=665, hidden_dim=64, num_layers=3, dropout=0.2, input_type=\"feature\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    attention_lstm_model = nn.DataParallel(attention_lstm_model)\n",
    "attention_lstm_model = attention_lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(attention_lstm_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí™ Starting training from epoch 1 to 300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%| | 11/300 [00:06<03:00,  1.60it/s, train_loss=0.4969, val_loss=0.4447, best_val_f1=0.0000, lr=5.00e-04, bad"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõë Early stopping: no 'val_f1' improvement in 10 epochs.\n",
      "\n",
      "‚úÖ Training complete.\n",
      "‚Ü©Ô∏è Loading best model state from .checkpoints/attention_lstm_feature_best_model.pt for return.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ATTENTION_LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"attention_lstm_feature_best_model.pt\"\n",
    "ATTENTION_LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"attention_lstm_feature_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        attention_lstm_model, timeseries_loader_tr[\"feature\"], timeseries_loader_val[\"feature\"],\n",
    "        criterion, optimizer, device,\n",
    "        save_path=ATTENTION_LSTM_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        scheduler=scheduler,\n",
    "        overwrite=False,\n",
    "        use_gnn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"], \"LSTM with Attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set and generate submission file\n",
    "evaluate_model(\n",
    "    attention_lstm_model, test_loader, device,\n",
    "    checkpoint_path=ATTENTION_LSTM_SAVE_PATH,\n",
    "    submission_path=ATTENTION_LSTM_SUBMISSION_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Signal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.lstm\n",
    "from src.layers.lstm import LSTMAttention\n",
    "\n",
    "# Create model and fit it\n",
    "# For signals [batch_size, 19, 3000] (processing each of the 19 sensors as a sequence of (3000,1) ):\n",
    "attention_lstm_model = LSTMAttention(input_dim=1, hidden_dim=64, num_layers=3, dropout=0.2, input_type=\"feature\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    attention_lstm_model = nn.DataParallel(attention_lstm_model)\n",
    "attention_lstm_model = attention_lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(attention_lstm_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. EEG CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.cnn\n",
    "from src.layers.cnn import EEG_CNN\n",
    "\n",
    "eeg_cnn_model = EEG_CNN(\n",
    "    input_channels=19,\n",
    "    dropout=0.3\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    eeg_cnn_model = nn.DataParallel(eeg_cnn_model)\n",
    "eeg_cnn_model = eeg_cnn_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(eeg_cnn_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    patience=5,\n",
    "    factor=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m EEG_CNN_SUBMISSION_PATH \u001b[38;5;241m=\u001b[39m SUBMISSION_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meeg_cnn_submission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# train model on training set (or load existing)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m train_history, val_history \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[0;32m----> 6\u001b[0m         eeg_cnn_model, \u001b[43mtrain_loader\u001b[49m, val_loader,\n\u001b[1;32m      7\u001b[0m         criterion, optimizer, device,\n\u001b[1;32m      8\u001b[0m         save_path\u001b[38;5;241m=\u001b[39mEEG_CNN_SUBMISSION_PATH,\n\u001b[1;32m      9\u001b[0m         num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n\u001b[1;32m     10\u001b[0m         patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     11\u001b[0m         monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m         scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[1;32m     13\u001b[0m         overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m         use_gnn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "EEG_CNN_SAVE_PATH = CHECKPOINT_ROOT / \"eeg_cnn_best_model.pt\"\n",
    "EEG_CNN_SUBMISSION_PATH = SUBMISSION_ROOT / \"eeg_cnn_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        eeg_cnn_model, train_loader, val_loader,\n",
    "        criterion, optimizer, device,\n",
    "        save_path=EEG_CNN_SUBMISSION_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        monitor=\"val_loss\",\n",
    "        scheduler=scheduler,\n",
    "        overwrite=True,\n",
    "        use_gnn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"], \"EEG CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.utils.train\n",
    "from src.utils.train import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set and generate submission file\n",
    "evaluate_model(\n",
    "    eeg_cnn_model, test_loader, device,\n",
    "    checkpoint_path=EEG_CNN_SAVE_PATH,\n",
    "    submission_path=EEG_CNN_SUBMISSION_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. EEG CNN LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.cnn\n",
    "from src.layers.cnn import EEG_CNN_LSTM\n",
    "\n",
    "cnn_lstm_model = EEG_CNN_LSTM(\n",
    "    input_channels=19,\n",
    "    cnn_output_channels=128,\n",
    "    lstm_hidden_dim=128, \n",
    "    fc_dropout=0.3,\n",
    "    lstm_dropout=0.3,\n",
    "    num_classes=1,\n",
    "    bidirectional_lstm=False # unidirectional\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    cnn_lstm_model = nn.DataParallel(cnn_lstm_model)\n",
    "cnn_lstm_model = cnn_lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(cnn_lstm_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    patience=5,\n",
    "    factor=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"cnn_lstm_best_model.pt\"\n",
    "CNN_LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"cnn_lstm_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        cnn_lstm_model, train_loader, val_loader,\n",
    "        criterion, optimizer, device,\n",
    "        save_path=CNN_LSTM_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=30,\n",
    "        monitor=\"val_loss\",\n",
    "        scheduler=scheduler,\n",
    "        overwrite=True,\n",
    "        use_gnn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"], \"EEG CNN + LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set and generate submission file\n",
    "evaluate_model(\n",
    "    cnn_lstm_model, test_loader, device,\n",
    "    save_path=CNN_LSTM_SAVE_PATH,\n",
    "    submission_path=CNN_LSTM_SUBMISSION_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. EEG CNN BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport layers.cnn\n",
    "from layers.cnn import EEG_CNN_LSTM\n",
    "\n",
    "cnn_bi_lstm_model = EEG_CNN_LSTM(\n",
    "    input_channels=19,\n",
    "    cnn_output_channels=128,\n",
    "    lstm_hidden_dim=128, \n",
    "    fc_dropout=0.3,\n",
    "    lstm_dropout=0.3,\n",
    "    num_classes=1,\n",
    "    bidirectional_lstm=True\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    cnn_bi_lstm_model = nn.DataParallel(cnn_bi_lstm_model)\n",
    "cnn_bi_lstm_model = cnn_bi_lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(cnn_bi_lstm_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    patience=5,\n",
    "    factor=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_BI_LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bi_lstm_best_model.pt\"\n",
    "CNN_BI_LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"cnn_bi_lstm_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "    cnn_bi_lstm_model, train_loader, val_loader,\n",
    "    criterion, optimizer, device,\n",
    "    save_path=CNN_BI_LSTM_SAVE_PATH,\n",
    "    num_epochs=300,\n",
    "    patience=30,\n",
    "    monitor=\"val_loss\",\n",
    "    scheduler=scheduler,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"], \"EEG CNN + BiLSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set and generate submission file\n",
    "evaluate_model(\n",
    "    cnn_bi_lstm_model, test_loader, device,\n",
    "    save_path=CNN_BI_LSTM_SAVE_PATH,\n",
    "    submission_path=CNN_BI_LSTM_SUBMISSION_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.eeggcn import EEGGCN\n",
    "\n",
    "# Get number of time points (in_channels) from first sample\n",
    "eeg_gnc_model = EEGGCN(\n",
    "    in_channels=19,\n",
    "    hidden_channels=128,\n",
    "    out_channels=32,\n",
    "    num_classes=2,\n",
    "    num_conv_layers=3,\n",
    "    dropout=0.5\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    eeg_gnc_model = nn.DataParallel(eeg_gnc_model)\n",
    "eeg_gnc_model = eeg_gnc_model.to(device)\n",
    "\n",
    "# Set up optimizer + scheduler\n",
    "optimizer = optim.Adam(eeg_gnc_model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "EEGGCN_SAVE_PATH = CHECKPOINT_ROOT / \"eeggcn_best_model.pt\"\n",
    "EEGCN_SUBMISSION_PATH = SUBMISSION_ROOT / \"eeggcn_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "    eeg_gnc_model, train_loader, val_loader,\n",
    "    criterion, optimizer, device,\n",
    "    save_path=EEGGCN_SAVE_PATH,\n",
    "    num_epochs=300,\n",
    "    patience=30,\n",
    "    monitor=\"val_loss\",\n",
    "    scheduler=scheduler,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# plot losses if any\n",
    "plt.figure()\n",
    "plt.plot(train_history[\"loss\"], label=\"train\")\n",
    "plt.plot(val_history[\"loss\"],   label=\"val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\n",
    "    eeg_gnc_model, test_loader, device,\n",
    "    save_path       = Path(\"eeggcn_attn.pt\"),\n",
    "    submission_path = Path(\"eeggcn_submission.csv\"),\n",
    "    threshold       = 0.5,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
