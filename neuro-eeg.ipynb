{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroGraphNet\n",
    "\n",
    "*A graph-based deep learning framework for EEG seizure detection, designed to improve accuracy and interpretability by leveraging Graph Neural Networks (GNNs) to capture spatial and temporal brain dynamics.*\n",
    "\n",
    "<hr />\n",
    "\n",
    "This notebook presents **NeuroGraphNet**, a model that applies Graph Neural Networks to EEG data for seizure detection. The primary goal is to **compare the performance and interpretability of graph-based methods versus traditional deep learning approaches**. Through this comparison, we aim to demonstrate the advantages of incorporating brain connectivity information into the learning process.\n",
    "\n",
    "**Authors**: Luca Di Bello, Guillaume Andr√© B√©lissent, Abdessalem Ben Ali, Beatriz Izquierdo Gonz√°lez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from src.utils.seeder import seed_everything\n",
    "\n",
    "# set seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# create useful constants\n",
    "RANDOM_SEED = 42\n",
    "IS_SCITAS = False # set to True if running on SCITAS cluster\n",
    "LOCAL_DATA_ROOT = Path(\"./data\")\n",
    "DATA_ROOT = Path(\"/home/ogut/data\") if IS_SCITAS else LOCAL_DATA_ROOT\n",
    "CHECKPOINT_ROOT = Path(\"./.checkpoints\")\n",
    "SUBMISSION_ROOT = Path(\"./.submissions\")\n",
    "\n",
    "# create directories if they do not exist\n",
    "CHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# set dataset root\n",
    "seed_everything(RANDOM_SEED)\n",
    "\n",
    "# setup torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading EEG segment tables‚Ä¶\n",
      "‚úîÔ∏è Loaded: 12993 train rows, 3614 test rows (took 0.1s)\n"
     ]
    }
   ],
   "source": [
    "from src.utils.signal import time_filtering, normalize\n",
    "%aimport src.utils.signal\n",
    "from src.utils.index import ensure_eeg_multiindex\n",
    "\n",
    "start = time.time()\n",
    "print(\"‚è≥ Loading EEG segment tables‚Ä¶\")\n",
    "clips_tr = pd.read_parquet(DATA_ROOT / \"train\" / \"segments.parquet\").dropna()\n",
    "clips_te = pd.read_parquet(DATA_ROOT / \"test\" / \"segments.parquet\").dropna()\n",
    "\n",
    "# load clips with label\n",
    "clips_tr = ensure_eeg_multiindex(clips_tr, id_col_name='id')\n",
    "clips_te = ensure_eeg_multiindex(clips_te, id_col_name='id')\n",
    "\n",
    "print(f\"‚úîÔ∏è Loaded: {len(clips_tr)} train rows, {len(clips_te)} test rows \"\n",
    "      f\"(took {time.time()-start:.1f}s)\")\n",
    "# NOTE: Merge clips for sanity checks\n",
    "clips = pd.concat([clips_tr, clips_te]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train extracted features shape: (12993, 228)\n",
      "Test extracted features shape: (3614, 228)\n"
     ]
    }
   ],
   "source": [
    "# print feature shapes\n",
    "X_train = np.load(LOCAL_DATA_ROOT / \"extracted_features\" / \"X_train.npy\", allow_pickle=True)\n",
    "X_test = np.load(LOCAL_DATA_ROOT / \"extracted_features\" / \"X_test.npy\", allow_pickle=True)\n",
    "y_train = np.load(LOCAL_DATA_ROOT / \"labels\" / \"y_train.npy\", allow_pickle=True)\n",
    "sample_subject_array = np.load(LOCAL_DATA_ROOT / \"extracted_features\" / \"sample_subject_array_train.npy\",allow_pickle=True)\n",
    "\n",
    "# sanity checks to ensure validity of the data\n",
    "assert X_train.shape[0]  == y_train.shape[0], \"Mismatch in number of training samples and labels\"\n",
    "assert X_train.shape[1] == X_test.shape[1], \"Mismatch in number of features between train and test sets\"\n",
    "assert clips_tr.shape[0] == y_train.shape[0], \"Mismatch in number of training samples and segments\"\n",
    "assert X_train.shape[0] == sample_subject_array.shape[0], \"Mismatch in number of training samples and subjects\"\n",
    "assert clips_tr.shape[0] == sample_subject_array.shape[0], \"Mismatch in number of training segments and subjects\"\n",
    "\n",
    "print(\"Train extracted features shape:\", X_train.shape)\n",
    "print(\"Test extracted features shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- Extracted features -----------------------------------------------#\n",
    "channels = ['FP1', 'FP2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2', 'F7', 'F8', 'T3', 'T4', 'T5', 'T6', 'FZ', 'CZ', 'PZ']\n",
    "features = [\n",
    "    \"rms\", \"linelen\", \"hj_mob\", \"hj_cmp\", \"spec_ent\",\n",
    "    \"alpha_pow\", \"beta_pow\", \"theta_pow\", \"gamma_pow\",\n",
    "    \"rel_alpha\", \"rel_theta\", \"theta_alpha_ratio\"\n",
    "]\n",
    "n_features = len(features)\n",
    "n_channels = len(channels)\n",
    "\n",
    "feature_names = [f\"{ch} - {ft}\" for ch in channels for ft in features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings (WORK IN PROGRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating timeseries dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing TimeseriesEEGDataset in SIGNAL mode.\n",
      "   - Sampling rate: 250 Hz\n",
      "   - Derived segment length: 3000 timesteps.\n",
      "   - Segment length: 3000 timesteps.\n",
      "   ‚úÖ Using existing cached data from data/timeseries_dataset_train_signal/processed\n",
      "üèÅ TimeseriesEEGDataset initialization complete. Loaded 12993 samples.\n",
      "üöÄ Initializing TimeseriesEEGDataset in FEATURE mode.\n",
      "   ‚úÖ Using existing cached data from data/timeseries_dataset_train_features/processed\n",
      "üèÅ TimeseriesEEGDataset initialization complete. Loaded 12993 samples.\n",
      "üöÄ Initializing TimeseriesEEGDataset in SIGNAL mode.\n",
      "   - Sampling rate: 250 Hz\n",
      "   - Derived segment length: 3000 timesteps.\n",
      "   - Segment length: 3000 timesteps.\n",
      "   ‚ö†Ô∏è Info: Column 'label' not found in clips_df. Processing without labels (e.g., test set).\n",
      "   ‚úÖ Using existing cached data from data/timeseries_dataset_test_signal/processed\n",
      "üèÅ TimeseriesEEGDataset initialization complete. Loaded 3614 samples.\n",
      "üöÄ Initializing TimeseriesEEGDataset in FEATURE mode.\n",
      "   ‚ö†Ô∏è Info: Column 'label' not found in clips_df. Processing without labels (e.g., test set).\n",
      "   ‚úÖ Using existing cached data from data/timeseries_dataset_test_features/processed\n",
      "üèÅ TimeseriesEEGDataset initialization complete. Loaded 3614 samples.\n"
     ]
    }
   ],
   "source": [
    "%aimport src.utils.timeseries_eeg_dataset\n",
    "from src.utils.timeseries_eeg_dataset import TimeseriesEEGDataset\n",
    "\n",
    "LOCAL_DATA_ROOT = Path(\"./data\")\n",
    "\n",
    "timeseries_datasets_tr = {\n",
    "    \"signal\": TimeseriesEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_signal\"),\n",
    "        signal_folder=str(DATA_ROOT / 'train'),\n",
    "        clips_df=clips_tr,\n",
    "        mode='signal',\n",
    "    ),\n",
    "    \"feature\": TimeseriesEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_features\"),\n",
    "        signal_folder=str(DATA_ROOT / 'train'),\n",
    "        clips_df=clips_tr,\n",
    "        mode='feature',\n",
    "        feature_file_path=str(LOCAL_DATA_ROOT / \"extracted_features\" / \"X_train.npy\"),\n",
    "    ),\n",
    "    # FIXME: Uncomment the embedding dataset as soon as the embedding file is available\n",
    "    # \"embedding\": TimeseriesEEGDataset(\n",
    "    #     root=str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_embedding\"),\n",
    "    #     signal_folder=str(DATA_ROOT / 'train'),\n",
    "    #     clips_df=clips_tr,\n",
    "    #     mode='embedding',\n",
    "    #     embedding_file_path=str(LOCAL_DATA_ROOT / \"embeddings\" / \"X_train_embedding.npy\"),\n",
    "    #     labels_for_embedding_file_path=str(LOCAL_DATA_ROOT / \"labels\" / \"y_train.npy\")\n",
    "    # ),\n",
    "}\n",
    "\n",
    "timeseries_datasets_te = {\n",
    "    \"signal\": TimeseriesEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_signal\"),\n",
    "        signal_folder=str(DATA_ROOT / 'test'),\n",
    "        clips_df=clips_te,\n",
    "        mode='signal',\n",
    "    ),\n",
    "    \"feature\": TimeseriesEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_features\"),\n",
    "        signal_folder=str(DATA_ROOT / 'test'),\n",
    "        clips_df=clips_te,\n",
    "        mode='feature',\n",
    "        feature_file_path=str(LOCAL_DATA_ROOT / \"extracted_features\" / \"X_test.npy\"),\n",
    "    ),\n",
    "    # FIXME: Uncomment the embedding dataset as soon as the embedding file is available\n",
    "    # \"embedding\": TimeseriesEEGDataset(\n",
    "    #     root=str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_embedding\"),\n",
    "    #     signal_folder=str(DATA_ROOT / 'test'),\n",
    "    #     clips_df=clips_te,\n",
    "    #     mode='embedding',\n",
    "    #     embedding_file_path=str(LOCAL_DATA_ROOT / \"embeddings\" / \"X_test_embedding.npy\"),\n",
    "    #     labels_for_embedding_file_path=str(LOCAL_DATA_ROOT / \"labels\" / \"y_test.npy\")\n",
    "    # ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing GraphEEGDataset in FEATURE mode.\n",
      "   - Root: data/graph_dataset_train\n",
      "   - Mode: FEATURE\n",
      "   - Edge strategy: full\n",
      "   - Node Feature Normalization: True\n",
      "‚öôÔ∏è process() called for FEATURE mode. Target processed directory: data/graph_dataset_train/processed\n",
      "   - Starting processing from pre-extracted features...\n",
      "   - Total feature sets to process: 12993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Processing feature item 1299/12993...\n",
      "     - Processing feature item 2598/12993...\n",
      "     - Processing feature item 3897/12993...\n",
      "     - Processing feature item 5196/12993...\n",
      "     - Processing feature item 6495/12993...\n",
      "     - Processing feature item 7794/12993...\n",
      "     - Processing feature item 9093/12993...\n",
      "     - Processing feature item 10392/12993...\n",
      "     - Processing feature item 11691/12993...\n",
      "     - Processing feature item 12990/12993...\n",
      "     - Processing feature item 12993/12993...\n",
      "   ‚úÖ Processed and saved 12993 items from features.\n",
      "üèÅ process() finished. Total items processed and saved in this run for feature mode: 12993\n",
      "   - Found 12993 existing processed files for feature mode.\n",
      "üèÅ GraphEEGDataset initialization complete. Current mode: FEATURE. Known processed items: 12993\n",
      "üöÄ Initializing GraphEEGDataset in FEATURE mode.\n",
      "   - Root: data/graph_dataset_train\n",
      "   - Mode: FEATURE\n",
      "   - Edge strategy: spatial\n",
      "   - Node Feature Normalization: True\n",
      "   - Loading spatial distances...\n",
      "     - Loaded 180 unique spatial distances relevant to defined channels.\n",
      "‚öôÔ∏è process() called for FEATURE mode. Target processed directory: data/graph_dataset_train/processed\n",
      "   - Starting processing from pre-extracted features...\n",
      "   - Total feature sets to process: 12993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Processing feature item 1299/12993...\n",
      "     - Processing feature item 2598/12993...\n",
      "     - Processing feature item 3897/12993...\n",
      "     - Processing feature item 5196/12993...\n",
      "     - Processing feature item 6495/12993...\n",
      "     - Processing feature item 7794/12993...\n",
      "     - Processing feature item 9093/12993...\n",
      "     - Processing feature item 10392/12993...\n",
      "     - Processing feature item 11691/12993...\n",
      "     - Processing feature item 12990/12993...\n",
      "     - Processing feature item 12993/12993...\n",
      "   ‚úÖ Processed and saved 12993 items from features.\n",
      "üèÅ process() finished. Total items processed and saved in this run for feature mode: 12993\n",
      "   - Found 12993 existing processed files for feature mode.\n",
      "üèÅ GraphEEGDataset initialization complete. Current mode: FEATURE. Known processed items: 12993\n",
      "üöÄ Initializing GraphEEGDataset in FEATURE mode.\n",
      "   - Root: data/graph_dataset_train\n",
      "   - Mode: FEATURE\n",
      "   - Edge strategy: spatial\n",
      "   - Node Feature Normalization: True\n",
      "   - Loading spatial distances...\n",
      "     - Loaded 180 unique spatial distances relevant to defined channels.\n",
      "‚öôÔ∏è process() called for FEATURE mode. Target processed directory: data/graph_dataset_train/processed\n",
      "   - Starting processing from pre-extracted features...\n",
      "   - Total feature sets to process: 12993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Processing feature item 1299/12993...\n",
      "     - Processing feature item 2598/12993...\n",
      "     - Processing feature item 3897/12993...\n",
      "     - Processing feature item 5196/12993...\n",
      "     - Processing feature item 6495/12993...\n",
      "     - Processing feature item 7794/12993...\n",
      "     - Processing feature item 9093/12993...\n",
      "     - Processing feature item 10392/12993...\n",
      "     - Processing feature item 11691/12993...\n",
      "     - Processing feature item 12990/12993...\n",
      "     - Processing feature item 12993/12993...\n",
      "   ‚úÖ Processed and saved 12993 items from features.\n",
      "üèÅ process() finished. Total items processed and saved in this run for feature mode: 12993\n",
      "   - Found 12993 existing processed files for feature mode.\n",
      "üèÅ GraphEEGDataset initialization complete. Current mode: FEATURE. Known processed items: 12993\n",
      "üöÄ Initializing GraphEEGDataset in FEATURE mode.\n",
      "   - Root: data/graph_dataset_test\n",
      "   - Mode: FEATURE\n",
      "   - Edge strategy: full\n",
      "   - Node Feature Normalization: True\n",
      "‚öôÔ∏è process() called for FEATURE mode. Target processed directory: data/graph_dataset_test/processed\n",
      "   - Starting processing from pre-extracted features...\n",
      "   - Total feature sets to process: 3614\n",
      "   ‚ö†Ô∏è Warning: Labels for features will be None or use a default. 'label' column not in clips_df or length mismatch (3614 vs 3614).\n",
      "     - Processing feature item 361/3614...\n",
      "     - Processing feature item 722/3614...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Processing feature item 1083/3614...\n",
      "     - Processing feature item 1444/3614...\n",
      "     - Processing feature item 1805/3614...\n",
      "     - Processing feature item 2166/3614...\n",
      "     - Processing feature item 2527/3614...\n",
      "     - Processing feature item 2888/3614...\n",
      "     - Processing feature item 3249/3614...\n",
      "     - Processing feature item 3610/3614...\n",
      "     - Processing feature item 3614/3614...\n",
      "   ‚úÖ Processed and saved 3614 items from features.\n",
      "üèÅ process() finished. Total items processed and saved in this run for feature mode: 3614\n",
      "   - Found 3614 existing processed files for feature mode.\n",
      "üèÅ GraphEEGDataset initialization complete. Current mode: FEATURE. Known processed items: 3614\n",
      "üöÄ Initializing GraphEEGDataset in FEATURE mode.\n",
      "   - Root: data/graph_dataset_test\n",
      "   - Mode: FEATURE\n",
      "   - Edge strategy: spatial\n",
      "   - Node Feature Normalization: True\n",
      "   - Loading spatial distances...\n",
      "     - Loaded 180 unique spatial distances relevant to defined channels.\n",
      "‚öôÔ∏è process() called for FEATURE mode. Target processed directory: data/graph_dataset_test/processed\n",
      "   - Starting processing from pre-extracted features...\n",
      "   - Total feature sets to process: 3614\n",
      "   ‚ö†Ô∏è Warning: Labels for features will be None or use a default. 'label' column not in clips_df or length mismatch (3614 vs 3614).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Processing feature item 361/3614...\n",
      "     - Processing feature item 722/3614...\n",
      "     - Processing feature item 1083/3614...\n",
      "     - Processing feature item 1444/3614...\n",
      "     - Processing feature item 1805/3614...\n",
      "     - Processing feature item 2166/3614...\n",
      "     - Processing feature item 2527/3614...\n",
      "     - Processing feature item 2888/3614...\n",
      "     - Processing feature item 3249/3614...\n",
      "     - Processing feature item 3610/3614...\n",
      "     - Processing feature item 3614/3614...\n",
      "   ‚úÖ Processed and saved 3614 items from features.\n",
      "üèÅ process() finished. Total items processed and saved in this run for feature mode: 3614\n",
      "   - Found 3614 existing processed files for feature mode.\n",
      "üèÅ GraphEEGDataset initialization complete. Current mode: FEATURE. Known processed items: 3614\n",
      "üöÄ Initializing GraphEEGDataset in FEATURE mode.\n",
      "   - Root: data/graph_dataset_test\n",
      "   - Mode: FEATURE\n",
      "   - Edge strategy: correlation\n",
      "   - Node Feature Normalization: True\n",
      "‚öôÔ∏è process() called for FEATURE mode. Target processed directory: data/graph_dataset_test/processed\n",
      "   - Starting processing from pre-extracted features...\n",
      "   - Total feature sets to process: 3614\n",
      "   ‚ö†Ô∏è Warning: Labels for features will be None or use a default. 'label' column not in clips_df or length mismatch (3614 vs 3614).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n",
      "/Users/lucadibello/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/numpy/lib/_function_base_impl.py:3045: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Processing feature item 361/3614...\n",
      "     - Processing feature item 722/3614...\n",
      "     - Processing feature item 1083/3614...\n",
      "     - Processing feature item 1444/3614...\n",
      "     - Processing feature item 1805/3614...\n",
      "     - Processing feature item 2166/3614...\n",
      "     - Processing feature item 2527/3614...\n",
      "     - Processing feature item 2888/3614...\n",
      "     - Processing feature item 3249/3614...\n",
      "     - Processing feature item 3610/3614...\n",
      "     - Processing feature item 3614/3614...\n",
      "   ‚úÖ Processed and saved 3614 items from features.\n",
      "üèÅ process() finished. Total items processed and saved in this run for feature mode: 3614\n",
      "   - Found 3614 existing processed files for feature mode.\n",
      "üèÅ GraphEEGDataset initialization complete. Current mode: FEATURE. Known processed items: 3614\n",
      "Graph training datasets created:\n",
      "  full: GraphEEGDataset(12993) (length: 12993)\n",
      "     INFO: Attempting torch.load with weights_only=False for data/graph_dataset_train/processed/data_feat_0.pt\n",
      "    First training sample: Data(x=[19, 12], edge_index=[2, 342], y=[1], original_idx=[1])\n",
      "  spatial: GraphEEGDataset(12993) (length: 12993)\n",
      "     INFO: Attempting torch.load with weights_only=False for data/graph_dataset_train/processed/data_feat_0.pt\n",
      "    First training sample: Data(x=[19, 12], edge_index=[2, 342], y=[1], original_idx=[1])\n",
      "  correlation: GraphEEGDataset(12993) (length: 12993)\n",
      "     INFO: Attempting torch.load with weights_only=False for data/graph_dataset_train/processed/data_feat_0.pt\n",
      "    First training sample: Data(x=[19, 12], edge_index=[2, 342], y=[1], original_idx=[1])\n",
      "\n",
      "Graph test datasets created:\n",
      "  full: GraphEEGDataset(3614) (length: 3614)\n",
      "     INFO: Attempting torch.load with weights_only=False for data/graph_dataset_test/processed/data_feat_0.pt\n",
      "    First test sample: Data(x=[19, 12], edge_index=[2, 342], original_idx=[1]), Label (y): None\n",
      "  spatial: GraphEEGDataset(3614) (length: 3614)\n",
      "     INFO: Attempting torch.load with weights_only=False for data/graph_dataset_test/processed/data_feat_0.pt\n",
      "    First test sample: Data(x=[19, 12], edge_index=[2, 342], original_idx=[1]), Label (y): None\n",
      "  correlation: GraphEEGDataset(3614) (length: 3614)\n",
      "     INFO: Attempting torch.load with weights_only=False for data/graph_dataset_test/processed/data_feat_0.pt\n",
      "    First test sample: Data(x=[19, 12], edge_index=[2, 342], original_idx=[1]), Label (y): None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from src.utils.graph_eeg_dataset import GraphEEGDataset\n",
    "\n",
    "LOCAL_DATA_ROOT = Path(\"./data\")\n",
    "\n",
    "graph_datasets_tr = {\n",
    "    'full': GraphEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / 'graph_dataset_train'),\n",
    "        clips_df=clips_tr,\n",
    "        signal_folder=str(DATA_ROOT / 'train'),\n",
    "        extracted_features_array=X_train,\n",
    "        use_extracted_features=True,\n",
    "        edge_strategy='full',\n",
    "        spatial_distance_file=str(LOCAL_DATA_ROOT / 'distances_3d.csv'),\n",
    "        force_reprocess=False,\n",
    "        prefetch_data=False,\n",
    "    ),\n",
    "    'spatial': GraphEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / 'graph_dataset_train'),\n",
    "        clips_df=clips_tr,\n",
    "        signal_folder=str(DATA_ROOT / 'train'),\n",
    "        extracted_features_array=X_train,\n",
    "        use_extracted_features=True,\n",
    "        edge_strategy='spatial',\n",
    "        spatial_distance_file=str(LOCAL_DATA_ROOT / 'distances_3d.csv'),\n",
    "        force_reprocess=False,\n",
    "        prefetch_data=False,\n",
    "    ),\n",
    "    'correlation': GraphEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / 'graph_dataset_train'),\n",
    "        clips_df=clips_tr,\n",
    "        signal_folder=str(DATA_ROOT / 'train'),\n",
    "        extracted_features_array=X_train,\n",
    "        use_extracted_features=True,\n",
    "        edge_strategy='spatial',\n",
    "        spatial_distance_file=str(LOCAL_DATA_ROOT / 'distances_3d.csv'),\n",
    "        force_reprocess=False,\n",
    "        prefetch_data=False,\n",
    "    ),\n",
    "}\n",
    "\n",
    "graph_datasets_te = {\n",
    "    'full': GraphEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / 'graph_dataset_test'),\n",
    "        clips_df=clips_te,\n",
    "        signal_folder=str(DATA_ROOT / 'test'),\n",
    "        extracted_features_array=X_test,\n",
    "        use_extracted_features=True,\n",
    "        edge_strategy='full',\n",
    "        spatial_distance_file=str(LOCAL_DATA_ROOT / 'distances_3d.csv'),\n",
    "        force_reprocess=False, # Force reprocess for test dataset\n",
    "        prefetch_data=False,\n",
    "    ),\n",
    "    'spatial': GraphEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / 'graph_dataset_test'),\n",
    "        clips_df=clips_te,\n",
    "        signal_folder=str(DATA_ROOT / 'test'),\n",
    "        extracted_features_array=X_test,\n",
    "        use_extracted_features=True,\n",
    "        edge_strategy='spatial',\n",
    "        spatial_distance_file=str(LOCAL_DATA_ROOT / 'distances_3d.csv'),\n",
    "        force_reprocess=False, # Force reprocess for test dataset\n",
    "        prefetch_data=False,\n",
    "    ),\n",
    "    'correlation': GraphEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / 'graph_dataset_test'),\n",
    "        clips_df=clips_te,\n",
    "        signal_folder=str(DATA_ROOT / 'test'),\n",
    "        extracted_features_array=X_test,\n",
    "        use_extracted_features=True,\n",
    "        edge_strategy='correlation',\n",
    "        spatial_distance_file=str(LOCAL_DATA_ROOT / 'distances_3d.csv'),\n",
    "        force_reprocess=False, # Force reprocess for test dataset\n",
    "        prefetch_data=False,\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Graph training datasets created:\")\n",
    "for key, ds in graph_datasets_tr.items():\n",
    "    print(f\"  {key}: {ds} (length: {len(ds)})\")\n",
    "    if len(ds) > 0:\n",
    "        sample = ds[0]\n",
    "        print(f\"    First training sample: {sample}\")\n",
    "\n",
    "print(\"\\nGraph test datasets created:\")\n",
    "for key, ds in graph_datasets_te.items():\n",
    "    print(f\"  {key}: {ds} (length: {len(ds)})\")\n",
    "    if len(ds) > 0:\n",
    "        sample = ds[0]\n",
    "        print(f\"    First test sample: {sample}, Label (y): {getattr(sample, 'y', None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries datasets train-validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal: Train size = 10394, Val size = 2599\n",
      "feature: Train size = 10394, Val size = 2599\n",
      "{'signal': <torch.utils.data.dataset.Subset object at 0x105c9ae40>, 'feature': <torch.utils.data.dataset.Subset object at 0x1742d8e10>}\n",
      "{'signal': <torch.utils.data.dataset.Subset object at 0x1742d8a50>, 'feature': <torch.utils.data.dataset.Subset object at 0x119f5e8b0>}\n",
      "{'signal': <src.utils.timeseries_eeg_dataset.TimeseriesEEGDataset object at 0x16c367ed0>, 'feature': <src.utils.timeseries_eeg_dataset.TimeseriesEEGDataset object at 0x16c3c3820>}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# split training dataset into train/val sets if not already done\n",
    "TRAIN_RATIO = 0.8\n",
    "TIMESERIES_TRAIN_SIZE = int(len(timeseries_datasets_tr[\"signal\"]) * TRAIN_RATIO)\n",
    "TIMESERIES_VAL_SIZE = len(timeseries_datasets_tr[\"signal\"]) - TIMESERIES_TRAIN_SIZE\n",
    "\n",
    "# Store original datasets before reassigning\n",
    "original_timeseries_datasets_tr = timeseries_datasets_tr.copy()\n",
    "\n",
    "# Split each training (both timeseries and graph) dataset into train/val\n",
    "timeseries_datasets_tr = {}\n",
    "timeseries_datasets_val = {}\n",
    "for key, ds in original_timeseries_datasets_tr.items():\n",
    "    tr, val = random_split(ds, [TIMESERIES_TRAIN_SIZE, TIMESERIES_VAL_SIZE], generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
    "    timeseries_datasets_tr[key] = tr\n",
    "    timeseries_datasets_val[key] = val\n",
    "    print(f\"{key}: Train size = {len(tr)}, Val size = {len(val)}\")\n",
    "print(timeseries_datasets_tr)\n",
    "print(timeseries_datasets_val)\n",
    "print(timeseries_datasets_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph datasets train-validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full: Train size = 10394, Val size = 2599\n",
      "spatial: Train size = 10394, Val size = 2599\n",
      "correlation: Train size = 10394, Val size = 2599\n",
      "{'full': <torch.utils.data.dataset.Subset object at 0x16c3c3a80>, 'spatial': <torch.utils.data.dataset.Subset object at 0x173816e00>, 'correlation': <torch.utils.data.dataset.Subset object at 0x17382f250>}\n",
      "{'full': <torch.utils.data.dataset.Subset object at 0x16ad4b650>, 'spatial': <torch.utils.data.dataset.Subset object at 0x173815bf0>, 'correlation': <torch.utils.data.dataset.Subset object at 0x17382f150>}\n",
      "{'full': GraphEEGDataset(3614), 'spatial': GraphEEGDataset(3614), 'correlation': GraphEEGDataset(3614)}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# split training dataset into train/val sets if not already done\n",
    "TRAIN_RATIO = 0.8\n",
    "GRAPH_TRAIN_SIZE = int(len(graph_datasets_tr[\"full\"]) * TRAIN_RATIO)\n",
    "GRAPH_VAL_SIZE = len(graph_datasets_tr[\"full\"]) - GRAPH_TRAIN_SIZE\n",
    "\n",
    "# Store original datasets before reassigning\n",
    "original_graph_datasets_tr = graph_datasets_tr\n",
    "\n",
    "# Split each training (both timeseries and graph) dataset into train/val\n",
    "graph_datasets_tr = {}\n",
    "graph_datasets_val = {}\n",
    "for key, ds in original_graph_datasets_tr.items():\n",
    "    tr, val = random_split(ds, [GRAPH_TRAIN_SIZE, GRAPH_VAL_SIZE], generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
    "    graph_datasets_tr[key] = tr\n",
    "    graph_datasets_val[key] = val\n",
    "    print(f\"{key}: Train size = {len(tr)}, Val size = {len(val)}\")\n",
    "print(graph_datasets_tr)\n",
    "print(graph_datasets_val)\n",
    "print(graph_datasets_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Creating Timeseries DataLoaders‚Ä¶\n",
      "\t  signal: Train loader size = 21, Val loader size = 6, Test loader size = 8\n",
      "\t  feature: Train loader size = 21, Val loader size = 6, Test loader size = 8\n",
      "‚úîÔ∏è DataLoaders created in 0.0s\n",
      "‚è≥ Creating Graph DataLoaders‚Ä¶\n",
      "\t  full: Train loader size = 21, Val loader size = 6, Test loader size = 8\n",
      "\t  spatial: Train loader size = 21, Val loader size = 6, Test loader size = 8\n",
      "\t  correlation: Train loader size = 21, Val loader size = 6, Test loader size = 8\n",
      "‚úîÔ∏è GeoDataLoaders created in 0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucadibello/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders for both timeseries and graph datasets\n",
    "common_loader_kwargs = dict(\n",
    "    batch_size=512,\n",
    "    num_workers=16,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "dl_start = time.time()\n",
    "timeseries_loader_tr= {}\n",
    "timeseries_loader_val = {}\n",
    "timeseries_loader_te = {}\n",
    "print(\"‚è≥ Creating Timeseries DataLoaders‚Ä¶\")\n",
    "for kind in timeseries_datasets_tr.keys():\n",
    "    timeseries_loader_tr[kind] = DataLoader(timeseries_datasets_tr[kind], shuffle=True,  **common_loader_kwargs) # type: ignore\n",
    "    timeseries_loader_val[kind]   = DataLoader(timeseries_datasets_val[kind], shuffle=False, **common_loader_kwargs) # type: ignore\n",
    "    timeseries_loader_te[kind]  = DataLoader(timeseries_datasets_te[kind], shuffle=False, **common_loader_kwargs) # type: ignore\n",
    "    print(f\"\\t  {kind}: Train loader size = {len(timeseries_loader_tr[kind])}, \"\n",
    "            f\"Val loader size = {len(timeseries_loader_val[kind])}, \"\n",
    "            f\"Test loader size = {len(timeseries_loader_te[kind])}\")\n",
    "print(f\"‚úîÔ∏è DataLoaders created in {time.time() - dl_start:.1f}s\")\n",
    "\n",
    "dl_start = time.time()\n",
    "graph_loader_tr= {}\n",
    "graph_loader_val = {}\n",
    "graph_loader_te = {}\n",
    "print(\"‚è≥ Creating Graph DataLoaders‚Ä¶\")\n",
    "for kind in graph_datasets_tr.keys():\n",
    "    graph_loader_tr[kind] = GeoDataLoader(graph_datasets_tr[kind], shuffle=True,  **common_loader_kwargs) # type: ignore\n",
    "    graph_loader_val[kind]   = GeoDataLoader(graph_datasets_val[kind], shuffle=False, **common_loader_kwargs) # type: ignore\n",
    "    graph_loader_te[kind]  = GeoDataLoader(graph_datasets_te[kind], shuffle=False, **common_loader_kwargs) # type: ignore\n",
    "    print(f\"\\t  {kind}: Train loader size = {len(graph_loader_tr[kind])}, \"\n",
    "            f\"Val loader size = {len(graph_loader_val[kind])}, \"\n",
    "            f\"Test loader size = {len(graph_loader_te[kind])}\")\n",
    "print(f\"‚úîÔ∏è GeoDataLoaders created in {time.time() - dl_start:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch type: <class 'list'>\n",
      "Batch length: 2\n",
      "torch.Size([512, 19, 3000])\n",
      "torch.Size([512, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch in timeseries_loader_tr['signal']:\n",
    "    # Print batch shape and type\n",
    "    print(f\"Batch type: {type(batch)}\")\n",
    "    print(f\"Batch length: {len(batch)}\")\n",
    "    print(batch[0].shape)    \n",
    "    print(batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional approaches (no additional features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.utils.train\n",
    "from src.utils.train import train_model, evaluate_model\n",
    "from src.utils.plot import plot_training_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LSTM classifier (baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.lstm\n",
    "from src.layers.lstm import LSTM\n",
    "\n",
    "# build model with current parameters\n",
    "lstm_model = LSTM(input_dim=228,\n",
    "                hidden_dim=64,\n",
    "                num_layers=4,\n",
    "                dropout=0.1)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    lstm_model = nn.DataParallel(lstm_model)\n",
    "lstm_model = lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí™ Starting training from epoch 1 to 300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  88%|‚ñâ| 263/300 [01:36<00:13,  2.71it/s, train_loss=0.4569, val_loss=0.4346, best_val_loss=0.4345, lr=9.37e-06, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõë Early stopping: no 'val_loss' improvement in 10 epochs.\n",
      "\n",
      "‚úÖ Training complete.\n",
      "‚Ü©Ô∏è Loading best model state from .checkpoints/lstm_features_best_model.pt for return.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"lstm_features_best_model.pt\"\n",
    "LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"lstm_features_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        lstm_model, timeseries_loader_tr[\"feature\"], timeseries_loader_val[\"feature\"],\n",
    "        criterion, optimizer, device,\n",
    "        save_path=LSTM_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        monitor=\"val_loss\",\n",
    "        scheduler=scheduler,\n",
    "        overwrite=True,\n",
    "        use_gnn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"], \"LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features + oversampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Applying oversampling to the training data...\n",
      "  Attribute-based label extraction not applicable or failed. Iterating through 10394 dataset samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucadibello/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Finished iterating. Collected 10394 labels.\n",
      "  Class counts before oversampling: [8375, 2019]\n",
      "  Successfully created an oversampled DataLoader.\n",
      "üí™ Starting training from epoch 1 to 300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%| | 4/300 [06:29<10:40:57, 129.92s/it, train_loss=0.7002, val_loss=0.5278, best_val_loss=0.4417, lr=4.69e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \u001b[33m\"\u001b[39m\u001b[33mlstm_features_oversampling_submission.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# train model on training set (or load existing)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m train_history, val_history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeseries_loader_tr\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeseries_loader_val\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLSTM_SAVE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mval_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_oversampling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:109\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, use_oversampling)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1446\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1443\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1444\u001b[39m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._persistent_workers:\n\u001b[32m-> \u001b[39m\u001b[32m1446\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_shutdown_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m   1449\u001b[39m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[32m   1450\u001b[39m \n\u001b[32m   1451\u001b[39m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1582\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1577\u001b[39m         \u001b[38;5;28mself\u001b[39m._mark_worker_as_unavailable(worker_id, shutdown=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1578\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._workers:\n\u001b[32m   1579\u001b[39m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[32m   1580\u001b[39m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[32m   1581\u001b[39m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1582\u001b[39m     \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMP_STATUS_CHECK_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1583\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._index_queues:\n\u001b[32m   1584\u001b[39m     q.cancel_join_thread()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py:149\u001b[39m, in \u001b[36mBaseProcess.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parent_pid == os.getpid(), \u001b[33m'\u001b[39m\u001b[33mcan only join a child process\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mcan only join a started process\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_popen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    151\u001b[39m     _children.discard(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_fork.py:41\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmultiprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wait\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msentinel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     42\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/connection.py:1148\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1148\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1149\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1150\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/selectors.py:398\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    396\u001b[39m ready = []\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"lstm_features_oversampling_best_model.pt\"\n",
    "LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"lstm_features_oversampling_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        lstm_model, timeseries_loader_tr[\"feature\"], timeseries_loader_val[\"feature\"],\n",
    "        criterion, optimizer, device,\n",
    "        save_path=LSTM_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        scheduler=scheduler,\n",
    "        overwrite=True,\n",
    "        use_gnn=False,\n",
    "        use_oversampling=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Signal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.lstm\n",
    "from src.layers.lstm import LSTM\n",
    "\n",
    "# build model with current parameters\n",
    "lstm_model = LSTM(input_dim=19,\n",
    "                hidden_dim=64,\n",
    "                num_layers=4,\n",
    "                dropout=0.3)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    lstm_model = nn.DataParallel(lstm_model)\n",
    "lstm_model = lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"lstm_signal_best_model.pt\"\n",
    "LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"lstm_signal_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        lstm_model, timeseries_loader_tr[\"signal\"], timeseries_loader_val[\"signal\"],\n",
    "        criterion, optimizer, device,\n",
    "        save_path=LSTM_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        monitor=\"val_loss\",\n",
    "        scheduler=scheduler,\n",
    "        overwrite=True,\n",
    "        use_gnn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bidirectional LSTM with early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.lstm\n",
    "from src.layers.lstm import LSTM\n",
    "\n",
    "# build model with current parameters\n",
    "lstm_model = LSTM(input_dim=228,\n",
    "                hidden_dim=64,\n",
    "                num_layers=4,\n",
    "                dropout=0.1,\n",
    "                bidirectional=True)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    lstm_model = nn.DataParallel(lstm_model)\n",
    "lstm_model = lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí™ Starting training from epoch 1 to 300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  86%|‚ñä| 258/300 [02:49<00:27,  1.52it/s, train_loss=0.4566, val_loss=0.4319, best_val_loss=0.4319, lr=1.46e-07, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõë Early stopping: no 'val_loss' improvement in 10 epochs.\n",
      "\n",
      "‚úÖ Training complete.\n",
      "‚Ü©Ô∏è Loading best model state from .checkpoints/bi_lstm_feature_best_model.pt for return.\n",
      "   - Loading checkpoint from: .checkpoints/bi_lstm_feature_best_model.pt\n",
      "   - Detected full checkpoint dictionary.\n",
      "   - Model state successfully loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BI_LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"bi_lstm_feature_best_model.pt\"\n",
    "BI_LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"bi_lstm_feature_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        lstm_model, timeseries_loader_tr[\"feature\"], timeseries_loader_val[\"feature\"],\n",
    "        criterion, optimizer, device,\n",
    "        save_path=BI_LSTM_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        monitor=\"val_loss\",\n",
    "        scheduler=scheduler,\n",
    "        overwrite=False,\n",
    "        use_gnn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAAHkCAYAAACuZcnbAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAvuxJREFUeJztnQeYE0Ubx9+067QTkCogvYiAgCCoKKKoqIgV7L1X7OWzd+wo9oZiV+wVrCgIFkTp2ECqHOVa7i65fM9/jg2bXMoml9wlmf/vee65u+zuZN7Z2dl55y1j8/l8PiGEEEIIIYQQC9itnEQIIYQQQgghgAoEIYQQQgghxDJUIAghhBBCCCGWoQJBCCGEEEIIsQwVCEIIIYQQQohlqEAQQgghhBBCLEMFghBCCCGEEGIZKhCEEEIIIYQQy1CBIIQQQgghhFiGCgTJSB5++GHp3r27pZ+rrroqId+5atUqVd5ll10W1/X77ruv7LXXXqIDr7/+umqrt956K+w5N9xwgzrntddei1jWunXrpFevXnLkkUda/v45c+aosu+///6Y2//vv/+uc7/ZunWrFBUV1eqv3333nTQEkD34uUCb7rHHHnL22WfLjz/+GLWvoz3wGdrWCpWVlfLss8/K0UcfLQMHDpRddtlFRowYIZdeemmt7zPKtvKDtgy+ZsaMGWHrUVZWJv369bP07BpyGz9nnXWW+hz92ErdcN+TDZ4HyJTK3HbbbQHtEku///TTT+Wcc86R4cOHS58+fVSfOf300+XDDz8Ur9crqUio5yvUz6JFiyQVOOGEE1R9PB5PQ1eFpDDOhq4AIclg1KhRstNOOwV8dscdd8imTZvk7rvvDvg8+Lx4KSwsVGW3b98+ruuvueYa8fl8CalLJgCF4JVXXpH33ntPTTLD8e6776qJw1FHHVWn76uv9v/qq6/kiiuukIceekh23333gP7atWtXaUjMzwYm+JiMvv3223LiiSfKU089JUOHDk1IX8cE9+STT5bff/9dRo8eLQcddJDk5eWpCTom4x988IFcffXV6hxwzDHH+L/bAG3YrFkzdZ4ZTHyC+fjjj2XkyJEh6/LFF19IeXl5TPXv37+/jB8/Xlq1ahXwOe4jfsKRm5sryeSNN95Qk3M8M2jPVOWwww5Tk//PPvtM/VihpKRELrnkEvn666+lW7ducuyxx8qOO+4oGzZskM8//1wdGzx4sFoUaN68uaQiwe+eYNq0aVNvdSGkrlCBIBlJjx491I+ZBx98UCkQeHklA7yw61L2fvvtl9D6pDtYkcZkcO7cubJmzRpp3bp1yPPeeecd1fYHH3xwnb6vvtr/p59+ks2bN0ftrw1BqP4LxQxtc9999ynLUSL6+rRp02T+/Ply1113ydixYwOOYTV53Lhxcs8996jJeNu2bdWEHT/BCoSVenTo0EFmzpypFKKsrKxax7FyvcMOO8jGjRst179du3Yhvxf9NVnjixVmz56d8tYHAOUBP//8848lBaK6ulouuugi+fbbb2XixIlyxhlniM1m8x8/99xzVZ+69dZblcUMCw9OZ+pNbxqybxCSaOjCRAhJaSsErALvv/9+yOO//fabLFu2TK1iFxQU1Hv9dACrvLCMLF26NGFlzps3T/3eZ599ah1r3LixTJgwQVmVoGzVlQMPPFCtXmPyGQw+x4o2+g9JXWBBwv2D1eHMM88MUB4M0GdOOukkWbBggbzwwgsNUk9CdIIKBCEmH3SsVMKdAqtjcFEAmMDCDx8vKPhq9+7dW/nfwlcb/vCR/MLhS4rJyZIlS9SLb7fddlMrqXDNwAqsmWAffKNOmCD/73//k2HDhqlV+UMPPVSmT59eS4YVK1bIhRdeKEOGDFHfgVU6fAZfdiv++n/99Zdce+21qh6QH37hWB1+6aWXAs5DWajHv//+q9wG4IbTt29f5WYC95xg4BMPF5gBAwaoumGV0OoqKWTFqjFcMkJhtIPhvoRVZrjaHHHEEaoNDB/p6667LuoKc6gYiJUrV6oVT6NN4XuNz0Lx/fffq+OIG0AfGTRokJrQmP270R8ee+wx9TfaBN8ZKQYC7lloV9wL/OBvWFxCxXPg3Mcff1yt2kNulA13jqqqKqkL8IOGKxNW8hMV75Ofn69+v/jiiyHdxtA2cG865JBDpK7AeuJyudQkNBi4vqB98MzXN2jT66+/XvU53C8oU3g2YCUN5pNPPpFTTz1VPWvoW/iNlXYo0Aa438ZzAnct9DVzLIh5rAoXy2OMV2+++abqx7vuumuA243hToi+iOfhuOOOU2NmKKsOJvtwKcK5WHl/5plnlCUhHow4KFinIoGYFIfD4Y+tev7555WMocbLL7/8Uh1DvQz++OMPNa7DXQ73ZP/995cHHnhA3G53wLW4DvfulltuUfJBTsRmJArj+Xr00UeVDLi3GGPxPMC6EgzqN3nyZHXvUG/UB/3jl19+qXVuaWmpsiYecMABqkz0O7xf1q9fX+tcvHtQDsZu3O9TTjkloM8BxHLhvYHnDN+N99TFF1+sriWZTerZ+AhpQDAhwuQTL0ZMOgB8iqdOnaomZni5YMKDIE+8JH/++Wf14jDODQUG2OOPP15NFC6//HL1cnjuueeUEoGXWJMmTaK+FFu2bKl+Y4KMF8qVV16pPsNLHkBRwAsbkz1MAuADjAkTlB4rL21MirHan5OT4/ctxgsFPtU333yzeinjcwOUibLxkoPSApccBMRiAg1rwc4776zOw+Ti/PPPV+Wh/na7XZUJv2UrNG3aVL2Y0NZYAYfvswEmfviuLl26qBccgJsDfNrhAoOJTkVFhVphhusNXmivvvqqWGX16tVKMYGyg/sH/2S4W0DeUBM8fDeUNSiKmCDj+/C9UOQwgcEqPl7GjRo1UkG9+BuKWDgwOcEEGxNGtCGAvHDdwSorlCIzmOigb0LJQJ/CJMpQVqDoWcEc2A0LAPrAk08+qSa1eA4SBZTzjz76SMWBoJ6YxEDhgoKNvptI9xNYNDCpCeXGhFgL9J3gWIZ4QSyFuQ3NIG7E/LyhDVAf3C+4aS1evFhNDtFf8ds4H2MF4regNKAfYKzBJA596ocfflBKEM5FDA+UZ4xJiAuJN54G7oJQGk477TT1PybIAC5lKB9tiXEQzxbaD8+8OV4F4yGOGxNJWAswFsFdDUo8xsBYwFgDmTCGRIu5wXiBSSwWZ/BdWIBAvaFcB7vKIbYHbWm4Ff36669KBlgyMf6jTTEBxzOExQFYNbKzs/3X41mEWyWeR9xP9N9ohOsbAP0y2IqKsRJyoD6QH9+J5BL4PqMd0eewUAGZMVZi/P/vv/9UH8J1kyZNUlY441yM4xhLoYxAXpSFRSIsRGC8wvNigDEeZULJ/PPPP9V7EN+F+9miRQs1RkCpwzsN34V+jPIwbsFihGcc55HMhAoEISYwWGI1xQATp5dffllNcLDCY4DBEoMnBlK8+CNNBLds2aImcJgwGmCijlVnTDwjBQiDTp06qRe3YbbHqhFeEsYqIcALv7i4WK3U4bhRR3wnJiTRwICP6zFZwQvYAKtUiC3ApNysQEBRgUKESa4BJth40WCFHPJiMosVVbzUUVdjQoRyoKRhJcwKUGygQGD1E9YAA1g7cH+MTDi4D5gkYrKP1UHzajYUAUwG8AI3T+QiYcTM4KWJFT3jhYrJEV6MZjDJwMQX7WgOXsWqPRQw3ANM6DCpwqQPCgTunRFEHcrFB2XB8oF7byiokAUr0agTVkeNehmrkKiXoZBi8gRLGSboVhWI4EBlA0w0EpkhDJa8Rx55RE2GMPmAPPgBUBIPP/xwdR9DxSzEAyZQUNYxqTGsPlB6MTEMDsKuC08//bT6CQWskAboE5jMYRJrTuKAe4pVXihWN954oxpj0Ld69uypFHQo8gaY6OG70J+w8oyxC+MRJtv4G3Ea8YB+hOfHnNUMk2v0Q/R/3DNzv4CigUkq6gBFDM86gsWheGLBAGCMw8QTCx2xgvuEOllViFAHTKaN7GywxkCpwf+YhANkw8JYsffee6v4F4xVUMDQplDMMGYByAvFAMo6FAgsBhhgYQEWv1gSCYR7vgDqCYtD8CIGxgE8LwBKJ8Z1WE1wf/BuwN+QF3EgWMQwwDiLMQD3EuMAFi5wLpQH9C3Dwm48c1iUQn/EfTKAcnDeeef5/8d9xXOLZwlj6sKFC5WlEAtv5rbBwhLOwzFYgElmQgWCEBPGhNwAWV4wmQtOD4gXkJFRBX7U0cBAbsaYpFtZicdKkdnn17gWq0wAE39MjDA5NZQHgMkGVgetKBCY+OMFYM5egpU/I41fKBmjyYSUhHBzwqTXPGnHiwyrrtEykphfulBOsPqGybvRFnjRm1cQEYQMy5AxaTHACh6+E0BpsaJAYEKBST7KNE/SUTYmTMEKBFbu0CfMygNWl426WOkjZozy8fI2W7cwob7gggtUm0KpMtcNkyGzNQtWEFiC8JK3CiapBrj3aDtYXaBYYqIPpTe4feMFSjkmNpjEo/8iWB5KICY4WK2Gwghrm3lFNF4woUbbYYJtKBCYVKKPQ7lIVOAx+mLwSneoBQXIC4UMK87mVWn0N0xI0eaY5OEZxvMLZcOsPKC+Rr+ItW/FMw4aMUhw9QpeRcdnWL3GIoORmQr1g8UKlkBM4lF3TITrglWrlNFOhmscFitw3yGDYVXBs4PnE8cM5Q4WQ0y60SfMMqKfwvKAe2KeJENBizULmfn5CibUuIQx3VAeAPowFEwsCMDyhPpANixIGQspBlCWoIRj4eubb75R9wkLVnieghetsEiEsQIKiZngvgyXNmC4O8EKjvbGwhWsMXvuuacag/BdDeEWSOoXKhCEmMBqVDB4eWC1ChNKxAlgUozVLGMiayX1Z3BaQWNl1Yp7UbRrkckEk73gwR/AvccKkAVl4GUD9wjIiHLhphCunlbqBTp27Bh3vQAmrJiIoG5QEPBChWUAq2BYtTO/eFEHTA5mzZqlfLwx6cUk2LhXVn2wUT4UM7Pff6S6Y3KD1UKsIGKVFe2H7za+L1bfb6PtQn2XsRKL8s2EchVAe8Ty3cETRwBrANy2MPnAJApWqUSBSTAm0oZ1A5NrTOyxegnFB78TYSHARB2TG7MbE/oJLDzoP4lSIDChDNWGZtAvcU/QfyOtSOPZw9iDuqLfQ6mEGwn6FtyMjHEnGamHg59tfC/AhDQcqBeAgovFAygM+EH7op2hxKHvxOqehkUctINVt8e1a9eq34ZbGpRUTG5hGTUUCCw+4Hkx+h1iHwDcfkLFGJjlM4gnVWy0vhGM2WXTwHAPNWKxMFag30GJiDZW4Bpcb1ZGjefQvPgUbkwxvgPPkKGkwDoDhR/WYYzVUBjxrEH5CDX2k8yBCgQhJoIHVgyUeOnATQBuSvBHx8oKBkm40MCEbYW6rNpGu9YIkg0VhxHqpRIKKEfwV8b5mNTgZY+XD3zSw7muWJXJUELMxDqhhgKBySR8maFAYPIHuc17P2DyCZcDTOBxDl6ImPzivmElG9fGivGijFb3e++9V5544gm1KonvRhsaGzHBtSBWIk0Kje8Pdu8JlZkmUcDaBAUC1ri6KhBQvuGuhGcoeJUSq5e4p5hwwp0H1olEAUsD+jmUS/QNPNNmF7z6wrh/eMbgjhJtLMLEzIj1QTwCLCiwVGBSf9NNN8Vdj0ibhAU/20adocQbAfDBGGmWMbHGijTidDBGIrUsFE88s6g/lIpIMWPBoF/D0oZVdEyAI636Q+mH2wzaylgMgiyYzE6ZMkVZuDDGwc0L7jmGMmM8b7BAhOvfwYpPoixxkQjlwmdYw4PrbmWswD2PZZywIiPGXFgwcK/xbMEahbaGCxvisiLti0LSGyoQhEQAq36YaECJQLCcGfiLpgJYJcdLwVglNGOsrEXjzjvvVC8ZBEXCLG2e7MWLsfoUqg7BGWGigeA8rN5hEouMIXBvMT4zQCDg8uXL1fHgiZnh7hXLqicms1bqDssDXpYIxoWiYn7px6O0AMMvHvKY3ZSAkd2kPjedMiYpiZo0ob3g8hbOzQGTRExUE7nxGibemDyiD+GeQZaGmNwYsQnw6w+1Ig3XFPjgY4IIhQ3KA5QfZNQyT/5CZdiJpIgEK8OxPBNGnbEibQRVm58HPCdw30M/Qf+EbFDSoLwj8BtuVgj6hQUI7luh0vdGAi6PUCAwMb399tvDngcffyxYBO9KD1clxJJgPDcm3ob7klk+1D/4nmASjj4T76aJdQEW72CMMcmwOGOsgGKFNg9eMAoeKyAnLBaQyfwsQymBoorFj0hWplCWWnwHFFosMhhurVD88c7E/aICkbkwjSshETBSKgabkvHSxEsl2kpefYDJLla88WI2p87Dy9CcojCanHA1CDZZY6IHgmNArICXCl5ymOyb3W3gzx2cGtYKmBQgoBLlYfIEq4T5JWjcq+CdiLHaCP/6WO4VJmoICsV9xsqpmeAgWVg+0NaQ1aw8QE4jMNjcfsaELpIVxlgFhdXFXGdYXYxAy0S6EkXDSIMJn+y6ArcHBFbCVQ6BuaGAuwlWkxO5PwMUEljT4KuPSTn+TkR8RaxghR6WPazWGv3SAKu4iHuBNQsYGw7CGmhWHuCjjww9wNw/gv3/gbEgEJx+MzgdcCSM+4AYmOD+CBczJGsw3DqhMCD2CvfP7EJmjKHBVl4rYBKK/o4AbSgCoVbd0R6wCENxCZ4EY/JvpFrF84yUpIYrEIAyiwUJtEnwQgwyt8E6i++ub2AxMy9iQAnE+INxBhY6gHaB8hBsDYfLFzbXQ7+HG5fRjhivghe/8C6DchXrjuxQ6pDQI9jtC4ojFLVU3MyPJA7eXUIiAF9OuKdghR4+sHgZY5KOl4nxIjW/KBsKvMRhfscPXp5QBPDyMTbiima2RiwBJol48WN1EC8SvGxxPV5WCBCOB2RhQtYguKWgXniZ4UVvNQOTGbh8YGUW9wLyBK8yYoUZE3ZYimBWx+QQkya8LDFpwWQnlnuFSQOUMqycoh2gIGCCFzwRg7sErEBoP6zCQoFBkCG+1/DbNref4VqB7F44L9TutMjOhFVXTF4Q8Gjsso2JL2IDjOwwiSZ4Uon2woQLPviY9FvJxIRAUViyQoHgTyi8CLBFthek2ET56H9QLLBSDZcXxAcgKDyW1VArYCXf6NfIhd9QIJMRZENALO4zJteYKGIihj6OjDgAVi38D0Uek0SsNkMZx/hj9GVznzb6FhQzjF14ZrAqjMklnkWMYRgboETBumXVlQgLFHje8Owa/RHjAixsyNCE/mj40EMBwjMIuaDkw5KHmAi4NcFtLdY4AAP42WNCCksM+gyUGvQZKFMY67BQACUB43UouWBxMKzIaAszGB/wGQKRISfGUTzTcMNCW6Pd43FFDCaa0ta5c+eALHhYIEFgOibpUMIwxqAtkTHKyCiFlX7cTyws4N2Ee4W4L/Ql9A0kqzCSOyDFNNoKcQt4pnHPYOXAWIRMX5Fc6kIBhQR9FxnrYAWB4oB4ImR+g7KDsZ9kLlQgCIkyoGM1EGkVjdV8+Pri5Y8XGHxrsQpjTPAaCgziWG3CpAiTaKzQYRKKly2UgmgTBbj9YKKCyRVWRmGNQJlIXYhJLCaE0fyPQwGTOF5OeMEYGUgwqcFk1Jxy0AqYsCAjFeTDRDY4dz9enJAfky2s3ON8mO6hCGCSj5cn7lWwC0Y40AbIroQ2xOocJre4Fv3APOlH22LChlSWmNjgGiiakB2TKbyU0aa4J1B8MKHDCx+TZJj6w5n4keoTL3hMBND/MMmBVQffk4gN1kJhdtPD5AUKHxQnTGitTuYhWzhwD6BAYBUeChZkg388LFJQsvB96HdGBp9Ex3VAOTZcbWJ1o0kkUDIxycKkD6u/eMYwsceYgomqEbyPPoj+hn6NCTgmZZg4YtUZygfOR59GuwLcIyhHmPRCEcOzhvuHMQzxC/gNtzAoF2jzWCw8mGCj/6OusESgP8JNEZ+blXk8G7iPcOfDajkmsRgzMQnGWBTvqjTqjXZAv0AdjP1k0J/QnjgGecJZONBmiHmBNTCU6xwUG7Qx3G5wb1BvjDFQjqBYJGI/g2A32GCQXc2sQMDKgP/RB/B84PnH2Ib7aoD+jLgS3FuMUxhXkHUOVi7EeZjHOyghGI9RBp47KICQEUoK7o05i5zVe4JxHQoqFldQHsZDKBIYh9HPSOZi8yUjhQMhpF7BixSTsuAJFyYTeDnArQDZUQgh8YPVf1hLoMBBkSN1A4oIFBtMQuO1TGQi7GckHWAMBCEZAFausPoWHKtgBPFaXXUnhBBCCIkGXZgIyQDg3wt/cviVw88b7idIpweTtrFhFyEkcSvE8GeH+0e43cRJeBBHhHTL5t25CSHpBRUIQjIA+LrCR9eIN4CvNOIVEAB88sknJ3WPAEJ0AwG7+EEsDxWI2IHyhfgqQkj6whgIQgghhBBCiGUYA0EIIYQQQghJHwUCmykhRSHSfSHQ84wzzlDpIsNlbEC6tlA/yINvgNSISPW26667qsDScDnJCSGEEEIIIWnmwoQUbshhjM2hEJCGQFAEqGG3WfOurgCbT2GTEjNI/wa/b+QThyKBwKzDDz9c5chGvnXkREZ+aORpR554QgghhBBCSJoqEAj0HDJkiFx22WVqsxaAzVJgjcBGQmPGjIl4PXZkxa6Y2BwGSoOxIRZ2asRmTgYTJ06UzZs3q01t4gXNVF3dcLoWYmB1i1bRUWZAufWCcusF5dYLyq0XtjSX2263WU660qBZmBYvXqysCmbLQOPGjdV293Pnzo2qQGCnVuz2aigPYN68eQG7NAIoKVBIjJ1g4wHKQ1FRqTTUDS0oyJGSEneDKjH1iY4yA8pNuXWAclNuHaDclDvdKCzMF4cjDRSItWvXqt/Y5t5My5Yt/cfC8cUXX6g0etOnT69VJlyhgssrLy+XTZs2SWFhYcLqTwghhBBCiG40qAKBST0IjnXIzs6WLVu2RLwWsQ/YIKtnz54Bn7vd7lrlGf/DZaqu2qUZmKkMD7DgY8DQQEMdw3W4FAaRUFYR87XG9cZvK+XGUyfjGOoTXKXElFtb1nBtGFxOrOVarVNd700y2jDU8cjlxt8P47k3yWhD8+/k98PUaUODZPXDVB0jwo1pie6HqTZGmP/XoX+He85TYZytjzYMPq+hx9lYy43n2kjvsfqeR9S1XKvX2kO8x9K5f6eFApGTk+Of2Bt/g4qKCsnNzQ173erVq9Uuu0888UStY1A+ghUF4/9IZUYDjQ3TlJmqKo+Ul1f5zVbBbN1aoyDl5maJwxGY8Kq8vFKqqrzicjkkJydQ4fF4vFJWVlNnlGt0wPz8bHWTi4vL1e+cHJc4nY6Aa93uKqms9KjP8/ICy/V6q6W0tGJbudmQKuC4YXbLyXGKyxXYNSoqqqSiwqPkQD3MoNMVF7vV3zgW/LDgO/HdWVlOyc52WWpDowijQ4dqQ7QR2gp1RVuEasNQ9818b0K3YaVUVnrV5/je8G1Yu1wrbeh02iUvL7tWNrKSkoptbnY17Wh+lo02zM52qnY0g/uN+x6qH5rvDfoDdqgObMMK8XiqQ7Yh+if6Ke5n5P7tEofDEbJ/h25Dr5SWbu/fwNzHt251q3qjPng+Qvfv8G0Yrh8a9yZSG0br35HaMJb+bYBnGW2F6+oyRoQqN5XHCMPP1ujn8YwR20pW/SVdxgh8F+7r9v4d+xgRbZxNxTECdcJ3m8e1WMcIM6hvOowRxrhmLIzEM0Ykah5Rn2ME2gPnBb/H6nseUd9jhM30HsO9aah5RLQ2jNS/00aBMFyX1q9fLzvttJP/c/yPjErh+Pzzz5Ur0rBhw0KWievN4P+8vDxp1KhR3HXFQ4CbGvwZwI0OPmYGHal2eTUX4+H3eMJfG6pc43trbnZVyHLR8SPVyehkobRTt9ujOmioctH5IpVrPBShykUHhbyxtKH5ZROuXAwekNfqfTNTtzZ0x9WGeBmHK7dGQQxfLspEOwZfY5wTqb7Gy8RqGxrlot6R+3di29C4FvcGg2XocuPvh5HaMFr/jtSG8fZvXIPvrX2MY0Rdx9lMHCPq0r8bcoyoGSfM13KMyOR5hDHxDlUux4jUHSPSRoHo0aOHFBQUKGuCoUAgCxOyKx1//PFhr0Og9ODBg8XprF19BFX/8MMPAZ/Nnj1bBgwYUGtlJVYiBcXEeyya2QjX4iHECgMeGvOp0YJ04q/TdhNZfctqECxzospNVn0T1YaQGysLwfe6ruVGr2/DtqH5fidf1tRpw3DPdl3LtXKsIdswnNwN3Q+TXS7kzsoy5M78/m2uU7h+3hDjbH2VG9zPU6UfWj0W77U1cod/j9WlTqnchrYQ41o69u+0UCAQmwBFYdKkScqi0LZtW7UPBIKg999/f2XGLCoqUpYDs4sTFIwjjjgiZJknnHCCysqEMvH7q6++ko8//ljtA5GuwAwF8yRWGBp42456Q0eZAeWm3DpAuVNTbrhAeL2BK5OJoEZxypaqKrhviTZQbsqdSjgccCVM3P7RDapAgAsvvFA8Ho9cd911KgB60KBBar8Gl8ulNpQbOXKk3HHHHWpnaYMNGzZI06ZNQ5bXtWtXefTRR5Ui8vzzz0u7du3U39xEjhBCCKkNlJmtW4ukvLwkad+xcWNNYKluUG692JjicufmFkjjxoVxb2mQUjtRpwvw2eM+EPWHjjIDyk25dYByp5bcW7ZsVMpDQUEztYKaiMlFpm2wFS+UWy9sKSo3pvqVlRVSUrJJKRFNmuwQYR8Ie3pYIAghhBDSMFRXe/3KQ0FB46R9jzlFs05Qbr2wp7DcWBwAUCIaNWpWZ3emxDlDkaQSLjtAJqOjzIBy6wXl1otUkxuxhubJBSEkc8na9pwnItaJFog0ANpsqNRwmYyOMgPKrReUWy9SWe5kuC2ZSdVV2WRDufWiOsXlTuRzTgUiDTrj0pWbZXNphTTNz5Zu7ZtG3NGWEEIIIYSQZEIFIoX5ccl6mfb5MtlUvH1DkGaNsmXCfl1lt+4tJZNJ1WDDZEO5KbcOUG695K4P3/DbbrtRPvro/YjnfPvtvLjKPv/8M6V16zZy7bU3Wjr/yCMPkQMPHCOnnXZWUuRes2a1HHXUoQGfIdV9p047yymnnCl77DHc//nw4QPlmmtukIMOOkSefvpx1UZvvPFe2LIrKyvlxRefk88//0R9T3Z2jvTq1VuOP/5kGTBgoL89fvnlp7BltGrVWt56630599wz1HlHHnmMXHzx5bXOmzr1OXn88cmqrcK1Lepvvn+Q4dlnnwx5bvfuPeXpp6dKooKOP/74AxkyZA9p1qzQ8nX2BN7v+fN/kfPOO1393a/fAJk8+QlJJahApLDy8Mjbv9X6HMoEPj/v8D4Zr0QQQghJPxrCcn7RRZfJ2Wef7///sMNGy4UXTpSRI0fVuezbb79H7HaH5fOffPIFyc5OfkzJbbfdLX369FVZf0pLS9Sk/+qrJ8pTT70gXbt2V+e8887HasNeq9x1162yaNHvcv75lyiFpKSkWN555y255JLz5L77Jstuuw1S7VFVVbPz8vr16+SMM07y1wWY2wob/n711Rfq/gS7z8yc+akll5rg+9iy5Y7y5JPP1zov1ObC8QLFB0rp66+/Kw1F79591P178MF7pahoo6QaVCBSdPCF5SESL3++TPp3bUF3JkIIISlvOT9uVDcZ0K1F0r4Xk+TgiTL+32GH5nUuu3HjJjGd36xZM6kPGjVq7JevefPmyuIBJeKTTz7yKxCxyA8l5NNPP5Jbb707wIpx2WVXy7JlS+XNN19TCoS5PWCxCK6LmQEDBsm8eXNkwYL50rdvP//n//zzt6xc+Y+yGkQj+D4ie1Ai7mskUmGHA6fTqeSsD2U0HpiFKQXByo158A1FUXGFOo8QQghJJct58PsL/09+a4E63pB8+OF7cswxY+WBBybJAQfsrVbrwddff6lW0ffbb7jsu+8ecuqpx8ucOd/7r4PLDlajzWUYv/fZZ6g6/9dffwlwYYKrDXjqqcfloovOVW5Bhx9+kCof5f3115/+8zdt2iQ33HC1jB49Qg4+eKRMmfKwXHjh2f4yYgGuTMEuQKirFWANwOR87tzZaoNfM7feepdcckltN6Ro7LDDDkpx+OKLGQGfz5z5meyxx5616psoZs36Rt2Xffcdpu7Tk09O8Ss74I8/lssVV1wso0fvIyNGDJGjjjpMXn75RXXsp5/mqfYHcBVD++HH7E4Fgj/DfX/44fvl+OOPUvfx559/VIrISy89r8ofOXKYnHzyBKWkmZk2baocffRhqi/h+5577qmUUGCiQQUiBYHZN5HnEUIIIbGACUxFpdfyT7nbIy99tjRimbBM4Dwr5SVrAvXvv6vkv/82yDPPvCRnnHGuLF68SK677goZNeoAeeGFV+Xxx59VPu+33PI/v5tOMOvWrZXp09+U66+/RZ5++kXJzc1VCka4Ov/6689Kwbj77gfk0Uefkk2biuS+++5Sx6qrq9VEduXKlTJp0sNy332PyO+/L1CTz1jAhP+TTz6Uv//+S0aPPjiOlhHJy8uXww8/Ssk2duyBctNN16m/0WYtWrRUP/EwcuT+8tVXMwPaZ8aMT9XnyWD27O/kf/+7Sg499HCZOvVVmTjxKqWw4J4Ct9utXLJgSXnssWdk6tTXZJ99Rsojjzwgy5YtkV122VW5ZAG4SsXiBvfmm68pd617731YevfeRZ544lHVhlC+0L+OOupYmTTpTnnrrdfV+d9++7VMnfqsXH751fLyy28rN7znn3+6lpKRitCFKQWBz2giz0tXN67i4vKU3NExmVBu0QrKLVqRLnJjonfHiz/J8n+3JLRcWCLOe+BrS+d2addErj5uQFLSy5588unStm079TcmjJdccoUcfviR/uOY5F122YXK73zHHVuFnKxjwme4CR177HFy9dWXycaNG5UrUXBb4vzrrrtZGjeu2ajvsMOOkClTHvL72iPmYNq0N2SnnTqqz26++Q458sjAIOlQXHbZRf5dgysqKpQyMm7cUbLzzp3jbpuLL75M+d5/8MG7atL/2Wcfq88HDx6igrGbN7fmhmYOJB4xYl954IF75LffflWTc6z+I3Zi6NBh8vrrL8dcRyhwo0btWevzzz77Rv1+4YVn5NBDx8nYsUeo/3GvL7/8GmVVQGB4Tk6uHHXUeBk37mjJy8tT58D9a9q0F2TFiuXqvsIlCzRt2kwFkltlyJBhMmjQ7urv8vJyefXVaXLjjbf5XcJQl7Vr16jvwr1avXqVZGW5pFWrNtKqVSv107x5y5D9LtWgApGCIOAMPqOR3JgKG9UEpmUyqf6STRaUWy8ot16kjdwZHF7Xvn17/981k8UmysUIq/erVq2U5ctrLCmYkIejQ4dO/r/z82tiLzye0BaLwsJCv/Jg+PQb1o0lSxaryaqhPNScv4PstFOHqHJcddV10qtXH/+q+uLFv8vkyQ+qyftll10l8TJq1Gj1U1Hhlt9+W6AUiffemy7XXHO5PPHEczGXB4sOsgh9+eUMpUDMmPGZ7LXXPpKVlRVX/aDEPPxwePeupUsXK6Xs/fen+z8zrB9wHYPigsk7lCMokDX3fFnUe26Fdu22962//vpDKisr5Kabrg3Y9RmbN8KdCu27//4HKWVt/Phx0rHjzkr5GDFipFIkUh0qECkIAqORqjVUFiaD8ft1zegAasiWk+MSt7tKq3SHlJty6wDlTm25seqP1f/KKuuTKcTk3f/6/KjnXXLUrpYWv7Jc9qRtbmdeUYar0MSJF6hJJXz1999/tJqMw6IQsX4hJr+hXJggg8sVfqLscDjE56uOeyJtnrB26dJVWUGeeuoxOffcC5RLUizA93/WrK/lggsu9bcTgqbxA4Xp/vvvls2bN0vTptHvX/D8ZN99RynXHGR3ggIBS0e8oM3McgeDZ2vChBNVethgEJS8ceN/ctZZp6hg92HD9pJBg4ZIz569ZNy4g+Paxd2MOabDeMZvvvlO6dBhu4JogH6BNn722WnKOjN37hwVewOrDCwip5xyhqQyjIFIUZCiFala87KdtSwPuqRwdTqtp83LJCi3XlBuvUgXuTHxzc5yWP7p3alQWc4jgfcXzrNSXrJ3xjZ45ZUXpX//gXLbbffIMcccpyaTcJEB9RHIikl/SUmJsn4YbNmyWVat+ieu8ow6x6OglpWVKpeb33+vvXjZqFEjlQ0oPz82pcRg7733VRN3pIQtKdkqAwcOlmQBFy5keYKSYfzAZeqRRx5UMsLysHXrVpky5Rnlzrb33vtIcXFxQPsF9z+n0+XPVGWwcmXkewSlAcoO+pO5Lt9/P0tefnmqskog1uHtt99QyiuUBlh4DjlkrIoRSXVogUhhoCSUlFfJ8x8vkY6tG8sx+3aRrm2bZLTlgRBCSPqRrpbzli1byTfffKk27WrZsqVahccKPggXRJ1IsDkb3JAQ4IvN1jBJR3wErCDRlKji4q1qUm643iD4+rXXXpbhw/cKu/cDYiUQZBwMNotDViS4Gl111aVy2mlnqhSsWGVfvHihPPbYw3LccSeJy1UzkY4VWC0gK2Tbb78DErpnQzDHHXei/O9/V6sN5xCoDeXhzjtvkTZt2ioLBO65210uM2d+ribu//zzlzz00H3q2qqqmkxNubk1sRFIX9ukSVMVF4L78cwzT8iRRx4rixYtjLpxIe4B4jCQAQqKF/bJgMULbYCN+QBcnKDY4Piuu/aX9evXy88//yT9+vWXVIcKRIqTtW21qnF+lvTs0Cylzd6EEEL0xbCcB+8DAcvDhCTvAxEvp59+lhQV/SdXXnmx+h9+6Fdf/T+5+ebrlR99KNeTRION2e699y65+OJzlAKBTEiwSESbrF977RX+v7HSjSxJyCZ15pnnhr0GGaAQIB7MQw89pib4kyY9pAJ8kSUIE1soJmiTM844Rw4++LA6yYlsRnDTSVb2JYN99tlPbroJO10/owKqEX8CV6VzzqmRGxmXliw5QSZPvl9ZFLDL+Jgxh6mMSFAMxo4V6dy5i3JrQ3rdM888T8aPP17th4GMSbAY7LJLPzn33Av96X3DAXcwBGJDKUX2L2yCB0sDXKzAmDFjZcuWLSp1KxQdWHoQA2HUNZWx+dIh2WwK4PVWS1FRab1/7w+L1slj7/wuvToVyhXj+2ujQGCVqqAgR0pK3NrIDCg35dYByp06cmPFdePGNbLDDq0j+urXdSdqp9OeMjLX9z2PJDdiCmA52H33of5VeVg+DjpopEyceGXcKVlTXW6rYJ8FZH866KBDRCe5zUBJQfaoyZOfkGQ/74WF+f7MXtGgBSLFcW27kVVVycuLnYpAVgQa6iQzoNyUWwcod2bLjUlUjw6BuzFnuszhiCY3LAdY5UZqV6SShfIA/3ik9kRK0HQlkfcbMSJw1Ur27tOpJrfH41HxMHA7S0UYRJ3iGJpgpac6fdL/JQDIWlnp0UpmQLlFKyi3aIWucgMdZbYiN1xWsMHcwoUL5JRTjpOzzz5F7T/x0EOPW8p2pMP9fuihe+Www0aLbnL//vtvSm5sgpeK0AKR4jgdNUFUXq9Py2wlHk/tNGmZDuXWC8qtF7rKTcJTE1z8TENXIyX59tt5oiu77tovpeWnBSLFcW6zQFT7fCmXvSKZQNa8vCytZAaUm3LrAOXWS26go8yAcuuFXSO5qUCkOI5tFgiPt267IxJCCCGEEJIIqECkSRA1FQhCCCGEEJIKUIFIkyDqKg8VCEIIIYQQ0vBQgUiTIGodLRDYe0NHKLdeUG690FVuQkhmwSxMKY7TblggfFptwgNZS0tTM/dxMqHcekG59UJXuYFO7y8zlFsvqjWSmxaIFAe7dxqrVrpuxEMIIYQQQlIHWiDSxIVJqQ42vVKhFRRkS0lJhV4aPeWm3BpAuTNbbl91tXjXLhFf2Rax5TURR6vu4nA6kirzBRecJaWlpfLMMy+GPH7XXbfKL7/8JC+//FbEcp5++nH56KP35Y033lP/Dx8+UK655gY56KBDQp5/2203ypo1q2Xy5CfC3nOz3Nhd+M03X5Vjjjku5Pclg/PPP1PJbt79unnzFrLffgfI6aefLS6Xq5Ys+H3UUYfKQw89pvapCMecOd/Liy8+J0uWLBav1yNt27aTUaMOlOOOO17sdqd8+OF7cvvtN0WsH75j7do16rzGjZvIu+9+Ik5n4PT0v/82yLhxB0t1dXXYvRFQf7QlwD3r3383JUM4XnjhFdl55y6SCP74Y4WSYfjwPRvs2T7mmLHy77+r1N+vv/6utG7dJqnfRwUiTVyYgMfjE5dTIy1CJ40pAMqtF5RbLzJb7qo/50nFdy+Jr3ST/zNbfjPJHXa8ODrulrTvHTPmMLnllv/J33//JR06dAw4VlFRIV988bmccMIpMZf7zjsfS0FBQcLq+dlnH8vDD9/vVyDGjz9Bxo07WpLNvvuOkosumqj+rqqqUhPeu+66RU3IzzvvIvX5RRddJtXV1jc5nDt3tlx55SVy5pnnycSJV6lJ/4IF8+Whh+6TVav+kauv/p+MHDlKdt99qP+aa6+9Qlq23NFfFwClAZNvUFZWKj/9NE8GDx4S8F0zZ35uyQujT5++ctttd6t7VlRUpD7D//g8mCZNErfT95VXXiKjRx+sFIiG4vHHn5P5839SbVwf0IUpTfaB0DWQmhBCSPooD+7PJgcoDwD/l336sDqeLEaM2FdNGj/99KNax7755kspLy9XE7xY2WGH5pKdnZOgWkqtSXBeXp40a9ZMkk12draSBT+tWrWWPfYYLkceeax8+OG7/nPQfpjMW+Wdd96SIUP2kAkTTpCOHTtJu3bt5cADx8hZZ52rLAHFxcWq7YzvxQ+UDHNd8GNYQMDAgYOVshfMzJmfya679o9aJ5QffM8aNWoc8H3muiQKXwq4mDdt2lTJWl9QgUhxHKZdDalAEEIIqS98VRXhfzyVAedWV5RLxayXIpZX8d005d4US7lWwYQRLjlY4Q/mo48+UBNmTBr/+GO5XHHFxTJ69D4yYsQQOeqow+Tll0O7PRkuTHDDUe3h88lzzz0lhx9+kOy333DlclNZGRgUP3/+z3LhhWfL/vvvLfvsM1TGjz9CPvnkQ3XM7M6DcrHSDhemI4/c7h61bt1aufnm6+XQQw9Q33HppefL8uXLAtx08DN58gMyZswoGTlymJIHLj7xtJkZlAt3J6vYbHZZtmypbNiwPuDz0aPHyIsvvia5ublxWUq+/voL5eplsHbtWlm6dInstdcISQao/w03XC2jR4+Qgw4aqawJK1f+4z9eWVkpjzzyoHKHQp858MB95frrr5JNm2oUZdw/WFCeffZJOe+8M2v1GwPzZ7jvaGt8L/rK/fffrT6HBee8886Qffcdply27r33LiktLfGXsXDhb3LuuafLqFF7qj587bWXq/ZpCKhApDg2m02cxmZyGewzSwghJLUoefassD/ln00OOLd06gXiKwu0PATjKy1SsRGlL18Wttyy9+6Iu74HH3yorF79r/z226/+zzZu/E/mzZsjY8aMFbfbLZdccp5aZX/ssWdk6tTXZJ99Rsojjzwgy5YtiVo+fP2nTZsq5557oYq1aNSokcyY8VnARBQT/h49eqnjzzzzkvTq1VvuvPMWKSraqNx5Lrxwot81apdddg0oH+4755xzmqxfv07uvPNemTLlGTXJP//8M/wuPuDzzz+RrVu3yCOPPCmTJj0kS5YskieeeDSmtvrnn79k+vQ3VLvEy9FHj1eTaEygL7roHDWB/vnnH9XKPiwS8azw77nnCCkrK5Off95urZo581MZPHh3KShoJIkGlinEz4CHH35CJk9+XLk2nXnmyX7F6NFHH5Ivv5yp4ipeeeVtufbaG+XHH+fKCy88o44/+eQLyi3r2GOPlzvuuMfydyMupbCwuTz77EvKGgRF8eKLz1UuX88//7LccMNt6t5ecsn5Snn1er1yxRWXSL9+A+T551+RBx+cIuvWrZM77rhZGgLGQKRJILXHK1JZZd03Md1BEFJJiTujAw1DQbkptw5Q7kyU25pMCKxOFj179pbOnbsoNybD5/2TTz6SZs0KlavN1q1b5aijxquYA7gOgdNOO0umTXtBVqxYLl27dg9fb59P3njjVTnqqGNl1KjR6rMLLrhUWRHMK9UoD3ENWPwDxx9/irKAYEUbLjhGPAWsIcGgrlu2bJann37R79Z04423ytFHj5W33npNzj23JlYhP79ArrjiWjVBR7zHyJH7y/ffz4rYNmiTL7+cof7G6j7iIBDwjPaIFyhATz89VV599SX1/ZhUAwRoT5x4pVIGYgVK2aBBQ2TmzBnqN4CSduyxx6k6x8Nll13k35TX4PLLr5H99z9QZsz4REpKiuX662/xKzxXXXW9UoTeffdtdT979uylFE3DhQouYIMGDVbWLIB7ZbfblcWloCA2FyKUb/SJW265XsV+nHjiqer/9u13khtvvE2OPvowVZ8uXbqp/oH2RR3atGkrN910u98SUt9QgUgDaiwQXvF4M/GlE57MfMlGh3LrBeXWi3SSu+CUx8Mf3DZBNsjZ/yJxf3xf1DKRlSl//CTL5cZjhXjhhWfVSj8mhJ988oHyy0fmIUz0xo07Srk5weKwatVKv3sQgokjsWXLFmXNwGTSTO/efeWvv/5Qf2NCftBBh8rrr7+iJpfm8rF6HA0oMe3bdwiIiYAFAlaMFStW+D/D95hX96FQmF1+QjF8+F5yzjkX+uuyfv1aef75Z+TMM09SK+DxBhR36rSzmnCDv/76U3744XulaF133ZXKAgOFLlb23Xc/5aJ12WVXKcsLrCXDh+8dMjbCCldddZ306tUn4LPCwkL1e8mSJUqxPPDAfQKOQxlEQD444ICDZO7cOTJlysNKEUR9/vnnb+nbt5/UBSi25gB91AXB53BPCgZ1QTasCRNOVO5OTz31mOy22yAZOnSYcvtqCKhApFEq13R68dQVrN7k5DjF7fakRHBSfUG5KbcOUO70kNvmyrZ8rrNdH5VtKTiAOqC8/EKV0tVmyi6YaPbf/yA10UOGoJqYhxVy2201biVQAM466xQ1QR82bC+1wg2FAL7m0TD0muD3sHki/+effyj/9O7de8igQbvL3nvvoyaJp59+osXah+4TUG6cTof/f3PQsf/KKP0pLy9fBTkbwHLRsePOKp7j888/lSOOODpm15/HH5+sFDbDcgO3JfzgHhxxxMEqxWs8CsSee+4td999m1p1h88/4lfiiacwwIq9WXYzPl+17LRTB7nzztrKr/Gd99xzu3zxxQw58EBkWdpLunY9XcXNwNUs1DMe6l6EUvAQTB5cF1hFDAuEmaZNa5TKc865QA4//CiZPftbmTfvB6VMwIIGZS0rK0vqE8ZApAGG6c0bZYUkk8Bg7XI567oYlXZQbtEKyi1akclyQynI3qMmNWk4sveYkFTlwchEA+UAbi+YGMNf3Jg8wvKA1WbEFpx88ulqgo9MQVYm4Fihh587glzNLFmy0P/3O++8qVa2H3jgUTnuuJNk6NDhKvbBjOHaFIrOnbvKypV/y6ZNNelHjRS0ixcvUpP9RGPIjIlrrGDyi/ZEJqZQE2+Hw+lf5Y8VWFQQBwCXK6RvHTnyAEkWnTp1VlYOxFegn+AH7kGPPfaw/PLLz8plCDLCJQsua9gPBArTX3/9GfK+GrcXiiX2JTGANcpKXaCEGvXAD6xFSIsLixEsH5Mm3aEU4LFjj5Rbb71b7r33YVWX5cuXSn1DC0Qa4NqmQFR59FEgCCGEpBeuTgNFRp0fYh+IQskddlxS94EI3hPippuuU/708DE3aNmylbjd5WpSCvcTTMgwOQNVVdGzPx1//MnKtaZDhw7St29/lV1p4cLf/cHQUDCwKo14ALj2IAD2gQcm+V1izKvaUAo6deoUUD5iK6ZOfVZl+MHeDC5Xljz77BNqtf+ww8bVqU2giMACY7BhwwZ54olHVH323nvfsNctWvS7v+4GLVq0VJaFs8++QG3QBzCxhrUFE+VXXnlJdtxxRxU3EC/77DNK7r33DjUxR/xKsoB70ksvPS/XXXeFcvGCSxGCwWfP/k5OP/0cpczgs2+++Uq6d++p2hEuWkuXLg5wi0I7QnYojE2bFqoYnPfee1v69euvFDX0s2gWAgRhn3fe6SrzEixCiM2499471XfCtQ1B9gigRzIA9EXEXSBdLlK3Bu99Uh9QgUhxkPKuk321tMnaJI4NjcTXZrekr+AQQggh8SoRzg4D6n0najMIRMWEDpmKsD+EASa0S5acIJMn369SY2KnXigb3377tSxatFDGRklIhPgJbLSG2IGNGzeqVXJcb/jKI5MO/saGdgj4bd++vZx99nny1FOPy+LFC9VEeMCAQWriec45p6rAXTOYqD788ONKSbnoonPVZ3377ipTpjytAmbrAvZRwA/ApBwr7gg6v//+R5VCEA64gwWDmBJkIjrkkLHKTey116bJ5ZdfpFbcCwt3UC5IN9xwc532z4CrEDa6g39/Ml1z0ObYeRuZuCZOPF+83mrlgnb//Y8odyxwyy13qnty4onHSuPGjVUswllnnSdTpz6nJvM5OTnq3qOMP/9cIc8997LaWA+Tf7jM7bBDCznjjLNrpbsNpk+fXeS++ybLU09NkVNPPV7y8nJVnMN5512s3NZgBUPWrccemyxnnXWysk4gBgcWLyg69Y3Nlw6OmCkAOlVR0XZzVEPu6AkzsVrpyWDsdgxwORmcsSQ0lJty6wDlTh25sfK+ceMa2WGH1mrFO5myp4rM9Qnlrj+wj8WaNauVQqDr/f7pp3lqH5LXX39XKcixPu+Fhfm1MlaFg0vZabijJz5P5o6eqQD02oqKqrQINEwklJty6wDl1ktuoKPMgHLXLwhWhqtWRYVbu/u9efNmKS7eWm/fRwUiRd2WYHmwsqNnpoJnsKICmUpEKyi3aAXlFq3QVW6go8yActcv2ETwsMNGB2zwp8v9Puusk+Xaa6+ot++jC1MKujB5Vi+S8vfvinpe7pgrxdmmp2QqMKOh3XWDcusF5daLVJO7vlyYCCEND12YMhyrO3Umc0fPhgZ+hPn52eq3TlBuyq0DlFsvuYGOMgPKrRd2jeSmApGCIGtFIs8jhBBCCCEkUVCBSEHUTp3527eyj7SjJyGEEFJX6M1MSObjS+BzTgUiBUmVHT0JIYRkNg6HQ/2urKxo6KoQQpKM8Zxjp/C6wo3kUnxHT/dXT4tUlgdYHqA8ZPo+EDqviFFuvaDcepFqctvtDsnNLZCSkpqU4VlZ2WqjsUSDIlNM9HqBcuuFLUXlxrgD5QHPOZ537GJdV5iFKYU3kgMV8z+UyjmvyR9VLWRT5wNl7/1H0PJACCEkYWAasHVrkZSXlzR0VQghSQTKQ+PGhWEXCWLJwkQLRIpjs9fcok3V+fJfTgcqD4QQQhIKJhNNmuwgjRo1E6/X09DVIYQkAbgtJcLyYEAFItXZdrPtNp94G3B79IZKd1haWtGg28LXN5SbcusA5U5NuTG5sNuztJM7WVBuyp3JUIFIcZw7D5avV+fJRz//JwNSaPOh+iAZfrjpAOXWC8qtF5RbLyi3Xtg0kpsKRIpjz20s7vw2sqnaLR7NFAhCCCGEEJJ60KE+DXBuC2jxeDPfJEYIIYQQQlIbWiBSHG/RKmm//ivZLatCvN4dG7o6hBBCCCFEc2iBSHGqN/4jO62ZIbtnL5cqjz4uTAhA0iUQyQzlptw6QLkptw5QbsqdyVCBSHXsNbuE2sWnXQwE9t7QEcqtF5RbLyi3XlBuvfBqJDcViFTHZt+uQGii1QIkMsjOdqrfOkG5RSsot2gF5RatoNyiFTbN5KYCkS77QIhPK80WqdCys11apUQDlJty6wDlptw6QLkpdyZDBSLFsRkWCFu1VhYIQgghhBCSmjS4AlFdXS0PPfSQ7LnnntKvXz8544wzZOXKlWHPr6qqknvvvdd//vHHHy+LFi0KOOeUU06R7t27B/yccMIJkvYuTBoFURNCCCGEkNSkwdO4PvroozJt2jS58847pVWrVnLPPffI6aefLu+9955kZWXVOv/GG2+UL7/8Up3fpk0befDBB5XS8dFHH0mjRo3UOUuWLFHn7bfffv7rXC6XpHMQtU0FUdMCQQghhBBCNLZAVFZWyjPPPCMXXnihjBgxQnr06CH333+/rF27Vj799NNa58My8eabb8ptt92mLBCdO3eWW2+9VSkav/32mzpn48aN6mfXXXeVFi1a+H+aNm0q6YijRSf5b8iF8lLpMK2yMPl8sDZ51G+doNyiFZRbtIJyi1ZQbtEKn2ZyN6gCsXjxYiktLZWhQ4f6P2vcuLH06tVL5s6dW+v8WbNmKSvDXnvtFXD+zJkz/WXA+oAAlk6dOkkmYMvOl+odOstqb6FmCoRPysur1G+doNyUWwcoN+XWAcpNuTOZBnVhgqUBtG7dOuDzli1b+o+Z+fPPP6V9+/bKOvHEE0/IunXrlLJx1VVXKWsEWLp0qVIybr75ZqVw5OXlyejRo+Xcc88N6RIVC3Z7YGQ9+ojRUYKPAWMzkVDHcB0uRbB+qIh987VZrho3Jrgw4X8r5cZTJ+MY6hNcpcSUW1vWSG2I/424j1jLtVqnut6bZLSh02mvtRFN5HLj74fx3ptktKHRt5PfD1OrDUN9bpY1Wf27brLW/d6Yx7Jk9cNUHCOM47r0b+OY+X6nwjhbX21ofo+lwjgbS7nxXGscczjstSbSDTGPqEu5Vq+1h3iPpXP/TgsFory8XP0OnthnZ2fLli1bap1fUlIif//9t4qbuOKKK5T1YcqUKTJhwgT58MMPZYcddlAKREVFhfTt21cFUyPA+u6775bVq1er3/GCxi4oyAn4DKYqaJu4UcHHwNatNfLl5maphylQ9kqpqvKKy+WQnJxA+T0er5SVVdZca3NL41XfyJDsf2Shr7f6nuLicnWzc3Jc4nTWKBcGbneVVFZ61Od5eYHlIg0sdkkEBQXZKrLCTEmJW3WwnBynuFyBXaOiokoqKjxKjvx8XLsddLriYrf6G8eCHxZ8J747K8upUpxZaUMUge/avLlM1SlUG6KN0FaoK9oiVBuGum/mexO6DSulstKrPsf3hm/D2uVaaUMoCHl52bWSCZSUVKh2aNw4V32P+Vk22hA5ptGOZnC/cd9D9UPzvUF/sNuD27BCvdxCtSH6J/op7mfk/u0Sh8MRsn+HbkOvlJbW9G+jXON+Q8atW92q3qgPno/Q/Tt8G4brh8a9idSG0fp3pDaMpX+b64TPUffgcq2OEaHKTfUxAu1h3G/UM54xYlvJqr+AdBgj8F2QF+2IfhjPGBFtnE3FMQLlok7mcS3WMcIM6psOY4QxruEYZI1njEjEPKK+xwjQuDHGte2T3YaYR9T3GGEzvcdwbxpqHhGtDSP171iw+RrQ1vLJJ5+o+If58+dLTs72RrzoootUfASUAzM33HCDvPLKK0pZMCwObrdb9t57bxVIjeBrj8ej3KKaNGnivw7nX3LJJcoi0bx587jqikbHZLa+Vw6q//tTSt+6SYq8+XK3+2iZMnFEWqwu1nXlAH/j4TAepHRZXaxrGxoDIF7a5tW6VF9drGsbGvfbmKyky+piXdsQhLrfqb66WNd7Y77fgVan1F5dDJY11nLN45pxfahyM6V/m62q5vttljUdV2hjWY02v8caepyNtdx4ro30Hst0C4TdNK4ZynK69e/CwvxaSlZKWiAM16X169fLTjvt5P8c/yP1ajDI0uR0Ov3KA4DiAbemVatWqf9x3Kw8gK5du6rfcIuKV4EAwS/4RByLZjbybdPu7baaLEyhJpWJr1PgqkF9yZpJ5dalDY3joc7J9HsT7mWT2DqlThsag3y4+x1vuVaOpUIbBsudKv2wPsrVoX8HHwvVz9mGqVtuIq4Nd06qyZrIcqtNlpd07N9pEUSNrEsFBQUyZ84c/2dbt26VhQsXyqBBg2qdj89gYViwYIH/M1ggkJ2pQ4cO6n/s93D11VcHXIfzkca1Y8eOknbYakyLdqlWK7O6BOcQQgghhJDUpEEVCMQ+YCO4SZMmyYwZM1RWJrgawdKw//77Kz/IDRs2KCUBDBw4UPbYYw+58sorZd68ebJ8+XIVCwH/68MOO0ydc8ABB8g777wjL7/8slIs4L6E2IfTTjtNKSvpvJEcVIdqrRQInWQ1Q7n1gnLrBeXWC8qtFz7RhQaNgQBQEu677z556623lKIAK8P//vc/adeunXJLGjlypNxxxx0ybtw4fyA1FI6PP/5YnT9gwAC55pprpEuXLv4yX3rpJfUDBQJ7QBx99NFy5pln1goOi62e1VJUVJoQmWP63s2rpey1a6SsOkuu3nysTJm4t2QHBY0RQgghhBBSF2KJgWhwBSJdaCgFonrLWil99Spx+1xy5abxMvniPSUvKFMAIYQQQggh9aVANKgLE7EeA2HbZhZDILUOILjUSPeoE5SbcusA5abcOkC5KXcmQwUixbHlNZG8gy+Tp0tHqv912o3aqhacaVBuvaDcekG59YJy64VDI7kbNI0riY7NmSWu9rvI37IRjlRaKRCEEEIIIST10EdVSnOc27RaXVyYCCGEEEJIakIFIsXxeaukcuEXMjRrsYqDoAWCEEIIIYQ0JFQgUh1PpZR//ZyMccyq2UxOEwsEdlEsK6uMutNlpkG5KbcOUG7KrQOUm3JnMlQgUh379j0f7JpZIDwer+gI5dYLyq0XlFsvKLdeeDSSmwpEmuxEbSgQ2I9CB2w27FTuVL91gnKLVlBu0QrKLVpBuUUrbJrJTQUinRQIm088mpjGbDab5OS41G+doNyUWwcoN+XWAcpNuTMZKhCpjn37LbKpGAg9LBCEEEIIISQ1oQKR4thMFgiHioHQwwJBCCGEEEJSEyoQ6cA2JYJpXAkhhBBCSENDBSINyNv/ApnR6DAp92VppUDolM3ADOXWC8qtF5RbLyi3Xng0ktvZ0BUg0XF06C8r5zmkSv4TryYuTEY+Zd2g3HpBufWCcusF5daLas3kpgUiTXA6a25VlUYWCE0SGdSCcusF5dYLyq0XlFsvbBrJTQUiDfD+OU+6exZLrq1SGwuE3W6TRo1y1W+doNyUWwcoN+XWAcpNuTMZKhBpQPk3z8ugje9LE3uZVjEQhBBCCCEk9aACkUZZmLATNRUIQgghhBDSkFCBSKPN5Go2ktPDhYkQQgghhKQmVCDSyAJRs5EcLRCEEEIIIaThYBrXNNtITpcgaqRD27q1XHSDcusF5dYLyq0XlFsvqjWTmxaINHJhctiqxVNNCwQhhBBCCGk4qECkATaTBUIXFyakQcvLy9ImHZoB5abcOkC5KbcOUG7KnclQgUgDcoYeK392OVbWeZtoFUTtdDpERyi3XlBuvaDcekG59cKpkdyMgUgDXB36SUlxKynx/a6NBYIQQgghhKQmtECkCU5Hza3SJYiaEEIIIYSkJrRApAGe1Yul6cbV0tjGnagJIYQQQkjDQgUiDXDPeV1ar1suHZwjpMrbRnTA5/OJ212pfusE5abcOkC5KbcOUG7KnclQgUgHtmVhsqssTHp0TDx/lZVe0Q3KrReUWy8ot15Qbr3waSY3YyDSaB8Im02fNK42m4jL5VC/dYJyi1ZQbtEKyi1aQblFK2yayU0FIo0sEA6p1sYCYbPZJDc3S/3WCcpNuXWAclNuHaDclDuToQKRZhvJebkTNSGEEEIIaUCoQKQDdoc/BqLKQwWCEEIIIYQ0HFQg0oFt5jC7DRYIPVyYCCGEEEJIasIsTGlAVp/9xN2qr/zxyRbxOPSxQHg1CRgPhnLrBeXWC8qtF5RbL7wayU0LRBrgaNtHytvuLuurm4i70iuL/94k1RluiYB8paUVGS9nMJSbcusA5abcOkC5KXcmY/PpsuNFArTKoqLSBvnuH5eslxc/XSpbSiv9nzVrlC0T9usqu3Vv2SB1IoQQQgghmUNhYb44HNZsC7RApDhQHt569xtpXfmnFNqL/Z9vKq6QR97+TR3PROx2mzRunKt+6wTlptw6QLkptw5QbsqdyVCBSGFgBpv2+TLZN+d3OafRDNk1659a57z8+TJtzGWEEEIIIaThoQKRwixduVlZGqrF5t9ILpii4gp1HiGEEEIIIfUBFYgUZnNphfpd7du+kVyk8wghhBBCCEk2VCBSmKb52er3dguEL+J5hBBCCCGEJBsqEClMt/ZNVbYlQ4Gw2WorEIWNstV5mQbiOkpK3NrFd1Buyq0DlJty6wDlptyZDBWIFAaR/EjVWr3tNtlDWCDG79c1YyP+dXkIg6HcekG59YJy6wXl1otqjeSmApHiYJ+Hvl1aqL/tpiBqWB7OO7xPxu4DYbPZJDfXpX7rBOWm3DpAuSm3DlBuyp3JOBu6AiQ67XcbLvYunWTxBzV7Ppwxpqfs3qtVxloeAJ4/l8spFRUe0WmrQ8pNuXWAclNuHaDclDuToQUiDXC27i6NBx4kq22t1f8dWjXOaOWBEEIIIYSkLlQg0gjHNqVBJx87QgghhBCSWtCFKQ2oLtkoFZu3Sgv7VtkqueKlAkEIIYQQQhoIKhBpQOWSb6V47lsyzNlDVshgLRQIn88nFRVV6rdOUG7KrQOUm3LrAOWm3KK7AnHiiSfGVTgi0Z9//vm4riXb8W3zNHNs2wdCBxcmPH8IRNINyq0XlFsvKLdeUG698Gkmt6UYiB9++EFKSkqUVmX1B+fjOpIAbNv2gdimQHirt6dzzWScTj1DdCi3XlBuvaDcekG59cKpkdyWXZhuvPFG6du3r+WCf/nlFzn22GPjrRcxYXc41G/HtsRLOrgwIctUXl62Vrs6AspNuXWAclNuHaDclDuTsaQqnX322bLjjjvGVHDr1q3VdSRxFgjHto3kdOiYhBBCCCEkjS0QF198ca3PysrKJC8vT/39ySefyOrVq2WfffaRjh07qs+gcIS6jsTBtvSthguThwoEIYQQQghpIGJ21vrjjz9k1KhR8sQTT6j/H3jgAaUo3HXXXXLYYYfJjz/+mIx66o1tmwuT6BNETQghhBBCMkSBmDRpkjidThk5cqRUVlbKtGnT5MADD5R58+bJnnvuqRQKklicO3aWRsOPkaWuHlopENWaBIsHQ7n1gnLrBeXWC8qtF9UayR2zAgFFYeLEibLLLruoLEvFxcVyzDHHSEFBgQqa/u2332Ju7IceekgpH/369ZMzzjhDVq5cGfb8qqoquffee/3nH3/88bJo0aKAc77//nsZN26c7LrrrjJ69Gj54IMPJJ2xFe4k0utA+cvVRf3v0aCDQkkqKanQRlkyoNyUWwcoN+XWAcpNuTOZmBUITOAbN26s/v76668lNzdXdtttN/W/1+tV1olYePTRR5UV45ZbbpFXXnlFKRSnn366sm6Eywb11ltvye233y5vvvmmFBYWKqUDigxYsWKFnHXWWUrBwHlHHXWUXHHFFUqpSHccjprbpUvnJIQQQgghGaBAdOvWTT799FPZsGGDfPzxxzJ8+HClNECxeOmll9Rxq0BJeOaZZ+TCCy+UESNGSI8ePeT++++XtWvXqu8IBpYJKA233XabUhA6d+4st956q2RlZfktH9i4rnv37nLJJZeo46eddpqyQjz11FOSrtiqyiS7bLUU+jZplca1UaMc9VsnKDfl1gHKTbl1gHJT7kwmZgUCk/033nhD9tprL9myZYta/QcHHHCAzJ49W8477zzLZS1evFhKS0tl6NCh/s9g3ejVq5fMnTu31vmzZs2SRo0aqe82nz9z5kx/GXCxMpcHhgwZooK703V78aqVC+S/F6+RYWUztLJAYCdzHaHcekG59YJy6wXl1gubRnLH5m8kIsOGDZP33ntPFixYoGIM2rZtqz4/6aST1EQdq/9WgaXB2DPCTMuWLf3HzPz555/Svn17ZZ1AFqh169YpZeOqq65S1gajzFatWtUqr7y8XDZt2qRcntIN27YsTNt3otZDgSCEEEIIIRmgQABM4vGDuAPEHOBvBDM7tu2YbBVM6gFckMxkZ2cr60YwJSUl8vfff6u4CcQ1wPowZcoUmTBhgnz44Yeyww47iNvtrlWe8X+4uAqrBJulYNAwrBqhTFaGpSDUMVyHS6GshtJYzddWb4t9sEugAhGp3HjqZBxDfYKrlJhya8sarg2Dy4m1XKt1quu9SUYbhjoeudz4+2E89yYZbWj+nfx+mDptaJCsfpiqY0TwfU9WP0y1McL8vw79O9xzngrjbH20YfB5DT3OxlpuPNdGeo/V9zyiruVavdYe4j2Wzv07qQrEnDlzVDpXxB2gIq+//rqKMcDmcbAGWCUnJ8c/sTf+BhUVFSo4u1ZlnU6lRCBOwrA44O+9995b3n77bRV8DeUjWFEw/g9VplXQ2AUF2+sIqqo8Ul5epW5U8DGwdWuNgpSbm+UPgDYoL6+UqiqvuFwOyckJVHg8Hq+UldXUGeW6c3OkFEHU2ywQxk3OyXGJ0xmotLndVVJZ6VGf5+UFluv1VktpacW2crMhVcBxY/v1nBynuFyBXaOiokoqKjxKjvx8XLsd1Ke42K3+xrHghwXfie/OynJKdrbLUhuiCHObhWpDtBHaCnVFW4Rqw1D3zXxvQrdhpVRWIiGAQ31v+DasXa6VNnQ67Wq7ezNIHoDsDQDH0Y7mZ9low+xsp2pHM7jfuO+h+qH53qA/2O3BbVghHk91yDZE/0Q/xf2M3L9dtRYPjP4dug29Ulq6vX+b7zfk3rrVreqN+uD5CN2/I7dhqH5o3JtIbRitf0dqw1j6t7lOALIGlxvLGBFMcXG56j+pOkagPYzzUM94xohtJav+ki5jBL7LeK7rMkZEGmdTcYyoacPAcS3WMcIM6psOY4QxruGnutob1xiRqHlEfY4RwOGw1XqP1fc8or7HCJvpPYZ705DziHjHiKQqEMhmhLiH/v37y2WXXaYUCQDXJaRjhRJxyimnWCrLcF1av3697LTTTv7P8X8oVyi4JkGJMJQHAMUDFpBVq1b5y8T1ZvA/ds1G/ES84CEwHgzzZ9tTdwUeM4OOVLu8movx8Hs84a9FuVVuT8AXerw1v2tudlXIctHxI9XJ6GShtFO326M6aKhy0fkilWs8FKHKRQeFvIHlbj8nuNyaQdcXtg2NYxg8IK/V+2ambm3ojqsN8TIOVy6uxyCKdg5VLspEOwbLaJwTqb7Gy8RqGxrlot6R+3di2hD3G3Ib1+LeYLAMXW78/TBSG0br35HaMNb+bXyOuoZa+YlljKh9raT8GGHc77q2oUG6jBF4qaNe8Y4RdenfDTVGoFzUyzyu1aUN02mMQD83+nSi+3cqjxF4jwXHazbEPKK+xwhH0HusIeYRdenfSVUgsFEcNpF78MEHxePxyD333KM+P/vss6WsrExZI6wqEMi6hP0jYNEwFIitW7fKwoULlUtUMIMGDVLfifgL7EMB4LKE7EwHH3yw+n/gwIFqfwozCO4eMGBArZWVWIkUvBzvsWhmI1zr26bh26U6wIUpWjB1/HXabiKrb1kD//cmpdxk1TdRbRg8OCaq3FRvQ/P9Tq6sqdWGwcpiosqNdqwu1yaiDYPvd6r0w+SX69OqfxvHQt3vupabDm1olju1+mH0Y3W5FhPcZJSb6m1YXes9ln792yoxz6ixadsRRxyh/g42jyDA+t9//7VcFmIToCjAijFjxgyVlQnpV2Fp2H///ZUZE+lioSQYysEee+whV155pcq2tHz5chULAfeJww47TJ1zwgknyK+//qrKRHwG0sQi3Szcm9IVm73GvGjb9uKJ9sBnAuhbMAvqlNEAUG7KrQOUm3LrAOWm3JlMzAoE3IAwqQ/FmjVrYnYTQlrYI488Uq677joZP368UgaefvppcblcqjzsM4EAaYOHH35YBg8eLOeff766DjERL7zwgj+7UteuXVWQ9VdffSVjx45VFhFYSYJTu6YTjiYtpNGQw+Wfxv3V/14NdqLG8wcfPU2eQz+UW7SCcotWUG7RCsotWmHTTO6YXZjgvoTAZWwYhxSqANoW0qc+9thjakO4WIDCcPnll6ufYNq1aydLliwJ+AwuT9iNGj/hwD4R5r0i0h1745ZSMPxYWfnfr9hOj2lcCSGEEEJI+igQEydOlPnz58vRRx8tzZs3V59deumlSoFAADP+JskhOOUhIYQQQgghKa9ANGnSRLkFTZ8+XQUnb968WbktIfZg3LhxdUqVSkLj81RKVVGRNPJsUv97t2VhIoQQQgghJOUViOuvv17FHsACgR+SfLxF/8r6t26U3ZyN5DU5XLyJCJ9PcSAi0oxpIGoAlFu0gnKLVlBu0QrKLVrh00zumIOo3333XSktxbZmpL7w2ezaZWFCmjHkVo51Z8R0h3JTbh2g3JRbByg35c5kYlYgsIEc9m0g9YihQPiqtXJhCrUVuw5Qbr2g3HpBufWCcuuFXSO5Y3Zhwg7RSLOKvRWwERx2eDaDjEy33357IuuoPXZH4D4QOmRhMrakN7Z01wXKTbl1gHJTbh2g3JQ7k4lZgfjss8+kZcuWUlVVpXaEDkaXDTQa0gJRrYl5jBBCCCGEZIACMXPmzOTUhITFZjdiIAwXpszfSI4QQgghhGRIDARpAGzbXJh8+rgwEUIIIYSQDLFAnHjiiVHPeeGFF+KtDwmBLStH8gccKH+sKRXZqEcWJqBLJoNgKLdeUG69oNx6Qbn1wqeR3M5ENE5ZWZmsWLFCBVTvv//+iaob2YbPlSf2gcfIugVrRH5fpIUFAkpScbFbdINy6wXl1gvKrReUWy+qNZM7ZgVi6tSpIT/fsmWLnHHGGbLzzjsnol4kBI5t6cF0UCAIIYQQQkiGx0A0adJEzjzzTHnuuecSVSTZhs3mk1xfieRUbYY9QgsXppp0aNla5VQGlJty6wDlptw6QLkpdyYTswUiGhs3bkx0kaSqQtY/c750FhGXTNDGAmHfln1KNyi3XlBuvaDcekG59cKukdwxKxBz586t9ZnX65W1a9fKo48+Kr17905U3UjQPhDqT/Fpo0AQQgghhJAMUCBOOOGEkJvFIbi6devWcs011ySqbsTAXpPGFThsergwEUIIIYSQDFEgQqVohUJRUFAg3bt318p80zAWiGrxVnMjOUIIIYQQkiYKxOrVq2XvvfeWZs2a1Tq2YcMGmT59usrGRBKH2eBg18SFCVaWsrIK7awtlJty6wDlptw6QLkpdyYTs7ng6quvlpUrV4Y8tmjRInnooYcSUS9iQrmMbXMbgwKhS+f0ePS0tFBuvaDcekG59YJy64VHI7ktWSCQnhUbxRmxDuedd55kZWWFzMC00047Jb6WmqN0B5tDxOcRu00PCwRkdrmcUlXlEY02dqTclFsLKDfl1gHKTblFdwXi7LPPltdff139/fbbb0uvXr2ksLAw4BzEPjRu3FjGjRuXnJpqboHI7ztSijaXSuXPDrFpoUDYJCfHJR6PV6ut4Sk35dYByk25dYByU27RXYEYMGCA+jE499xzpX379smsFwmi6X6nyuoVG6Tspx/EpYECQQghhBBCMiQG4o477girPJSVlcnXX3+diHqREDi27W6ogwsTIYQQQgjJoCxMN9xwg/zwww9SWVkZNpiaJJZqd6k4qkrFrtK4UoEghBBCCCFpokDcfvvt8tNPP8lRRx2lfufm5kq/fv1k1qxZsnTpUnn44YeTU1ONgSvdumcukdyyLdLKcYhsrG4uOshcVQU/QtEKyi1aQblFKyi3aAXlFq3waSZ3zC5Mc+fOlUsuuUSuu+46FTCdnZ0tl19+ubz55psyaNAgmTFjRnJqqjEIxvGJTauN5CBzeXmlFoFIZig35dYByk25dYByU+5MJmYForS0VO04DXbeeWdZuHCh+tvhcMiECRNk9uzZia8lEbE71C+HJhvJ+fe/0BDKrReUWy8ot15Qbr2waSR3zApEy5Yt5b///lN/d+jQQbZs2aJ2oAZNmzZVe0GQxGK328TuqFEgbOJT5rHqDNdwIXOjRjnqt05QbsqtA5SbcusA5abcmUzMCsTee+8tDzzwgPz888/Stm1badWqlTzzzDNSUlKi3Jh23HHH5NRUd7ZZILCRHNBlN2pCCCGEEJLmCsSFF16oNox78MEH1f+Ih3j++edV/MN7770np5xySjLqqT2GWQxZmIAubkyEEEIIISTNszA1a9ZM7Uq9fv169f+hhx4qbdq0kV9++UX69u0rgwcPTkY9ia1G17MLLRCEEEIIISSNFAhzLERxcbFSJKA49O/fXwVSk+SQ02WguAs7SPG8XPU/LRCEEEIIISRtFIg5c+bIpEmT5LffflOuNbBIPPXUUyr+4aqrrkp8LTUH1gbbrmMlx+eTtXO+0EKBgMxbt5aLblBuvaDcekG59YJy60W1ZnLHHAPx/fffy2mnnSY5OTly2WWX+fPdIrXrCy+8IM8++2wy6km2xUE4tkX304WJEEIIIYSkhQKBDEwjR46UqVOnykknneRXIM4++2w5/fTTlTWCJBakBMvLsYvNWykue017Z/pmcpA5Pz9Lm3RoBpSbcusA5abcOkC5KXcmE7MCsWjRIjniiCNCbpgxbNgw+ffffxNXO+Jn05u3y9anz5Q+rpVauDABXWNqKLdeUG69oNx6Qbn1wqGR3DErEI0aNfJvHBfMmjVr1HGSvCxMzm0WCLowEUIIIYSQtFAg4L50//33y4IFC/yfwRKxdu1aeeyxx2TEiBGJriNR+kPNrXJsM/roYIEghBBCCCEZkIVp4sSJMn/+fDn66KOlefPm6rNLL71UKRCtW7dWf5MkYCgQRgyElwoEIYQQQghJAwWiSZMmKlB6+vTpMnv2bNm8ebNyWzrhhBNk3Lhxkptbs08BSRwIVDdipp3bLBDV24LXM1nm8vJKf5C+LlBuyq0DlJty6wDlptyZTFz7QGRlZSkLBH5I8kFfrN7mbea0GVmYfBkvc1WVV3SDcusF5dYLyq0XlFsvfJrJbSkG4pBDDpGlS5cmvzYkJEh2Zd8W2b/Nk0m83uqMl9nlcqjfOkG5RSsot2gF5RatoNyiFTbN5LakQCxbtkzcbrf//+rqatl1111VSleSfBCkntu+hzh3HijFtsZaZGFSMudm1UoVnOlQbsqtA5SbcusA5abcmUxcLkzw76qoqFCKBKkfGg0+VGy93LLyqTkiUiJeTXzsCCGEEEJImqdxJQ2LscMhszARQgghhJCGgApEmmHsA5HpLkyEEEIIISSDXJhI/VP08WNS/tsXMsg1VFZI14zPwgS8Xn2yGZih3HpBufWCcusF5dYLr0ZyW1YgvvrqK/njjz/U34h9QJDIl19+qQKsgxk7dmxia6k5sDZ4PDWdcpsHU8YrEJC5tLRSdINy6wXl1gvKrReUWy+qNZPbsgLxyCOP1Prs4YcfrvUZFAsqEEnAVpPG1bFtHwi6MBFCCCGEkJRVIGbMmJH8mpCIgdNZ2VlSZVIgMt0CAZkLCnKkpMStlbJEuSm3DlBuyq0DlJtyi+4KxD///CN9+/aV/Px8ywWXlJTIggULZOjQoXWpHzHYtoOcXQwFgil0CSGEEEJIimZhOvXUU2XFihUxFYzzcR1JDDZbza2iCxMhhBBCCEl5CwQ2jnvjjTfk66+/tlzwunXr6lIvEs4CoYkLEyGEEEIISfMg6tdeey3mwnXZzrs+cBa2FWf7XaRka6H6nwoEIYQQQghpCGw+mBdIVLzeaikqKm2w74cyhlv19PsLZdZva+WoEZ3lwCEdJJMxZNYNyq0XlFsvKLdeUG69sKW53IWF+eJwWNtjmjtRpwlGh3Rs24paBwtEOj+EdYFy6wXl1gvKrReUWy98GslNBSJNNNrc3Cz1274tFiLTFQizzDpBuSm3DlBuyq0DlJtyZzJUINIA9EX3/I9k69NnSv+ijzVRIERcLof6rROUW7SCcotWUG7RCsotWmHTTO4GVyCqq6vloYcekj333FP69esnZ5xxhqxcuTLs+e+++65079691s+qVav85+y///61jl911VWS1lR7RTwV4hBvzb8ZrkAQQgghhJA0z8KULB599FGZNm2a3HnnndKqVSu555575PTTT5f33ntPsrKyap2/ZMkSGTx4sNx3330BnxcW1mQnKisrUwrI448/Lr179/Yfz8nJkbRm2z4QdqnZQI4byRFCCCGEEO0UiMrKSnnmmWfksssukxEjRqjP7r//fmWN+PTTT2XMmDG1rlm6dKmyKLRo0SJkmcuXL1dWjf79+0uTJk0kU7DZHUE7UdMCQQghhBBCUlSB6Nmzp+UCETyycOFCS+cuXrxYSktLZejQof7PGjduLL169ZK5c+eGVCBggdh3333DlonjzZs3zyjlAVH9Hq8vQIHIdBcmyOx2V2mV0QBQbsqtA5SbcusA5abcmYzlnagBJvawDrhcroR8+dq1a9Xv1q1bB3zesmVL/zEzW7ZsUTtcz5s3T7k9bdq0Sfr27SuXX365dOrUya9A5OXlyYUXXig//fSTNGvWTI444gg58cQT/RmM4sVuD4yMQbMYbRN8zDzJD3UM1+FSBNuEitgPvrbaZ6u1E3WkcuOpk3EM9QmuUmLKrS1rpDb0eLyWvtNqGybr3iSyDXEccgdfH7nc+PthvPcmGW1oyG18Z/L6YWq1YWWlRx0LJ2uy+nfdZK37vTH382T1w1QcI3C/derfRp2Cx7WGHGfrsw3N77FUGGdjKTeea433WFVV7fdYQ8wj6lKu1WvtId5jIunbvxOqQDz55JPy4Ycfyueffy6vvPKKjBo1Sg4++GAZMmRIndJVlZeXq9/BsQ7Z2dlKWQhm2bJl6jeEvOOOO8TtdsuUKVNkwoQJKmYClgecs3XrVjnggAPkvPPOkx9//FHFVaC8iy66KO66QsyCgsA4iqoqj5SXV6kbFXwMbN1aIx/SegVvzFFeXqkeMETs5+QEyo8OWFZWqf42yrXlZosbN8y+XYHIyXGJ01nj2mQA7RcvKHyel5dVazO80tKKbeVmo9SA4yUlbtXBcnKc4nIFdo2KiiqpqPAoOfLzce12cD+Ki1E7UceC+wS+E9+dleWU7GyX5TZEMVu2hG9DtBHaCnVFW4Rqw1D3zXxvQrdhpVRWetXn+N7wbVi7XCtt6HTaJS8vsA3hdldSEr5cow2zs52qHc3gfuO+h2pD871BfwhWosvKKsTjqQ7Zhuif6Ke4n5H7t0scDkfI/h26Db1SWhrYvwHuFcYv1Bf1Rn3wfITu35HbMFQ/NO5NpDaM1r8jtWGs/dtoQ8iC6+o6RpgpLi5XbZnKY4Rxv+syRuAlvXWrO63GCOO+1mWMiDTOpuoYgbYyz0/iGSMM0mmMwFfg3tRljEjUPKI+x4i8vNrvhYaYR9T3GGHbNq419Dwi3jEiaTtRI2bh66+/lo8++khmzpwp+fn5Mnr0aDnkkENk1113lVj55JNPlKVg/vz5AUHOmOjju6AcBFNUVKSsCkbDQAlB/MRpp50mZ555prquoqJCGjVq5L/miSeeUGVBmYjXCoFG37y5rEFWDtTPmt9ly+zp8pe3pdyzuJPs0aeVnHlo77DlprsFAn/j4TAepHRZXaxrGxoDIF7aZje1VF5dTEQbGvfbmKyky+piXdsQhLrfqby6mIh7Y77f+DxdVheDZY21XPO4ZlwfqtxM6d/GtcZkx9zPdbBABL/HGnqcjbXceK6N9B7LdAuE3TSuYc6Yjv07lp2oYwqihqVgv/32Uz9Y/YcSAWUC7kFY/YdV4qCDDpIePXpYKs9wXVq/fr3stNNO/s/xPwKlQ2FkWzLIzc2Vdu3aKdcmo47BFo1u3bqp7EywQkD5iJfgF3wijkUzGxnX5nUeIJ4de8nKOf+ILF6mLBCRyq1bnbZ30IaQNRPKrUsbGsdDnZPp9ybcyyaxdUqdNjQG+XD3O95yrRxLhTYMljtV+mF9lKtD/w4+Fqqfsw1Tt9xEXBvunFSTNZHlVm9z40rX/m2VuIMCYDGAsvDwww/Ld999J4cffrjKqITfVoGiUVBQIHPmzPF/BvcjBGEPGjSo1vmvvvqq7L777koZMCgpKZG//vpLunTpohoFys3kyZMDrluwYIHK2lQX5SFVcGybcDALEyGEEEIISbs0rn///bd8/PHH6mfRokXKCgGXJqvAUnD88cfLpEmTlGWhbdu2Kl4B+0FgMzj4QcJlCe5IUFj22msvde4VV1yh3JxgBcF+ELh23LhxyiyD+Iynn35adt55Z+nTp498//338tRTT8m1114rmYChQETT/AkhhBBCCEkJBcJQGuC6hIxHTZs2VZN97PSMDd5iDapGDITH45HrrrtOKQSwPEABQKYn7C49cuRIFTANBQEuT88995zce++9Mn78eGVxGDZsmLzwwgsq8BpMnDhRWTWgWCCTE9yboDwcffTRks6ULZ8nWz9+TLrktBWRwcq/LtNBQJCOUG69oNx6Qbn1gnLrRbVGclsKog5WGrBXA1yF4MKETEzBkfaZCCbsRUWlDfb9VX/+KO7PHpbSRh3kmr/3lj47F8qlR/drsPoQQgghhJDMIeFB1EiJCiVhwIABcv3118vw4cPF6ay51AheNtOmTZtY60yiYLPV3FCbr0a7pQsTIYQQQghJaRcmxCNgd2hs4hYNxEOQxIFMLbn5OYKMw7ZtO1F7t+1MnckyI48x8hbrpCxRbsqtA5SbcusA5abcorsCgRgE0rDYtu1fYZMaC4Q3ETm4Upy6bFKYzlBuvaDcekG59YJy64VNI7ktKRCxpGYlScJQILYpDjpot4QQQgghJPWwFCnRs2dP+fXXX5NfGxIWm80RaIHIcBcmQgghhBCSxhYIC4maSJKxZeeKvXkH8dibqv+5kRwhhBBCCGkI4t6JmtQfcFeqzG8j+eNukk39T675LMOVOshcUuLWzlWLclNuHaDclFsHKDflzmQsZ2HasGGDrF692tK5TOOaeIwOiSh/oMdGcno8hMFQbr2g3HpBufWCcutFtUZyW1Ygzj//fMuFMo1r4qP6s7OdUlHhEYehQGR4JzXLrJMLHeWm3DpAuSm3DlBuyp3JWFYgzj77bNlpp52SWxsSEmQFsxWvleI37pRCR66IjMh4FybInJXllMpKPIiiDZSbcusA5abcOkC5KXcmY1mB2GeffaRv377JrQ0JT3W1+Io3iD2rQAsLBCGEEEIISU0YRJ12+0AwjSshhBBCCGk4qECkCTZ7zT4Qsk2B0ClQhxBCCCGEpJkCcccdd0j79u2TXxsSEvjSVXkMhaHmtzfDHewgni5+hGYot2gF5RatoNyiFZRbtMKnmdyWYiAOP/zwsMeWL1+ufrp27SqdO3dOZN3INhDNX1Hp3faPHi5MkNntrhLdoNx6Qbn1gnLrBeXWC59mclt2Yfr888/lkEMOkRdffNH/2V133aU+u/jii2XMmDFy8803J6ue2uNwbtP1qr3auDA5HHp62FFuvaDcekG59YJy64VDI7ktSTp37ly58MILJSsry29l+O677+TZZ5+V3XbbTaZPny733nuv+v3mm28mu87agc3jcvNzxZZfKJLbWLo414rP583oPMOQOT8/279xni5QbsqtA5SbcusA5abcorsL09NPPy3Dhg2Txx9/XOzbsgG9/PLLatMMIz6iR48esmzZMnnttdfkiCOOSHa9taLqj3lS/N1L4istEnTLCxp/Kpu8eVL5RxPJ7jyooatHCCGEEEI0wpIFYv78+XLUUUf5lYfq6mr5/vvvpUuXLgHB1YMHD1ZKBEkcVX/Ok7JPH5bqkqKAz5vay6RyxiPqOCGEEEIIISmlQBQXF0thYaH//yVLlkhJSYnsvvvugYXZ7Uq5IInBV10tFd+9FHbHQ1Dx3TR1HiGEEEIIISmjQDRv3lzWrFnj/x/WB7gvDRkyJOC8RYsWSYsWLRJfS03xrl0ivtJNEc+BWxPOy0QyOcYjEpRbLyi3XlBuvaDceuHTSG5LCgTiH1544QUpKytT1ohXX31VCgoKZM899/Sfs3nzZnVOsFWCxI+vbEtCz0snkGWquNitRbYpM5SbcusA5abcOkC5KbfoHkR93nnnydFHHy177LGHsjyUl5fLDTfcINnZ2er45MmTVfalrVu3yllnnZXsOmuDLa9JQs8jhBBCCCGkXhSINm3aqBStsDxs3LhRRowYIXvttZf/+FtvvSWtWrVSigR3rE4cjlbdxZbfLKwbE3Rce36hOi/TQBq0vLwsKSur1EabB5SbcusA5abcOkC5KbforkAYcRCwRITbZM7I0EQSh81ul+w9jhP3Z5NrHVNudjaR7D0mqPMyEV37FOXWC8qtF5RbLyi3Xtg1ktueyAb79ddf5bHHHktEkWQbrk4DJW//C8ResD0LFthcnSfuIWeo44QQQgghhNQXCVWVfv75Z3nwwQcTWSSBErHzQGl15iNi32En9f8XVbvKTVvGSWXrXRu6aoQQQgghRDMsuzCRhgVuSo4d2ouvyi3rNjYXn9i18LEjhBBCCCGpBRWINACKQllZheSMOEP9v/TRWdhCTrwZrEAYMuumJFFuyq0DlJty6wDlptyZjD7RHmmOx7N9t2nHtpiTTFYggmXWCcqtF5RbLyi3XlBuvfBoJDcViDTAhmxL2U7120gVBjJZyw2WWRcot2gF5RatoNyiFZRbtMKmmdyWXJiuvvpqS4UtX768rvUhIcDmfdnZLin9/Wup+PUTGSE7yCvSJ6MtEIbMVVVerbaGp9yUWwcoN+XWAcpNuUV3BWLOnDmWC2zdunVd6kMi4Kssk+r//pZCW81t81brYyojhBBCCCFppEDMnDkz+TUhUbG5ctXvbKnKeBcmQgghhBCSmjAGIo2wZeWo31lSqX5nsgsTIYQQQghJYwvEiSeeKDfccIN07txZ/R3NB+z5559PVP0IXJd8IlVVHhFnoAXC6/VlvMwauBEGQLlFKyi3aAXlFq2g3KIVPs3ktqRAmINBogWG6BA4Ut+gTcvLq0RcgRaI6gxua7/MmkG59YJy6wXl1gvKrRc+zeS2pEBMnTo15N+k/kDqVu82FyaXTw8XJsisY5wH5dYLyq0XlFsvKLde2DWSu84xEEVFRfLrr7/Kpk2bElMjErJDFhTkiCM7X2zZBVJhhyuTL6NdmAyZjT0vdIFyU24doNyUWwcoN+UW3S0QYMWKFfLWW2+pGIcjjzxSOnbsKA8++KA8+eST4vV6xeFwqM+vv/569TdJPPb8plJw0mR55o1fRf77L6NdmAghhBBCSBorEHPnzpXTTjtN7Ha7ZGdny0svvSTnnHOOPPbYY0pp6NOnj8yfP19eeeUVadOmjZx55pnJr7nGOLZpt5nuwkQIIYQQQtJUgZg8ebIMHjxYHn74YcnNzZVJkybJ/fffLyeddJJcddVV6pxjjjlGGjduLO+99x4ViCTjcGxTILzcSI4QQgghhKRgDMTChQtl/PjxSnkAJ598soo232uvvQLOGzlypKxcuTI5NSWK8s8fldFFU6W1Y5M2gTqEEEIIISTNLBDFxcVSWFjo/79p06bqNywOZrKysqSioiLRddQeKApbt5arv71Fq2SHqnWSb6uQNUVl6lgmBuyYZdYJyq0XlFsvKLdeUG69qNZMbstZmMyB0QikNv8m9cOPS9bLyiKP+jvHViVf/bJaLp/ynfqcEEIIIYSQlE/jSgWifoCF4be/Nskjb/8mJV6HX4EAm4or1OeZpkRA5vz87Iy0rkSCclNuHaDclFsHKDflzmQsp3G98cYbpaCgIGC3aaRszc/P959TUlKSjDpqD8xiz3+4SP1d4XOp3zm2ms3kDF7+fJn079oiozquw1HnbUrSEsqtF5RbLyi3XlBuvXBoJLclSQcNGqQUBSgOhvKAz/Ly8vyf4QfnDBw4MNl11o4lKzfLxq1u9bd7mwKRbatxZTIoKq6QpSs3N0j9CCGEEEKIPliyQEydOjX5NSFh2VKyPTDdUCAMFyYzm0sZwE4IIYQQQpKLPraWNKZJQbb/77LqbCmtzpJqqe2q1DR/+3mEEEIIIYQ0aAwEaTi6tWsihY2ylZvSx+5d1U8wON6tfU163UwALnHl5ZV+lzldoNyUWwcoN+XWAcpNuTMZWiDSAGS7Gr9f14jn7LVra/lh8TpZ/HdmbDCH56+qyqt+6wTlFq2g3KIVlFu0gnKLVvg0k9vm00VVqiNeb7UUFZU2yHcjW67L5ZDvf1sj0z5bplK3Gricdsly2qXUvT2oulmjbJmwX1fZrXtLSVcMmXV6GAHlptw6QLkptw5QbsqdbhQW5lvOJEUXpjSxQOTkZMmgHjtK3ybFsuWbl2WNO0fuXzVAqjzV6seMsTfE2OEdpWVhnoqNgHtTOqV4NWT2eNzamAMB5abcOkC5KbcOUG7KnclQgUgzbN4qyS5aLu0Kdox67vRv/8ooqwQhhBBCCGl4GAORZtiycmv+qCqP6bpM3bGaEEIIIYTUL1Qg0gybK6fmj8qajeViBTtWZ0KQNSGEEEII0VSBqK6uloceekj23HNP6devn5xxxhmycuXKsOe/++670r1791o/q1at8p/z0UcfyUEHHSR9+/aVsWPHyvfffy/pjsfjrfljmwXC6asUm8SuCCAV7PRv/kiLbE1+mTWDcusF5dYLyq0XlFsvPBrJ3eBZmCZPniwvvvii3HnnndKqVSu55557lDLw3nvvSVZWVq3zcfzXX3+V++67L+DzwsJCcTgcMnv2bDn99NPliiuukGHDhskbb7yhyp8+fbp07tw5LbMwBdSjslLKnjtT/X1F0bFSIbXbyCqMiyCEEEIIIbFmYWpQC0RlZaU888wzcuGFF8qIESOkR48ecv/998vatWvl008/DXnN0qVLlcWhRYsWAT9QHsCTTz4p++23n5x44olKYbjyyiuld+/e8vzzz0smsGx1iXh8Nbctx1ZVp7IYF0EIIYQQQmKlQRWIxYsXS2lpqQwdOtT/WePGjaVXr14yd+7ckNcsWbIkrCUB7lA//fRTQHlg9913D1teOoD0q40b56rfm8sqpcyXJaXVWZJlS4yp7JkPFsl3C9aklFuTWWadoNyUWwcoN+XWAcpNuTOZBk3jCksDaN26dcDnLVu29B8zs2XLFlm3bp3MmzdPpk2bJps2bVJxDpdffrl06tRJtm7dKmVlZcoVykp56Qj2dLh+81HwPktYmeWVXnnqg0Xqb7o1EUIIIYSQlFUgystrUpEGxzpkZ2crZSGYZcuWqd8I27jjjjvE7XbLlClTZMKECSpmwuPxhC2vomL77s3xEqxVInrECCEJpXEaq/mhjuE6XIqdC7H5SKRrjevxu3v7JtK/cZHY3Ftla3WurPC0FF8CDUmGW9P543aRgT1a1qpvPLIax0LJGq4Ng8uJtVyrdarrvQlfJ5u6PlS50a4NdTxyufH3w3juTTLa0Pw7EW0Y+71pmDY0SFY/jFRuPNcm6t4E3/dk9cNUGyPM/+vQv8M956kwztZHGwaf19DjbKzlxnNtpPdYfc8j6lqu1WvtId5j6dy/00KByMnJ8cdCGH8DTPZzc7ftd2Bi4MCBKqNSs2bN/DcEQdiIn3jrrbfkqKOO8pdnJlx5sYCvKyjYXkdQVeWR8vIqdaOCj4GtW2sUpNzcrFpBKeXllWq7c2x7jp0Lg6P4y8pqZEC5+G5cb//3Fymb8Zyc7CwSKag5d5M3T94qGyS/VnWQRPLyjGWy54D26u9FfxXJhqJSyc92Ss+OhbVkRacrLq5JK5ufn13rYSktrVBB6FlZTsnOdllqQ0Nmg1BtiDZCW7lcTsnJcYVsw1D3zXxvcJ3TWRM/Y+B2V0plpVd9ju81AzkgDwhVbkmJWz2kOTlOVS8zFRVVUlHhEafTLnl52bXc70pKasrFcbSj+Vk22jA726na0UxlpUfc7tD90Hxv8vKyxG4PbsMK8XiqQ7Yh+if6Ke5n5P7t8scgBffv0G3oldLS7f3bfL8h99atNbt4oj54PsxATsgbrQ1D9UPj3kRqQ6MO8bRhLP3bXCcAWYPLjWWMCKa4uFz1n9D922hDh5InfP9GO9hi7t/R2hDH0B7GeahnPGPEtpJVf0mXMQLfZTzXdRkjIo2zqThG1LRh4LgW6xhhBvVNhzHCGNfwU13tjWuMSNQ8oj7HCOBw2Gq9x2IZIxIxj6jvMcJmeo/h3jTkPCLeMSJtFAjDdWn9+vWy0047+T/H/wiUDgWyLZmBYtCuXTvl2tS0aVPJy8tT15vB/zvuGH3n5kjgITAeDPNnADc6+JgZdKTa5dVcjIcf256HA+XiobCt+kWK3g3MPAWa2svk1IKv5JmSvROqRGzc4pZ7XpwrS/7ZLMVl2zsVXJyOG9UtwDphxngoQmm96KCQ10obQmbzwxGqDY1yMXiES50W6r6ZqXlgqkLeG5QZ6dpQx4w6ud0e9ZCHLrc6Yrk4jpe2eSXH+Btloh2DZTTOiVSu8TKx2oZGuah35P5d9zY07jfkNq7FvcFgGU8bRuqHkdoQg2u8bRhL/w4GsgbHH8UyRgRjfG/d+ndFXP07Whvi3pjvN8pMRBumwxhhHtfqMkbE278baoyoUV7sAeNaXcbZdBkjjPuNa5LRv1N1jIDcXq+v1nssljEiVLmpPkbYTeOacc8bah4Rb/9OGwUCWZcKCgpkzpw5fgUCcQwLFy6U448/vtb5r776qkrf+sUXXyhFAZSUlMhff/0lRx55pNK2BgwYID/88IPfGgFQPqwXdSVSgHG8x6KZjXCt1+OV0hnPhjyuVrRE5Ij8ebJgc/uEujPNW7whpIvT5LcWyHmH9wkZJ1FXWc1/GyskiSw3WfWtfe12M2Ms1wbLnahyU70NQ8mdPFlTqw0j3e+6lBvtWEO2Ybh+3tD9MNnlBsutQ/82Jjvh+nlDjLP1VW6o+50K/dDqsXivjfYeq0udUrkNq0O+x9Kvf6dFFibEKkBRmDRpksyYMUNlZbrkkktUEPT++++vzJgbNmxQsQ5gr732UiYa7PGAeIgFCxbIBRdcoKwS48aNU+eccsop8sEHH8izzz4rK1askLvvvlsWLVokJ510kqQr3rVLxFe6KexxGKma2kuli3NdvdWpPna0TkQHT0cot15Qbr2g3HpBufXCp5HcDb4TNfaAgPXguuuuk/Hjxytf6qefflpcLpesWbNGhg8fLh9++KHf5em5555TmZZw7sknnyyNGjWSF154QQVKA5x/++23y8svvyyHH3642ljuscceq9Mmcg1Oee2A8lCc2/RLOazJb2KTGtNZMsGO1ktXbk5a+TWmwCxt0qEZUG7KrQOUm3LrAOWm3JlMg+9EnS405E7U1WsWS+l7d1o+3+vIlk07DhRf236yMae9vDJzhXI9SjRnHtpLhvQKTJmbKIxgKCOYSBcoN+XWAcpNuXWAclPuTN6JukFjIIg1HK27i72gUKpLiqyd762Q5qtniayeJS3zm8kdoyfIH85esrm0Qu0jUVxeJa/MWFZnpWJ9UU0GAkIIIYQQog9UINIAm90uTfc9OWQWpmggdqJixiOy837nSlavwf7Pd+vWQrkgzV2yTr74aXVc9Zr+7Z/StkU+N50jhBBCCNGIBo+BINbI7ba75O1/gUh2flzXV3z+qLhnvSSe1YvEV12tTG09OjSTQd3rlt72hY+XqAwbhBBCCCFEDxgDkQYxEEjVis1EkKe4cuVCcX94d93Ky28m2XscJ65OA5Wf3uVTvquTO1OjXJecOLp7Qi0RZpl16qGUm3LrAOWm3DpAuSl3JsdA0AKRBqAjYtMP/Ha26aEUgDqVV7pJ3J9Nlso/flCWiAn7da1TeYipeOTt3+THJYEb+CVKZp2g3KIVlFu0gnKLVlBu0QqfZnJTgUgTjK3mEQ8B60EiqJgxRapWzFWWA2wMh12mzeS47A26N4Qhs25Qbr2g3HpBufWCcuuFUyO56cKUBi5MoVKDVf05T9xfPytSUfc6uXYbK9n9DxWf2FRgtZGtqUvbJnLl49/H5N50xfj+KrairmRCOrR4oNyUWwcoN+XWAcpNudMNpnHVAMQvODsMkMqf35XK3z6rkyJR9eN08Sz+Slk2enQaGHAM7k1wT7IKlA9CCCGEEJK50IUpjVHuTLuNlYITHpbcMVeKs/cofFrnuAgzhntTQa7LUjmwXNQVaO6L/t4k387/V/1OV02eEEIIISQToQUiQxQJZ5ue6qeqVTdxz3ikTnERNp9NXJ0HBSgRu3ZuLhMfmaUCpsNR2ChburVvKnUBgdjTPg/c5A6xGbCEcL8JQgghhJCGhxaINIrBsAIm/jmjzo8/U5PPpxQQ94/T1X4RBk6nXaVqjcTwvq3lh8XrZHGcVgMoD3CXCo65wP+JzvKUCfc606DcekG59YJy6wXlznwYRJ0GQdTxgMm/d+0SqfrzJ/H8/jk+qdN+EZEsBKHIz3HKqIHtZMwenVRgUTSs7EcBC8fd5+xhqTxCCCGEEGId7gNB/G5NucOOk5yR5yYsLgJuRMeOjL5vRKnbI9O//UsueugbS5YDZH+KppQUFVeo8wghhBBCSMNBBSINwIp748Y5ca+8G25Nkte0TvtFGJaCV2Yss3wtFAkr7kdWszdlepanut7rdIVyU24doNyUWwcot010gEHUaYMtcWlff5weV1yE2M+XFfbOMe0LYfDCx0uksqpaBUQj0Dr4AbOavSkRWZ5SHz0Gn9pQbr2g3HpBufWCcmc6VCA0TPtqa9ZGWRVi3W/d/c3zsnnAlXF9N7I3Pfn+QvU3UsIO7b2j9O/awq9M4DeUi2gxEOGyPMEyYt4EL5SSQgghhBBC6g4VCA3J2nmwStUac7pXd7H0mHuH9HUNlF+rOsT9/SXlVfLZvFXqB0rDsSO7SKPcLBnYvYX6LBzj9+saoBQYSsPPyzbI7N/XBaSYZepXQgghhJDkQAVCU9Q+D/bzxT3rRZEy64HJjqpSObXgK3mmZO86KREGsDhMmf57xHOyXQ45fUxPvzIAxeH97/6Sz+atVDEW4cpF7AU2wTMrEbRUEEIIIYTUDaZxTZM0rpjkJmNHZqR7jTUuArXY7M2Vm7YcIb4kxuF3bddElq3aIh12LJAbThmsPkMw9nMfLQ6rOERK/RpukzrDApIqSkWy7nWqQ7n1gnLrBeXWC8qd+WlcqUCkiQKRbJCqNda4iJneAfLOlj5Jq5PNVlMdzOVPGt1DWQ5m/bY25nKuGN9fSt1VyiJhhXjcn2jZIIQQQkg6QwUiwxQIm80mOTlOcbs9kszbhVStscZFbOx3iqwt6C6Nc7Nk6arN8skP/0hFVWrtxHjqmB7y+owVATESVgh2fwqnLKBcpLY1WzZCBYpHA+XC4lJa6ZH8LKeywOiihNRXH081KDfl1gHKTbl1wJYBclOByDAFApPIgoIcKSlxJ900VvXnPHF/85yIu8TS+bb8QskfP0lleAIL/yqSSa/8IqmEy2mXKk/sSk2zgiy559xh/km8ldiLeC0aodyr4lFC0pX67OOpBOWm3DpAuSm3DtgzQO5YFAgGUZNa+0U42veT0mmXqKxL0fCVFolnzSJxte2t/u+xU7Oo6Vjrm3iUB7CppFLem/WndN+pmcr09M2va8Rd6Y29nDAB3WblIZR7VXC2KmaVIoQQQkgqwJ2oSS3sTqfk7HmS5fPdnz+qLBfqWrtNTXQzhXdm/SV3v/yzmsTHozyYefnzZeLxVMvivzfJ7IVr1W/8D8uDVSVk7uJ1daoDSW2wamXuH+m6ikUIISSzoQuTRXRxYTLj/nG6VMWQnSln1PnKghHOJSfLaZNKj97dLTfbIeUV2xURl8MmVV5fTIHlZx/WRwb1aJlRQdyZYPqtq9xzF60LmSUsEy1PvN+UWwcoN+VONxgDkXFB1CJZWU6prERgTv19L1K8lr48UXylm+KKhwg1qf1x6QaZMt1aNiQSHsMdKlxcRiwTz3iUj0QrLA3VxxsaQ+7vFqyWR94K/1yEc39LV3S/35RbDyg35U43qEAkgUxP4xoxqPqzyZbPzx1zpTjb9Ix4TijrRF3IdtlTLvNTsmmU65LjRnWTqZ8uiRjQPXZ4RxmzR6ewk/twe2OEUj6483dyQLtePuW7iM8D7ve95w0Tp5Nep4QQQpIDFYgMVCBwQ1GHBlMivn5WpCK6/Dn7ni2uLkOinmclFaoVMEHu0q5pymV+SiXyc5wyamC7WopEuODtUMqHVaWvLivlDdnHGxKkP77zxZ+ingcl4sTR3TNGSdP1flNuvaDceuFIc7mZhSnDwAQuPz+7wfzqVFyDK0/cH94d9VxbXhPLMvXo0Czgs926tfArFc0a5QiMCs++/3vISSt2mB6/bcUbbZJqmZ9SCVgopn8LN6dVcvKBPfxtFi14G9d8NX+N7N6zpXz8w0pL34VdwrNdDnV/t5ZV+t2bAO5tUYlbSkqrpFFeljTJzxKxiToP93tAz1ZSXlYRdx9PxzgQ1M9t0XoGJTtSNq+GJpb2b+gxraGg3JRbByi3Wwu5qUAQSzjb9BBbfrOI8RCIgXC06h73d5iVCiMYqU+HpiobjXniCWXBPDkxMj9Z3WlaZ0UCbQTLwsr1JZYULpxjVXkwvuO+1+bXsoAYxyKxQ+McOWZkFynIccWsBMTiipUIEqmsNG2UE3M2L+wNkkrKUX23PyGEkIaFLkwW0TELU6zxEOYsTA0hczQ3mxyX3fJqr9XYC+w8aU7vCjeTWHe8JuExT0LDTdqjuWIlesU+kZNl1D83L1vOvmtGTBa0K8b3r2XBayjiaf9UGdPqG8pNuXWAcrvTVm66MJGkoJSDUedLxXcvBVoisvPF1WeUODsMaMjqqUkKVmaNSWbj3O0uMphwdmnbRK58/PuIE7WcLEfE/R5wfM++rf27QwPzpNbKdxDrGPtfjB7cXuYsWl9r0n7syC7yyozlUVfsd+3cXJb/u6XOFoNwk+VomwVGwmG3qYD4yW8tsHwN5EgFrLjCpaLFhBBCSN2gBSJNLBDwqystjd8/PNHpXSt+eleqfv8sILAaLk7ZexyXECtEsmS2sloKgleYwwUix/Md0dihSY6MH9lV8Gg+//EStSM1qRvBlqFgi4EVlyRs+jfxkVkRLUyIzbn7nD0sT5aNfl5c7JY3vlwhH87+O2YLREPGfsC9EBstxmoxSbUxrb6g3JRbByh3RdrKzSxMGZiFKZWoT1emZBDKBcUclJ2ISVkkdypYMfDUVVQFuj4N6b2j37JhfJeVSSuJH8SDtG5eUCsDWNOCLBnRr420LMzzZwmb+ok1Ze6yY/qp+2e178Sa1tispDR07AF2zH7i3YVRzzvz0F4ypFerpNeHEEJI/FCBSAJUIKxvLhe8oVwqUh+rtsZ3hAoAB1a/v64WDVK/wFpldVO/eO6t4SZV37EfibRAEEIIST0YA5FhpJJZzLt2SdSdqX2lReq8aBvKNaTModLI1vd3hDoWSm5MAjEZRIrUaJvGYTX9hY8jn0eSS3DbB8dHmBXLV6PEb0RKXRwt9uCFj5dIZVV1raxliaS4vFLtvhppGarQpDQD1H3Zv1tUQgMkNujatok28RGpNJbXJ5SbcuuAXTO5qUCkCcj2kwr4yrZYOq86ipKRTjLXN6HkNgLE3/8O+zmsDJikBrtfYT8NnPfxnL8TmnWK1A1M6L1en7z6xfKYg+x3bJYrt50xxD/RhgISrQy4XT35/sKEujWZlZ9Ff26SWb+tjXoN+qZR74Z2uWpoa6ex34mOcDzXC8qd+VCBIDFhdaO4iu9fFpszK6VjIdINTMIOHd5JxuzRMaL7k3HeQUM6xBQ/0awgSyo91bReJAnch8fe/T2ua//b4hZvdbXY7Q71f6wKiNkKYs5UZnWjv3h2i8d79OzDtrtRJSODVTzUZ9B5KIUJ+51AqRrQrUVSvpMQQuoDKhAkJrBRXLQN5RTu4ppA6xQPqE5HrLpfOZ12OXF094h+8sFpaX9etiGqT75h8QDBk6O8bIfAchspFW5ulkNd/9oXK5hhygI5WXZxV1bLR3P+kS5tmsjSVZvl0x/+iassuMGFyjAGEq04wq0JyQFSKd1rfVpAwilMG7e6VcreVN1RnBBCrEAFgsQEAqORqjVSFiYz7lkvqf0hUjmguq5B5SoupGyLss4oBSuFZDXiJ6ympQ13frgsUaFWs6MpIace3FN9T262k8HhUSjIcYp7W7au6d/8WefyoCQEKwrJtDgZ+1VYcbkqKq5Q5yUrNqk+LSCpojARQkiyYBamNMnChKh41CGlUrl+85yIuyTqufaOu4mzVTex5TYSe36zsJPs4Ml4VtueajU7VUEbBG+ql4i9MJJxr2N120hGGtvgWI1w55HMYczQDtKrY6G6v0Y8RiT2GdBGBnXfMWx/i7df4rrLp3wXsZ9F2sMj1u9ldqrUf4fVF5RbLxxpLjfTuGagApGKVC77Tiq+eCLm60JNspM1GU8W6b4XRn1gddJlPm99UblM/7buK+3pTkGuU7kAZUo8SkGuKyZ3tVBuRdHcjyL1t7pM6ONxe+L+GISQdIRpXDMMBCNmZTmlstITMV1ifQNrQjxASTDHR4SbjAeflyqonbi/eyniORXfTYvLdStV73UyYzVwXs+Ozfxyt22Rr71VYq9+beTD7+OLc0hFYo11CXYriuZ+1KN9E1m5oTRA4YLSMnSb253VvmS4XNXV7QkKjBUa52Yp5aYhdhGvTzJpXItEsBLbfaemkpPjyni5db3fustNBSJN0oJlZ7ukqsorqWQwshxQHWGS7WjfL2mT8XTcCyNV73WyMcttpKw1XsSYZCEbEP5+9fPllrJKwR3lmJFda2UNqvm8i6z5r6xWOlyroIzBPVvKxz+slGRhh8BExQns2rl51HiCxSu3hFRaPpu3Sv1gvwkrrN5Qqib0RmaqaN877bOlqn7L/90SoATgB1aKaIrLlHd+s7zpYH1nkIqFaPWyMq4lWrb6bqtwlqrTDuktfTo203Y8z2S5q4P6GBbMdJDbgC5MaeDChEGvoCBHSkrcKbc5STRXnmhkDRkvlbNfjnpe7pgr67QxXSKpWj5b3DMfi3pezr5ni6vLkIy518nEqtxWd24O3rQt1CTCOLbw7yJ5/7u/LfvzG2UkK34DCgoCzSe98ktCy01XhvTaUWYvXFev34mJH7KTvTvrr6jnIpOZOeuYoQSAeJMEYFNIc4ID9NVQe8CYrSyxTJBDPRcgVpfDcOl9g5M0hHu+jbKQeGH27+sCFgfqkh2rLu5u8RBtXDp/3C4qbW+qKoCJJtXeY8lo9x+jKIypIHc8MAYiCVCBCI/7x+lS9eP0uK61t+0l1f9G9xXO3udM5TKVCtmOPKsXSfn7dyVF6Un1e50sYpE70sQ9VKB2MgNszS8mw1Ji3jchnl3Bjb0aotUplUD77Na9hVrtJ9uVAKRs/ebX6JvthcKsiETbhd58frS+H+r5CZXKN9SEO9REPxIo9+QDe8ignjvWer6tKuCxZseKNpkfPbi9zFm0PmGpfC2NH42z5Zh9a1tEw2XDSwR1URLrYzyvL2WqrqmbQ9UzWqbB87cpjOkIFYgkQAUickxA6csT43ZlsoTDJeKtiivAOtGpVq3Ia8svlPzxk2L+nlS/18kiVrnNOyIbG5/hpRDvSyjapCPeFJ9hV45znAIpI+0obtXaYqyCO+y2eg26HtZnR+nVcQd/u+N+WAlU1gmnwyYeb/0+x+E2CzSsZrFaRQb3aCG//7WpTn1r7J6dpGObJpLttEvXtk3kx6UbZMr03yxvcHnPucMSlm3LiuUHWJ3cWg3QjwRc7PbctY3fkhTp+61MvONREhM5uY82nidjUh+qzaJN9KON66Hq2bQgS6qibLha2Dhb7j57+4JTOlmeqEBkmAIBf8Ic5IN3IzAnNW9XXV2Z4sXRbZg42/b2p4cFZmXBV14iFbOnWcruZFY0JLeRiM8m4t4aUulIVhamdLjXySAV5LaaejYewr3wlq3aIqWVHsnPckrXdk0sTQTMmFcwQTTXEitY3egv2CpT18kbSQyhlEm4Ou3es4V89/s6Ka8Iv8ljfZDtsktFVWxpLscO7ySHDq/p45FY+FdRnV3/UD+MR6Hc0kKNAy9/vjShlrdQ3288562bF9R6poPrFo+SaJ5Ihxpz0H+G9GopzZvkWlqsQf2zshzy67L/ZFOJOyYlNp5JfSjlyNJEv1G23HnW0FoxTPEq28EZ3VCWFddDkCoKBhWIJMA0rtEJlYq1XnHl1Gx/67E2gXHtNlay+x+qFINodQ+ldJS+c6tUr1sedF6hZO8xIaWyRhHrpOJKUSQ3Kav7ecTieoIXeH6OK660p3V96RISz8TSsPR9NOfvmJWTWDh0WEdpWZjrt3pu2Jw6aadhPTloSEe58vHvY1bisVHovecNk/kr/rP8/EZyvwqnhFhRYpM5qbeSYjpRyvaoge3ku9/WRrXcxWIZqg+oQGSgAoGHJx1cWoxVfM9fP0vV8u9F3MWSykAxcHQeLJ5fP7F0PiwLyAgFGauWfC3e/1aKveXO4l32ndjymkr+sXfXOTYjXe51oqHcySV4v42v5q8Oa22pyz4G7377h0z/NnrwMSGxEMriZSjH3/y6JmDFXlfgCuWOU4EK5VYZS5xLXSwgVhITHDuyi7wyY7lWFs5zxvaWQT12rNfvpAKRBBgDEb8yUfXHXPEsnCkZQVa+2FxZgS5ReU3FV7YZf0nByY+KLSs37riM+rzXiY4N0bGPp7Pcydp4zaor0w5NcuSYfbtIQY5L5i5ZJ1/8tLqOEpFMJ5JbCGl4C0jzZrnyyufLY973hYTfV+Lsw/rIoB71Z4ngRnIkJcBk1MhClDEKRGWp+CoDFcka5UH9JRXzPxRHmx5R4ydquUvlFIiz81BxNm4utmY7SJU9T6qxkBSmjLqSbjt/k/rd6M/KPgZYETb8d4PLhfk90iokgmrHH9BTyssq/IpTohQI+JCjSPg/k8ziw9l/yUNvbqW1IQWh1THxYHkfyQbscSbxSDa0QFiEFogUz9KUyuQ1FVfPEeJo0kqqt6yVyjhS3iZycp+sAHCd+3gmyp2MYEfDTSo4rWe8AdjGqmdwJi7AVWpCSCZQGCaNeFK+iy5MiYcKRHpmaco06jq5T2YKWt37eCbKXdfMVOHcpELJHYvvtNU6hApAX/zPJksbB4bisGEdVYaZhg6cRZYlbwr2F0JIcgjlLpoM6MKUkaT3y0JNekedby1LExz/qNeGxD3rJRXEHe/kXsU8RGl/X2mROi/SJnjJiZ9I7XuevJiR1JUbE/RwewrU1U0qWG58F6waoVI07rdbW+navpnl7FORvr/HTs1k1oK1MVs7sB/BIcNqMs60bZFfq551CWCNNWMOYkee+mBR0r+LEJIabC5NveBxKhBpAFbRtm51S7oDJcLIYFRdukl85cViy22kJmPmmAF7y65SvX5Z2mRyqlfKNkn5jEclq/fIuCawao+LOp6XjPiJVO/jyYoZSXW5oysBiZW7rgqLFazEaIRiwqhu/nqEqmeXtk2iptCE5eSYkaF3JLbqanXi6O4qzW5DEGk/Asi2166t4/KFR+rMk0bX7OPz/MdLMjoIF+tjZx7aW5rkZTGDFLEMxphUgy5MaZLGVVeCV31DbQynLdn54uy9nzixgZ4p2BqEWyn3rF4k5e/fFbXo3DFXhrRApGL8RLLRUWYdCJmr3sIO4YmIGwl27ar2+SxtgGZsqGY1ZiTUpmSRZLWyS3Ok3XVBLLEsofYS+P63tfLk+9FTCFtVTE44oJus+a8sottZInbbjjc1Z7jd6pHK1Ov1SZWXyQAyBZfDJlVx7EzPGIg0p6FjIHJzs6S8vDIl/aTrW2ZDqTCsGNUl/4ln+WxaKoAzu2aJq8odcqW82uOR0mmXRGyr4BgIc3tXfP9yTNemex9PdsxIsNyplFo3maTK/Q43Ca6LBSRa8Hgoua0oBHChuufcYf66RFNWjAm/IU9RiTtksHm03ctj3Y3dSr1aFuaFbVurKYRjVUyixfME94V4d3IHowe3lzmL1sfUjqH6Ij6b+MgsSxtARsPc7j8t2yCfx7B7tpGsYNGfm2TekvVJ3awvWUCGHXfIlyfe/b1BPKTH1mGjv2gJK7RVIKqrq2Xy5Mny+uuvS3FxsQwaNEj+97//Sfv27aNe++6778rll18uM2bMkHbt2vk/33///eXvvwOD5A4//HC5884701aBSOVAy1SQ2YqlAhM9R+dBUTeNw3nOzrtL1a8fSSbh7HuAeFf8ENV64xpwmDiato7b4hPOepGOfbyuFptY5K5YMVeb1Lqper8TRSzB43XJelXXIPdY6h4LoeqFfT/Gj+wqA7q1iPr98WTkwor9nn1bK9eycHWOVbZQQfj4G4rYf1vcMmdh4O7uwQrJsn+3SIWnWrKddunatklcK8jR+kU060mo/mBVSWuU55ITD+gecO3Cv4osWcuiZUyrr128g+Wfu3i9So1qpZ75uS55bebyiApcYRi3xHDfn4xkEdoqEFAeXnzxRTW5b9Wqldxzzz2yatUqee+99yQrKyvsdf/++68cdthhSukwKxBlZWWy2267yZQpU6R3797+83NycqRRo0Zx15MKRPrJHG41N/Q+DI3E1WWoODv2j3weiYqr/yGSvdvhMa2cp+oGelXLZ4t75mNRy8zZ92xxdRkSc10MuYvmfy3lnz1i2U0q3S0VOo5pVuSORyFIxIQ/GZjr1axRjgzo2Spg349IxDLJCmVtqC+itX2i+nks1hND0YmUcMCKkoZg/XvPGyZOp71OCl6k/htKLnzvkN47KkUwXitQNCtXLM9ZLG6Jy/7dIuWVXtmwqUxtkGlY+qx+/zEju0ij3KwGfZbTRoGorKyUIUOGyGWXXSYTJkxQn23dulX23HNPue2222TMmDFhrRbHH3+8uFwumT17doAC8euvv8pRRx0lP/zwgzRp0iRhdaUCkVkyW52AGeeFCuiGpSJ7j5p+S0UjkHAr5w29A3eswdD1YYGw//uzbHr/wYiZx8xuUpmwCaCOY5pVuVNVIajv+x1qkhVqkpnK7ZPIfp7oflGXfV6iXXvY8I7SqW1TS5aXaHIZx4Nd8OrqbhdLe1pVOOwx3O9Ufc7TJo3r4sWLpbS0VIYOHer/rHHjxtKrVy+ZO3duWAXisccek6qqKjn//POVAmFmyZIl0rx584QqDySzd8m2ch5+soccG1bpMLJL4Zh38xqp+umdmOrj2m2s2Bu3FJu7WLKbNpOtX0wVXxrHdGByi8Djqm7DxNm2t9jzm4V2h6rHHbjDBUMbdUWa4eAJuPp+1D1KDIQRvB5znf6YJ2WfPhz1PCO1rq+iNGYZSHqRjKxX6Yg501Wo+I1UmGylc78IlzbZyiQ82rXBG0XWRa5Ix3frlqwU08nPDmfPgOe8QRWItWvXqt+tW7cO+Lxly5b+Y8HAwvDMM8/IG2+8IevWrat1HApEXl6eXHjhhfLTTz9Js2bN5IgjjpATTzxR7HWciAR3FiwYGgaccP6W4Y7hOlyKeFdkyYh2rdtd5f/fSrnx1Mk4hvoEVykx5daWNVIbQuZ4y7Vap5jujd0hjna9Ql9rOoYEi84ddhL3dy8GTjxDBjgXSu6w48S180B/uc4sp+R6HZYmlqmOd+ks9RMWd4l4fv9M4L0bMplpdr5k9RklztbdxVdeo7jZd9xutbB6X2H5wKp9JCq+m6YUQYfTsf1Du0Nyhx0f8V7ACoX6xNq/UafyWS9GrFPA+WWbpGLO61FlyOq0m/jgw5DiY0SoMS3WNrRe33oaIyxcW1ZWE0CdvHE2VLnJaUOr5Qbfb6ttiHN7dSqMUdbUakPzeyzR/bsu/RDHMDHGz5KVm2VLSYU0KciW7u2bBmzFFOu1xnEkCgi+PtHzCKN/1McYAVeu4L5YHaJco58bbViXsaeh+ndaKBDl5eXqd3CsQ3Z2tmzZUjsPPeIb4O6En44dO4ZUIJYtW6bcoA444AA577zz5Mcff1RxFSjvoosuiruuaGxo1GaqqjxSXl7lN1sFs3VrjXzIuhFsEsLDVVXlFZfLITk5gfJ7PF71kgGB5dbk/i4uLlc3OyfHJU7zZGdb562s9KjP8/KyarlhlW7bjKSgADmFA3uQsVqQk+MUlyuwa1RUVElFhUfJkR+Ujxidrri4ZvqHY8EPC74T352V5ZTsbFcMbehTsoRrQ7QR2gp1RVuEasNQ9818b0K3YaVUVnrV5/je8G1Yu1yjDRv1GiLNdtlDKlctEm/pZnHkNxVfiy6qDWXDMnF5StVnWe16qskn3PJKSmrKzc52Sk7f4apum2c+J9UlRaItFaVS+eN0qXkaaoBFIG/48dJ4l+EBp5rbMLgfuv/+zdIGer71y6SgWz81uTfuXW7TplI98jxxf/WkiGd7Tey5jaTpqDPE166feDyx9++Kf36Pye0tq9otbgsyZG3+Uyqb7ZwmY4SrzmOEsZ9EOo0RNeWGb0NMVvLysi33b3MbYvxAO5rB/cZ9D9WG5nuD/hC80FZWVqH6d6g2xDsM7zLUJfI70CUOB9rQVesdGLoNvVJaGuodWAPqi3qjPniHhu7fsbeh0b8jtWG0/h2qDfG98YwRyZlHSMA8AvUd1Lt1XGNE48Y5ta412hDtH3xfG2oeUf9jhCsh84iGGCPSRoFAYLMRC2H8DSoqKiQ3N7fW+bfeeqt06tRJjj322LBlPvnkk+p6I2C6e/fuUlJSooKqL7jggritEHjQjIHf/BnAjQ4+ZsbQxAOvrbkYD7/HE/5alIs+gE6Eh6RGS6w5VnOzq0KWi44fqU5GJwulnbrdnpqJbohy0fkilWs8FKHKRQeFvIHlbj/HXC5kdjqdfi0+VBsa5aJdIK/V+2ambm3ojtiG6tkt7CxSKAKvHJ+nusYdZ8fuUo1VV3zztgHeLDcGHY/HI742u0rBhHvFu2aJVP75k1Qt+TrAeqErmHSXfvKwVP23UhxNW4k9r6k4Wte4EHlWL1ZuZFvyt39W8dO7UvHLh5bK9pZskk2/fqssA8ExBuLKqVEgsvJEKsvE2XNf8bbZVao91TH3b3X+xvWWZYalqspee0wMRenG9eJsunO9jREqrmXNEqku2yyS20R8rTDm+iKOEcFjWjxjRDDpMEbUjGsOVZ9I4ywmm/GOsyjTWHgxy2icE6lcY8JptQ2NclHvaG3ocvn899u4Jt5x1rgW5WJCVZ9tGO0daG5Do5/jmmT071jmEbWvlaSNEZAbv0tLoeg13DyivscIm2lcM8qt6zyivseItFEgDNel9evXy0477eT/HP9j4h/Mm2++qawV/fv3969SAMRKnH322eoHx4MtGt26dVPWC1gh4NIUL5F8+eI9Fs1sZJhxoS3ihpvLshKkE1+dtpvI6ltWA8gMLbnmheNLWLnJqm+i2hArBobcNefYxN66h+S07qFiMCp/flcqf/tMrczrDiwTkdzD1Ge+ahGv9VWV6i1rxW0udxuGMuHoNEjszdqoGJfqolUB9zHm/pJrPU5LBetn51s7GZN4i0kC4h7TvN4IyQUiB3RHGtNS8VlOZLmQG6uVmFTgWHLG2YYfv0NdG+p+173c1G5Dcz83ykyFfmj1WLzX1rzHXGqiH+6cVJM1EeXaA+53+vbvtFAgevToIQUFBTJnzhy/AgH3o4ULF6osS8F8+umnAf/Pnz9f7QPxxBNPKCUBjTJq1CgZO3asCrA2WLBggbRo0aJOygMhDQ0mgNm7jZWs/of6J4eS20i8a5ZJ1e+aKxWeCmufRSIrv0Y5i4D3z7niXVNQ83fRSokH88Z8tpxGUYPlXb1HiS07X+wtu1oK6EawevDmd4nM0hQtvTEDugkhJPNpUAUClgIoCpMmTZLCwkJp27atilfAfhDYDA4WhqKiIuWOBBenDh06BFxvBFq3adNGmjatCd6BAvH000/LzjvvLH369JHvv/9ennrqKbn22msbREZCkp1BytW2t2QP2K5UxLIBnNXN5bSg0qIC5i5Rv3xb10t1RZlUb/zb8n4M8ewtAuUQP1ACnJ2HRNzg0L5jF3HPeCRpk/qqFXNDlh8pKD2d9qcghBCSBgoEQLYk+Htfd9114na71U7UUACwxwM2lBs5cqTccccdMm7cOEvlTZw4UVk17rvvPqVgYH8IKA9HH3100mUhJJXS0jo77RZ1DwtMJn2Dj4lq0cD5WUOPFTtWzGNUUjKZ0mmXBrpNbUtLa2/UXGy5jVT6WvPGhKFSsFoF7QzlwdX3QKn69WMVHOjHmSVid4n3jx+SNqmv/OMHqZg5JYb61qSejWd/DEIIIalNg+9EnS409EZy8Js1p4PLdHSUOZlyx7pzcawb7cEdx/vvQvEs/TZhdc4YcgrEsfMQ8S6fJVJZk7UjJHaXZO11klTOfjVA2QvGltdUfAhYRkxG+13Fu3J+TNUJteldtPsdr/ITboduPt+UWwcoN+VON9JmJ+p0oiEVCELShXhcdMh2nN2GiSfSvhkm7E3biGPnQTFvWpi9z5k1G/tFsCSZYyagXATHVFgl3h26CSGE1D9UIDJQgTBv7KILOsqcCXKbrRK+8mKpLvlPPMtn13KhMlyijPOwr4LkNWFQuAVszdqpncu9f/8U+8VIR2sxHXDOqPNVAHf5+3fF/j05jSRryDEi7tIAdy6A/iHlcJlL7G7j6UC6P9/xQrn1gnKnJ1QgMtCFyeq28JmCjjJnstzRXGSC5Taf7928JuZVdpIYlKI3+Eip+OKJxBSoUtHaRCpKTN/RTLKGjg+Ir0mGUhGrG18yysjU5zsalJty64A9A+SORYFo8CBqQoieQd5Wz8feoI4d2ifGNQqbwGFvCG7IZwm1O3d55DSzMRHCqoR7WvH5owGfJSrtrDHhj2e/CivueYlMj0sIIekEFQhCSMqDCRqyB0Xd/wLuOTCqmveAyGkkri5Dxdmxv1ox9vz9U52yIemGZ+UC1YaRAruBs9d+4vljTtTzrJCItLOJ3K8iXPpa7nlBCNEVKhCEkIzZ/8LsYx/OzURN9Eadb8miERyrUfH9ywmZIEfD0WmgeP+cJ6lA9b+/RT0ne79zxZZVIJ6Fnyf0u2NJO2t2L8KO4gE7ldfhO6ykr3V/87z4PJUBaXsJISSToQJBCMk416ho7lKhLBris4mvfLNy2Qnew8H/fc6spFsvoLS4eu6bMgpEJIz9RIDVDeZiwepeEnXJ/hXuO6CQVP78rjVFxF3sjxOhWxMhRAcYRJ0mWZgIIalByMmqM7sm/UYCYiuQ+QjKTbypU+sL125jJbv/oUl3CXP22ldtihhOwUvE97t6jRTnzgMDN/2b9aLItv024r2PVCIIIekEszAlASoQhJBI2XhAtPS1frILan4HZCPavjs4iHvnamQ6qocUuKhv3jF3S9mrlzecogNZvZ7AmJc6AOuBs/MQteN33csqlPzxk6JuwBjJ4pVqWagIIZlNIRWIzEvjmu67G8aKjjIDyp1ZckeaKKrj65aKo6pEvK4Cse3YrdaEzqprDqwBjiatajaGq/aJ+8O7pT7IGjJeKme/XC/flY64+o0RZ7veAfe7euV8cS/+VnwhFMtYUtrGooQ0dAaphni+Iyn59aVEZeq4Fg3KXZW2clOByEAFIt1zC8eKjjIDyk25Y0tFGmi1MM635v5kw9l1dv2pWjijTmVoQajsYFbJzhdn7/3E2aq734UrknUrWAmJFlAeq6tVPNaT+n6+rboZJnsPEo5rlDvdoAKRBKhA1C86ygwoN+VOhBtKNPcnWCyydh0j1euXxZy1yAwtEBmAK1ey9zhebAVNa8WZoI/hM3FvrbFulZdIxexpoZXTnAJxdh4q9kbNQ1rasqrLpNKeF9LSVhdXwWAFJm7XvyRYZjiuUe50gxvJEUKIxhvyhUtVG2yxsJvKshe2iymTkcoWBQvEgo9TOtibRKGqXCq+erLu5bhLxIN9WcJYXkpDKBqSky/iLg1QViJaWSIkK8DE3zXkGKmc9VKdxDD29vAOOEzsTXYM6XpoJXsb8KxZKmXVZeKJUXEiJB2gBcIitEDULzrKDCg35W7IwNkAd6klX0fMKmW4vkRd8UXAuClYnJCMd0ULpeiEcEULpTil+meRlD7jM7vNLlnVpeLetFF8Oekvj9XP7AmUu6H2lKELU4YpEBiHXC6HVFV51dilAzrKDCg35U4V/Psg/Ba423eouIvQQbqmPSK48zchhMREQ+wpQwUiCTCNKyFER6xaMSKdFzKoNUQq21TH2W2YeJbOauhqEEI0Iqce95ShApGBFgin0yEeT+qtUiYLHWUGlJtyZypmBcOe30Sy2/VScnvWbFc6IgbpNjR5zST/2Hsadt8LQoh22CLsKZNoGESdYdhsNsnNzVJ+0rroezrKDCg35dYhAByxH3n5NbEfwUHh2HU6OMuOpQDbKJv0OTvvXqcN4nKGHSd2p1O5FNAlixBSX/hKi9SYaCWBRn1CBYIQQkjaZZryDRkfcZO+UO5Ujh07x+xKFRzzES7DVcwuWa4ccXbbU2VB8iz91to1hBAt8SHrV4pBBYIQQkjGKRqhjmHy7+wwIOIOxUZqTmPvg1AxH9HKMZSaUJYSW06BZPUZJa5+h2yPEenQL0QKXQsb/eU0EmfnIVRCCMlwbLDCphhUIAghhIjuikes7gFWyzEsJVA0HPlNpUnXXaW0rDIgbW8ohcTesqt/o79I+w1EVEJCphNNXPC6o9swsWXlhXcnSxGwcaKjSau4N0wkpCGx5Rf6FyhSCSoQaRTErRs6ygwot15QbtEq9iPcdh+hFBLzRn/RsGJdibSbc7j89N41y6Tq98ipfIPdyUJvApdTY1SJsLeI5b0XrFpesgskZ6+TAzLYhNwwMQ0zghF9yN5jQkpuQsgsTBZhGldCCCE6EuuGhOGuAWZFI9omXtFiWsKnCM4XV59Rkt3/UMsph83fEWzxCakQBW0M5/13oXj+/jlA0bK88RwhIQi1506yYRrXJEAFghBCCMkMBScZ3xGrYpJKOzI3xM7NqfBZqspj507UmUNDKhAwexcU1KQ8NPvNZjI6ygwoN+XWAcpNuXWAclPudCMWBSL1nKoIIYQQQgghKQsVCEIIIYQQQohlqEAQQgghhBBCLEMFghBCCCGEEGIZBlGnSRYmlT88TYNy4kVHmQHl1gvKrReUWy8ot17Y01xuBlFnIOncIeNFR5kB5dYLyq0XlFsvKLdeVGskNxWINMBms0lurkv91gUdZQaUm3LrAOWm3DpAuSl3JkMFIg1AX3S5nOq3LugoM6DcohWUW7SCcotWUG7RCptmclOBIIQQQgghhFiGCgQhhBBCCCHEMszCZBE0U0MGx6R7ZH886CgzoNx6Qbn1gnLrBeXWC3uay436W43hoAJBCCGEEEIIsQxdmAghhBBCCCGWoQJBCCGEEEIIsQwVCEIIIYQQQohlqEAQQgghhBBCLEMFghBCCCGEEGIZKhCEEEIIIYQQy1CBIIQQQgghhFiGCgQhhBBCCCHEMlQgCCGEEEIIIZahAkEIIYQQQgixDBUIQgghhBBCiGWoQBBCCCGEEEIsQwUiTXj88cflhBNOkExj8+bN8r///U/22msvGTBggIwfP17mzZvnP/7nn3/KmWeeKf3795dhw4bJzTffLOXl5ZIJrFu3Trp3717r56233go4b9OmTTJ8+HCZM2eOpDOofyh58TNy5Eh1TklJidxwww0yZMgQ2W233eTss8+WlStXSqY9u4sWLZLjjz9e+vXrJ/vuu6+88MILMV2fDoSq98yZM+WII45QzzPkvuuuu8Ttdgec8/TTT6v+0LdvXxk3bpzMnj1b0l3u6667rlafh/xWr08HguuNv8M979OnT/ef99VXX6n7vMsuu8h+++0nL730kqT7e+v7779XMu26664yevRo+eCDD8KWhXKuuuoqSQeiyf3mm2/KIYccosa1/fffX5544gnxer3+41VVVXLvvffKnnvuqc7BGIixMN3lPuWUU2r18XDPcDrd76j4SMrz4osv+nr06OE7/vjjfZnGKaec4hszZoxv7ty5vj/++MN30003+fr27etbsWKFr6ioyLfHHnv4zjnnHN+yZct8s2bN8g0fPtx3ww03+DKBL7/80rfLLrv41q1b51u/fr3/p7y83H/O2rVrfYcffrivW7duvtmzZ/vSmYqKigA58fPpp5/6unfv7nvjjTfUOSeffLLvgAMO8M2bN8+3aNEi34QJE3wHH3ywz+v1+jLl2UW/3n333X1XX321b/ny5Up29AOjDaJdnw6Eqjee8Z49e/qmTJni+/PPP1X/32uvvXxXXXWV/5xHHnnE169fP98HH3ygzsF4gP//+ecfXzoQ7n4deeSRvvvuuy+g72/cuNHy9alOqHpv2rQpQF6Mc8bzXFJSos6ZM2eO6hMPPvig7++///a99tpr6n/c/3R9b+GZxvOM+42/n3rqKV+vXr183333XUAZGNPuvfdeNbZfeeWVvnQgktzvvPOOr3fv3r5XXnlF3UvcwwEDBvgefvhh//XXXHONeqd//fXXqm0uuOAC37Bhw3xbt271pavcYOjQob5p06YF9Hf0/3S/39GgApHCYPJ41llnqRfo6NGj0+6lEo2//vpLPUyYLBpUV1f79ttvP98DDzzge+ihh9QEw+12+4/jBYMJNc5Ld5544gnfIYccEvb466+/7hs8eHDGKBDBlJaW+vbZZx//BBLyQZlYvHix/xwojiNGjPAP1Jnw7D722GNKEa6qqvJ/hhfL/vvvb+n6VCZSvSdOnKgURDNvv/22mnRAuUR/wHWYjBp4PB71jOC8dJUbYxU+h7Icz/WpTCz1njp1qq9Pnz4BzzLOxyTSDBRrTNDS9b11/fXXK4XRzKWXXuo79dRT/f9j8nzMMcf4hgwZosa3dJhQRpP72GOP9V177bUB10yePNm39957q7+xCIDx/YsvvvAf37Jli3oHBCtX6ST3f//9p47//vvvYctIx/ttBbowpTC///67uFwueffdd5UpNNNo1qyZMnHCdG1gs9nUz9atW+Xbb7+VUaNGSXZ2tv/4UUcdpVx8cE66s2TJEuncuXPY45999plccskl8uCDD0om8thjjyl3tCuvvFL9j/vdrVs3Zf416NKli3zxxRey8847S6Y8uzB9Dx48WJxOp/8zuGz99ddf8t9//0W9PpWJVO9TTz3Vf68N7Ha7cmuA69qPP/6o+sPBBx/sP+5wOFRZY8eOlXSV+59//pGysrKIfTgT77eZoqIieeCBB+Scc87xtwPuNZ4FuLyYuf3225WbR7q+tyDT0KFDA67B843+jUVbALc8jP3vv/++tGvXTtKBaHJfdtllctppp9V6vrds2aL+njVrljRq1Ei5ARk0btxYuTUGt1c6yb1kyRL1d6dOncKWkY732wrb32Ak5YCPbDg/2UwAg8fee+8d8Nknn3wif//9t1xzzTXy3nvvKV/oO+64Q32OFxUUiosuuihAqUhXli5dqgan4447TsV6dOjQQb1gjQEWPsVg1apVkmlgQvHcc8/JxIkTpWnTpuozow2mTZum/KAxOCMO4uqrr5Ydd9xRMuXZXbt2rVKUzLRs2VL9XrNmjTRv3jxtn/1I9e7Vq1fA/1Ac0Af69OkjhYWF6v43adJEvZAx2YRCBQUSSjT8jtNVbjznYOrUqfL111+rSRWecciFCVW061MZq/V+8sknJScnJ2CCiXG+urpaKYkXXnihzJ07Vz0H8IvHQlG6vrfefvttadWqVcBxyAWFCfFs6OsY89ONaHJjrDZTXFwsL7/8sop3AHi+27dvL59++qmakCMGEGMC4gEiLaSlutxLly5VzzHiM6Ek5eXlqbiXc889V7KystT56Xi/rUALBEkZfvrpJzVZRPDViBEj1KokXjwVFRUyefJkufzyy5VSgYDEdMfj8cgff/yhVmcuuOACNaAiqAwB4wjAy3SgJGDQPeaYY/yf4X5jpebDDz+Um266Se6//3412T7xxBNVH8gUEDRsvFgMDIU4k+SM1v+vuOIKWbZsmQqaN+4/2garzwhKxLPfsWNHOemkk2TFihWSrmCCAaUBk0hY3TBhgrUNEwxMoDMd3NfXXntNKQ/mhR98DnC/Bw4cKM8884wcfvjh6tl//fXXJV3fW6Geb+P/yspKyRSC5TZTWlqq+jfGMzznxv3GpPvRRx+VSy+9VKZMmaKssBMmTJCNGzdKusq9dOlSJSeSPjz11FNqERD9NxPmKdGgBYKkBJ9//rkygWKlcdKkSeozDC4wC954443qf6xUIqPDxRdfrF7CO+ywg6QrkA1ZibD6hpU5Qz5MqJCFJpVNuokAWVjglmLIbrQJBuJHHnlErUQDKI5YwYKZ+8ADD5RMADIHTyQMxQGrV5kOJhJ4hn/44Qd1f/HiNe4/Jl9Y1TNW/Hr37i0///yzvPjii35FI93AhAKTJFgbAaxPLVq0kKOPPloWLFiQVi5L8Y7t6O/IvmUGFmVw2GGHqUUC0LNnTzXJhGUqla0Qkd5bUJKCn2/j/9zcXMkEQsltsGHDBjnrrLOU5RzvMsNlB883nn0sDBkWB/yNZx1Wm9NPP13SUe6bb75ZuWYa7yw83+jbsDBCeYJFOVOhBYI0OJgcYBV+n332USt0xioVzMBdu3YNONf4/99//5V0Jz8/P2ACbcgH024ms3jxYpWaNdj3GfcbrkrGQAww+MLFKZPcuCDn+vXrAz4z/k83V61YgZww5//yyy9qcmF2DTDcPswxMPAtxmQjne8/rA+G8hA8jsHClulg0oX7DFcQM8b9Dnbng9taOtzvcO+t1q1bh3y+sThguKylM+HkBrAUQjGGRQFuqOa4AdxvKBFmdyW8/+DWlM732+l0BryzdHq+qUCQBndlueWWW9Sk4r777gsw/Q4aNEh+/fVXf+AZgLkQq/bpHogESwNWMYL3dvjtt9/UCzSTQZAhrEc9evQI+Bz3e/Xq1QEvX/wNv2HERmQKkBMBleb86HDdgrUtna1q0YC7HtyREP+CyQXawQzcWKAwQLkwwLO/fPnytL7/WIU8+eSTAz6D5QFk+rMOQgUVG8ryTjvtJPPnzw/4HGM8Pk/X9xb6MaxrZvB8Y7yHMpnORJIbi0J4vmFleeWVV2ot/uF5h+ui0fcBLI64LtWf70hyn3DCCcqlyQxkhBUCLpiZTHr3ZpLWIKgKGTcQGA2TJzLQwPyJHwRgwWcWgwtcF3DuN998ozaegskbgWjpDFZhkI0E5k+8YLFyg2BxTJ7g8pDJLFy4MGCV2QAuShhwESQPRQrnwVcWE+tgH9t0Bq4cMOVfe+21anKMrGJw2cAzkMmgf+N5vueee9Tzazzr+IEy1aZNG9U2t956q9pcDM88XtpYnYQLULpywAEHqLgmuGshIxNkg5vWmDFjUjp4NBEgKQAWAIIXCwzOP/98efXVV5VCib6BiSc2IwvO5pNO7y1MKLHwBRcXjOuI7fj444/TwkWnLnKjT8NVCxNsrMqbn29Dsdpjjz2Uuw/eeRj7oFxjQRDv9HSV+4ADDpB33nlHBYyjDyOG7+6771Z9uKCgQDIZxkCQBgOZDJCJBelK8WMGwXR33nmn2qEXDyMGGJh/Dz30UOVbmO5gJQpmUOzKCX9wZBxCRopnn322lkk/08DAa2ReMoNVHUykcd+xkoXVZ+w+jjYKDkpMZ2BlQLDdbbfdpvo5/OHxIsXfmQoUBLxY8bzj3gYzY8YMZVVEvBMm2ghAhMUCzwQmYOmWxtcMMskhqxQSJSAwHOMY3Pfw3Gc6xuQx1PMOjIkjMs5BwWzbtq1aMErltL1W3lsIFIai/Pzzz6t+jb/TPa4tktwYpw2rSyhlAJnVwMMPP6wUKyiOsD7AKoN3fCovCFq53zabTWVZg6KB8RwWRyREyXRs2AyioStBCCGEEEIISQ/owkQIIYQQQgixDBUIQgghhBBCiGWoQBBCCCGEEEIsQwWCEEIIIYQQYhkqEIQQQgghhBDLUIEghBBCCCGEWIYKBCGEEEIIIcQyVCAIIYQQQgghluFO1IQQQurEVVddJW+//XbY482bN5dZs2bVa526d++udry94IIL6vV7CSFEB6hAEEIIqTMtWrSQyZMnhzzmcrnqvT6EEEKSBxUIQgghdSYrK0v69evX0NUghBBSD1CBIIQQUi+ccMIJ0rZtW+nYsaO88MILUlFRIbvvvrtce+216nODBQsWyAMPPCC//fabVFVVyeDBg2XixInStWtX/znr16+Xe++9V77++mtxu93Su3dvdU7//v3955SUlKiyP/vsM1XOnnvuKf/73/+USxUhhJD4YRA1IYSQhODxeEL++Hw+/zkzZsyQt956S6677jq56aabZNGiRUqxKC8vV8dnz54t48ePV3/ffvvtcuutt8qaNWvk2GOPlRUrVqjPS0tL1Tlz5syRyy+/XLlOZWdny6mnnip//fWX/7ugpEBxePDBB5VyMXPmTLn55pvrvV0IISTToAWCEEJInfn333+VFSAUV1xxhZx22mnqbygKUCDat2+v/t95553l8MMPl+nTpyulAFaFDh06yBNPPCEOh0OdM3z4cBk1apQ89NBDShlAwDa+D7979uypzhkwYICMHTtW5s6dqywcYJdddpG7775b/T106FCZP3++fPXVV/XSHoQQkslQgSCEEJKQIOopU6aEPNa6dWv/35joG8oD6NWrl/ofE//DDjtMuS8he5KhPIDGjRvLPvvs45/8//jjj9KuXTu/8gByc3Plk08+Cfje3XbbLeB/XLN169YESEsIIXpDBYIQQkhCgqix4h+NHXfcsdZnO+ywg2zZskWKi4uVu1OoGAV8huNg8+bN6ppo5OXlBfxvt9sD3KkIIYTEB2MgCCGE1BubNm2q9dl///0nhYWF0qhRI7HZbOr/YDZs2CBNmzZVf+O8oqKiWuf89NNP/jgJQgghyYMKBCGEkHoD7kdmJQKZllatWqViFGAx6NOnj3z00Ufi9Xr958Dy8OWXX/pdkgYOHCgrV66UZcuW+c9BRidsGvfGG2/Us0SEEKIfdGEihBBSZyorK+WXX36JuDO0EUR9+umnyznnnKOyKd1///3SrVs3GTNmjDqObEkIuD7zzDNlwoQJKosSAqpR/nnnnafOGTdunEydOlWVceGFF0qzZs38GZdwDSGEkORCBYIQQkidgYvRMcccE/Y4siwZ1oMhQ4ao/RnAvvvuq7I0IYYCwBLx7LPPqoxLl156qfoc19x1113+fSAKCgrkxRdfVBmWbrnlFqmurlab2EGJMAdoE0IISQ42HyPKCCGE1APY7wHAekAIISR9YQwEIYQQQgghxDJUIAghhBBCCCGWoQsTIYQQQgghxDK0QBBCCCGEEEIsQwWCEEIIIYQQYhkqEIQQQgghhBDLUIEghBBCCCGEWIYKBCGEEEIIIcQyVCAIIYQQQgghlqECQQghhBBCCLEMFQhCCCGEEEKIZahAEEIIIYQQQsQq/wc6x9oBS4Ab9wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"], \"BiLSTM [Features]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Evaluating model. Loading model from: .checkpoints/bi_lstm_feature_best_model.pt\n",
      "   - Loading checkpoint from: .checkpoints/bi_lstm_feature_best_model.pt\n",
      "   - Detected full checkpoint dictionary.\n",
      "   - Model state successfully loaded.\n",
      "üß™ Performing inference on the test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucadibello/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/lucadibello/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/Users/lucadibello/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ~~~~~~~~~~~~~~~^^^^^^\n  File \"/Users/lucadibello/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/Users/lucadibello/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 212, in collate\n    collate(samples, collate_fn_map=collate_fn_map)\n    ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/lucadibello/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 240, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate_model\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# evaluate model on test set and generate submission file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeseries_loader_te\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBI_LSTM_SAVE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubmission_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBI_LSTM_SUBMISSION_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/src/utils/train.py:462\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, test_loader, device, checkpoint_path, submission_path, id_attribute, threshold, use_gnn, input_type)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müß™ Performing inference on the test set...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m# for batch_data in tqdm(test_loader, desc=\"Evaluating\"):\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBATCH: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbatch_data\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m, feature: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbatch_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Handle different batch formats for device transfer\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1480\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1478\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._task_info[idx]\n\u001b[32m   1479\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1480\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1505\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1503\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1504\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1505\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/_utils.py:733\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mTypeError\u001b[39m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/lucadibello/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/Users/lucadibello/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ~~~~~~~~~~~~~~~^^^^^^\n  File \"/Users/lucadibello/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/Users/lucadibello/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 212, in collate\n    collate(samples, collate_fn_map=collate_fn_map)\n    ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/lucadibello/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 240, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "%aimport src.utils.train\n",
    "from src.utils.train import evaluate_model\n",
    "# evaluate model on test set and generate submission file\n",
    "evaluate_model(\n",
    "    lstm_model, timeseries_loader_te[\"feature\"], device,\n",
    "    checkpoint_path=BI_LSTM_SAVE_PATH,\n",
    "    submission_path=BI_LSTM_SUBMISSION_PATH,\n",
    "    use_gnn=False,\n",
    "    input_type=\"feature\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features + Oversampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Attempting to load checkpoint from .checkpoints/bi_lstm_feature_best_model.pt...\n",
      "   - Loading checkpoint from: .checkpoints/bi_lstm_feature_best_model.pt\n",
      "   - Detected full checkpoint dictionary.\n",
      "   - Warning: Could not load optimizer state from checkpoint: loaded state dict contains a parameter group that doesn't match the size of optimizer's group\n",
      " ‚ö†Ô∏è Could not load checkpoint: Error(s) in loading state_dict for LSTM:\n",
      "\tUnexpected key(s) in state_dict: \"lstm.weight_ih_l0_reverse\", \"lstm.weight_hh_l0_reverse\", \"lstm.bias_ih_l0_reverse\", \"lstm.bias_hh_l0_reverse\", \"lstm.weight_ih_l1_reverse\", \"lstm.weight_hh_l1_reverse\", \"lstm.bias_ih_l1_reverse\", \"lstm.bias_hh_l1_reverse\", \"lstm.weight_ih_l2_reverse\", \"lstm.weight_hh_l2_reverse\", \"lstm.bias_ih_l2_reverse\", \"lstm.bias_hh_l2_reverse\", \"lstm.weight_ih_l3_reverse\", \"lstm.weight_hh_l3_reverse\", \"lstm.bias_ih_l3_reverse\", \"lstm.bias_hh_l3_reverse\". \n",
      "\tsize mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n",
      "\tsize mismatch for lstm.weight_ih_l2: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n",
      "\tsize mismatch for lstm.weight_ih_l3: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n",
      "\tsize mismatch for fc.weight: copying a param with shape torch.Size([1, 128]) from checkpoint, the shape in current model is torch.Size([1, 64]).. Starting training from scratch.\n",
      "üîç Applying oversampling to the training data...\n",
      "  Attribute-based label extraction not applicable or failed. Iterating through 10394 dataset samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucadibello/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Finished iterating. Collected 10394 labels.\n",
      "  Class counts before oversampling: [8375, 2019]\n",
      "  Successfully created an oversampled DataLoader.\n",
      "üí™ Starting training from epoch 1 to 300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|‚ñé                                                                                  | 1/300 [01:43<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 23137, 23165, 23182, 23190, 23198, 23206, 23214, 23223, 23234, 23249, 23262, 23271, 23279, 23287, 23302, 23311) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmpty\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1251\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1250\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/queues.py:112\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[31mEmpty\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m BI_LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \u001b[33m\"\u001b[39m\u001b[33mbi_lstm_feature_submission.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# train model on training set (or load existing)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m train_history, val_history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeseries_loader_tr\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeseries_loader_val\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBI_LSTM_SAVE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mval_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_oversampling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:185\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, use_oversampling)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:488\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    486\u001b[39m         \u001b[38;5;28mself\u001b[39m._iterator = \u001b[38;5;28mself\u001b[39m._get_iterator()\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1230\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._reset\u001b[39m\u001b[34m(self, loader, first_iter)\u001b[39m\n\u001b[32m   1228\u001b[39m resume_iteration_cnt = \u001b[38;5;28mself\u001b[39m._num_workers\n\u001b[32m   1229\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m resume_iteration_cnt > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1230\u001b[39m     return_idx, return_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1231\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_idx, _utils.worker._ResumeIteration):\n\u001b[32m   1232\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m return_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1420\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1416\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1417\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1418\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1420\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1421\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1422\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1264\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1263\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1265\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1266\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1267\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 23137, 23165, 23182, 23190, 23198, 23206, 23214, 23223, 23234, 23249, 23262, 23271, 23279, 23287, 23302, 23311) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "BI_LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"bi_lstm_feature_best_model.pt\"\n",
    "BI_LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"bi_lstm_feature_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "    lstm_model, timeseries_loader_tr[\"feature\"], timeseries_loader_val[\"feature\"],\n",
    "    criterion, optimizer, device,\n",
    "    save_path=BI_LSTM_SAVE_PATH,\n",
    "    num_epochs=300,\n",
    "    patience=10,\n",
    "    monitor=\"val_loss\",\n",
    "    scheduler=scheduler,\n",
    "    overwrite=False,\n",
    "    use_gnn=False,\n",
    "    use_oversampling=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LSTM with Attention and early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.lstm\n",
    "from src.layers.lstm import LSTMAttention\n",
    "\n",
    "# Create model and fit it\n",
    "attention_lstm_model = LSTMAttention(input_dim=228, hidden_dim=64, num_layers=3, dropout=0.2, input_type=\"feature\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    attention_lstm_model = nn.DataParallel(attention_lstm_model)\n",
    "attention_lstm_model = attention_lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(attention_lstm_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí™ Starting training from epoch 1 to 300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|‚ñé                                                                                  | 1/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([154, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%| | 2/300 [00:01<05:00,  1.01s/it, train_loss=0.6008, val_loss=0.5081, best_val_loss=0.5081, lr=1.00e-03, ba"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([39, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%| | 3/300 [00:01<03:41,  1.34it/s, train_loss=0.5076, val_loss=0.4942, best_val_loss=0.4942, lr=1.00e-03, ba"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([154, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([39, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%| | 4/300 [00:02<03:13,  1.53it/s, train_loss=0.5047, val_loss=0.4909, best_val_loss=0.4909, lr=1.00e-03, ba"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([154, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([39, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%| | 5/300 [00:02<02:53,  1.70it/s, train_loss=0.5030, val_loss=0.4883, best_val_loss=0.4883, lr=1.00e-03, ba"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([154, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([39, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%| | 6/300 [00:03<02:51,  1.71it/s, train_loss=0.4983, val_loss=0.4855, best_val_loss=0.4855, lr=1.00e-03, ba"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([154, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([39, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%| | 7/300 [00:03<02:48,  1.73it/s, train_loss=0.4941, val_loss=0.4828, best_val_loss=0.4828, lr=1.00e-03, ba"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([154, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([39, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%| | 8/300 [00:04<02:37,  1.85it/s, train_loss=0.4931, val_loss=0.4777, best_val_loss=0.4777, lr=1.00e-03, ba"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([154, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([39, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%| | 9/300 [00:04<02:40,  1.81it/s, train_loss=0.4915, val_loss=0.4722, best_val_loss=0.4722, lr=1.00e-03, ba"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([154, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([39, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%| | 10/300 [00:05<02:36,  1.85it/s, train_loss=0.4891, val_loss=0.4672, best_val_loss=0.4672, lr=1.00e-03, b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([154, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([39, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%| | 11/300 [00:05<02:32,  1.89it/s, train_loss=0.4833, val_loss=0.4669, best_val_loss=0.4669, lr=1.00e-03, b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([154, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([39, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([154, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%| | 12/300 [00:06<02:44,  1.75it/s, train_loss=0.4815, val_loss=0.4626, best_val_loss=0.4626, lr=1.00e-03, b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([39, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([154, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%| | 13/300 [00:07<02:53,  1.66it/s, train_loss=0.4821, val_loss=0.4637, best_val_loss=0.4626, lr=1.00e-03, b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([39, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([154, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%| | 14/300 [00:07<02:48,  1.70it/s, train_loss=0.4840, val_loss=0.4677, best_val_loss=0.4626, lr=1.00e-03, b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([39, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%| | 15/300 [00:08<02:40,  1.78it/s, train_loss=0.4844, val_loss=0.4625, best_val_loss=0.4625, lr=1.00e-03, b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([154, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([39, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%| | 16/300 [00:08<02:36,  1.81it/s, train_loss=0.4814, val_loss=0.4604, best_val_loss=0.4604, lr=1.00e-03, b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([154, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([39, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   6%| | 17/300 [00:09<02:38,  1.78it/s, train_loss=0.4810, val_loss=0.4618, best_val_loss=0.4604, lr=1.00e-03, b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([154, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([39, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   6%| | 17/300 [00:09<02:49,  1.67it/s, train_loss=0.4810, val_loss=0.4618, best_val_loss=0.4604, lr=1.00e-03, b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n",
      "LSTMAttention forward pass with input_type: feature, input shape: torch.Size([512, 228])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m ATTENTION_LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \u001b[33m\"\u001b[39m\u001b[33mattention_lstm_feature_submission.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# train model on training set (or load existing)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m train_history, val_history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_lstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeseries_loader_tr\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeseries_loader_val\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mATTENTION_LSTM_SAVE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mval_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/src/utils/train.py:239\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# if grad_clip is specified, clip gradients\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grad_clip > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# step optimizer\u001b[39;00m\n\u001b[32m    241\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:34\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:216\u001b[39m, in \u001b[36mclip_grad_norm_\u001b[39m\u001b[34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m    214\u001b[39m grads = [p.grad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    215\u001b[39m total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[43m_clip_grads_with_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:34\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:164\u001b[39m, in \u001b[36m_clip_grads_with_norm_\u001b[39m\u001b[34m(parameters, max_norm, total_norm, foreach)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (device, _), ([device_grads], _) \u001b[38;5;129;01min\u001b[39;00m grouped_grads.items():\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m _has_foreach_support(device_grads, device)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    162\u001b[39m         foreach \u001b[38;5;129;01mand\u001b[39;00m _device_has_foreach_support(device)\n\u001b[32m    163\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_mul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_coef_clamped\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n\u001b[32m    166\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    167\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mforeach=True was passed, but can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m         )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "ATTENTION_LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"attention_lstm_feature_best_model.pt\"\n",
    "ATTENTION_LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"attention_lstm_feature_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        attention_lstm_model, timeseries_loader_tr[\"feature\"], timeseries_loader_val[\"feature\"],\n",
    "        criterion, optimizer, device,\n",
    "        save_path=ATTENTION_LSTM_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        monitor=\"val_loss\",\n",
    "        scheduler=scheduler,\n",
    "        overwrite=False,\n",
    "        use_gnn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"], \"LSTM with Attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set and generate submission file\n",
    "evaluate_model(\n",
    "    attention_lstm_model, test_loader, device,\n",
    "    checkpoint_path=ATTENTION_LSTM_SAVE_PATH,\n",
    "    submission_path=ATTENTION_LSTM_SUBMISSION_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Signal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.lstm\n",
    "from src.layers.lstm import LSTMAttention\n",
    "\n",
    "# Create model and fit it\n",
    "# For signals [batch_size, 19, 3000] (processing each of the 19 sensors as a sequence of (3000,1) ):\n",
    "attention_lstm_model = LSTMAttention(input_dim=1, hidden_dim=64, num_layers=3, dropout=0.2, input_type=\"feature\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    attention_lstm_model = nn.DataParallel(attention_lstm_model)\n",
    "attention_lstm_model = attention_lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(attention_lstm_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. EEG CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.cnn\n",
    "from src.layers.cnn import EEG_CNN\n",
    "\n",
    "eeg_cnn_model = EEG_CNN(\n",
    "    input_channels=19,\n",
    "    dropout=0.3\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    eeg_cnn_model = nn.DataParallel(eeg_cnn_model)\n",
    "eeg_cnn_model = eeg_cnn_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(eeg_cnn_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    patience=5,\n",
    "    factor=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG_CNN_SAVE_PATH = CHECKPOINT_ROOT / \"eeg_cnn_best_model.pt\"\n",
    "EEG_CNN_SUBMISSION_PATH = SUBMISSION_ROOT / \"eeg_cnn_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        eeg_cnn_model, train_loader, val_loader,\n",
    "        criterion, optimizer, device,\n",
    "        save_path=EEG_CNN_SUBMISSION_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        monitor=\"val_loss\",\n",
    "        scheduler=scheduler,\n",
    "        overwrite=True,\n",
    "        use_gnn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"], \"EEG CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.utils.train\n",
    "from src.utils.train import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set and generate submission file\n",
    "evaluate_model(\n",
    "    eeg_cnn_model, test_loader, device,\n",
    "    checkpoint_path=EEG_CNN_SAVE_PATH,\n",
    "    submission_path=EEG_CNN_SUBMISSION_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. EEG CNN LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.cnn\n",
    "from src.layers.cnn import EEG_CNN_LSTM\n",
    "\n",
    "cnn_lstm_model = EEG_CNN_LSTM(\n",
    "    input_channels=19,\n",
    "    cnn_output_channels=128,\n",
    "    lstm_hidden_dim=128, \n",
    "    fc_dropout=0.3,\n",
    "    lstm_dropout=0.3,\n",
    "    num_classes=1,\n",
    "    bidirectional_lstm=False # unidirectional\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    cnn_lstm_model = nn.DataParallel(cnn_lstm_model)\n",
    "cnn_lstm_model = cnn_lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(cnn_lstm_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    patience=5,\n",
    "    factor=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"cnn_lstm_best_model.pt\"\n",
    "CNN_LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"cnn_lstm_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        cnn_lstm_model, train_loader, val_loader,\n",
    "        criterion, optimizer, device,\n",
    "        save_path=CNN_LSTM_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=30,\n",
    "        monitor=\"val_loss\",\n",
    "        scheduler=scheduler,\n",
    "        overwrite=True,\n",
    "        use_gnn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"], \"EEG CNN + LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set and generate submission file\n",
    "evaluate_model(\n",
    "    cnn_lstm_model, test_loader, device,\n",
    "    save_path=CNN_LSTM_SAVE_PATH,\n",
    "    submission_path=CNN_LSTM_SUBMISSION_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. EEG CNN BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport layers.cnn\n",
    "from layers.cnn import EEG_CNN_LSTM\n",
    "\n",
    "cnn_bi_lstm_model = EEG_CNN_LSTM(\n",
    "    input_channels=19,\n",
    "    cnn_output_channels=128,\n",
    "    lstm_hidden_dim=128, \n",
    "    fc_dropout=0.3,\n",
    "    lstm_dropout=0.3,\n",
    "    num_classes=1,\n",
    "    bidirectional_lstm=True\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    cnn_bi_lstm_model = nn.DataParallel(cnn_bi_lstm_model)\n",
    "cnn_bi_lstm_model = cnn_bi_lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(cnn_bi_lstm_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    patience=5,\n",
    "    factor=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_BI_LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bi_lstm_best_model.pt\"\n",
    "CNN_BI_LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"cnn_bi_lstm_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "    cnn_bi_lstm_model, train_loader, val_loader,\n",
    "    criterion, optimizer, device,\n",
    "    save_path=CNN_BI_LSTM_SAVE_PATH,\n",
    "    num_epochs=300,\n",
    "    patience=30,\n",
    "    monitor=\"val_loss\",\n",
    "    scheduler=scheduler,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"], \"EEG CNN + BiLSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set and generate submission file\n",
    "evaluate_model(\n",
    "    cnn_bi_lstm_model, test_loader, device,\n",
    "    save_path=CNN_BI_LSTM_SAVE_PATH,\n",
    "    submission_path=CNN_BI_LSTM_SUBMISSION_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.eeggcn import EEGGCN\n",
    "\n",
    "# Get number of time points (in_channels) from first sample\n",
    "eeg_gnc_model = EEGGCN(\n",
    "    in_channels=19,\n",
    "    hidden_channels=128,\n",
    "    out_channels=32,\n",
    "    num_classes=2,\n",
    "    num_conv_layers=3,\n",
    "    dropout=0.5\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    eeg_gnc_model = nn.DataParallel(eeg_gnc_model)\n",
    "eeg_gnc_model = eeg_gnc_model.to(device)\n",
    "\n",
    "# Set up optimizer + scheduler\n",
    "optimizer = optim.Adam(eeg_gnc_model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "EEGGCN_SAVE_PATH = CHECKPOINT_ROOT / \"eeggcn_best_model.pt\"\n",
    "EEGCN_SUBMISSION_PATH = SUBMISSION_ROOT / \"eeggcn_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "    eeg_gnc_model, train_loader, val_loader,\n",
    "    criterion, optimizer, device,\n",
    "    save_path=EEGGCN_SAVE_PATH,\n",
    "    num_epochs=300,\n",
    "    patience=30,\n",
    "    monitor=\"val_loss\",\n",
    "    scheduler=scheduler,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# plot losses if any\n",
    "plt.figure()\n",
    "plt.plot(train_history[\"loss\"], label=\"train\")\n",
    "plt.plot(val_history[\"loss\"],   label=\"val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\n",
    "    eeg_gnc_model, test_loader, device,\n",
    "    save_path       = Path(\"eeggcn_attn.pt\"),\n",
    "    submission_path = Path(\"eeggcn_submission.csv\"),\n",
    "    threshold       = 0.5,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
