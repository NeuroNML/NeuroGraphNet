{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroGraphNet\n",
    "\n",
    "*A graph-based deep learning framework for EEG seizure detection, designed to improve accuracy and interpretability by leveraging Graph Neural Networks (GNNs) to capture spatial and temporal brain dynamics.*\n",
    "\n",
    "<hr />\n",
    "\n",
    "This notebook presents **NeuroGraphNet**, a model that applies Graph Neural Networks to EEG data for seizure detection. The primary goal is to **compare the performance and interpretability of graph-based methods versus traditional deep learning approaches**. Through this comparison, we aim to demonstrate the advantages of incorporating brain connectivity information into the learning process.\n",
    "\n",
    "**Authors**: Luca Di Bello, Guillaume Andr√© B√©lissent, Abdessalem Ben Ali, Beatriz Izquierdo Gonz√°lez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run feature extraction on same node\n",
    "# import subprocess\n",
    "# subprocess.run('python3 ./feature_extraction.py', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from src.utils.seeder import seed_everything\n",
    "\n",
    "# set seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# create useful constants\n",
    "RANDOM_SEED = 42\n",
    "IS_SCITAS = True # set to True if running on SCITAS cluster\n",
    "LOCAL_DATA_ROOT = Path(\"./data\")\n",
    "DATA_ROOT = Path(\"/home/ogut/data\") if IS_SCITAS else LOCAL_DATA_ROOT\n",
    "CHECKPOINT_ROOT = Path(\"./.checkpoints\")\n",
    "SUBMISSION_ROOT = Path(\"./.submissions\")\n",
    "\n",
    "# create directories if they do not exist\n",
    "CHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# set dataset root\n",
    "seed_everything(RANDOM_SEED)\n",
    "\n",
    "# setup torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading EEG segment tables‚Ä¶\n",
      "‚úîÔ∏è Loaded: 12993 train rows, 3614 test rows (took 0.5s)\n"
     ]
    }
   ],
   "source": [
    "%aimport src.utils.signal\n",
    "from src.utils.index import ensure_eeg_multiindex\n",
    "\n",
    "start = time.time()\n",
    "print(\"‚è≥ Loading EEG segment tables‚Ä¶\")\n",
    "clips_tr = pd.read_parquet(DATA_ROOT / \"train\" / \"segments.parquet\").dropna()\n",
    "clips_te = pd.read_parquet(DATA_ROOT / \"test\" / \"segments.parquet\").dropna()\n",
    "\n",
    "# load clips with label\n",
    "clips_tr = ensure_eeg_multiindex(clips_tr, id_col_name='id')\n",
    "clips_te = ensure_eeg_multiindex(clips_te, id_col_name='id')\n",
    "\n",
    "print(f\"‚úîÔ∏è Loaded: {len(clips_tr)} train rows, {len(clips_te)} test rows \"\n",
    "      f\"(took {time.time()-start:.1f}s)\")\n",
    "# NOTE: Merge clips for sanity checks\n",
    "clips = pd.concat([clips_tr, clips_te]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train extracted features shape: (12993, 228)\n",
      "Test extracted features shape: (3614, 228)\n"
     ]
    }
   ],
   "source": [
    "# print feature shapes\n",
    "X_train = np.load(LOCAL_DATA_ROOT / \"extracted_features\" / \"X_train.npy\", allow_pickle=True)\n",
    "X_test = np.load(LOCAL_DATA_ROOT / \"extracted_features\" / \"X_test.npy\", allow_pickle=True)\n",
    "y_train = np.load(LOCAL_DATA_ROOT / \"labels\" / \"y_train.npy\", allow_pickle=True)\n",
    "sample_subject_array = np.load(LOCAL_DATA_ROOT / \"extracted_features\" / \"sample_subject_array_train.npy\",allow_pickle=True)\n",
    "\n",
    "# sanity checks to ensure validity of the data\n",
    "assert X_train.shape[0]  == y_train.shape[0], \"Mismatch in number of training samples and labels\"\n",
    "assert X_train.shape[1] == X_test.shape[1], \"Mismatch in number of features between train and test sets\"\n",
    "assert clips_tr.shape[0] == y_train.shape[0], \"Mismatch in number of training samples and segments\"\n",
    "assert X_train.shape[0] == sample_subject_array.shape[0], \"Mismatch in number of training samples and subjects\"\n",
    "assert clips_tr.shape[0] == sample_subject_array.shape[0], \"Mismatch in number of training segments and subjects\"\n",
    "\n",
    "print(\"Train extracted features shape:\", X_train.shape)\n",
    "print(\"Test extracted features shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- Extracted features -----------------------------------------------#\n",
    "channels = ['FP1', 'FP2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2', 'F7', 'F8', 'T3', 'T4', 'T5', 'T6', 'FZ', 'CZ', 'PZ']\n",
    "features = [\n",
    "    \"rms\", \"linelen\", \"hj_mob\", \"hj_cmp\", \"spec_ent\",\n",
    "    \"alpha_pow\", \"beta_pow\", \"theta_pow\", \"gamma_pow\",\n",
    "    \"rel_alpha\", \"rel_theta\", \"theta_alpha_ratio\"\n",
    "]\n",
    "n_features = len(features)\n",
    "n_channels = len(channels)\n",
    "\n",
    "feature_names = [f\"{ch} - {ft}\" for ch in channels for ft in features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings (WORK IN PROGRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating timeseries dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing TimeseriesEEGDataset in SIGNAL mode.\n",
      "   - Sampling rate: 250 Hz\n",
      "   - Derived segment length: 3000 timesteps.\n",
      "   - Segment length: 3000 timesteps.\n",
      "   ‚úÖ Using existing cached data from data/timeseries_dataset_train_signal/processed\n",
      "üèÅ TimeseriesEEGDataset initialization complete. Loaded 12993 samples.\n",
      "üöÄ Initializing TimeseriesEEGDataset in FEATURE mode.\n",
      "   ‚úÖ Using existing cached data from data/timeseries_dataset_train_features/processed\n",
      "üèÅ TimeseriesEEGDataset initialization complete. Loaded 12993 samples.\n",
      "üöÄ Initializing TimeseriesEEGDataset in SIGNAL mode.\n",
      "   - Sampling rate: 250 Hz\n",
      "   - Derived segment length: 3000 timesteps.\n",
      "   - Segment length: 3000 timesteps.\n",
      "   ‚ö†Ô∏è Info: Column 'label' not found in clips_df. Processing without labels (e.g., test set).\n",
      "   ‚úÖ Using existing cached data from data/timeseries_dataset_test_signal/processed\n",
      "üèÅ TimeseriesEEGDataset initialization complete. Loaded 3614 samples.\n",
      "üöÄ Initializing TimeseriesEEGDataset in FEATURE mode.\n",
      "   ‚ö†Ô∏è Info: Column 'label' not found in clips_df. Processing without labels (e.g., test set).\n",
      "   ‚úÖ Using existing cached data from data/timeseries_dataset_test_features/processed\n",
      "üèÅ TimeseriesEEGDataset initialization complete. Loaded 3614 samples.\n"
     ]
    }
   ],
   "source": [
    "%aimport src.utils.timeseries_eeg_dataset\n",
    "from src.utils.timeseries_eeg_dataset import TimeseriesEEGDataset\n",
    "\n",
    "LOCAL_DATA_ROOT = Path(\"./data\")\n",
    "\n",
    "timeseries_datasets_tr = {\n",
    "    \"signal\": TimeseriesEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_signal\"),\n",
    "        signal_folder=str(DATA_ROOT / 'train'),\n",
    "        clips_df=clips_tr,\n",
    "        mode='signal',\n",
    "    ),\n",
    "    \"feature\": TimeseriesEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_features\"),\n",
    "        signal_folder=str(DATA_ROOT / 'train'),\n",
    "        clips_df=clips_tr,\n",
    "        mode='feature',\n",
    "        feature_file_path=str(LOCAL_DATA_ROOT / \"extracted_features\" / \"X_train.npy\"),\n",
    "    ),\n",
    "    # FIXME: Uncomment the embedding dataset as soon as the embedding file is available\n",
    "    # \"embedding\": TimeseriesEEGDataset(\n",
    "    #     root=str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_embedding\"),\n",
    "    #     signal_folder=str(DATA_ROOT / 'train'),\n",
    "    #     clips_df=clips_tr,\n",
    "    #     mode='embedding',\n",
    "    #     embedding_file_path=str(LOCAL_DATA_ROOT / \"embeddings\" / \"X_train_embedding.npy\"),\n",
    "    #     labels_for_embedding_file_path=str(LOCAL_DATA_ROOT / \"labels\" / \"y_train.npy\")\n",
    "    # ),\n",
    "}\n",
    "\n",
    "timeseries_datasets_te = {\n",
    "    \"signal\": TimeseriesEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_signal\"),\n",
    "        signal_folder=str(DATA_ROOT / 'test'),\n",
    "        clips_df=clips_te,\n",
    "        mode='signal',\n",
    "    ),\n",
    "    \"feature\": TimeseriesEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_features\"),\n",
    "        signal_folder=str(DATA_ROOT / 'test'),\n",
    "        clips_df=clips_te,\n",
    "        mode='feature',\n",
    "        feature_file_path=str(LOCAL_DATA_ROOT / \"extracted_features\" / \"X_test.npy\"),\n",
    "    ),\n",
    "    # FIXME: Uncomment the embedding dataset as soon as the embedding file is available\n",
    "    # \"embedding\": TimeseriesEEGDataset(\n",
    "    #     root=str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_embedding\"),\n",
    "    #     signal_folder=str(DATA_ROOT / 'test'),\n",
    "    #     clips_df=clips_te,\n",
    "    #     mode='embedding',\n",
    "    #     embedding_file_path=str(LOCAL_DATA_ROOT / \"embeddings\" / \"X_test_embedding.npy\"),\n",
    "    #     labels_for_embedding_file_path=str(LOCAL_DATA_ROOT / \"labels\" / \"y_test.npy\")\n",
    "    # ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing GraphEEGDataset in FEATURE mode.\n",
      "   - Root: data/graph_dataset_train\n",
      "   - Mode: FEATURE\n",
      "   - Edge strategy: spatial\n",
      "   - Node Feature Normalization: True\n",
      "   - Loading spatial distances...\n",
      "     - Loaded 180 unique spatial distances relevant to defined channels.\n",
      "‚öôÔ∏è process() called for FEATURE mode. Target processed directory: data/graph_dataset_train/processed\n",
      "   - Starting processing from pre-extracted features...\n",
      "   - Total feature sets to process: 12993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - Processing feature item 1299/12993...\n",
      "     - Processing feature item 2598/12993...\n",
      "     - Processing feature item 3897/12993...\n",
      "     - Processing feature item 5196/12993...\n",
      "     - Processing feature item 6495/12993...\n",
      "     - Processing feature item 7794/12993...\n",
      "     - Processing feature item 9093/12993...\n",
      "     - Processing feature item 10392/12993...\n",
      "     - Processing feature item 11691/12993...\n",
      "     - Processing feature item 12990/12993...\n",
      "     - Processing feature item 12993/12993...\n",
      "   ‚úÖ Processed and saved 12993 items from features.\n",
      "üèÅ process() finished. Total items processed and saved in this run for feature mode: 12993\n",
      "   - Found 12993 existing processed files for feature mode.\n",
      "üèÅ GraphEEGDataset initialization complete. Current mode: FEATURE. Known processed items: 12993\n",
      "Graph training datasets created:\n",
      "  spatial: GraphEEGDataset(12993) (length: 12993)\n",
      "     INFO: Attempting torch.load with weights_only=False for data/graph_dataset_train/processed/data_feat_0.pt\n",
      "    First training sample: Data(x=[19, 12], edge_index=[2, 342], y=[1], original_idx=[1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from src.utils.graph_eeg_dataset import GraphEEGDataset\n",
    "\n",
    "LOCAL_DATA_ROOT = Path(\"./data\")\n",
    "\n",
    "graph_datasets_tr = {\n",
    "    # 'full': GraphEEGDataset(\n",
    "    #     root=str(LOCAL_DATA_ROOT / 'graph_dataset_train'),\n",
    "    #     clips_df=clips_tr,\n",
    "    #     signal_folder=str(DATA_ROOT / 'train'),\n",
    "    #     extracted_features_array=X_train,\n",
    "    #     use_extracted_features=True,\n",
    "    #     edge_strategy='full',\n",
    "    #     spatial_distance_file=str(LOCAL_DATA_ROOT / 'distances_3d.csv'),\n",
    "    #     force_reprocess=False,\n",
    "    #     prefetch_data=False,\n",
    "    # ),\n",
    "    'spatial': GraphEEGDataset(\n",
    "        root=str(LOCAL_DATA_ROOT / 'graph_dataset_train'),\n",
    "        clips_df=clips_tr,\n",
    "        signal_folder=str(DATA_ROOT / 'train'),\n",
    "        extracted_features_array=X_train,\n",
    "        use_extracted_features=True,\n",
    "        edge_strategy='spatial',\n",
    "        spatial_distance_file=str(LOCAL_DATA_ROOT / 'distances_3d.csv'),\n",
    "        force_reprocess=False,\n",
    "        prefetch_data=False,\n",
    "    ),\n",
    "    # 'correlation': GraphEEGDataset(\n",
    "    #     root=str(LOCAL_DATA_ROOT / 'graph_dataset_train'),\n",
    "    #     clips_df=clips_tr,\n",
    "    #     signal_folder=str(DATA_ROOT / 'train'),\n",
    "    #     extracted_features_array=X_train,\n",
    "    #     use_extracted_features=True,\n",
    "    #     edge_strategy='spatial',\n",
    "    #     spatial_distance_file=str(LOCAL_DATA_ROOT / 'distances_3d.csv'),\n",
    "    #     force_reprocess=False,\n",
    "    #     prefetch_data=False,\n",
    "    # ),\n",
    "}\n",
    "\n",
    "# graph_datasets_te = {\n",
    "#     'full': GraphEEGDataset(\n",
    "#         root=str(LOCAL_DATA_ROOT / 'graph_dataset_test'),\n",
    "#         clips_df=clips_te,\n",
    "#         signal_folder=str(DATA_ROOT / 'test'),\n",
    "#         extracted_features_array=X_test,\n",
    "#         use_extracted_features=True,\n",
    "#         edge_strategy='full',\n",
    "#         spatial_distance_file=str(LOCAL_DATA_ROOT / 'distances_3d.csv'),\n",
    "#         force_reprocess=False, # Force reprocess for test dataset\n",
    "#         prefetch_data=False,\n",
    "#     ),\n",
    "#     'spatial': GraphEEGDataset(\n",
    "#         root=str(LOCAL_DATA_ROOT / 'graph_dataset_test'),\n",
    "#         clips_df=clips_te,\n",
    "#         signal_folder=str(DATA_ROOT / 'test'),\n",
    "#         extracted_features_array=X_test,\n",
    "#         use_extracted_features=True,\n",
    "#         edge_strategy='spatial',\n",
    "#         spatial_distance_file=str(LOCAL_DATA_ROOT / 'distances_3d.csv'),\n",
    "#         force_reprocess=False, # Force reprocess for test dataset\n",
    "#         prefetch_data=False,\n",
    "#     ),\n",
    "#     'correlation': GraphEEGDataset(\n",
    "#         root=str(LOCAL_DATA_ROOT / 'graph_dataset_test'),\n",
    "#         clips_df=clips_te,\n",
    "#         signal_folder=str(DATA_ROOT / 'test'),\n",
    "#         extracted_features_array=X_test,\n",
    "#         use_extracted_features=True,\n",
    "#         edge_strategy='correlation',\n",
    "#         spatial_distance_file=str(LOCAL_DATA_ROOT / 'distances_3d.csv'),\n",
    "#         force_reprocess=False, # Force reprocess for test dataset\n",
    "#         prefetch_data=False,\n",
    "#     )\n",
    "# }\n",
    "\n",
    "print(\"Graph training datasets created:\")\n",
    "for key, ds in graph_datasets_tr.items():\n",
    "    print(f\"  {key}: {ds} (length: {len(ds)})\")\n",
    "    if len(ds) > 0:\n",
    "        sample = ds[0]\n",
    "        print(f\"    First training sample: {sample}\")\n",
    "\n",
    "# print(\"\\nGraph test datasets created:\")\n",
    "# for key, ds in graph_datasets_te.items():\n",
    "#     print(f\"  {key}: {ds} (length: {len(ds)})\")\n",
    "#     if len(ds) > 0:\n",
    "#         sample = ds[0]\n",
    "#         print(f\"    First test sample: {sample}, Label (y): {getattr(sample, 'y', None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries datasets train-validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal: Train size = 10394, Val size = 2599\n",
      "feature: Train size = 10394, Val size = 2599\n",
      "{'signal': <torch.utils.data.dataset.Subset object at 0x7fe733a72da0>, 'feature': <torch.utils.data.dataset.Subset object at 0x7fe733a73250>}\n",
      "{'signal': <torch.utils.data.dataset.Subset object at 0x7fe733a73490>, 'feature': <torch.utils.data.dataset.Subset object at 0x7fe733a73c70>}\n",
      "{'signal': <src.utils.timeseries_eeg_dataset.TimeseriesEEGDataset object at 0x7fe96eb83a90>, 'feature': <src.utils.timeseries_eeg_dataset.TimeseriesEEGDataset object at 0x7fe96eb80790>}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# split training dataset into train/val sets if not already done\n",
    "TRAIN_RATIO = 0.8\n",
    "TIMESERIES_TRAIN_SIZE = int(len(timeseries_datasets_tr[\"signal\"]) * TRAIN_RATIO)\n",
    "TIMESERIES_VAL_SIZE = len(timeseries_datasets_tr[\"signal\"]) - TIMESERIES_TRAIN_SIZE\n",
    "\n",
    "# Store original datasets before reassigning\n",
    "original_timeseries_datasets_tr = timeseries_datasets_tr.copy()\n",
    "\n",
    "# Split each training (both timeseries and graph) dataset into train/val\n",
    "timeseries_datasets_tr = {}\n",
    "timeseries_datasets_val = {}\n",
    "for key, ds in original_timeseries_datasets_tr.items():\n",
    "    tr, val = random_split(ds, [TIMESERIES_TRAIN_SIZE, TIMESERIES_VAL_SIZE], generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
    "    timeseries_datasets_tr[key] = tr\n",
    "    timeseries_datasets_val[key] = val\n",
    "    print(f\"{key}: Train size = {len(tr)}, Val size = {len(val)}\")\n",
    "print(timeseries_datasets_tr)\n",
    "print(timeseries_datasets_val)\n",
    "print(timeseries_datasets_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph datasets train-validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spatial: Train size = 10394, Val size = 2599\n",
      "{'spatial': <torch.utils.data.dataset.Subset object at 0x7fe7184f46a0>}\n",
      "{'spatial': <torch.utils.data.dataset.Subset object at 0x7fe7184f4280>}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# split training dataset into train/val sets if not already done\n",
    "TRAIN_RATIO = 0.8\n",
    "GRAPH_TRAIN_SIZE = int(len(graph_datasets_tr[\"spatial\"]) * TRAIN_RATIO)\n",
    "GRAPH_VAL_SIZE = len(graph_datasets_tr[\"spatial\"]) - GRAPH_TRAIN_SIZE\n",
    "\n",
    "# Store original datasets before reassigning\n",
    "original_graph_datasets_tr = graph_datasets_tr\n",
    "\n",
    "# Split each training (both timeseries and graph) dataset into train/val\n",
    "graph_datasets_tr = {}\n",
    "graph_datasets_val = {}\n",
    "for key, ds in original_graph_datasets_tr.items():\n",
    "    tr, val = random_split(ds, [GRAPH_TRAIN_SIZE, GRAPH_VAL_SIZE], generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
    "    graph_datasets_tr[key] = tr\n",
    "    graph_datasets_val[key] = val\n",
    "    print(f\"{key}: Train size = {len(tr)}, Val size = {len(val)}\")\n",
    "print(graph_datasets_tr)\n",
    "print(graph_datasets_val)\n",
    "# print(graph_datasets_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Creating Timeseries DataLoaders‚Ä¶\n",
      "\t  signal: Train loader size = 21, Val loader size = 6, Test loader size = len(timeseries_loader_te[kind])\n",
      "\t  feature: Train loader size = 21, Val loader size = 6, Test loader size = len(timeseries_loader_te[kind])\n",
      "‚úîÔ∏è DataLoaders created in 0.0s\n",
      "‚è≥ Creating Graph DataLoaders‚Ä¶\n",
      "\t  spatial: Train loader size = 21, Val loader size = 6, Test loader size = len(graph_loader_te[kind])\n",
      "‚úîÔ∏è GeoDataLoaders created in 0.0s\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders for both timeseries and graph datasets\n",
    "common_loader_kwargs = dict(\n",
    "    batch_size=512,\n",
    "    num_workers=16,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=8,\n",
    ")\n",
    "\n",
    "dl_start = time.time()\n",
    "timeseries_loader_tr= {}\n",
    "timeseries_loader_val = {}\n",
    "timeseries_loader_te = {}\n",
    "print(\"‚è≥ Creating Timeseries DataLoaders‚Ä¶\")\n",
    "for kind in timeseries_datasets_tr.keys():\n",
    "    timeseries_loader_tr[kind] = DataLoader(timeseries_datasets_tr[kind], shuffle=True,  **common_loader_kwargs) # type: ignore\n",
    "    timeseries_loader_val[kind]   = DataLoader(timeseries_datasets_val[kind], shuffle=False, **common_loader_kwargs) # type: ignore\n",
    "    # timeseries_loader_te[kind]  = DataLoader(timeseries_datasets_te[kind], shuffle=False, **common_loader_kwargs) # type: ignore\n",
    "    print(f\"\\t  {kind}: Train loader size = {len(timeseries_loader_tr[kind])}, \"\n",
    "            f\"Val loader size = {len(timeseries_loader_val[kind])}, \"\n",
    "            f\"Test loader size = len(timeseries_loader_te[kind])\")\n",
    "print(f\"‚úîÔ∏è DataLoaders created in {time.time() - dl_start:.1f}s\")\n",
    "\n",
    "dl_start = time.time()\n",
    "graph_loader_tr= {}\n",
    "graph_loader_val = {}\n",
    "graph_loader_te = {}\n",
    "print(\"‚è≥ Creating Graph DataLoaders‚Ä¶\")\n",
    "for kind in graph_datasets_tr.keys():\n",
    "    graph_loader_tr[kind] = GeoDataLoader(graph_datasets_tr[kind], shuffle=True,  **common_loader_kwargs) # type: ignore\n",
    "    graph_loader_val[kind]   = GeoDataLoader(graph_datasets_val[kind], shuffle=False, **common_loader_kwargs) # type: ignore\n",
    "    # graph_loader_te[kind]  = GeoDataLoader(graph_datasets_te[kind], shuffle=False, **common_loader_kwargs) # type: ignore\n",
    "    print(f\"\\t  {kind}: Train loader size = {len(graph_loader_tr[kind])}, \"\n",
    "            f\"Val loader size = {len(graph_loader_val[kind])}, \"\n",
    "            f\"Test loader size = len(graph_loader_te[kind])\")\n",
    "print(f\"‚úîÔ∏è GeoDataLoaders created in {time.time() - dl_start:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch type: <class 'list'>\n",
      "Batch length: 2\n",
      "torch.Size([512, 19, 3000])\n",
      "torch.Size([512, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch in timeseries_loader_tr['signal']:\n",
    "    # Print batch shape and type\n",
    "    print(f\"Batch type: {type(batch)}\")\n",
    "    print(f\"Batch length: {len(batch)}\")\n",
    "    print(batch[0].shape)    \n",
    "    print(batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional approaches (no additional features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.train import train_model, evaluate_model\n",
    "from src.utils.plot import plot_training_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. LSTM + GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí™ Starting training from epoch 1 to 300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|‚ñé                                                                                  | 1/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     INFO: Attempting torch.load with weights_only=False for data/graph_dataset_train/processed/data_feat_198.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m LSTM_GNN_SUBMISSION_PATH \u001b[38;5;241m=\u001b[39m SUBMISSION_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlstm_gnn_submission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# train model on training set (or load existing)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_datasets_tr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspatial\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_datasets_val\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspatial\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLSTM_GNN_SAVE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_oversampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:260\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, use_oversampling)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(curr_batch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(curr_batch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor GNN mode, batch_data must have \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 260\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcurr_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcurr_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcurr_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# Non-GNN\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_batch_item, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_batch_item) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/layers/cnn_lstm_gnn.py:104\u001b[0m, in \u001b[0;36mLSTM_GNN_Model.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03mForward pass of the LSTM-GNN model.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Class logits for each graph in the batch. Shape: [num_graphs_in_batch, num_classes].\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# The input 'x' represents a batch of EEG recordings,\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# where each recording has multiple channels (nodes) and time steps.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# x shape: [num_graphs_in_batch, num_channels, time_steps]\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m batch_size, num_channels, time_steps \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Reshape the input to process each channel's time series independently\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# through the channel_encoder.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# New shape: [num_graphs_in_batch * num_channels, time_steps]\u001b[39;00m\n\u001b[1;32m    109\u001b[0m x_reshaped_for_encoder \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m num_channels, time_steps)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "%aimport src.layers.cnn_lstm_gnn\n",
    "from src.layers.cnn_lstm_gnn import LSTM_GNN_Model\n",
    "\n",
    "# build model with current parameters\n",
    "model = LSTM_GNN_Model(\n",
    "    cnn_dropout=0.25,\n",
    "    lstm_hidden_dim=64,\n",
    "    lstm_out_dim=64,\n",
    "    lstm_dropout=0.25,\n",
    "    gcn_hidden_channels=64,\n",
    "    gcn_out_channels=32,\n",
    "    num_gcn_layers=3,\n",
    "    gcn_dropout=0.5,\n",
    "    num_classes=1,\n",
    "    num_channels=19\n",
    ")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "\n",
    "LSTM_GNN_SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_best_model.pt\"\n",
    "LSTM_GNN_SUBMISSION_PATH = SUBMISSION_ROOT / \"lstm_gnn_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        model, graph_datasets_tr[\"spatial\"], graph_datasets_val[\"spatial\"],\n",
    "        criterion, optimizer, device,\n",
    "        save_path=LSTM_GNN_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        scheduler=scheduler,\n",
    "        overwrite=True,\n",
    "        use_gnn=True,\n",
    "        use_oversampling=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "from src.layers.simple_mlp import SimpleMLP\n",
    "\n",
    "# build model with current parameters\n",
    "model = SimpleMLP(\n",
    "    input_dim=228,\n",
    "    hidden_dims=[256, 128],\n",
    "    dropout=0.3,\n",
    "    num_classes=1\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "SIMPLE_MLP_SAVE_PATH = CHECKPOINT_ROOT / \"simple_mlp_best_model.pt\"\n",
    "SIMPLE_MLP_SUBMISSION_PATH = SUBMISSION_ROOT / \"simple_mlp_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        model, timeseries_loader_tr[\"feature\"], timeseries_loader_val[\"feature\"],\n",
    "        criterion, optimizer, device,\n",
    "        save_path=SIMPLE_MLP_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        scheduler=scheduler,\n",
    "        overwrite=True,\n",
    "        use_gnn=False,\n",
    "        use_oversampling=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Feature Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "from src.layers.feature_net import FeatureNet\n",
    "\n",
    "# build model with current parameters\n",
    "model = FeatureNet(\n",
    "    input_dim=665,\n",
    "    hidden_dims=[512, 256, 128],\n",
    "    dropout=0.3,\n",
    "    num_classes=1\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "FEATURE_NET_SAVE_PATH = CHECKPOINT_ROOT / \"feature_net_best_model.pt\"\n",
    "FEATURE_NET_SUBMISSION_PATH = SUBMISSION_ROOT / \"feature_net_submission.csv\"\n",
    "\n",
    "train_history, val_history = train_model(\n",
    "        model, timeseries_loader_tr[\"feature\"], timeseries_loader_val[\"feature\"],\n",
    "        criterion, optimizer, device,\n",
    "        save_path=FEATURE_NET_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        scheduler=scheduler,\n",
    "        overwrite=True,\n",
    "        use_gnn=False,\n",
    "        use_oversampling=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and val loss\n",
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_NET_SAVE_PATH = CHECKPOINT_ROOT / \"feature_net_best_model.pt\"\n",
    "FEATURE_NET_SUBMISSION_PATH = SUBMISSION_ROOT / \"feature_net_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        model, timeseries_loader_tr[\"feature\"], timeseries_loader_val[\"feature\"],\n",
    "        criterion, optimizer, device,\n",
    "        save_path=FEATURE_NET_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        scheduler=scheduler,\n",
    "        overwrite=True,\n",
    "        use_gnn=False,\n",
    "        use_oversampling=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LSTM classifier (baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Signal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.lstm\n",
    "from src.layers.lstm import LSTM\n",
    "\n",
    "# build model with current parameters\n",
    "lstm_model = LSTM(input_dim=19,\n",
    "                hidden_dim=64,\n",
    "                num_layers=4,\n",
    "                dropout=0.3, input_type=\"signal\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    lstm_model = nn.DataParallel(lstm_model)\n",
    "lstm_model = lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"lstm_signal_best_model.pt\"\n",
    "LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"lstm_signal_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        model=lstm_model,\n",
    "        train_loader=timeseries_loader_tr[\"signal\"],\n",
    "        val_loader=timeseries_loader_val[\"signal\"],\n",
    "        criterion=nn.BCEWithLogitsLoss(),\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=100,\n",
    "        use_oversampling=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bidirectional LSTM with early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.lstm\n",
    "from src.layers.lstm import LSTM\n",
    "\n",
    "# build model with current parameters\n",
    "lstm_model = LSTM(input_dim=665,\n",
    "                hidden_dim=64,\n",
    "                num_layers=4,\n",
    "                dropout=0.1,\n",
    "                bidirectional=True)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    lstm_model = nn.DataParallel(lstm_model)\n",
    "lstm_model = lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BI_LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"bi_lstm_feature_best_model.pt\"\n",
    "BI_LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"bi_lstm_feature_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        lstm_model, timeseries_loader_tr[\"feature\"], timeseries_loader_val[\"feature\"],\n",
    "        criterion, optimizer, device,\n",
    "        save_path=BI_LSTM_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        scheduler=scheduler,\n",
    "        overwrite=False,\n",
    "        use_gnn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"], \"BiLSTM [Features]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.utils.train\n",
    "from src.utils.train import evaluate_model\n",
    "# evaluate model on test set and generate submission file\n",
    "evaluate_model(\n",
    "    lstm_model, timeseries_loader_te[\"feature\"], device,\n",
    "    checkpoint_path=BI_LSTM_SAVE_PATH,\n",
    "    submission_path=BI_LSTM_SUBMISSION_PATH,\n",
    "    use_gnn=False,\n",
    "    input_type=\"feature\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features + Oversampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BI_LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"bi_lstm_feature_best_model.pt\"\n",
    "BI_LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"bi_lstm_feature_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "    lstm_model, timeseries_loader_tr[\"feature\"], timeseries_loader_val[\"feature\"],\n",
    "    criterion, optimizer, device,\n",
    "    save_path=BI_LSTM_SAVE_PATH,\n",
    "    num_epochs=300,\n",
    "    patience=10,\n",
    "    monitor=\"val_loss\",\n",
    "    scheduler=scheduler,\n",
    "    overwrite=False,\n",
    "    use_gnn=False,\n",
    "    use_oversampling=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LSTM with Attention and early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.lstm\n",
    "from src.layers.lstm import LSTMAttention\n",
    "\n",
    "# Create model and fit it\n",
    "attention_lstm_model = LSTMAttention(input_dim=665, hidden_dim=64, num_layers=3, dropout=0.2, input_type=\"feature\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    attention_lstm_model = nn.DataParallel(attention_lstm_model)\n",
    "attention_lstm_model = attention_lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(attention_lstm_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTENTION_LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"attention_lstm_feature_best_model.pt\"\n",
    "ATTENTION_LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"attention_lstm_feature_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        attention_lstm_model, timeseries_loader_tr[\"feature\"], timeseries_loader_val[\"feature\"],\n",
    "        criterion, optimizer, device,\n",
    "        save_path=ATTENTION_LSTM_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        scheduler=scheduler,\n",
    "        overwrite=False,\n",
    "        use_gnn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"], \"LSTM with Attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set and generate submission file\n",
    "evaluate_model(\n",
    "    attention_lstm_model, test_loader, device,\n",
    "    checkpoint_path=ATTENTION_LSTM_SAVE_PATH,\n",
    "    submission_path=ATTENTION_LSTM_SUBMISSION_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Signal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.lstm\n",
    "from src.layers.lstm import LSTMAttention\n",
    "\n",
    "# Create model and fit it\n",
    "# For signals [batch_size, 19, 3000] (processing each of the 19 sensors as a sequence of (3000,1) ):\n",
    "attention_lstm_model = LSTMAttention(input_dim=1, hidden_dim=64, num_layers=3, dropout=0.2, input_type=\"feature\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    attention_lstm_model = nn.DataParallel(attention_lstm_model)\n",
    "attention_lstm_model = attention_lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(attention_lstm_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. EEG CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.cnn\n",
    "from src.layers.cnn import EEG_CNN\n",
    "\n",
    "eeg_cnn_model = EEG_CNN(\n",
    "    input_channels=19,\n",
    "    dropout=0.3\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    eeg_cnn_model = nn.DataParallel(eeg_cnn_model)\n",
    "eeg_cnn_model = eeg_cnn_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(eeg_cnn_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    patience=5,\n",
    "    factor=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG_CNN_SAVE_PATH = CHECKPOINT_ROOT / \"eeg_cnn_best_model.pt\"\n",
    "EEG_CNN_SUBMISSION_PATH = SUBMISSION_ROOT / \"eeg_cnn_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        eeg_cnn_model, train_loader, val_loader,\n",
    "        criterion, optimizer, device,\n",
    "        save_path=EEG_CNN_SUBMISSION_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=10,\n",
    "        monitor=\"val_loss\",\n",
    "        scheduler=scheduler,\n",
    "        overwrite=True,\n",
    "        use_gnn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"], \"EEG CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.utils.train\n",
    "from src.utils.train import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set and generate submission file\n",
    "evaluate_model(\n",
    "    eeg_cnn_model, test_loader, device,\n",
    "    checkpoint_path=EEG_CNN_SAVE_PATH,\n",
    "    submission_path=EEG_CNN_SUBMISSION_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. EEG CNN LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.cnn\n",
    "from src.layers.cnn import EEG_CNN_LSTM\n",
    "\n",
    "cnn_lstm_model = EEG_CNN_LSTM(\n",
    "    input_channels=19,\n",
    "    cnn_output_channels=128,\n",
    "    lstm_hidden_dim=128, \n",
    "    fc_dropout=0.3,\n",
    "    lstm_dropout=0.3,\n",
    "    num_classes=1,\n",
    "    bidirectional_lstm=False # unidirectional\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    cnn_lstm_model = nn.DataParallel(cnn_lstm_model)\n",
    "cnn_lstm_model = cnn_lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(cnn_lstm_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    patience=5,\n",
    "    factor=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"cnn_lstm_best_model.pt\"\n",
    "CNN_LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"cnn_lstm_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "        cnn_lstm_model, train_loader, val_loader,\n",
    "        criterion, optimizer, device,\n",
    "        save_path=CNN_LSTM_SAVE_PATH,\n",
    "        num_epochs=300,\n",
    "        patience=30,\n",
    "        monitor=\"val_loss\",\n",
    "        scheduler=scheduler,\n",
    "        overwrite=True,\n",
    "        use_gnn=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"], \"EEG CNN + LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set and generate submission file\n",
    "evaluate_model(\n",
    "    cnn_lstm_model, test_loader, device,\n",
    "    save_path=CNN_LSTM_SAVE_PATH,\n",
    "    submission_path=CNN_LSTM_SUBMISSION_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. EEG CNN BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport layers.cnn\n",
    "from layers.cnn import EEG_CNN_LSTM\n",
    "\n",
    "cnn_bi_lstm_model = EEG_CNN_LSTM(\n",
    "    input_channels=19,\n",
    "    cnn_output_channels=128,\n",
    "    lstm_hidden_dim=128, \n",
    "    fc_dropout=0.3,\n",
    "    lstm_dropout=0.3,\n",
    "    num_classes=1,\n",
    "    bidirectional_lstm=True\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    cnn_bi_lstm_model = nn.DataParallel(cnn_bi_lstm_model)\n",
    "cnn_bi_lstm_model = cnn_bi_lstm_model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(cnn_bi_lstm_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    patience=5,\n",
    "    factor=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_BI_LSTM_SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bi_lstm_best_model.pt\"\n",
    "CNN_BI_LSTM_SUBMISSION_PATH = SUBMISSION_ROOT / \"cnn_bi_lstm_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "    cnn_bi_lstm_model, train_loader, val_loader,\n",
    "    criterion, optimizer, device,\n",
    "    save_path=CNN_BI_LSTM_SAVE_PATH,\n",
    "    num_epochs=300,\n",
    "    patience=30,\n",
    "    monitor=\"val_loss\",\n",
    "    scheduler=scheduler,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"], \"EEG CNN + BiLSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set and generate submission file\n",
    "evaluate_model(\n",
    "    cnn_bi_lstm_model, test_loader, device,\n",
    "    save_path=CNN_BI_LSTM_SAVE_PATH,\n",
    "    submission_path=CNN_BI_LSTM_SUBMISSION_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.eeggcn import EEGGCN\n",
    "\n",
    "# Get number of time points (in_channels) from first sample\n",
    "eeg_gnc_model = EEGGCN(\n",
    "    in_channels=19,\n",
    "    hidden_channels=128,\n",
    "    out_channels=32,\n",
    "    num_classes=2,\n",
    "    num_conv_layers=3,\n",
    "    dropout=0.5\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    eeg_gnc_model = nn.DataParallel(eeg_gnc_model)\n",
    "eeg_gnc_model = eeg_gnc_model.to(device)\n",
    "\n",
    "# Set up optimizer + scheduler\n",
    "optimizer = optim.Adam(eeg_gnc_model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "EEGGCN_SAVE_PATH = CHECKPOINT_ROOT / \"eeggcn_best_model.pt\"\n",
    "EEGCN_SUBMISSION_PATH = SUBMISSION_ROOT / \"eeggcn_submission.csv\"\n",
    "\n",
    "# train model on training set (or load existing)\n",
    "train_history, val_history = train_model(\n",
    "    eeg_gnc_model, train_loader, val_loader,\n",
    "    criterion, optimizer, device,\n",
    "    save_path=EEGGCN_SAVE_PATH,\n",
    "    num_epochs=300,\n",
    "    patience=30,\n",
    "    monitor=\"val_loss\",\n",
    "    scheduler=scheduler,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# plot losses if any\n",
    "plt.figure()\n",
    "plt.plot(train_history[\"loss\"], label=\"train\")\n",
    "plt.plot(val_history[\"loss\"],   label=\"val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\n",
    "    eeg_gnc_model, test_loader, device,\n",
    "    save_path       = Path(\"eeggcn_attn.pt\"),\n",
    "    submission_path = Path(\"eeggcn_submission.csv\"),\n",
    "    threshold       = 0.5,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
