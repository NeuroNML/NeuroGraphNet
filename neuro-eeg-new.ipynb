{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b998b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from torch.utils.data import Subset, WeightedRandomSampler\n",
    "# from torch.utils.data import DataLoader\n",
    "from src.utils.seeder import seed_everything\n",
    "\n",
    "# set seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# create useful constants\n",
    "RANDOM_SEED = 42\n",
    "IS_SCITAS = True # set to True if running on SCITAS cluster\n",
    "LOCAL_DATA_ROOT = Path(\"./data\")\n",
    "DATA_ROOT = Path(\"/home/ogut/data\") if IS_SCITAS else LOCAL_DATA_ROOT\n",
    "CHECKPOINT_ROOT = Path(\"./.checkpoints\")\n",
    "SUBMISSION_ROOT = Path(\"./.submissions\")\n",
    "\n",
    "# create directories if they do not exist\n",
    "CHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# set dataset root\n",
    "seed_everything(RANDOM_SEED)\n",
    "\n",
    "# setup torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d28586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# # execute feature extraction script\n",
    "# try:\n",
    "#     process = subprocess.Popen([\"python3\", \"scripts/feature_extractor.py\"])\n",
    "#     process.wait()\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"Process interrupted, terminating...\")\n",
    "#     process.terminate()\n",
    "#     process.wait()\n",
    "# except Exception as e:\n",
    "#     print(f\"Error occurred: {e}\")\n",
    "#     if 'process' in locals():\n",
    "#         process.terminate()\n",
    "#         process.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45999291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacial distance matrix between sensors\n",
    "spatial_distance_file = LOCAL_DATA_ROOT / \"distances_3d.csv\"\n",
    "\n",
    "# training data\n",
    "train_dir = DATA_ROOT / \"train\"\n",
    "train_dir_metadata = train_dir / \"segments.parquet\"\n",
    "train_dataset_dir = LOCAL_DATA_ROOT / \"graph_dataset_train\"\n",
    "\n",
    "# test data\n",
    "test_dir = DATA_ROOT / \"test\"\n",
    "test_dir_metadata = test_dir / \"segments.parquet\"\n",
    "test_dataset_dir = LOCAL_DATA_ROOT / \"graph_dataset_test\"\n",
    "\n",
    "# additional features\n",
    "extracted_features_dir = LOCAL_DATA_ROOT / \"extracted_features\"\n",
    "embeddings_dir =  LOCAL_DATA_ROOT / \"embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d93851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient   session    segment\n",
      "pqejgcvm  s001_t000  0          pqejgcvm_s001_t000_0\n",
      "                     1          pqejgcvm_s001_t000_1\n",
      "                     2          pqejgcvm_s001_t000_2\n",
      "                     3          pqejgcvm_s001_t000_3\n",
      "                     4          pqejgcvm_s001_t000_4\n",
      "Name: id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from src.utils.index import ensure_eeg_multiindex \n",
    "\n",
    "# Load clips from datasets\n",
    "clips_tr = pd.read_parquet(train_dir_metadata)\n",
    "clips_tr = ensure_eeg_multiindex(clips_tr)\n",
    "clips_tr = clips_tr[~clips_tr.label.isna()].reset_index()  # Filter NaN values out of clips_tr\n",
    "\n",
    "# Load clips from datasets\n",
    "clips_te = pd.read_parquet(test_dir_metadata)\n",
    "clips_te = ensure_eeg_multiindex(clips_te)\n",
    "\n",
    "# Create unique IDs by converting all index components to strings and store in new column\n",
    "clips_te['id'] = clips_te.index.map(lambda x: '_'.join(str(i) for i in x))\n",
    "assert clips_te.id.nunique() == len(clips_te), \"There are duplicate IDs\"\n",
    "print(clips_te[\"id\"].head())\n",
    "\n",
    "# sort in order to maintain the same submission order\n",
    "clips_te = clips_te.sort_values(by=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd45b398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 23:54:36 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-05 23:54:36 - INFO - Dataset parameters:\n",
      "2025-06-05 23:54:36 - INFO -   - Root directory: data/graph_dataset_train\n",
      "2025-06-05 23:54:36 - INFO -   - Edge strategy: spatial\n",
      "2025-06-05 23:54:36 - INFO -   - Top-k neighbors: None\n",
      "2025-06-05 23:54:36 - INFO -   - Correlation threshold: 0.5\n",
      "2025-06-05 23:54:36 - INFO -   - Force reprocess: False\n",
      "2025-06-05 23:54:36 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-05 23:54:36 - INFO -   - Segment length: 3000\n",
      "2025-06-05 23:54:36 - INFO -   - Apply filtering: True\n",
      "2025-06-05 23:54:36 - INFO -   - Apply rereferencing: False\n",
      "2025-06-05 23:54:36 - INFO -   - Apply normalization: False\n",
      "2025-06-05 23:54:36 - INFO -   - Sampling rate: 250\n",
      "2025-06-05 23:54:36 - INFO -   - Test mode: False\n",
      "2025-06-05 23:54:36 - INFO - Number of EEG channels: 19\n",
      "2025-06-05 23:54:36 - INFO - Setting up signal filters...\n",
      "2025-06-05 23:54:36 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-05 23:54:36 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-05 23:54:36 - INFO - Loaded 361 spatial distances in 0.01s\n",
      "2025-06-05 23:54:36 - INFO - Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "Length of train_dataset: 12993\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# dataset settings\n",
    "batch_size = 64\n",
    "selected_features = []\n",
    "embeddings = []\n",
    "edge_strategy = \"spatial\"\n",
    "correlation_threshold = 0.5\n",
    "top_k = None\n",
    "low_bandpass_frequency = 0.5\n",
    "high_bandpass_frequency = 50\n",
    "\n",
    "# additional settings\n",
    "oversampling_power = 1.0\n",
    "\n",
    "# load training dataset\n",
    "dataset_tr = GraphEEGDataset(\n",
    "    root=train_dataset_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    selected_features_train=selected_features,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    embeddings_train=embeddings,\n",
    "    edge_strategy=edge_strategy,\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file if edge_strategy == \"spatial\" else None\n",
    "    ),\n",
    "    top_k=top_k,\n",
    "    correlation_threshold=correlation_threshold,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=False,\n",
    "    apply_normalization=False,\n",
    "    sampling_rate=250,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of train_dataset: {len(dataset_tr)}\")\n",
    "print(f' Eliminated IDs: {dataset_tr.ids_to_eliminate}')\n",
    "\n",
    "# Eliminate ids that did not have electrodes above correlation threshols\n",
    "clips_tr = clips_tr[~clips_tr.index.isin(dataset_tr.ids_to_eliminate)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a32d044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 23:54:32 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-05 23:54:32 - INFO - Dataset parameters:\n",
      "2025-06-05 23:54:32 - INFO -   - Root directory: data/graph_dataset_test\n",
      "2025-06-05 23:54:32 - INFO -   - Edge strategy: spatial\n",
      "2025-06-05 23:54:32 - INFO -   - Top-k neighbors: None\n",
      "2025-06-05 23:54:32 - INFO -   - Correlation threshold: 0.5\n",
      "2025-06-05 23:54:32 - INFO -   - Force reprocess: False\n",
      "2025-06-05 23:54:32 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-05 23:54:32 - INFO -   - Segment length: 3000\n",
      "2025-06-05 23:54:32 - INFO -   - Apply filtering: True\n",
      "2025-06-05 23:54:32 - INFO -   - Apply rereferencing: False\n",
      "2025-06-05 23:54:32 - INFO -   - Apply normalization: False\n",
      "2025-06-05 23:54:32 - INFO -   - Sampling rate: 250\n",
      "2025-06-05 23:54:32 - INFO -   - Test mode: True\n",
      "2025-06-05 23:54:32 - INFO - Number of EEG channels: 19\n",
      "2025-06-05 23:54:32 - INFO - Setting up signal filters...\n",
      "2025-06-05 23:54:32 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-05 23:54:32 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-05 23:54:33 - INFO - Loaded 361 spatial distances in 0.01s\n",
      "2025-06-05 23:54:33 - INFO - Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "Length of test_dataset: 3614\n",
      " Eliminated IDs:[]\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load test dataset\n",
    "te_dataset = GraphEEGDataset(\n",
    "    root=test_dataset_dir,\n",
    "    clips=clips_te,\n",
    "    signal_folder=test_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    selected_features_train=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    embeddings_train=False,\n",
    "    edge_strategy=\"spatial\",\n",
    "    spatial_distance_file=spatial_distance_file,\n",
    "    top_k=None,\n",
    "    correlation_threshold=0.5,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=False,\n",
    "    apply_normalization=False,\n",
    "    sampling_rate=250,\n",
    "    is_test = True,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of test_dataset: {len(te_dataset)}\")\n",
    "print(f' Eliminated IDs:{te_dataset.ids_to_eliminate}')\n",
    "\n",
    "# Eliminate ids that did not have electrodes above correlation threshols\n",
    "clips_te = clips_te[~clips_te.index.isin(te_dataset.ids_to_eliminate)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a52b6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[19, 3000], edge_index=[2, 342], id='pqejgcvm_s001_t000_0')\n"
     ]
    }
   ],
   "source": [
    "for batch in te_dataset:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6943f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels before split\n",
      "[1 1 1 ... 1 1 0]\n",
      "[23:54:42] Train labels: 0 -> 8375, 1 -> 2019\n",
      "[23:54:42] Val labels:   0 -> 2101, 1 -> 498\n",
      "Train batches: 163\n",
      "Val batches: 41\n",
      "Test batches: 57\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "from src.utils.general_funcs import labels_stats\n",
    "\n",
    "# Get total samples and split sizes\n",
    "total_samples = len(dataset_tr)\n",
    "train_size = int(0.8 * total_samples)\n",
    "val_size = total_samples - train_size\n",
    "\n",
    "# Get labels for initial split\n",
    "y = clips_tr[\"label\"].values\n",
    "\n",
    "# Create initial train/val split\n",
    "train_indices, val_indices = random_split(\n",
    "    range(total_samples), \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(RANDOM_SEED)\n",
    ")\n",
    "\n",
    "# Convert to numpy arrays for easier indexing\n",
    "train_indices = np.array(train_indices)\n",
    "val_indices = np.array(val_indices)\n",
    "\n",
    "print('Labels before split', flush=True)\n",
    "print(y, flush=True)\n",
    "\n",
    "# Print stats for class 0 and 1\n",
    "labels_stats(y, train_indices, val_indices)\n",
    "\n",
    "# Create train and val datasets\n",
    "train_dataset = Subset(dataset_tr, train_indices)\n",
    "val_dataset = Subset(dataset_tr, val_indices)\n",
    "\n",
    "# 3. Compute sample weights for oversampling\n",
    "train_labels = [clips_tr.iloc[i][\"label\"] for i in train_indices]\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = (1. / class_counts) ** oversampling_power  # Higher weights for not frequent classes\n",
    "sample_weights = [class_weights[label] for label in train_labels]  # Assign weight to each sample based on its class\n",
    "\n",
    "# 4. Define sampler\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Define dataloaders\n",
    "BATCH_SIZE = 64\n",
    "train_loader = GeoDataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, shuffle=False)\n",
    "val_loader = GeoDataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "te_loader = GeoDataLoader(te_dataset, batch_size=BATCH_SIZE)\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(te_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e3d2ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[1216, 3000], edge_index=[2, 21888], y=[64], batch=[1216], ptr=[65])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1427c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 00:03:31 - INFO - Starting training setup...\n",
      "2025-06-06 00:03:31 - INFO - Model type: GNN\n",
      "2025-06-06 00:03:31 - INFO - Device: cuda\n",
      "2025-06-06 00:03:31 - INFO - Batch size: 64\n",
      "2025-06-06 00:03:31 - INFO - Number of epochs: 100\n",
      "2025-06-06 00:03:31 - INFO - Patience: 15\n",
      "2025-06-06 00:03:31 - INFO - Monitor metric: val_f1\n",
      "2025-06-06 00:03:31 - INFO - Total training batches per epoch: 163\n",
      "2025-06-06 00:03:31 - INFO - Starting training from epoch 1 to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "pos_weight:tensor([1.5000], device='cuda:0')\n",
      "🚀 Attempting to load checkpoint from .checkpoints/lstm_gnn_best_model_35_epochs.pt...\n",
      "   - Loading checkpoint from: .checkpoints/lstm_gnn_best_model_35_epochs.pt\n",
      "   - Detected full checkpoint dictionary.\n",
      "   - Warning: Could not load optimizer state from checkpoint: loaded state dict contains a parameter group that doesn't match the size of optimizer's group\n",
      " ⚠️ Could not load checkpoint: Error(s) in loading state_dict for LSTM_GNN_Model:\n",
      "\tMissing key(s) in state_dict: \"gcn.conv_layers.3.bias\", \"gcn.conv_layers.3.lin.weight\", \"gcn.bn_layers.3.weight\", \"gcn.bn_layers.3.bias\", \"gcn.bn_layers.3.running_mean\", \"gcn.bn_layers.3.running_var\". \n",
      "\tsize mismatch for gcn.conv_layers.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for gcn.conv_layers.2.lin.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for gcn.bn_layers.2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for gcn.bn_layers.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for gcn.bn_layers.2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for gcn.bn_layers.2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).. Starting training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|▊                                                                                  | 1/100 [00:00<?, ?it/s]2025-06-06 00:03:31 - INFO - \n",
      "Epoch 1/100 - Training phase\n",
      "2025-06-06 00:03:32 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:03:32 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:03:32 - INFO - Batch 1/163 - Loss: 0.9557 - Avg batch time: 0.25s\n",
      "2025-06-06 00:03:48 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:03:48 - INFO - Batch 11/163 - Loss: 0.5846 - Avg batch time: 0.25s\n",
      "2025-06-06 00:04:04 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:04:04 - INFO - Batch 21/163 - Loss: 0.6532 - Avg batch time: 0.25s\n",
      "2025-06-06 00:04:19 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:04:19 - INFO - Batch 31/163 - Loss: 0.5424 - Avg batch time: 0.25s\n",
      "2025-06-06 00:04:33 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:04:33 - INFO - Batch 41/163 - Loss: 0.5044 - Avg batch time: 0.25s\n",
      "2025-06-06 00:04:46 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:04:47 - INFO - Batch 51/163 - Loss: 0.6488 - Avg batch time: 0.25s\n",
      "2025-06-06 00:05:01 - INFO - Processing batch 61/163\n",
      "2025-06-06 00:05:01 - INFO - Batch 61/163 - Loss: 0.4821 - Avg batch time: 0.25s\n",
      "2025-06-06 00:05:14 - INFO - Processing batch 71/163\n",
      "2025-06-06 00:05:14 - INFO - Batch 71/163 - Loss: 0.5356 - Avg batch time: 0.25s\n",
      "2025-06-06 00:05:27 - INFO - Processing batch 81/163\n",
      "2025-06-06 00:05:28 - INFO - Batch 81/163 - Loss: 0.5983 - Avg batch time: 0.25s\n",
      "2025-06-06 00:05:40 - INFO - Processing batch 91/163\n",
      "2025-06-06 00:05:40 - INFO - Batch 91/163 - Loss: 0.6043 - Avg batch time: 0.25s\n",
      "2025-06-06 00:05:53 - INFO - Processing batch 101/163\n",
      "2025-06-06 00:05:53 - INFO - Batch 101/163 - Loss: 0.5462 - Avg batch time: 0.25s\n",
      "2025-06-06 00:06:06 - INFO - Processing batch 111/163\n",
      "2025-06-06 00:06:06 - INFO - Batch 111/163 - Loss: 0.5915 - Avg batch time: 0.25s\n",
      "2025-06-06 00:06:18 - INFO - Processing batch 121/163\n",
      "2025-06-06 00:06:18 - INFO - Batch 121/163 - Loss: 0.5067 - Avg batch time: 0.25s\n",
      "2025-06-06 00:06:30 - INFO - Processing batch 131/163\n",
      "2025-06-06 00:06:30 - INFO - Batch 131/163 - Loss: 0.4152 - Avg batch time: 0.25s\n",
      "2025-06-06 00:06:42 - INFO - Processing batch 141/163\n",
      "2025-06-06 00:06:42 - INFO - Batch 141/163 - Loss: 0.6386 - Avg batch time: 0.25s\n",
      "2025-06-06 00:06:53 - INFO - Processing batch 151/163\n",
      "2025-06-06 00:06:54 - INFO - Batch 151/163 - Loss: 0.6149 - Avg batch time: 0.25s\n",
      "2025-06-06 00:07:05 - INFO - Processing batch 161/163\n",
      "2025-06-06 00:07:05 - INFO - Batch 161/163 - Loss: 0.6294 - Avg batch time: 0.25s\n",
      "2025-06-06 00:07:07 - INFO - \n",
      "Epoch 1 training completed in 216.49s\n",
      "2025-06-06 00:07:07 - INFO - Average training loss: 0.5635\n",
      "Epochs:   2%| | 2/100 [04:54<8:01:46, 294.97s/it, train_loss=0.5635, val_loss=0.6869, best_val_f1=0.5291, lr=1.00e-03, b2025-06-06 00:08:26 - INFO - \n",
      "Epoch 2/100 - Training phase\n",
      "2025-06-06 00:08:26 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:08:26 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:08:27 - INFO - Batch 1/163 - Loss: 0.8414 - Avg batch time: 0.25s\n",
      "2025-06-06 00:08:38 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:08:39 - INFO - Batch 11/163 - Loss: 0.6091 - Avg batch time: 0.25s\n",
      "2025-06-06 00:08:50 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:08:50 - INFO - Batch 21/163 - Loss: 0.5537 - Avg batch time: 0.25s\n",
      "2025-06-06 00:09:01 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:09:01 - INFO - Batch 31/163 - Loss: 0.5345 - Avg batch time: 0.25s\n",
      "2025-06-06 00:09:12 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:09:12 - INFO - Batch 41/163 - Loss: 0.5446 - Avg batch time: 0.25s\n",
      "2025-06-06 00:09:23 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:09:23 - INFO - Batch 51/163 - Loss: 0.5149 - Avg batch time: 0.25s\n",
      "2025-06-06 00:09:34 - INFO - Processing batch 61/163\n",
      "2025-06-06 00:09:34 - INFO - Batch 61/163 - Loss: 0.5813 - Avg batch time: 0.25s\n",
      "2025-06-06 00:09:45 - INFO - Processing batch 71/163\n",
      "2025-06-06 00:09:45 - INFO - Batch 71/163 - Loss: 0.5724 - Avg batch time: 0.25s\n",
      "2025-06-06 00:09:55 - INFO - Processing batch 81/163\n",
      "2025-06-06 00:09:56 - INFO - Batch 81/163 - Loss: 0.4211 - Avg batch time: 0.25s\n",
      "2025-06-06 00:10:06 - INFO - Processing batch 91/163\n",
      "2025-06-06 00:10:06 - INFO - Batch 91/163 - Loss: 0.5812 - Avg batch time: 0.25s\n",
      "2025-06-06 00:10:17 - INFO - Processing batch 101/163\n",
      "2025-06-06 00:10:17 - INFO - Batch 101/163 - Loss: 0.5853 - Avg batch time: 0.25s\n",
      "2025-06-06 00:10:27 - INFO - Processing batch 111/163\n",
      "2025-06-06 00:10:27 - INFO - Batch 111/163 - Loss: 0.5579 - Avg batch time: 0.25s\n",
      "2025-06-06 00:10:38 - INFO - Processing batch 121/163\n",
      "2025-06-06 00:10:38 - INFO - Batch 121/163 - Loss: 0.5344 - Avg batch time: 0.25s\n",
      "2025-06-06 00:10:48 - INFO - Processing batch 131/163\n",
      "2025-06-06 00:10:49 - INFO - Batch 131/163 - Loss: 0.4108 - Avg batch time: 0.25s\n",
      "2025-06-06 00:10:58 - INFO - Processing batch 141/163\n",
      "2025-06-06 00:10:59 - INFO - Batch 141/163 - Loss: 0.4942 - Avg batch time: 0.25s\n",
      "2025-06-06 00:11:09 - INFO - Processing batch 151/163\n",
      "2025-06-06 00:11:09 - INFO - Batch 151/163 - Loss: 0.5302 - Avg batch time: 0.25s\n",
      "2025-06-06 00:11:19 - INFO - Processing batch 161/163\n",
      "2025-06-06 00:11:19 - INFO - Batch 161/163 - Loss: 0.7699 - Avg batch time: 0.25s\n",
      "2025-06-06 00:11:21 - INFO - \n",
      "Epoch 2 training completed in 175.00s\n",
      "2025-06-06 00:11:21 - INFO - Average training loss: 0.5597\n",
      "Epochs:   3%| | 3/100 [08:20<6:32:03, 242.52s/it, train_loss=0.5597, val_loss=0.6767, best_val_f1=0.5397, lr=1.00e-03, b2025-06-06 00:11:51 - INFO - \n",
      "Epoch 3/100 - Training phase\n",
      "2025-06-06 00:11:52 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:11:52 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:11:52 - INFO - Batch 1/163 - Loss: 0.4290 - Avg batch time: 0.25s\n",
      "2025-06-06 00:12:02 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:12:03 - INFO - Batch 11/163 - Loss: 0.4989 - Avg batch time: 0.25s\n",
      "2025-06-06 00:12:13 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:12:13 - INFO - Batch 21/163 - Loss: 0.5062 - Avg batch time: 0.25s\n",
      "2025-06-06 00:12:23 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:12:23 - INFO - Batch 31/163 - Loss: 0.4821 - Avg batch time: 0.25s\n",
      "2025-06-06 00:12:33 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:12:33 - INFO - Batch 41/163 - Loss: 0.5899 - Avg batch time: 0.25s\n",
      "2025-06-06 00:12:43 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:12:43 - INFO - Batch 51/163 - Loss: 0.5057 - Avg batch time: 0.25s\n",
      "2025-06-06 00:12:53 - INFO - Processing batch 61/163\n",
      "2025-06-06 00:12:53 - INFO - Batch 61/163 - Loss: 0.5313 - Avg batch time: 0.25s\n",
      "2025-06-06 00:13:03 - INFO - Processing batch 71/163\n",
      "2025-06-06 00:13:03 - INFO - Batch 71/163 - Loss: 0.4124 - Avg batch time: 0.25s\n",
      "2025-06-06 00:13:13 - INFO - Processing batch 81/163\n",
      "2025-06-06 00:13:13 - INFO - Batch 81/163 - Loss: 0.8368 - Avg batch time: 0.25s\n",
      "2025-06-06 00:13:23 - INFO - Processing batch 91/163\n",
      "2025-06-06 00:13:23 - INFO - Batch 91/163 - Loss: 0.6823 - Avg batch time: 0.25s\n",
      "2025-06-06 00:13:33 - INFO - Processing batch 101/163\n",
      "2025-06-06 00:13:33 - INFO - Batch 101/163 - Loss: 0.6886 - Avg batch time: 0.25s\n",
      "2025-06-06 00:13:43 - INFO - Processing batch 111/163\n",
      "2025-06-06 00:13:43 - INFO - Batch 111/163 - Loss: 0.5793 - Avg batch time: 0.25s\n",
      "2025-06-06 00:13:52 - INFO - Processing batch 121/163\n",
      "2025-06-06 00:13:53 - INFO - Batch 121/163 - Loss: 0.4539 - Avg batch time: 0.25s\n",
      "2025-06-06 00:14:02 - INFO - Processing batch 131/163\n",
      "2025-06-06 00:14:02 - INFO - Batch 131/163 - Loss: 0.4962 - Avg batch time: 0.25s\n",
      "2025-06-06 00:14:12 - INFO - Processing batch 141/163\n",
      "2025-06-06 00:14:12 - INFO - Batch 141/163 - Loss: 0.5397 - Avg batch time: 0.25s\n",
      "2025-06-06 00:14:21 - INFO - Processing batch 151/163\n",
      "2025-06-06 00:14:21 - INFO - Batch 151/163 - Loss: 0.6139 - Avg batch time: 0.25s\n",
      "2025-06-06 00:14:30 - INFO - Processing batch 161/163\n",
      "2025-06-06 00:14:31 - INFO - Batch 161/163 - Loss: 0.5873 - Avg batch time: 0.25s\n",
      "2025-06-06 00:14:32 - INFO - \n",
      "Epoch 3 training completed in 160.69s\n",
      "2025-06-06 00:14:32 - INFO - Average training loss: 0.5395\n",
      "Epochs:   4%| | 4/100 [11:30<5:49:44, 218.59s/it, train_loss=0.5395, val_loss=0.4361, best_val_f1=0.6839, lr=1.00e-03, b2025-06-06 00:15:02 - INFO - \n",
      "Epoch 4/100 - Training phase\n",
      "2025-06-06 00:15:02 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:15:02 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:15:03 - INFO - Batch 1/163 - Loss: 0.5255 - Avg batch time: 0.25s\n",
      "2025-06-06 00:15:12 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:15:13 - INFO - Batch 11/163 - Loss: 0.4552 - Avg batch time: 0.25s\n",
      "2025-06-06 00:15:22 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:15:22 - INFO - Batch 21/163 - Loss: 0.6723 - Avg batch time: 0.25s\n",
      "2025-06-06 00:15:31 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:15:32 - INFO - Batch 31/163 - Loss: 0.5213 - Avg batch time: 0.25s\n",
      "2025-06-06 00:15:41 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:15:41 - INFO - Batch 41/163 - Loss: 0.5642 - Avg batch time: 0.25s\n",
      "2025-06-06 00:15:51 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:15:51 - INFO - Batch 51/163 - Loss: 0.4852 - Avg batch time: 0.25s\n",
      "2025-06-06 00:16:00 - INFO - Processing batch 61/163\n",
      "2025-06-06 00:16:00 - INFO - Batch 61/163 - Loss: 0.4859 - Avg batch time: 0.25s\n",
      "2025-06-06 00:16:10 - INFO - Processing batch 71/163\n",
      "2025-06-06 00:16:10 - INFO - Batch 71/163 - Loss: 0.4924 - Avg batch time: 0.25s\n",
      "2025-06-06 00:16:19 - INFO - Processing batch 81/163\n",
      "2025-06-06 00:16:19 - INFO - Batch 81/163 - Loss: 0.3953 - Avg batch time: 0.25s\n",
      "2025-06-06 00:16:29 - INFO - Processing batch 91/163\n",
      "2025-06-06 00:16:29 - INFO - Batch 91/163 - Loss: 0.4916 - Avg batch time: 0.25s\n",
      "2025-06-06 00:16:38 - INFO - Processing batch 101/163\n",
      "2025-06-06 00:16:38 - INFO - Batch 101/163 - Loss: 0.5172 - Avg batch time: 0.25s\n",
      "2025-06-06 00:16:47 - INFO - Processing batch 111/163\n",
      "2025-06-06 00:16:48 - INFO - Batch 111/163 - Loss: 0.6189 - Avg batch time: 0.25s\n",
      "2025-06-06 00:16:57 - INFO - Processing batch 121/163\n",
      "2025-06-06 00:16:57 - INFO - Batch 121/163 - Loss: 0.5668 - Avg batch time: 0.25s\n",
      "2025-06-06 00:17:06 - INFO - Processing batch 131/163\n",
      "2025-06-06 00:17:06 - INFO - Batch 131/163 - Loss: 0.4848 - Avg batch time: 0.25s\n",
      "2025-06-06 00:17:15 - INFO - Processing batch 141/163\n",
      "2025-06-06 00:17:15 - INFO - Batch 141/163 - Loss: 0.5914 - Avg batch time: 0.25s\n",
      "2025-06-06 00:17:24 - INFO - Processing batch 151/163\n",
      "2025-06-06 00:17:24 - INFO - Batch 151/163 - Loss: 0.4552 - Avg batch time: 0.25s\n",
      "2025-06-06 00:17:33 - INFO - Processing batch 161/163\n",
      "2025-06-06 00:17:34 - INFO - Batch 161/163 - Loss: 0.4527 - Avg batch time: 0.25s\n",
      "2025-06-06 00:17:35 - INFO - \n",
      "Epoch 4 training completed in 153.30s\n",
      "2025-06-06 00:17:35 - INFO - Average training loss: 0.5344\n",
      "Epochs:   5%| | 5/100 [14:34<5:24:13, 204.77s/it, train_loss=0.5344, val_loss=0.5221, best_val_f1=0.6839, lr=1.00e-03, b2025-06-06 00:18:05 - INFO - \n",
      "Epoch 5/100 - Training phase\n",
      "2025-06-06 00:18:06 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:18:06 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:18:06 - INFO - Batch 1/163 - Loss: 0.6262 - Avg batch time: 0.25s\n",
      "2025-06-06 00:18:15 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:18:15 - INFO - Batch 11/163 - Loss: 0.5246 - Avg batch time: 0.25s\n",
      "2025-06-06 00:18:24 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:18:25 - INFO - Batch 21/163 - Loss: 0.4948 - Avg batch time: 0.25s\n",
      "2025-06-06 00:18:34 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:18:34 - INFO - Batch 31/163 - Loss: 0.4562 - Avg batch time: 0.25s\n",
      "2025-06-06 00:18:43 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:18:44 - INFO - Batch 41/163 - Loss: 0.3916 - Avg batch time: 0.25s\n",
      "2025-06-06 00:18:52 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:18:53 - INFO - Batch 51/163 - Loss: 0.4432 - Avg batch time: 0.25s\n",
      "2025-06-06 00:19:01 - INFO - Processing batch 61/163\n",
      "2025-06-06 00:19:02 - INFO - Batch 61/163 - Loss: 0.5253 - Avg batch time: 0.25s\n",
      "2025-06-06 00:19:10 - INFO - Processing batch 71/163\n",
      "2025-06-06 00:19:11 - INFO - Batch 71/163 - Loss: 0.4919 - Avg batch time: 0.25s\n",
      "2025-06-06 00:19:19 - INFO - Processing batch 81/163\n",
      "2025-06-06 00:19:20 - INFO - Batch 81/163 - Loss: 0.6105 - Avg batch time: 0.25s\n",
      "2025-06-06 00:19:28 - INFO - Processing batch 91/163\n",
      "2025-06-06 00:19:28 - INFO - Batch 91/163 - Loss: 0.5353 - Avg batch time: 0.25s\n",
      "2025-06-06 00:19:37 - INFO - Processing batch 101/163\n",
      "2025-06-06 00:19:37 - INFO - Batch 101/163 - Loss: 0.5187 - Avg batch time: 0.25s\n",
      "2025-06-06 00:19:46 - INFO - Processing batch 111/163\n",
      "2025-06-06 00:19:47 - INFO - Batch 111/163 - Loss: 0.4146 - Avg batch time: 0.25s\n",
      "2025-06-06 00:19:55 - INFO - Processing batch 121/163\n",
      "2025-06-06 00:19:56 - INFO - Batch 121/163 - Loss: 0.4307 - Avg batch time: 0.25s\n",
      "2025-06-06 00:20:05 - INFO - Processing batch 131/163\n",
      "2025-06-06 00:20:05 - INFO - Batch 131/163 - Loss: 0.4620 - Avg batch time: 0.25s\n",
      "2025-06-06 00:20:13 - INFO - Processing batch 141/163\n",
      "2025-06-06 00:20:14 - INFO - Batch 141/163 - Loss: 0.3769 - Avg batch time: 0.25s\n",
      "2025-06-06 00:20:23 - INFO - Processing batch 151/163\n",
      "2025-06-06 00:20:23 - INFO - Batch 151/163 - Loss: 0.3414 - Avg batch time: 0.25s\n",
      "2025-06-06 00:20:32 - INFO - Processing batch 161/163\n",
      "2025-06-06 00:20:32 - INFO - Batch 161/163 - Loss: 0.7605 - Avg batch time: 0.25s\n",
      "2025-06-06 00:20:33 - INFO - \n",
      "Epoch 5 training completed in 147.98s\n",
      "2025-06-06 00:20:33 - INFO - Average training loss: 0.5067\n",
      "Epochs:   6%| | 6/100 [17:32<5:05:31, 195.01s/it, train_loss=0.5067, val_loss=0.4994, best_val_f1=0.6839, lr=1.00e-03, b2025-06-06 00:21:03 - INFO - \n",
      "Epoch 6/100 - Training phase\n",
      "2025-06-06 00:21:03 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:21:03 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:21:04 - INFO - Batch 1/163 - Loss: 0.4327 - Avg batch time: 0.25s\n",
      "2025-06-06 00:21:12 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:21:13 - INFO - Batch 11/163 - Loss: 0.4291 - Avg batch time: 0.25s\n",
      "2025-06-06 00:21:22 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:21:22 - INFO - Batch 21/163 - Loss: 0.6106 - Avg batch time: 0.25s\n",
      "2025-06-06 00:21:31 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:21:31 - INFO - Batch 31/163 - Loss: 0.4661 - Avg batch time: 0.25s\n",
      "2025-06-06 00:21:40 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:21:40 - INFO - Batch 41/163 - Loss: 0.4841 - Avg batch time: 0.25s\n",
      "2025-06-06 00:21:50 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:21:50 - INFO - Batch 51/163 - Loss: 0.5679 - Avg batch time: 0.25s\n",
      "2025-06-06 00:21:59 - INFO - Processing batch 61/163\n",
      "2025-06-06 00:21:59 - INFO - Batch 61/163 - Loss: 0.4746 - Avg batch time: 0.25s\n",
      "2025-06-06 00:22:08 - INFO - Processing batch 71/163\n",
      "2025-06-06 00:22:08 - INFO - Batch 71/163 - Loss: 0.6252 - Avg batch time: 0.25s\n",
      "2025-06-06 00:22:17 - INFO - Processing batch 81/163\n",
      "2025-06-06 00:22:17 - INFO - Batch 81/163 - Loss: 0.4880 - Avg batch time: 0.25s\n",
      "2025-06-06 00:22:26 - INFO - Processing batch 91/163\n",
      "2025-06-06 00:22:26 - INFO - Batch 91/163 - Loss: 0.3670 - Avg batch time: 0.25s\n",
      "2025-06-06 00:22:35 - INFO - Processing batch 101/163\n",
      "2025-06-06 00:22:35 - INFO - Batch 101/163 - Loss: 0.3848 - Avg batch time: 0.25s\n",
      "2025-06-06 00:22:44 - INFO - Processing batch 111/163\n",
      "2025-06-06 00:22:44 - INFO - Batch 111/163 - Loss: 0.6921 - Avg batch time: 0.25s\n",
      "2025-06-06 00:22:53 - INFO - Processing batch 121/163\n",
      "2025-06-06 00:22:53 - INFO - Batch 121/163 - Loss: 0.3883 - Avg batch time: 0.25s\n",
      "2025-06-06 00:23:02 - INFO - Processing batch 131/163\n",
      "2025-06-06 00:23:02 - INFO - Batch 131/163 - Loss: 0.4028 - Avg batch time: 0.25s\n",
      "2025-06-06 00:23:11 - INFO - Processing batch 141/163\n",
      "2025-06-06 00:23:11 - INFO - Batch 141/163 - Loss: 0.5649 - Avg batch time: 0.25s\n",
      "2025-06-06 00:23:20 - INFO - Processing batch 151/163\n",
      "2025-06-06 00:23:20 - INFO - Batch 151/163 - Loss: 0.3775 - Avg batch time: 0.25s\n",
      "2025-06-06 00:23:29 - INFO - Processing batch 161/163\n",
      "2025-06-06 00:23:29 - INFO - Batch 161/163 - Loss: 0.5936 - Avg batch time: 0.25s\n",
      "2025-06-06 00:23:30 - INFO - \n",
      "Epoch 6 training completed in 147.33s\n",
      "2025-06-06 00:23:30 - INFO - Average training loss: 0.5110\n",
      "Epochs:   7%| | 7/100 [20:29<4:53:03, 189.07s/it, train_loss=0.5110, val_loss=0.4473, best_val_f1=0.6839, lr=1.00e-03, b2025-06-06 00:24:00 - INFO - \n",
      "Epoch 7/100 - Training phase\n",
      "2025-06-06 00:24:01 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:24:01 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:24:01 - INFO - Batch 1/163 - Loss: 0.3981 - Avg batch time: 0.25s\n",
      "2025-06-06 00:24:10 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:24:10 - INFO - Batch 11/163 - Loss: 0.5234 - Avg batch time: 0.25s\n",
      "2025-06-06 00:24:19 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:24:19 - INFO - Batch 21/163 - Loss: 0.3268 - Avg batch time: 0.25s\n",
      "2025-06-06 00:24:28 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:24:29 - INFO - Batch 31/163 - Loss: 0.4287 - Avg batch time: 0.25s\n",
      "2025-06-06 00:24:37 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:24:38 - INFO - Batch 41/163 - Loss: 0.5951 - Avg batch time: 0.25s\n",
      "2025-06-06 00:24:46 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:24:46 - INFO - Batch 51/163 - Loss: 0.5625 - Avg batch time: 0.25s\n",
      "2025-06-06 00:24:55 - INFO - Processing batch 61/163\n",
      "2025-06-06 00:24:55 - INFO - Batch 61/163 - Loss: 0.4281 - Avg batch time: 0.25s\n",
      "2025-06-06 00:25:04 - INFO - Processing batch 71/163\n",
      "2025-06-06 00:25:04 - INFO - Batch 71/163 - Loss: 0.4947 - Avg batch time: 0.25s\n",
      "2025-06-06 00:25:13 - INFO - Processing batch 81/163\n",
      "2025-06-06 00:25:13 - INFO - Batch 81/163 - Loss: 0.3650 - Avg batch time: 0.25s\n",
      "2025-06-06 00:25:22 - INFO - Processing batch 91/163\n",
      "2025-06-06 00:25:22 - INFO - Batch 91/163 - Loss: 0.6044 - Avg batch time: 0.25s\n",
      "2025-06-06 00:25:30 - INFO - Processing batch 101/163\n",
      "2025-06-06 00:25:31 - INFO - Batch 101/163 - Loss: 0.5440 - Avg batch time: 0.25s\n",
      "2025-06-06 00:25:39 - INFO - Processing batch 111/163\n",
      "2025-06-06 00:25:39 - INFO - Batch 111/163 - Loss: 0.4065 - Avg batch time: 0.25s\n",
      "2025-06-06 00:25:48 - INFO - Processing batch 121/163\n",
      "2025-06-06 00:25:48 - INFO - Batch 121/163 - Loss: 0.5198 - Avg batch time: 0.25s\n",
      "2025-06-06 00:25:57 - INFO - Processing batch 131/163\n",
      "2025-06-06 00:25:57 - INFO - Batch 131/163 - Loss: 0.6400 - Avg batch time: 0.25s\n",
      "2025-06-06 00:26:06 - INFO - Processing batch 141/163\n",
      "2025-06-06 00:26:06 - INFO - Batch 141/163 - Loss: 0.6459 - Avg batch time: 0.25s\n",
      "2025-06-06 00:26:15 - INFO - Processing batch 151/163\n",
      "2025-06-06 00:26:15 - INFO - Batch 151/163 - Loss: 0.3990 - Avg batch time: 0.25s\n",
      "2025-06-06 00:26:24 - INFO - Processing batch 161/163\n",
      "2025-06-06 00:26:24 - INFO - Batch 161/163 - Loss: 0.3898 - Avg batch time: 0.25s\n",
      "2025-06-06 00:26:25 - INFO - \n",
      "Epoch 7 training completed in 144.76s\n",
      "2025-06-06 00:26:25 - INFO - Average training loss: 0.4774\n",
      "Epochs:   8%| | 8/100 [23:24<4:42:41, 184.36s/it, train_loss=0.4774, val_loss=0.3685, best_val_f1=0.7181, lr=5.00e-04, b2025-06-06 00:26:55 - INFO - \n",
      "Epoch 8/100 - Training phase\n",
      "2025-06-06 00:26:56 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:26:56 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:26:56 - INFO - Batch 1/163 - Loss: 0.7501 - Avg batch time: 0.25s\n",
      "2025-06-06 00:27:05 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:27:05 - INFO - Batch 11/163 - Loss: 0.5747 - Avg batch time: 0.25s\n",
      "2025-06-06 00:27:14 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:27:14 - INFO - Batch 21/163 - Loss: 0.4677 - Avg batch time: 0.25s\n",
      "2025-06-06 00:27:24 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:27:24 - INFO - Batch 31/163 - Loss: 0.4164 - Avg batch time: 0.25s\n",
      "2025-06-06 00:27:33 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:27:33 - INFO - Batch 41/163 - Loss: 0.3259 - Avg batch time: 0.25s\n",
      "2025-06-06 00:27:42 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:27:42 - INFO - Batch 51/163 - Loss: 0.7096 - Avg batch time: 0.25s\n",
      "2025-06-06 00:27:51 - INFO - Processing batch 61/163\n",
      "2025-06-06 00:27:51 - INFO - Batch 61/163 - Loss: 0.5025 - Avg batch time: 0.25s\n",
      "2025-06-06 00:28:00 - INFO - Processing batch 71/163\n",
      "2025-06-06 00:28:00 - INFO - Batch 71/163 - Loss: 0.3719 - Avg batch time: 0.25s\n",
      "2025-06-06 00:28:09 - INFO - Processing batch 81/163\n",
      "2025-06-06 00:28:09 - INFO - Batch 81/163 - Loss: 0.4086 - Avg batch time: 0.25s\n",
      "2025-06-06 00:28:19 - INFO - Processing batch 91/163\n",
      "2025-06-06 00:28:19 - INFO - Batch 91/163 - Loss: 0.4572 - Avg batch time: 0.25s\n",
      "2025-06-06 00:28:28 - INFO - Processing batch 101/163\n",
      "2025-06-06 00:28:28 - INFO - Batch 101/163 - Loss: 0.4558 - Avg batch time: 0.25s\n",
      "2025-06-06 00:28:37 - INFO - Processing batch 111/163\n",
      "2025-06-06 00:28:37 - INFO - Batch 111/163 - Loss: 0.4140 - Avg batch time: 0.25s\n",
      "2025-06-06 00:28:46 - INFO - Processing batch 121/163\n",
      "2025-06-06 00:28:46 - INFO - Batch 121/163 - Loss: 0.3589 - Avg batch time: 0.25s\n",
      "2025-06-06 00:28:54 - INFO - Processing batch 131/163\n",
      "2025-06-06 00:28:55 - INFO - Batch 131/163 - Loss: 0.4315 - Avg batch time: 0.25s\n",
      "2025-06-06 00:29:03 - INFO - Processing batch 141/163\n",
      "2025-06-06 00:29:04 - INFO - Batch 141/163 - Loss: 0.5438 - Avg batch time: 0.25s\n",
      "2025-06-06 00:29:12 - INFO - Processing batch 151/163\n",
      "2025-06-06 00:29:13 - INFO - Batch 151/163 - Loss: 0.4252 - Avg batch time: 0.25s\n",
      "2025-06-06 00:29:21 - INFO - Processing batch 161/163\n",
      "2025-06-06 00:29:22 - INFO - Batch 161/163 - Loss: 0.4944 - Avg batch time: 0.25s\n",
      "2025-06-06 00:29:23 - INFO - \n",
      "Epoch 8 training completed in 147.91s\n",
      "2025-06-06 00:29:23 - INFO - Average training loss: 0.4422\n",
      "Epochs:   9%| | 9/100 [26:23<4:37:05, 182.70s/it, train_loss=0.4422, val_loss=0.4698, best_val_f1=0.7181, lr=5.00e-04, b2025-06-06 00:29:54 - INFO - \n",
      "Epoch 9/100 - Training phase\n",
      "2025-06-06 00:29:55 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:29:55 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:29:55 - INFO - Batch 1/163 - Loss: 0.4632 - Avg batch time: 0.25s\n",
      "2025-06-06 00:30:04 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:30:04 - INFO - Batch 11/163 - Loss: 0.4995 - Avg batch time: 0.25s\n",
      "2025-06-06 00:30:13 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:30:13 - INFO - Batch 21/163 - Loss: 0.3641 - Avg batch time: 0.25s\n",
      "2025-06-06 00:30:22 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:30:22 - INFO - Batch 31/163 - Loss: 0.5097 - Avg batch time: 0.25s\n",
      "2025-06-06 00:30:31 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:30:31 - INFO - Batch 41/163 - Loss: 0.6260 - Avg batch time: 0.25s\n",
      "2025-06-06 00:30:40 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:30:40 - INFO - Batch 51/163 - Loss: 0.3238 - Avg batch time: 0.25s\n",
      "2025-06-06 00:30:49 - INFO - Processing batch 61/163\n",
      "2025-06-06 00:30:49 - INFO - Batch 61/163 - Loss: 0.3053 - Avg batch time: 0.25s\n",
      "2025-06-06 00:30:58 - INFO - Processing batch 71/163\n",
      "2025-06-06 00:30:58 - INFO - Batch 71/163 - Loss: 0.4576 - Avg batch time: 0.25s\n",
      "2025-06-06 00:31:07 - INFO - Processing batch 81/163\n",
      "2025-06-06 00:31:07 - INFO - Batch 81/163 - Loss: 0.3119 - Avg batch time: 0.25s\n",
      "2025-06-06 00:31:16 - INFO - Processing batch 91/163\n",
      "2025-06-06 00:31:16 - INFO - Batch 91/163 - Loss: 0.3322 - Avg batch time: 0.25s\n",
      "2025-06-06 00:31:24 - INFO - Processing batch 101/163\n",
      "2025-06-06 00:31:25 - INFO - Batch 101/163 - Loss: 0.4173 - Avg batch time: 0.25s\n",
      "2025-06-06 00:31:34 - INFO - Processing batch 111/163\n",
      "2025-06-06 00:31:34 - INFO - Batch 111/163 - Loss: 0.5850 - Avg batch time: 0.25s\n",
      "2025-06-06 00:31:43 - INFO - Processing batch 121/163\n",
      "2025-06-06 00:31:43 - INFO - Batch 121/163 - Loss: 0.4126 - Avg batch time: 0.25s\n",
      "2025-06-06 00:31:52 - INFO - Processing batch 131/163\n",
      "2025-06-06 00:31:52 - INFO - Batch 131/163 - Loss: 0.4784 - Avg batch time: 0.25s\n",
      "2025-06-06 00:32:01 - INFO - Processing batch 141/163\n",
      "2025-06-06 00:32:01 - INFO - Batch 141/163 - Loss: 0.4536 - Avg batch time: 0.25s\n",
      "2025-06-06 00:32:10 - INFO - Processing batch 151/163\n",
      "2025-06-06 00:32:10 - INFO - Batch 151/163 - Loss: 0.3989 - Avg batch time: 0.25s\n",
      "2025-06-06 00:32:18 - INFO - Processing batch 161/163\n",
      "2025-06-06 00:32:19 - INFO - Batch 161/163 - Loss: 0.4402 - Avg batch time: 0.25s\n",
      "2025-06-06 00:32:20 - INFO - \n",
      "Epoch 9 training completed in 145.82s\n",
      "2025-06-06 00:32:20 - INFO - Average training loss: 0.4320\n",
      "Epochs:  10%| | 10/100 [29:19<4:30:50, 180.56s/it, train_loss=0.4320, val_loss=0.4022, best_val_f1=0.7181, lr=5.00e-04, 2025-06-06 00:32:50 - INFO - \n",
      "Epoch 10/100 - Training phase\n",
      "2025-06-06 00:32:51 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:32:51 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:32:51 - INFO - Batch 1/163 - Loss: 0.2954 - Avg batch time: 0.25s\n",
      "2025-06-06 00:33:00 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:33:00 - INFO - Batch 11/163 - Loss: 0.3716 - Avg batch time: 0.25s\n",
      "2025-06-06 00:33:09 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:33:09 - INFO - Batch 21/163 - Loss: 0.3825 - Avg batch time: 0.25s\n",
      "2025-06-06 00:33:18 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:33:18 - INFO - Batch 31/163 - Loss: 0.4769 - Avg batch time: 0.25s\n",
      "2025-06-06 00:33:27 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:33:27 - INFO - Batch 41/163 - Loss: 0.4241 - Avg batch time: 0.25s\n",
      "2025-06-06 00:33:36 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:33:36 - INFO - Batch 51/163 - Loss: 0.4078 - Avg batch time: 0.25s\n",
      "2025-06-06 00:33:45 - INFO - Processing batch 61/163\n",
      "2025-06-06 00:33:45 - INFO - Batch 61/163 - Loss: 0.4341 - Avg batch time: 0.25s\n",
      "2025-06-06 00:33:53 - INFO - Processing batch 71/163\n",
      "2025-06-06 00:33:54 - INFO - Batch 71/163 - Loss: 0.4895 - Avg batch time: 0.25s\n",
      "2025-06-06 00:34:03 - INFO - Processing batch 81/163\n",
      "2025-06-06 00:34:03 - INFO - Batch 81/163 - Loss: 0.4356 - Avg batch time: 0.25s\n",
      "2025-06-06 00:34:12 - INFO - Processing batch 91/163\n",
      "2025-06-06 00:34:12 - INFO - Batch 91/163 - Loss: 0.4181 - Avg batch time: 0.25s\n",
      "2025-06-06 00:34:21 - INFO - Processing batch 101/163\n",
      "2025-06-06 00:34:21 - INFO - Batch 101/163 - Loss: 0.4169 - Avg batch time: 0.25s\n",
      "2025-06-06 00:34:29 - INFO - Processing batch 111/163\n",
      "2025-06-06 00:34:30 - INFO - Batch 111/163 - Loss: 0.3193 - Avg batch time: 0.25s\n",
      "2025-06-06 00:34:38 - INFO - Processing batch 121/163\n",
      "2025-06-06 00:34:39 - INFO - Batch 121/163 - Loss: 0.3540 - Avg batch time: 0.25s\n",
      "2025-06-06 00:34:47 - INFO - Processing batch 131/163\n",
      "2025-06-06 00:34:48 - INFO - Batch 131/163 - Loss: 0.3761 - Avg batch time: 0.25s\n",
      "2025-06-06 00:34:56 - INFO - Processing batch 141/163\n",
      "2025-06-06 00:34:57 - INFO - Batch 141/163 - Loss: 0.6371 - Avg batch time: 0.25s\n",
      "2025-06-06 00:35:05 - INFO - Processing batch 151/163\n",
      "2025-06-06 00:35:05 - INFO - Batch 151/163 - Loss: 0.4242 - Avg batch time: 0.25s\n",
      "2025-06-06 00:35:15 - INFO - Processing batch 161/163\n",
      "2025-06-06 00:35:15 - INFO - Batch 161/163 - Loss: 0.4230 - Avg batch time: 0.25s\n",
      "2025-06-06 00:35:17 - INFO - \n",
      "Epoch 10 training completed in 146.74s\n",
      "2025-06-06 00:35:17 - INFO - Average training loss: 0.4281\n",
      "Epochs:  11%| | 11/100 [32:16<4:26:21, 179.56s/it, train_loss=0.4281, val_loss=0.4839, best_val_f1=0.7181, lr=5.00e-04, 2025-06-06 00:35:47 - INFO - \n",
      "Epoch 11/100 - Training phase\n",
      "2025-06-06 00:35:48 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:35:48 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:35:48 - INFO - Batch 1/163 - Loss: 0.3456 - Avg batch time: 0.25s\n",
      "2025-06-06 00:35:57 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:35:57 - INFO - Batch 11/163 - Loss: 0.4876 - Avg batch time: 0.25s\n",
      "2025-06-06 00:36:06 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:36:06 - INFO - Batch 21/163 - Loss: 0.4856 - Avg batch time: 0.25s\n",
      "2025-06-06 00:36:15 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:36:15 - INFO - Batch 31/163 - Loss: 0.4970 - Avg batch time: 0.25s\n",
      "2025-06-06 00:36:24 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:36:24 - INFO - Batch 41/163 - Loss: 0.4334 - Avg batch time: 0.25s\n",
      "2025-06-06 00:36:33 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:36:34 - INFO - Batch 51/163 - Loss: 0.4705 - Avg batch time: 0.25s\n",
      "2025-06-06 00:36:42 - INFO - Processing batch 61/163\n",
      "2025-06-06 00:36:42 - INFO - Batch 61/163 - Loss: 0.3928 - Avg batch time: 0.25s\n",
      "2025-06-06 00:36:51 - INFO - Processing batch 71/163\n",
      "2025-06-06 00:36:51 - INFO - Batch 71/163 - Loss: 0.2969 - Avg batch time: 0.25s\n",
      "2025-06-06 00:37:00 - INFO - Processing batch 81/163\n",
      "2025-06-06 00:37:00 - INFO - Batch 81/163 - Loss: 0.5634 - Avg batch time: 0.25s\n",
      "2025-06-06 00:37:09 - INFO - Processing batch 91/163\n",
      "2025-06-06 00:37:09 - INFO - Batch 91/163 - Loss: 0.6044 - Avg batch time: 0.25s\n",
      "2025-06-06 00:37:18 - INFO - Processing batch 101/163\n",
      "2025-06-06 00:37:18 - INFO - Batch 101/163 - Loss: 0.4628 - Avg batch time: 0.25s\n",
      "2025-06-06 00:37:26 - INFO - Processing batch 111/163\n",
      "2025-06-06 00:37:27 - INFO - Batch 111/163 - Loss: 0.3825 - Avg batch time: 0.25s\n",
      "2025-06-06 00:37:36 - INFO - Processing batch 121/163\n",
      "2025-06-06 00:37:36 - INFO - Batch 121/163 - Loss: 0.3086 - Avg batch time: 0.25s\n",
      "2025-06-06 00:37:45 - INFO - Processing batch 131/163\n",
      "2025-06-06 00:37:45 - INFO - Batch 131/163 - Loss: 0.4233 - Avg batch time: 0.25s\n",
      "2025-06-06 00:37:53 - INFO - Processing batch 141/163\n",
      "2025-06-06 00:37:54 - INFO - Batch 141/163 - Loss: 0.5847 - Avg batch time: 0.25s\n",
      "2025-06-06 00:38:03 - INFO - Processing batch 151/163\n",
      "2025-06-06 00:38:03 - INFO - Batch 151/163 - Loss: 0.5000 - Avg batch time: 0.25s\n",
      "2025-06-06 00:38:12 - INFO - Processing batch 161/163\n",
      "2025-06-06 00:38:12 - INFO - Batch 161/163 - Loss: 0.3421 - Avg batch time: 0.25s\n",
      "2025-06-06 00:38:13 - INFO - \n",
      "Epoch 11 training completed in 145.69s\n",
      "2025-06-06 00:38:13 - INFO - Average training loss: 0.4142\n",
      "Epochs:  12%| | 12/100 [35:11<4:21:24, 178.24s/it, train_loss=0.4142, val_loss=0.3724, best_val_f1=0.7181, lr=5.00e-04, 2025-06-06 00:38:43 - INFO - \n",
      "Epoch 12/100 - Training phase\n",
      "2025-06-06 00:38:43 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:38:43 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:38:43 - INFO - Batch 1/163 - Loss: 0.3369 - Avg batch time: 0.25s\n",
      "2025-06-06 00:38:52 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:38:52 - INFO - Batch 11/163 - Loss: 0.4625 - Avg batch time: 0.25s\n",
      "2025-06-06 00:39:01 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:39:01 - INFO - Batch 21/163 - Loss: 0.4908 - Avg batch time: 0.25s\n",
      "2025-06-06 00:39:10 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:39:11 - INFO - Batch 31/163 - Loss: 0.5484 - Avg batch time: 0.25s\n",
      "2025-06-06 00:39:19 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:39:19 - INFO - Batch 41/163 - Loss: 0.4950 - Avg batch time: 0.25s\n",
      "2025-06-06 00:39:28 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:39:28 - INFO - Batch 51/163 - Loss: 0.4638 - Avg batch time: 0.25s\n",
      "2025-06-06 00:39:37 - INFO - Processing batch 61/163\n",
      "2025-06-06 00:39:37 - INFO - Batch 61/163 - Loss: 0.4140 - Avg batch time: 0.25s\n",
      "2025-06-06 00:39:46 - INFO - Processing batch 71/163\n",
      "2025-06-06 00:39:47 - INFO - Batch 71/163 - Loss: 0.5071 - Avg batch time: 0.25s\n",
      "2025-06-06 00:39:55 - INFO - Processing batch 81/163\n",
      "2025-06-06 00:39:56 - INFO - Batch 81/163 - Loss: 0.3863 - Avg batch time: 0.25s\n",
      "2025-06-06 00:40:04 - INFO - Processing batch 91/163\n",
      "2025-06-06 00:40:04 - INFO - Batch 91/163 - Loss: 0.2513 - Avg batch time: 0.25s\n",
      "2025-06-06 00:40:13 - INFO - Processing batch 101/163\n",
      "2025-06-06 00:40:13 - INFO - Batch 101/163 - Loss: 0.5044 - Avg batch time: 0.25s\n",
      "2025-06-06 00:40:22 - INFO - Processing batch 111/163\n",
      "2025-06-06 00:40:22 - INFO - Batch 111/163 - Loss: 0.4345 - Avg batch time: 0.25s\n",
      "2025-06-06 00:40:31 - INFO - Processing batch 121/163\n",
      "2025-06-06 00:40:31 - INFO - Batch 121/163 - Loss: 0.5697 - Avg batch time: 0.25s\n",
      "2025-06-06 00:40:39 - INFO - Processing batch 131/163\n",
      "2025-06-06 00:40:40 - INFO - Batch 131/163 - Loss: 0.3558 - Avg batch time: 0.25s\n",
      "2025-06-06 00:40:48 - INFO - Processing batch 141/163\n",
      "2025-06-06 00:40:48 - INFO - Batch 141/163 - Loss: 0.3661 - Avg batch time: 0.25s\n",
      "2025-06-06 00:40:57 - INFO - Processing batch 151/163\n",
      "2025-06-06 00:40:57 - INFO - Batch 151/163 - Loss: 0.5653 - Avg batch time: 0.25s\n",
      "2025-06-06 00:41:06 - INFO - Processing batch 161/163\n",
      "2025-06-06 00:41:07 - INFO - Batch 161/163 - Loss: 0.5444 - Avg batch time: 0.25s\n",
      "2025-06-06 00:41:08 - INFO - \n",
      "Epoch 12 training completed in 145.19s\n",
      "2025-06-06 00:41:08 - INFO - Average training loss: 0.4116\n",
      "Epochs:  13%|▏| 13/100 [38:07<4:17:22, 177.50s/it, train_loss=0.4116, val_loss=0.4289, best_val_f1=0.7181, lr=5.00e-04, 2025-06-06 00:41:38 - INFO - \n",
      "Epoch 13/100 - Training phase\n",
      "2025-06-06 00:41:39 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:41:39 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:41:39 - INFO - Batch 1/163 - Loss: 0.2893 - Avg batch time: 0.25s\n",
      "2025-06-06 00:41:48 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:41:48 - INFO - Batch 11/163 - Loss: 0.3484 - Avg batch time: 0.25s\n",
      "2025-06-06 00:41:57 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:41:57 - INFO - Batch 21/163 - Loss: 0.4377 - Avg batch time: 0.25s\n",
      "2025-06-06 00:42:06 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:42:06 - INFO - Batch 31/163 - Loss: 0.3345 - Avg batch time: 0.25s\n",
      "2025-06-06 00:42:15 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:42:15 - INFO - Batch 41/163 - Loss: 0.3698 - Avg batch time: 0.25s\n",
      "2025-06-06 00:42:24 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:42:24 - INFO - Batch 51/163 - Loss: 0.3914 - Avg batch time: 0.25s\n",
      "2025-06-06 00:42:33 - INFO - Processing batch 61/163\n",
      "2025-06-06 00:42:33 - INFO - Batch 61/163 - Loss: 0.3313 - Avg batch time: 0.25s\n",
      "2025-06-06 00:42:42 - INFO - Processing batch 71/163\n",
      "2025-06-06 00:42:42 - INFO - Batch 71/163 - Loss: 0.3857 - Avg batch time: 0.25s\n",
      "2025-06-06 00:42:50 - INFO - Processing batch 81/163\n",
      "2025-06-06 00:42:51 - INFO - Batch 81/163 - Loss: 0.5067 - Avg batch time: 0.25s\n",
      "2025-06-06 00:42:59 - INFO - Processing batch 91/163\n",
      "2025-06-06 00:43:00 - INFO - Batch 91/163 - Loss: 0.3579 - Avg batch time: 0.25s\n",
      "2025-06-06 00:43:08 - INFO - Processing batch 101/163\n",
      "2025-06-06 00:43:08 - INFO - Batch 101/163 - Loss: 0.6093 - Avg batch time: 0.25s\n",
      "2025-06-06 00:43:17 - INFO - Processing batch 111/163\n",
      "2025-06-06 00:43:17 - INFO - Batch 111/163 - Loss: 0.4523 - Avg batch time: 0.25s\n",
      "2025-06-06 00:43:26 - INFO - Processing batch 121/163\n",
      "2025-06-06 00:43:26 - INFO - Batch 121/163 - Loss: 0.5502 - Avg batch time: 0.25s\n",
      "2025-06-06 00:43:35 - INFO - Processing batch 131/163\n",
      "2025-06-06 00:43:35 - INFO - Batch 131/163 - Loss: 0.3574 - Avg batch time: 0.25s\n",
      "2025-06-06 00:43:44 - INFO - Processing batch 141/163\n",
      "2025-06-06 00:43:44 - INFO - Batch 141/163 - Loss: 0.3428 - Avg batch time: 0.25s\n",
      "2025-06-06 00:43:53 - INFO - Processing batch 151/163\n",
      "2025-06-06 00:43:53 - INFO - Batch 151/163 - Loss: 0.3453 - Avg batch time: 0.25s\n",
      "2025-06-06 00:44:01 - INFO - Processing batch 161/163\n",
      "2025-06-06 00:44:02 - INFO - Batch 161/163 - Loss: 0.4729 - Avg batch time: 0.25s\n",
      "2025-06-06 00:44:03 - INFO - \n",
      "Epoch 13 training completed in 144.57s\n",
      "2025-06-06 00:44:03 - INFO - Average training loss: 0.4001\n",
      "Epochs:  14%|▏| 14/100 [41:02<4:13:13, 176.67s/it, train_loss=0.4001, val_loss=0.4056, best_val_f1=0.7181, lr=2.50e-04, 2025-06-06 00:44:33 - INFO - \n",
      "Epoch 14/100 - Training phase\n",
      "2025-06-06 00:44:34 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:44:34 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:44:34 - INFO - Batch 1/163 - Loss: 0.3584 - Avg batch time: 0.25s\n",
      "2025-06-06 00:44:43 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:44:43 - INFO - Batch 11/163 - Loss: 0.3122 - Avg batch time: 0.25s\n",
      "2025-06-06 00:44:52 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:44:52 - INFO - Batch 21/163 - Loss: 0.6484 - Avg batch time: 0.25s\n",
      "2025-06-06 00:45:01 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:45:01 - INFO - Batch 31/163 - Loss: 0.4292 - Avg batch time: 0.25s\n",
      "2025-06-06 00:45:10 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:45:10 - INFO - Batch 41/163 - Loss: 0.3763 - Avg batch time: 0.25s\n",
      "2025-06-06 00:45:19 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:45:19 - INFO - Batch 51/163 - Loss: 0.2783 - Avg batch time: 0.25s\n",
      "2025-06-06 00:45:28 - INFO - Processing batch 61/163\n",
      "2025-06-06 00:45:28 - INFO - Batch 61/163 - Loss: 0.3465 - Avg batch time: 0.25s\n",
      "2025-06-06 00:45:36 - INFO - Processing batch 71/163\n",
      "2025-06-06 00:45:37 - INFO - Batch 71/163 - Loss: 0.3376 - Avg batch time: 0.25s\n",
      "2025-06-06 00:45:45 - INFO - Processing batch 81/163\n",
      "2025-06-06 00:45:46 - INFO - Batch 81/163 - Loss: 0.3704 - Avg batch time: 0.25s\n",
      "2025-06-06 00:45:54 - INFO - Processing batch 91/163\n",
      "2025-06-06 00:45:55 - INFO - Batch 91/163 - Loss: 0.4032 - Avg batch time: 0.25s\n",
      "2025-06-06 00:46:03 - INFO - Processing batch 101/163\n",
      "2025-06-06 00:46:04 - INFO - Batch 101/163 - Loss: 0.2821 - Avg batch time: 0.25s\n",
      "2025-06-06 00:46:12 - INFO - Processing batch 111/163\n",
      "2025-06-06 00:46:13 - INFO - Batch 111/163 - Loss: 0.3665 - Avg batch time: 0.25s\n",
      "2025-06-06 00:46:22 - INFO - Processing batch 121/163\n",
      "2025-06-06 00:46:22 - INFO - Batch 121/163 - Loss: 0.3043 - Avg batch time: 0.25s\n",
      "2025-06-06 00:46:30 - INFO - Processing batch 131/163\n",
      "2025-06-06 00:46:31 - INFO - Batch 131/163 - Loss: 0.5250 - Avg batch time: 0.25s\n",
      "2025-06-06 00:46:39 - INFO - Processing batch 141/163\n",
      "2025-06-06 00:46:39 - INFO - Batch 141/163 - Loss: 0.2695 - Avg batch time: 0.25s\n",
      "2025-06-06 00:46:48 - INFO - Processing batch 151/163\n",
      "2025-06-06 00:46:48 - INFO - Batch 151/163 - Loss: 0.3676 - Avg batch time: 0.25s\n",
      "2025-06-06 00:46:57 - INFO - Processing batch 161/163\n",
      "2025-06-06 00:46:57 - INFO - Batch 161/163 - Loss: 0.3439 - Avg batch time: 0.25s\n",
      "2025-06-06 00:46:58 - INFO - \n",
      "Epoch 14 training completed in 145.17s\n",
      "2025-06-06 00:46:58 - INFO - Average training loss: 0.3589\n",
      "Epochs:  15%|▏| 15/100 [43:57<4:09:42, 176.27s/it, train_loss=0.3589, val_loss=0.3624, best_val_f1=0.7220, lr=2.50e-04, 2025-06-06 00:47:28 - INFO - \n",
      "Epoch 15/100 - Training phase\n",
      "2025-06-06 00:47:29 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:47:29 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:47:29 - INFO - Batch 1/163 - Loss: 0.4279 - Avg batch time: 0.25s\n",
      "2025-06-06 00:47:38 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:47:38 - INFO - Batch 11/163 - Loss: 0.3857 - Avg batch time: 0.25s\n",
      "2025-06-06 00:47:47 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:47:47 - INFO - Batch 21/163 - Loss: 0.3558 - Avg batch time: 0.25s\n",
      "2025-06-06 00:47:56 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:47:56 - INFO - Batch 31/163 - Loss: 0.4849 - Avg batch time: 0.25s\n",
      "2025-06-06 00:48:05 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:48:05 - INFO - Batch 41/163 - Loss: 0.3754 - Avg batch time: 0.25s\n",
      "2025-06-06 00:48:14 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:48:14 - INFO - Batch 51/163 - Loss: 0.4515 - Avg batch time: 0.25s\n",
      "2025-06-06 00:48:23 - INFO - Processing batch 61/163\n",
      "2025-06-06 00:48:23 - INFO - Batch 61/163 - Loss: 0.5456 - Avg batch time: 0.25s\n",
      "2025-06-06 00:48:32 - INFO - Processing batch 71/163\n",
      "2025-06-06 00:48:32 - INFO - Batch 71/163 - Loss: 0.3191 - Avg batch time: 0.25s\n",
      "2025-06-06 00:48:41 - INFO - Processing batch 81/163\n",
      "2025-06-06 00:48:41 - INFO - Batch 81/163 - Loss: 0.3529 - Avg batch time: 0.25s\n",
      "2025-06-06 00:48:49 - INFO - Processing batch 91/163\n",
      "2025-06-06 00:48:50 - INFO - Batch 91/163 - Loss: 0.3883 - Avg batch time: 0.25s\n",
      "2025-06-06 00:48:58 - INFO - Processing batch 101/163\n",
      "2025-06-06 00:48:58 - INFO - Batch 101/163 - Loss: 0.4226 - Avg batch time: 0.25s\n",
      "2025-06-06 00:49:07 - INFO - Processing batch 111/163\n",
      "2025-06-06 00:49:07 - INFO - Batch 111/163 - Loss: 0.2303 - Avg batch time: 0.25s\n",
      "2025-06-06 00:49:16 - INFO - Processing batch 121/163\n",
      "2025-06-06 00:49:16 - INFO - Batch 121/163 - Loss: 0.3212 - Avg batch time: 0.25s\n",
      "2025-06-06 00:49:24 - INFO - Processing batch 131/163\n",
      "2025-06-06 00:49:25 - INFO - Batch 131/163 - Loss: 0.4852 - Avg batch time: 0.25s\n",
      "2025-06-06 00:49:34 - INFO - Processing batch 141/163\n",
      "2025-06-06 00:49:34 - INFO - Batch 141/163 - Loss: 0.4600 - Avg batch time: 0.25s\n",
      "2025-06-06 00:49:43 - INFO - Processing batch 151/163\n",
      "2025-06-06 00:49:43 - INFO - Batch 151/163 - Loss: 0.2757 - Avg batch time: 0.25s\n",
      "2025-06-06 00:49:51 - INFO - Processing batch 161/163\n",
      "2025-06-06 00:49:52 - INFO - Batch 161/163 - Loss: 0.4430 - Avg batch time: 0.25s\n",
      "2025-06-06 00:49:53 - INFO - \n",
      "Epoch 15 training completed in 144.32s\n",
      "2025-06-06 00:49:53 - INFO - Average training loss: 0.3605\n",
      "Epochs:  16%|▏| 16/100 [46:51<4:05:46, 175.56s/it, train_loss=0.3605, val_loss=0.3829, best_val_f1=0.7220, lr=2.50e-04, 2025-06-06 00:50:22 - INFO - \n",
      "Epoch 16/100 - Training phase\n",
      "2025-06-06 00:50:23 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:50:23 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:50:24 - INFO - Batch 1/163 - Loss: 0.3846 - Avg batch time: 0.25s\n",
      "2025-06-06 00:50:32 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:50:33 - INFO - Batch 11/163 - Loss: 0.3010 - Avg batch time: 0.25s\n",
      "2025-06-06 00:50:41 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:50:41 - INFO - Batch 21/163 - Loss: 0.3298 - Avg batch time: 0.25s\n",
      "2025-06-06 00:50:50 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:50:50 - INFO - Batch 31/163 - Loss: 0.4550 - Avg batch time: 0.25s\n",
      "2025-06-06 00:50:59 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:50:59 - INFO - Batch 41/163 - Loss: 0.2395 - Avg batch time: 0.25s\n",
      "2025-06-06 00:51:08 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:51:08 - INFO - Batch 51/163 - Loss: 0.3557 - Avg batch time: 0.25s\n",
      "2025-06-06 00:51:17 - INFO - Processing batch 61/163\n",
      "2025-06-06 00:51:17 - INFO - Batch 61/163 - Loss: 0.3749 - Avg batch time: 0.25s\n",
      "2025-06-06 00:51:26 - INFO - Processing batch 71/163\n",
      "2025-06-06 00:51:26 - INFO - Batch 71/163 - Loss: 0.2783 - Avg batch time: 0.25s\n",
      "2025-06-06 00:51:35 - INFO - Processing batch 81/163\n",
      "2025-06-06 00:51:35 - INFO - Batch 81/163 - Loss: 0.4461 - Avg batch time: 0.25s\n",
      "2025-06-06 00:51:44 - INFO - Processing batch 91/163\n",
      "2025-06-06 00:51:44 - INFO - Batch 91/163 - Loss: 0.2923 - Avg batch time: 0.25s\n",
      "2025-06-06 00:51:52 - INFO - Processing batch 101/163\n",
      "2025-06-06 00:51:53 - INFO - Batch 101/163 - Loss: 0.3484 - Avg batch time: 0.25s\n",
      "2025-06-06 00:52:01 - INFO - Processing batch 111/163\n",
      "2025-06-06 00:52:01 - INFO - Batch 111/163 - Loss: 0.2045 - Avg batch time: 0.25s\n",
      "2025-06-06 00:52:10 - INFO - Processing batch 121/163\n",
      "2025-06-06 00:52:10 - INFO - Batch 121/163 - Loss: 0.2870 - Avg batch time: 0.25s\n",
      "2025-06-06 00:52:19 - INFO - Processing batch 131/163\n",
      "2025-06-06 00:52:19 - INFO - Batch 131/163 - Loss: 0.3669 - Avg batch time: 0.25s\n",
      "2025-06-06 00:52:28 - INFO - Processing batch 141/163\n",
      "2025-06-06 00:52:28 - INFO - Batch 141/163 - Loss: 0.3062 - Avg batch time: 0.25s\n",
      "2025-06-06 00:52:36 - INFO - Processing batch 151/163\n",
      "2025-06-06 00:52:36 - INFO - Batch 151/163 - Loss: 0.3253 - Avg batch time: 0.25s\n",
      "2025-06-06 00:52:45 - INFO - Processing batch 161/163\n",
      "2025-06-06 00:52:46 - INFO - Batch 161/163 - Loss: 0.4208 - Avg batch time: 0.25s\n",
      "2025-06-06 00:52:47 - INFO - \n",
      "Epoch 16 training completed in 144.47s\n",
      "2025-06-06 00:52:47 - INFO - Average training loss: 0.3480\n",
      "Epochs:  17%|▏| 17/100 [49:46<4:02:20, 175.19s/it, train_loss=0.3480, val_loss=0.4158, best_val_f1=0.7220, lr=2.50e-04, 2025-06-06 00:53:17 - INFO - \n",
      "Epoch 17/100 - Training phase\n",
      "2025-06-06 00:53:17 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:53:17 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:53:18 - INFO - Batch 1/163 - Loss: 0.2792 - Avg batch time: 0.25s\n",
      "2025-06-06 00:53:26 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:53:27 - INFO - Batch 11/163 - Loss: 0.2560 - Avg batch time: 0.25s\n",
      "2025-06-06 00:53:36 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:53:36 - INFO - Batch 21/163 - Loss: 0.4755 - Avg batch time: 0.25s\n",
      "2025-06-06 00:53:44 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:53:45 - INFO - Batch 31/163 - Loss: 0.3803 - Avg batch time: 0.25s\n",
      "2025-06-06 00:53:53 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:53:54 - INFO - Batch 41/163 - Loss: 0.3930 - Avg batch time: 0.25s\n",
      "2025-06-06 00:54:02 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:54:02 - INFO - Batch 51/163 - Loss: 0.2876 - Avg batch time: 0.25s\n",
      "2025-06-06 00:54:11 - INFO - Processing batch 61/163\n",
      "2025-06-06 00:54:11 - INFO - Batch 61/163 - Loss: 0.3579 - Avg batch time: 0.25s\n",
      "2025-06-06 00:54:20 - INFO - Processing batch 71/163\n",
      "2025-06-06 00:54:20 - INFO - Batch 71/163 - Loss: 0.4546 - Avg batch time: 0.25s\n",
      "2025-06-06 00:54:29 - INFO - Processing batch 81/163\n",
      "2025-06-06 00:54:30 - INFO - Batch 81/163 - Loss: 0.4585 - Avg batch time: 0.25s\n",
      "2025-06-06 00:54:38 - INFO - Processing batch 91/163\n",
      "2025-06-06 00:54:38 - INFO - Batch 91/163 - Loss: 0.2947 - Avg batch time: 0.25s\n",
      "2025-06-06 00:54:47 - INFO - Processing batch 101/163\n",
      "2025-06-06 00:54:47 - INFO - Batch 101/163 - Loss: 0.3273 - Avg batch time: 0.25s\n",
      "2025-06-06 00:54:56 - INFO - Processing batch 111/163\n",
      "2025-06-06 00:54:56 - INFO - Batch 111/163 - Loss: 0.2792 - Avg batch time: 0.25s\n",
      "2025-06-06 00:55:05 - INFO - Processing batch 121/163\n",
      "2025-06-06 00:55:05 - INFO - Batch 121/163 - Loss: 0.3752 - Avg batch time: 0.25s\n",
      "2025-06-06 00:55:14 - INFO - Processing batch 131/163\n",
      "2025-06-06 00:55:14 - INFO - Batch 131/163 - Loss: 0.4318 - Avg batch time: 0.25s\n",
      "2025-06-06 00:55:23 - INFO - Processing batch 141/163\n",
      "2025-06-06 00:55:23 - INFO - Batch 141/163 - Loss: 0.3035 - Avg batch time: 0.25s\n",
      "2025-06-06 00:55:32 - INFO - Processing batch 151/163\n",
      "2025-06-06 00:55:32 - INFO - Batch 151/163 - Loss: 0.3431 - Avg batch time: 0.25s\n",
      "2025-06-06 00:55:40 - INFO - Processing batch 161/163\n",
      "2025-06-06 00:55:41 - INFO - Batch 161/163 - Loss: 0.3974 - Avg batch time: 0.25s\n",
      "2025-06-06 00:55:42 - INFO - \n",
      "Epoch 17 training completed in 145.38s\n",
      "2025-06-06 00:55:42 - INFO - Average training loss: 0.3513\n",
      "Epochs:  18%|▏| 18/100 [52:42<3:59:43, 175.41s/it, train_loss=0.3513, val_loss=0.4725, best_val_f1=0.7220, lr=2.50e-04, 2025-06-06 00:56:13 - INFO - \n",
      "Epoch 18/100 - Training phase\n",
      "2025-06-06 00:56:13 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:56:13 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:56:14 - INFO - Batch 1/163 - Loss: 0.3423 - Avg batch time: 0.25s\n",
      "2025-06-06 00:56:22 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:56:23 - INFO - Batch 11/163 - Loss: 0.3408 - Avg batch time: 0.25s\n",
      "2025-06-06 00:56:31 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:56:32 - INFO - Batch 21/163 - Loss: 0.2831 - Avg batch time: 0.25s\n",
      "2025-06-06 00:56:40 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:56:41 - INFO - Batch 31/163 - Loss: 0.3554 - Avg batch time: 0.25s\n",
      "2025-06-06 00:56:49 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:56:49 - INFO - Batch 41/163 - Loss: 0.2986 - Avg batch time: 0.25s\n",
      "2025-06-06 00:56:58 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:56:58 - INFO - Batch 51/163 - Loss: 0.2243 - Avg batch time: 0.25s\n",
      "2025-06-06 00:57:07 - INFO - Processing batch 61/163\n",
      "2025-06-06 00:57:07 - INFO - Batch 61/163 - Loss: 0.3385 - Avg batch time: 0.25s\n",
      "2025-06-06 00:57:16 - INFO - Processing batch 71/163\n",
      "2025-06-06 00:57:16 - INFO - Batch 71/163 - Loss: 0.3607 - Avg batch time: 0.25s\n",
      "2025-06-06 00:57:25 - INFO - Processing batch 81/163\n",
      "2025-06-06 00:57:25 - INFO - Batch 81/163 - Loss: 0.4639 - Avg batch time: 0.25s\n",
      "2025-06-06 00:57:34 - INFO - Processing batch 91/163\n",
      "2025-06-06 00:57:34 - INFO - Batch 91/163 - Loss: 0.3189 - Avg batch time: 0.25s\n",
      "2025-06-06 00:57:43 - INFO - Processing batch 101/163\n",
      "2025-06-06 00:57:43 - INFO - Batch 101/163 - Loss: 0.4237 - Avg batch time: 0.25s\n",
      "2025-06-06 00:57:52 - INFO - Processing batch 111/163\n",
      "2025-06-06 00:57:52 - INFO - Batch 111/163 - Loss: 0.3185 - Avg batch time: 0.25s\n",
      "2025-06-06 00:58:01 - INFO - Processing batch 121/163\n",
      "2025-06-06 00:58:01 - INFO - Batch 121/163 - Loss: 0.4079 - Avg batch time: 0.25s\n",
      "2025-06-06 00:58:09 - INFO - Processing batch 131/163\n",
      "2025-06-06 00:58:10 - INFO - Batch 131/163 - Loss: 0.5434 - Avg batch time: 0.25s\n",
      "2025-06-06 00:58:18 - INFO - Processing batch 141/163\n",
      "2025-06-06 00:58:18 - INFO - Batch 141/163 - Loss: 0.2927 - Avg batch time: 0.25s\n",
      "2025-06-06 00:58:27 - INFO - Processing batch 151/163\n",
      "2025-06-06 00:58:27 - INFO - Batch 151/163 - Loss: 0.2555 - Avg batch time: 0.25s\n",
      "2025-06-06 00:58:36 - INFO - Processing batch 161/163\n",
      "2025-06-06 00:58:36 - INFO - Batch 161/163 - Loss: 0.2765 - Avg batch time: 0.25s\n",
      "2025-06-06 00:58:37 - INFO - \n",
      "Epoch 18 training completed in 144.58s\n",
      "2025-06-06 00:58:37 - INFO - Average training loss: 0.3379\n",
      "Epochs:  19%|▏| 19/100 [55:36<3:56:18, 175.05s/it, train_loss=0.3379, val_loss=0.3972, best_val_f1=0.7220, lr=2.50e-04, 2025-06-06 00:59:07 - INFO - \n",
      "Epoch 19/100 - Training phase\n",
      "2025-06-06 00:59:07 - INFO - Processing batch 1/163\n",
      "2025-06-06 00:59:07 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 00:59:08 - INFO - Batch 1/163 - Loss: 0.2883 - Avg batch time: 0.25s\n",
      "2025-06-06 00:59:16 - INFO - Processing batch 11/163\n",
      "2025-06-06 00:59:17 - INFO - Batch 11/163 - Loss: 0.2539 - Avg batch time: 0.25s\n",
      "2025-06-06 00:59:26 - INFO - Processing batch 21/163\n",
      "2025-06-06 00:59:26 - INFO - Batch 21/163 - Loss: 0.2018 - Avg batch time: 0.25s\n",
      "2025-06-06 00:59:35 - INFO - Processing batch 31/163\n",
      "2025-06-06 00:59:35 - INFO - Batch 31/163 - Loss: 0.3623 - Avg batch time: 0.25s\n",
      "2025-06-06 00:59:44 - INFO - Processing batch 41/163\n",
      "2025-06-06 00:59:44 - INFO - Batch 41/163 - Loss: 0.3220 - Avg batch time: 0.25s\n",
      "2025-06-06 00:59:52 - INFO - Processing batch 51/163\n",
      "2025-06-06 00:59:53 - INFO - Batch 51/163 - Loss: 0.3442 - Avg batch time: 0.25s\n",
      "2025-06-06 01:00:01 - INFO - Processing batch 61/163\n",
      "2025-06-06 01:00:01 - INFO - Batch 61/163 - Loss: 0.4238 - Avg batch time: 0.25s\n",
      "2025-06-06 01:00:10 - INFO - Processing batch 71/163\n",
      "2025-06-06 01:00:10 - INFO - Batch 71/163 - Loss: 0.4100 - Avg batch time: 0.25s\n",
      "2025-06-06 01:00:19 - INFO - Processing batch 81/163\n",
      "2025-06-06 01:00:19 - INFO - Batch 81/163 - Loss: 0.3170 - Avg batch time: 0.25s\n",
      "2025-06-06 01:00:27 - INFO - Processing batch 91/163\n",
      "2025-06-06 01:00:28 - INFO - Batch 91/163 - Loss: 0.3483 - Avg batch time: 0.25s\n",
      "2025-06-06 01:00:36 - INFO - Processing batch 101/163\n",
      "2025-06-06 01:00:36 - INFO - Batch 101/163 - Loss: 0.2505 - Avg batch time: 0.25s\n",
      "2025-06-06 01:00:45 - INFO - Processing batch 111/163\n",
      "2025-06-06 01:00:45 - INFO - Batch 111/163 - Loss: 0.2624 - Avg batch time: 0.25s\n",
      "2025-06-06 01:00:54 - INFO - Processing batch 121/163\n",
      "2025-06-06 01:00:54 - INFO - Batch 121/163 - Loss: 0.3975 - Avg batch time: 0.25s\n",
      "2025-06-06 01:01:05 - INFO - Processing batch 131/163\n",
      "2025-06-06 01:01:05 - INFO - Batch 131/163 - Loss: 0.4000 - Avg batch time: 0.25s\n",
      "2025-06-06 01:01:13 - INFO - Processing batch 141/163\n",
      "2025-06-06 01:01:14 - INFO - Batch 141/163 - Loss: 0.2724 - Avg batch time: 0.25s\n",
      "2025-06-06 01:01:22 - INFO - Processing batch 151/163\n",
      "2025-06-06 01:01:22 - INFO - Batch 151/163 - Loss: 0.2340 - Avg batch time: 0.25s\n",
      "2025-06-06 01:01:31 - INFO - Processing batch 161/163\n",
      "2025-06-06 01:01:31 - INFO - Batch 161/163 - Loss: 0.3566 - Avg batch time: 0.25s\n",
      "2025-06-06 01:01:32 - INFO - \n",
      "Epoch 19 training completed in 145.56s\n",
      "2025-06-06 01:01:32 - INFO - Average training loss: 0.3247\n",
      "Epochs:  20%|▏| 20/100 [58:32<3:53:44, 175.30s/it, train_loss=0.3247, val_loss=0.4372, best_val_f1=0.7220, lr=1.25e-04, 2025-06-06 01:02:03 - INFO - \n",
      "Epoch 20/100 - Training phase\n",
      "2025-06-06 01:02:03 - INFO - Processing batch 1/163\n",
      "2025-06-06 01:02:03 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 01:02:04 - INFO - Batch 1/163 - Loss: 0.4066 - Avg batch time: 0.25s\n",
      "2025-06-06 01:02:12 - INFO - Processing batch 11/163\n",
      "2025-06-06 01:02:13 - INFO - Batch 11/163 - Loss: 0.2741 - Avg batch time: 0.25s\n",
      "2025-06-06 01:02:22 - INFO - Processing batch 21/163\n",
      "2025-06-06 01:02:22 - INFO - Batch 21/163 - Loss: 0.2693 - Avg batch time: 0.25s\n",
      "2025-06-06 01:02:30 - INFO - Processing batch 31/163\n",
      "2025-06-06 01:02:31 - INFO - Batch 31/163 - Loss: 0.3039 - Avg batch time: 0.25s\n",
      "2025-06-06 01:02:40 - INFO - Processing batch 41/163\n",
      "2025-06-06 01:02:40 - INFO - Batch 41/163 - Loss: 0.3098 - Avg batch time: 0.25s\n",
      "2025-06-06 01:02:49 - INFO - Processing batch 51/163\n",
      "2025-06-06 01:02:49 - INFO - Batch 51/163 - Loss: 0.4088 - Avg batch time: 0.25s\n",
      "2025-06-06 01:02:57 - INFO - Processing batch 61/163\n",
      "2025-06-06 01:02:58 - INFO - Batch 61/163 - Loss: 0.3494 - Avg batch time: 0.25s\n",
      "2025-06-06 01:03:06 - INFO - Processing batch 71/163\n",
      "2025-06-06 01:03:06 - INFO - Batch 71/163 - Loss: 0.3115 - Avg batch time: 0.25s\n",
      "2025-06-06 01:03:15 - INFO - Processing batch 81/163\n",
      "2025-06-06 01:03:15 - INFO - Batch 81/163 - Loss: 0.2482 - Avg batch time: 0.25s\n",
      "2025-06-06 01:03:24 - INFO - Processing batch 91/163\n",
      "2025-06-06 01:03:24 - INFO - Batch 91/163 - Loss: 0.2944 - Avg batch time: 0.25s\n",
      "2025-06-06 01:03:33 - INFO - Processing batch 101/163\n",
      "2025-06-06 01:03:33 - INFO - Batch 101/163 - Loss: 0.3238 - Avg batch time: 0.25s\n",
      "2025-06-06 01:03:41 - INFO - Processing batch 111/163\n",
      "2025-06-06 01:03:42 - INFO - Batch 111/163 - Loss: 0.4357 - Avg batch time: 0.25s\n",
      "2025-06-06 01:03:50 - INFO - Processing batch 121/163\n",
      "2025-06-06 01:03:50 - INFO - Batch 121/163 - Loss: 0.3455 - Avg batch time: 0.25s\n",
      "2025-06-06 01:03:59 - INFO - Processing batch 131/163\n",
      "2025-06-06 01:03:59 - INFO - Batch 131/163 - Loss: 0.3202 - Avg batch time: 0.25s\n",
      "2025-06-06 01:04:08 - INFO - Processing batch 141/163\n",
      "2025-06-06 01:04:08 - INFO - Batch 141/163 - Loss: 0.3469 - Avg batch time: 0.25s\n",
      "2025-06-06 01:04:17 - INFO - Processing batch 151/163\n",
      "2025-06-06 01:04:17 - INFO - Batch 151/163 - Loss: 0.3549 - Avg batch time: 0.25s\n",
      "2025-06-06 01:04:25 - INFO - Processing batch 161/163\n",
      "2025-06-06 01:04:26 - INFO - Batch 161/163 - Loss: 0.2937 - Avg batch time: 0.25s\n",
      "2025-06-06 01:04:27 - INFO - \n",
      "Epoch 20 training completed in 144.11s\n",
      "2025-06-06 01:04:27 - INFO - Average training loss: 0.3091\n",
      "Epochs:  21%|▏| 21/100 [1:01:26<3:50:20, 174.94s/it, train_loss=0.3091, val_loss=0.3798, best_val_f1=0.7220, lr=1.25e-042025-06-06 01:04:57 - INFO - \n",
      "Epoch 21/100 - Training phase\n",
      "2025-06-06 01:04:57 - INFO - Processing batch 1/163\n",
      "2025-06-06 01:04:57 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 01:04:58 - INFO - Batch 1/163 - Loss: 0.3762 - Avg batch time: 0.25s\n",
      "2025-06-06 01:05:06 - INFO - Processing batch 11/163\n",
      "2025-06-06 01:05:07 - INFO - Batch 11/163 - Loss: 0.3038 - Avg batch time: 0.25s\n",
      "2025-06-06 01:05:15 - INFO - Processing batch 21/163\n",
      "2025-06-06 01:05:15 - INFO - Batch 21/163 - Loss: 0.3824 - Avg batch time: 0.25s\n",
      "2025-06-06 01:05:25 - INFO - Processing batch 31/163\n",
      "2025-06-06 01:05:25 - INFO - Batch 31/163 - Loss: 0.2854 - Avg batch time: 0.25s\n",
      "2025-06-06 01:05:33 - INFO - Processing batch 41/163\n",
      "2025-06-06 01:05:34 - INFO - Batch 41/163 - Loss: 0.3066 - Avg batch time: 0.25s\n",
      "2025-06-06 01:05:42 - INFO - Processing batch 51/163\n",
      "2025-06-06 01:05:42 - INFO - Batch 51/163 - Loss: 0.2354 - Avg batch time: 0.25s\n",
      "2025-06-06 01:05:51 - INFO - Processing batch 61/163\n",
      "2025-06-06 01:05:51 - INFO - Batch 61/163 - Loss: 0.3648 - Avg batch time: 0.25s\n",
      "2025-06-06 01:06:00 - INFO - Processing batch 71/163\n",
      "2025-06-06 01:06:00 - INFO - Batch 71/163 - Loss: 0.2778 - Avg batch time: 0.25s\n",
      "2025-06-06 01:06:09 - INFO - Processing batch 81/163\n",
      "2025-06-06 01:06:09 - INFO - Batch 81/163 - Loss: 0.2749 - Avg batch time: 0.25s\n",
      "2025-06-06 01:06:17 - INFO - Processing batch 91/163\n",
      "2025-06-06 01:06:18 - INFO - Batch 91/163 - Loss: 0.2923 - Avg batch time: 0.25s\n",
      "2025-06-06 01:06:26 - INFO - Processing batch 101/163\n",
      "2025-06-06 01:06:27 - INFO - Batch 101/163 - Loss: 0.2763 - Avg batch time: 0.25s\n",
      "2025-06-06 01:06:35 - INFO - Processing batch 111/163\n",
      "2025-06-06 01:06:35 - INFO - Batch 111/163 - Loss: 0.3187 - Avg batch time: 0.25s\n",
      "2025-06-06 01:06:44 - INFO - Processing batch 121/163\n",
      "2025-06-06 01:06:44 - INFO - Batch 121/163 - Loss: 0.3458 - Avg batch time: 0.25s\n",
      "2025-06-06 01:06:53 - INFO - Processing batch 131/163\n",
      "2025-06-06 01:06:53 - INFO - Batch 131/163 - Loss: 0.4132 - Avg batch time: 0.25s\n",
      "2025-06-06 01:07:02 - INFO - Processing batch 141/163\n",
      "2025-06-06 01:07:02 - INFO - Batch 141/163 - Loss: 0.2263 - Avg batch time: 0.25s\n",
      "2025-06-06 01:07:11 - INFO - Processing batch 151/163\n",
      "2025-06-06 01:07:11 - INFO - Batch 151/163 - Loss: 0.3322 - Avg batch time: 0.25s\n",
      "2025-06-06 01:07:20 - INFO - Processing batch 161/163\n",
      "2025-06-06 01:07:20 - INFO - Batch 161/163 - Loss: 0.2918 - Avg batch time: 0.25s\n",
      "2025-06-06 01:07:21 - INFO - \n",
      "Epoch 21 training completed in 144.32s\n",
      "2025-06-06 01:07:21 - INFO - Average training loss: 0.3107\n",
      "Epochs:  22%|▏| 22/100 [1:04:20<3:47:14, 174.81s/it, train_loss=0.3107, val_loss=0.4038, best_val_f1=0.7220, lr=1.25e-042025-06-06 01:07:51 - INFO - \n",
      "Epoch 22/100 - Training phase\n",
      "2025-06-06 01:07:52 - INFO - Processing batch 1/163\n",
      "2025-06-06 01:07:52 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 01:07:52 - INFO - Batch 1/163 - Loss: 0.2441 - Avg batch time: 0.25s\n",
      "2025-06-06 01:08:01 - INFO - Processing batch 11/163\n",
      "2025-06-06 01:08:01 - INFO - Batch 11/163 - Loss: 0.1836 - Avg batch time: 0.25s\n",
      "2025-06-06 01:08:10 - INFO - Processing batch 21/163\n",
      "2025-06-06 01:08:10 - INFO - Batch 21/163 - Loss: 0.2245 - Avg batch time: 0.25s\n",
      "2025-06-06 01:08:19 - INFO - Processing batch 31/163\n",
      "2025-06-06 01:08:19 - INFO - Batch 31/163 - Loss: 0.4713 - Avg batch time: 0.25s\n",
      "2025-06-06 01:08:28 - INFO - Processing batch 41/163\n",
      "2025-06-06 01:08:28 - INFO - Batch 41/163 - Loss: 0.2571 - Avg batch time: 0.25s\n",
      "2025-06-06 01:08:37 - INFO - Processing batch 51/163\n",
      "2025-06-06 01:08:37 - INFO - Batch 51/163 - Loss: 0.4214 - Avg batch time: 0.25s\n",
      "2025-06-06 01:08:46 - INFO - Processing batch 61/163\n",
      "2025-06-06 01:08:47 - INFO - Batch 61/163 - Loss: 0.3742 - Avg batch time: 0.25s\n",
      "2025-06-06 01:08:55 - INFO - Processing batch 71/163\n",
      "2025-06-06 01:08:56 - INFO - Batch 71/163 - Loss: 0.3424 - Avg batch time: 0.25s\n",
      "2025-06-06 01:09:04 - INFO - Processing batch 81/163\n",
      "2025-06-06 01:09:05 - INFO - Batch 81/163 - Loss: 0.2753 - Avg batch time: 0.25s\n",
      "2025-06-06 01:09:14 - INFO - Processing batch 91/163\n",
      "2025-06-06 01:09:14 - INFO - Batch 91/163 - Loss: 0.2416 - Avg batch time: 0.25s\n",
      "2025-06-06 01:09:23 - INFO - Processing batch 101/163\n",
      "2025-06-06 01:09:23 - INFO - Batch 101/163 - Loss: 0.2912 - Avg batch time: 0.25s\n",
      "2025-06-06 01:09:31 - INFO - Processing batch 111/163\n",
      "2025-06-06 01:09:32 - INFO - Batch 111/163 - Loss: 0.2006 - Avg batch time: 0.25s\n",
      "2025-06-06 01:09:40 - INFO - Processing batch 121/163\n",
      "2025-06-06 01:09:40 - INFO - Batch 121/163 - Loss: 0.2558 - Avg batch time: 0.25s\n",
      "2025-06-06 01:09:49 - INFO - Processing batch 131/163\n",
      "2025-06-06 01:09:49 - INFO - Batch 131/163 - Loss: 0.3609 - Avg batch time: 0.25s\n",
      "2025-06-06 01:09:58 - INFO - Processing batch 141/163\n",
      "2025-06-06 01:09:58 - INFO - Batch 141/163 - Loss: 0.2004 - Avg batch time: 0.25s\n",
      "2025-06-06 01:10:07 - INFO - Processing batch 151/163\n",
      "2025-06-06 01:10:07 - INFO - Batch 151/163 - Loss: 0.3256 - Avg batch time: 0.25s\n",
      "2025-06-06 01:10:16 - INFO - Processing batch 161/163\n",
      "2025-06-06 01:10:16 - INFO - Batch 161/163 - Loss: 0.2715 - Avg batch time: 0.25s\n",
      "2025-06-06 01:10:17 - INFO - \n",
      "Epoch 22 training completed in 145.84s\n",
      "2025-06-06 01:10:17 - INFO - Average training loss: 0.3007\n",
      "Epochs:  23%|▏| 23/100 [1:07:17<3:45:01, 175.34s/it, train_loss=0.3007, val_loss=0.3758, best_val_f1=0.7220, lr=1.25e-042025-06-06 01:10:48 - INFO - \n",
      "Epoch 23/100 - Training phase\n",
      "2025-06-06 01:10:49 - INFO - Processing batch 1/163\n",
      "2025-06-06 01:10:49 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 01:10:49 - INFO - Batch 1/163 - Loss: 0.3434 - Avg batch time: 0.25s\n",
      "2025-06-06 01:10:58 - INFO - Processing batch 11/163\n",
      "2025-06-06 01:10:58 - INFO - Batch 11/163 - Loss: 0.2092 - Avg batch time: 0.25s\n",
      "2025-06-06 01:11:07 - INFO - Processing batch 21/163\n",
      "2025-06-06 01:11:08 - INFO - Batch 21/163 - Loss: 0.2714 - Avg batch time: 0.25s\n",
      "2025-06-06 01:11:16 - INFO - Processing batch 31/163\n",
      "2025-06-06 01:11:17 - INFO - Batch 31/163 - Loss: 0.3053 - Avg batch time: 0.25s\n",
      "2025-06-06 01:11:25 - INFO - Processing batch 41/163\n",
      "2025-06-06 01:11:25 - INFO - Batch 41/163 - Loss: 0.2216 - Avg batch time: 0.25s\n",
      "2025-06-06 01:11:34 - INFO - Processing batch 51/163\n",
      "2025-06-06 01:11:34 - INFO - Batch 51/163 - Loss: 0.4043 - Avg batch time: 0.25s\n",
      "2025-06-06 01:11:43 - INFO - Processing batch 61/163\n",
      "2025-06-06 01:11:43 - INFO - Batch 61/163 - Loss: 0.3661 - Avg batch time: 0.25s\n",
      "2025-06-06 01:11:52 - INFO - Processing batch 71/163\n",
      "2025-06-06 01:11:52 - INFO - Batch 71/163 - Loss: 0.3187 - Avg batch time: 0.25s\n",
      "2025-06-06 01:12:00 - INFO - Processing batch 81/163\n",
      "2025-06-06 01:12:01 - INFO - Batch 81/163 - Loss: 0.2724 - Avg batch time: 0.25s\n",
      "2025-06-06 01:12:09 - INFO - Processing batch 91/163\n",
      "2025-06-06 01:12:09 - INFO - Batch 91/163 - Loss: 0.3338 - Avg batch time: 0.25s\n",
      "2025-06-06 01:12:18 - INFO - Processing batch 101/163\n",
      "2025-06-06 01:12:18 - INFO - Batch 101/163 - Loss: 0.3075 - Avg batch time: 0.25s\n",
      "2025-06-06 01:12:27 - INFO - Processing batch 111/163\n",
      "2025-06-06 01:12:27 - INFO - Batch 111/163 - Loss: 0.4455 - Avg batch time: 0.25s\n",
      "2025-06-06 01:12:36 - INFO - Processing batch 121/163\n",
      "2025-06-06 01:12:36 - INFO - Batch 121/163 - Loss: 0.2807 - Avg batch time: 0.25s\n",
      "2025-06-06 01:12:45 - INFO - Processing batch 131/163\n",
      "2025-06-06 01:12:45 - INFO - Batch 131/163 - Loss: 0.3466 - Avg batch time: 0.25s\n",
      "2025-06-06 01:12:53 - INFO - Processing batch 141/163\n",
      "2025-06-06 01:12:54 - INFO - Batch 141/163 - Loss: 0.1961 - Avg batch time: 0.25s\n",
      "2025-06-06 01:13:02 - INFO - Processing batch 151/163\n",
      "2025-06-06 01:13:02 - INFO - Batch 151/163 - Loss: 0.2681 - Avg batch time: 0.25s\n",
      "2025-06-06 01:13:11 - INFO - Processing batch 161/163\n",
      "2025-06-06 01:13:11 - INFO - Batch 161/163 - Loss: 0.2144 - Avg batch time: 0.25s\n",
      "2025-06-06 01:13:13 - INFO - \n",
      "Epoch 23 training completed in 144.60s\n",
      "2025-06-06 01:13:13 - INFO - Average training loss: 0.2975\n",
      "Epochs:  24%|▏| 24/100 [1:10:11<3:41:45, 175.07s/it, train_loss=0.2975, val_loss=0.3531, best_val_f1=0.7220, lr=1.25e-042025-06-06 01:13:42 - INFO - \n",
      "Epoch 24/100 - Training phase\n",
      "2025-06-06 01:13:43 - INFO - Processing batch 1/163\n",
      "2025-06-06 01:13:43 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 01:13:43 - INFO - Batch 1/163 - Loss: 0.1595 - Avg batch time: 0.25s\n",
      "2025-06-06 01:13:52 - INFO - Processing batch 11/163\n",
      "2025-06-06 01:13:52 - INFO - Batch 11/163 - Loss: 0.3877 - Avg batch time: 0.25s\n",
      "2025-06-06 01:14:01 - INFO - Processing batch 21/163\n",
      "2025-06-06 01:14:01 - INFO - Batch 21/163 - Loss: 0.3345 - Avg batch time: 0.25s\n",
      "2025-06-06 01:14:10 - INFO - Processing batch 31/163\n",
      "2025-06-06 01:14:10 - INFO - Batch 31/163 - Loss: 0.2097 - Avg batch time: 0.25s\n",
      "2025-06-06 01:14:19 - INFO - Processing batch 41/163\n",
      "2025-06-06 01:14:19 - INFO - Batch 41/163 - Loss: 0.3794 - Avg batch time: 0.25s\n",
      "2025-06-06 01:14:28 - INFO - Processing batch 51/163\n",
      "2025-06-06 01:14:28 - INFO - Batch 51/163 - Loss: 0.3001 - Avg batch time: 0.25s\n",
      "2025-06-06 01:14:37 - INFO - Processing batch 61/163\n",
      "2025-06-06 01:14:37 - INFO - Batch 61/163 - Loss: 0.3552 - Avg batch time: 0.25s\n",
      "2025-06-06 01:14:46 - INFO - Processing batch 71/163\n",
      "2025-06-06 01:14:46 - INFO - Batch 71/163 - Loss: 0.1902 - Avg batch time: 0.25s\n",
      "2025-06-06 01:14:54 - INFO - Processing batch 81/163\n",
      "2025-06-06 01:14:55 - INFO - Batch 81/163 - Loss: 0.3506 - Avg batch time: 0.25s\n",
      "2025-06-06 01:15:03 - INFO - Processing batch 91/163\n",
      "2025-06-06 01:15:04 - INFO - Batch 91/163 - Loss: 0.2270 - Avg batch time: 0.25s\n",
      "2025-06-06 01:15:12 - INFO - Processing batch 101/163\n",
      "2025-06-06 01:15:12 - INFO - Batch 101/163 - Loss: 0.2644 - Avg batch time: 0.25s\n",
      "2025-06-06 01:15:21 - INFO - Processing batch 111/163\n",
      "2025-06-06 01:15:21 - INFO - Batch 111/163 - Loss: 0.2956 - Avg batch time: 0.25s\n",
      "2025-06-06 01:15:30 - INFO - Processing batch 121/163\n",
      "2025-06-06 01:15:30 - INFO - Batch 121/163 - Loss: 0.1885 - Avg batch time: 0.25s\n",
      "2025-06-06 01:15:39 - INFO - Processing batch 131/163\n",
      "2025-06-06 01:15:39 - INFO - Batch 131/163 - Loss: 0.2083 - Avg batch time: 0.25s\n",
      "2025-06-06 01:15:48 - INFO - Processing batch 141/163\n",
      "2025-06-06 01:15:48 - INFO - Batch 141/163 - Loss: 0.1116 - Avg batch time: 0.25s\n",
      "2025-06-06 01:15:57 - INFO - Processing batch 151/163\n",
      "2025-06-06 01:15:57 - INFO - Batch 151/163 - Loss: 0.3621 - Avg batch time: 0.25s\n",
      "2025-06-06 01:16:06 - INFO - Processing batch 161/163\n",
      "2025-06-06 01:16:06 - INFO - Batch 161/163 - Loss: 0.2300 - Avg batch time: 0.25s\n",
      "2025-06-06 01:16:07 - INFO - \n",
      "Epoch 24 training completed in 145.11s\n",
      "2025-06-06 01:16:07 - INFO - Average training loss: 0.2906\n",
      "Epochs:  25%|▎| 25/100 [1:13:06<3:38:40, 174.95s/it, train_loss=0.2906, val_loss=0.3648, best_val_f1=0.7278, lr=1.25e-042025-06-06 01:16:37 - INFO - \n",
      "Epoch 25/100 - Training phase\n",
      "2025-06-06 01:16:38 - INFO - Processing batch 1/163\n",
      "2025-06-06 01:16:38 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 01:16:38 - INFO - Batch 1/163 - Loss: 0.2864 - Avg batch time: 0.25s\n",
      "2025-06-06 01:16:46 - INFO - Processing batch 11/163\n",
      "2025-06-06 01:16:47 - INFO - Batch 11/163 - Loss: 0.1531 - Avg batch time: 0.25s\n",
      "2025-06-06 01:16:55 - INFO - Processing batch 21/163\n",
      "2025-06-06 01:16:55 - INFO - Batch 21/163 - Loss: 0.2946 - Avg batch time: 0.25s\n",
      "2025-06-06 01:17:04 - INFO - Processing batch 31/163\n",
      "2025-06-06 01:17:04 - INFO - Batch 31/163 - Loss: 0.4068 - Avg batch time: 0.25s\n",
      "2025-06-06 01:17:13 - INFO - Processing batch 41/163\n",
      "2025-06-06 01:17:13 - INFO - Batch 41/163 - Loss: 0.2580 - Avg batch time: 0.25s\n",
      "2025-06-06 01:17:22 - INFO - Processing batch 51/163\n",
      "2025-06-06 01:17:22 - INFO - Batch 51/163 - Loss: 0.3733 - Avg batch time: 0.25s\n",
      "2025-06-06 01:17:31 - INFO - Processing batch 61/163\n",
      "2025-06-06 01:17:32 - INFO - Batch 61/163 - Loss: 0.3722 - Avg batch time: 0.25s\n",
      "2025-06-06 01:17:40 - INFO - Processing batch 71/163\n",
      "2025-06-06 01:17:40 - INFO - Batch 71/163 - Loss: 0.4481 - Avg batch time: 0.25s\n",
      "2025-06-06 01:17:49 - INFO - Processing batch 81/163\n",
      "2025-06-06 01:17:49 - INFO - Batch 81/163 - Loss: 0.3454 - Avg batch time: 0.25s\n",
      "2025-06-06 01:17:58 - INFO - Processing batch 91/163\n",
      "2025-06-06 01:17:58 - INFO - Batch 91/163 - Loss: 0.4633 - Avg batch time: 0.25s\n",
      "2025-06-06 01:18:07 - INFO - Processing batch 101/163\n",
      "2025-06-06 01:18:07 - INFO - Batch 101/163 - Loss: 0.2656 - Avg batch time: 0.25s\n",
      "2025-06-06 01:18:15 - INFO - Processing batch 111/163\n",
      "2025-06-06 01:18:16 - INFO - Batch 111/163 - Loss: 0.1608 - Avg batch time: 0.25s\n",
      "2025-06-06 01:18:25 - INFO - Processing batch 121/163\n",
      "2025-06-06 01:18:25 - INFO - Batch 121/163 - Loss: 0.2301 - Avg batch time: 0.25s\n",
      "2025-06-06 01:18:34 - INFO - Processing batch 131/163\n",
      "2025-06-06 01:18:34 - INFO - Batch 131/163 - Loss: 0.3470 - Avg batch time: 0.25s\n",
      "2025-06-06 01:18:43 - INFO - Processing batch 141/163\n",
      "2025-06-06 01:18:43 - INFO - Batch 141/163 - Loss: 0.2956 - Avg batch time: 0.25s\n",
      "2025-06-06 01:18:51 - INFO - Processing batch 151/163\n",
      "2025-06-06 01:18:52 - INFO - Batch 151/163 - Loss: 0.2197 - Avg batch time: 0.25s\n",
      "2025-06-06 01:19:00 - INFO - Processing batch 161/163\n",
      "2025-06-06 01:19:01 - INFO - Batch 161/163 - Loss: 0.2978 - Avg batch time: 0.25s\n",
      "2025-06-06 01:19:02 - INFO - \n",
      "Epoch 25 training completed in 144.99s\n",
      "2025-06-06 01:19:02 - INFO - Average training loss: 0.2740\n",
      "Epochs:  26%|▎| 26/100 [1:16:00<3:35:35, 174.81s/it, train_loss=0.2740, val_loss=0.4044, best_val_f1=0.7278, lr=6.25e-052025-06-06 01:19:31 - INFO - \n",
      "Epoch 26/100 - Training phase\n",
      "2025-06-06 01:19:32 - INFO - Processing batch 1/163\n",
      "2025-06-06 01:19:32 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 01:19:32 - INFO - Batch 1/163 - Loss: 0.3959 - Avg batch time: 0.25s\n",
      "2025-06-06 01:19:41 - INFO - Processing batch 11/163\n",
      "2025-06-06 01:19:41 - INFO - Batch 11/163 - Loss: 0.2230 - Avg batch time: 0.25s\n",
      "2025-06-06 01:19:50 - INFO - Processing batch 21/163\n",
      "2025-06-06 01:19:50 - INFO - Batch 21/163 - Loss: 0.3422 - Avg batch time: 0.25s\n",
      "2025-06-06 01:19:59 - INFO - Processing batch 31/163\n",
      "2025-06-06 01:19:59 - INFO - Batch 31/163 - Loss: 0.2690 - Avg batch time: 0.25s\n",
      "2025-06-06 01:20:07 - INFO - Processing batch 41/163\n",
      "2025-06-06 01:20:08 - INFO - Batch 41/163 - Loss: 0.4070 - Avg batch time: 0.25s\n",
      "2025-06-06 01:20:16 - INFO - Processing batch 51/163\n",
      "2025-06-06 01:20:17 - INFO - Batch 51/163 - Loss: 0.2542 - Avg batch time: 0.25s\n",
      "2025-06-06 01:20:25 - INFO - Processing batch 61/163\n",
      "2025-06-06 01:20:26 - INFO - Batch 61/163 - Loss: 0.2941 - Avg batch time: 0.25s\n",
      "2025-06-06 01:20:34 - INFO - Processing batch 71/163\n",
      "2025-06-06 01:20:34 - INFO - Batch 71/163 - Loss: 0.4129 - Avg batch time: 0.25s\n",
      "2025-06-06 01:20:43 - INFO - Processing batch 81/163\n",
      "2025-06-06 01:20:44 - INFO - Batch 81/163 - Loss: 0.1465 - Avg batch time: 0.25s\n",
      "2025-06-06 01:20:52 - INFO - Processing batch 91/163\n",
      "2025-06-06 01:20:53 - INFO - Batch 91/163 - Loss: 0.2224 - Avg batch time: 0.25s\n",
      "2025-06-06 01:21:01 - INFO - Processing batch 101/163\n",
      "2025-06-06 01:21:01 - INFO - Batch 101/163 - Loss: 0.3275 - Avg batch time: 0.25s\n",
      "2025-06-06 01:21:10 - INFO - Processing batch 111/163\n",
      "2025-06-06 01:21:10 - INFO - Batch 111/163 - Loss: 0.2734 - Avg batch time: 0.25s\n",
      "2025-06-06 01:21:19 - INFO - Processing batch 121/163\n",
      "2025-06-06 01:21:19 - INFO - Batch 121/163 - Loss: 0.2616 - Avg batch time: 0.25s\n",
      "2025-06-06 01:21:27 - INFO - Processing batch 131/163\n",
      "2025-06-06 01:21:28 - INFO - Batch 131/163 - Loss: 0.1699 - Avg batch time: 0.25s\n",
      "2025-06-06 01:21:36 - INFO - Processing batch 141/163\n",
      "2025-06-06 01:21:37 - INFO - Batch 141/163 - Loss: 0.2597 - Avg batch time: 0.25s\n",
      "2025-06-06 01:21:45 - INFO - Processing batch 151/163\n",
      "2025-06-06 01:21:45 - INFO - Batch 151/163 - Loss: 0.2646 - Avg batch time: 0.25s\n",
      "2025-06-06 01:21:54 - INFO - Processing batch 161/163\n",
      "2025-06-06 01:21:54 - INFO - Batch 161/163 - Loss: 0.1772 - Avg batch time: 0.25s\n",
      "2025-06-06 01:21:55 - INFO - \n",
      "Epoch 26 training completed in 143.86s\n",
      "2025-06-06 01:21:55 - INFO - Average training loss: 0.2760\n",
      "Epochs:  27%|▎| 27/100 [1:18:54<3:32:20, 174.53s/it, train_loss=0.2760, val_loss=0.4011, best_val_f1=0.7278, lr=6.25e-052025-06-06 01:22:25 - INFO - \n",
      "Epoch 27/100 - Training phase\n",
      "2025-06-06 01:22:26 - INFO - Processing batch 1/163\n",
      "2025-06-06 01:22:26 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "2025-06-06 01:22:26 - INFO - Batch 1/163 - Loss: 0.4042 - Avg batch time: 0.25s\n",
      "2025-06-06 01:22:35 - INFO - Processing batch 11/163\n",
      "2025-06-06 01:22:35 - INFO - Batch 11/163 - Loss: 0.2910 - Avg batch time: 0.25s\n",
      "2025-06-06 01:22:44 - INFO - Processing batch 21/163\n",
      "2025-06-06 01:22:44 - INFO - Batch 21/163 - Loss: 0.3314 - Avg batch time: 0.25s\n",
      "Epochs:  27%|▎| 27/100 [1:19:19<3:42:42, 183.04s/it, train_loss=0.2760, val_loss=0.4011, best_val_f1=0.7278, lr=6.25e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 168\u001b[0m\n\u001b[1;32m    163\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss(pos_weight\u001b[38;5;241m=\u001b[39madjusted_pos_weight)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# /home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAVE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hidden attribute\u001b[39;49;00m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_load_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    184\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:249\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, use_oversampling, wandb_config, wandb_project, wandb_run_name, log_wandb, try_load_checkpoint)\u001b[0m\n\u001b[1;32m    246\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training phase\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    247\u001b[0m batch_times \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, data_batch_item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader_to_iterate):\n\u001b[1;32m    250\u001b[0m     batch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Log every 10 batches\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch_geometric/data/dataset.py:291\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03mpresent).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03mbool, will return a subset of the dataset at the specified indices.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(idx))):\n\u001b[0;32m--> 291\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[idx])\n\u001b[1;32m    292\u001b[0m     data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch_geometric/data/dataset.py:118\u001b[0m, in \u001b[0;36mDataset.indices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mindices\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/data/dataset_graph.py:534\u001b[0m, in \u001b[0;36mGraphEEGDataset.len\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    531\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;124;03m    Returns the number of examples in the dataset (number of graphs saved)\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 534\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_file_names\u001b[49m)\n\u001b[1;32m    535\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlength\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m length\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/data/dataset_graph.py:570\u001b[0m, in \u001b[0;36mGraphEEGDataset.processed_file_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;124;03mReturns the names of all processed files.\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# return [f\"data_{i}.pt\" for i in range(len(self.clips))]\u001b[39;00m\n\u001b[1;32m    567\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m    568\u001b[0m     [\n\u001b[1;32m    569\u001b[0m         f\n\u001b[0;32m--> 570\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    572\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Guard against other files\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     ]\n\u001b[1;32m    574\u001b[0m )\n\u001b[1;32m    575\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processed files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m files\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.layers.cnn_lstm_gnn import LSTM_GNN_Model\n",
    "from src.utils.train import train_model\n",
    "\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_best_model.pt\"\n",
    "SUBMISSION_PATH = SUBMISSION_ROOT / \"lstm_gnn_submission.csv\"\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"patience\": 15,\n",
    "    \"epochs\": 100,\n",
    "}\n",
    "\n",
    "# NOTE: model with default parameters\n",
    "model_older = LSTM_GNN_Model(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25,\n",
    "    lstm_hidden_dim = 64,\n",
    "    lstm_out_dim = 64,  # This will be the time_encoder_output_dim for the GCN\n",
    "    lstm_dropout = 0.25,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 64,\n",
    "    gcn_out_channels = 32,\n",
    "    num_gcn_layers = 3,\n",
    "    gcn_dropout = 0.5,\n",
    "    num_classes = 1,  # For binary classification (seizure/non-seizure)\n",
    "    num_channels = 19,  # Number of EEG channels\n",
    ")\n",
    "\n",
    "# build model with current parameters\n",
    "# Epochs:   6%| | 6/100 [13:36<4:17:46, 164.54s/it, train_loss=0.6508, val_loss=0.6166, best_val_f1=0.2840, lr=3.00e-04, b2025-06-05 13:20:42 - INFO - \n",
    "# Epochs:   7%| | 7/100 [16:24<4:17:02, 165.83s/it, train_loss=0.6446, val_loss=0.6258, best_val_f1=0.2840, lr=3.00e-04, b2025-06-05 13:23:30 - INFO - \n",
    "model_improved = LSTM_GNN_Model(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25,\n",
    "    lstm_hidden_dim = 96, # 96 original\n",
    "    lstm_out_dim = 96,  # This will be the time_encoder_output_dim for the GCN\n",
    "    lstm_dropout = 0.25,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 96,\n",
    "    gcn_out_channels = 64,\n",
    "    num_gcn_layers = 3,\n",
    "    gcn_dropout = 0.5,\n",
    "    num_classes = 1,  # For binary classification (seizure/non-seizure)\n",
    "    num_channels = 19,  # Number of EEG channels\n",
    ")\n",
    "\n",
    "\n",
    "# build model with current parameters\n",
    "# Epochs:   2%| | 2/100 [03:21<5:28:40, 201.23s/it, train_loss=0.7174, val_loss=0.5724, best_val_f1=0.0848, lr=3.00e-04, b2025-06-05 13:32:42 - INFO - \n",
    "# Epochs:   3%| | 3/100 [06:28<5:12:31, 193.31s/it, train_loss=0.6813, val_loss=0.5767, best_val_f1=0.1734, lr=3.00e-04, b2025-06-05 13:35:50 - INFO - \n",
    "# Epochs:   4%| | 4/100 [09:26<4:57:36, 186.00s/it, train_loss=0.6638, val_loss=0.6072, best_val_f1=0.1734, lr=3.00e-04, b2025-06-05 13:38:47 - INFO - \n",
    "# Epochs:   5%| | 5/100 [12:26<4:50:40, 183.58s/it, train_loss=0.6704, val_loss=0.5754, best_val_f1=0.2178, lr=3.00e-04, b2025-06-05 13:41:47 - INFO - \n",
    "# ...\n",
    "# Epochs:   7%| | 7/100 [19:21<5:10:58, 200.62s/it, train_loss=0.6333, val_loss=0.5949, best_val_f1=0.3921, lr=3.00e-04, b2025-06-05 13:48:43 - INFO - \n",
    "# Epochs:   8%| | 8/100 [23:11<5:22:14, 210.15s/it, train_loss=0.6261, val_loss=0.5993, best_val_f1=0.3921, lr=3.00e-04, b2025-06-05 13:52:33 - INFO - \n",
    "# Epochs:   9%| | 9/100 [26:35<5:15:33, 208.06s/it, train_loss=0.6043, val_loss=0.5743, best_val_f1=0.3921, lr=3.00e-04, b2025-06-05 13:55:56 - INFO - \n",
    "# ...\n",
    "# Epochs:  12%| | 12/100 [36:17<4:49:57, 197.70s/it, train_loss=0.5935, val_loss=0.5691, best_val_f1=0.5043, lr=3.00e-04, 2025-06-05 14:05:38 - INFO - \n",
    "# Epochs:  13%|▏| 13/100 [39:27<4:43:05, 195.24s/it, train_loss=0.5701, val_loss=0.5855, best_val_f1=0.5380, lr=3.00e-04, 2025-06-05 14:08:48 - INFO - \n",
    "# Epochs:  14%|▏| 14/100 [42:32<4:35:35, 192.27s/it, train_loss=0.5329, val_loss=0.6952, best_val_f1=0.5380, lr=3.00e-04, 2025-06-05 14:11:54 - INFO -\n",
    "# Epochs:  18%|▏| 18/100 [55:14<4:22:12, 191.86s/it, train_loss=0.5042, val_loss=0.5616, best_val_f1=0.5623, lr=3.00e-04, 2025-06-05 14:24:36 - INFO -\n",
    "# Epochs:  19%|▏| 19/100 [58:26<4:19:03, 191.89s/it, train_loss=0.5092, val_loss=0.4702, best_val_f1=0.6405, lr=3.00e-04, 2025-06-05 14:27:48 - INFO - \n",
    "# Epochs:  20%|▏| 20/100 [04:25<5:53:37, 265.22s/it, train_loss=0.5077, val_loss=0.4850, best_val_f1=0.6405, lr=3.00e-04, 2025-06-05 15:35:20 - INFO - \n",
    "# Epochs:  21%|▏| 21/100 [07:55<5:06:16, 232.62s/it, train_loss=0.4657, val_loss=0.4666, best_val_f1=0.6405, lr=3.00e-04, 2025-06-05 15:38:49 - INFO - \n",
    "# ...\n",
    "# Epochs:  23%|▏| 23/100 [16:40<5:02:39, 235.83s/it, train_loss=0.4786, val_loss=0.4441, best_val_f1=0.6405, lr=3.00e-04, 2025-06-05 15:24:57 - INFO -\n",
    "# Epochs:  24%|▏| 24/100 [18:00<4:20:53, 205.96s/it, train_loss=0.4688, val_loss=0.5586, best_val_f1=0.6405, lr=3.00e-04, 2025-06-05 15:48:55 - INFO - \n",
    "# Epochs:  25%|▎| 25/100 [21:08<4:09:36, 199.69s/it, train_loss=0.4521, val_loss=0.4014, best_val_f1=0.6484, lr=3.00e-04, 2025-06-05 15:52:02 - INFO - \n",
    "# Epochs:  26%|▎| 26/100 [24:09<3:58:50, 193.65s/it, train_loss=0.4378, val_loss=0.3937, best_val_f1=0.6800, lr=3.00e-04, 2025-06-05 15:55:04 - INFO - \n",
    "# ---- FROM HERE IT DOES NOT LEARN ANYTHING!!!\n",
    "# ....\n",
    "#\n",
    "# Epochs:  31%|▎| 31/100 [39:30<3:35:33, 187.44s/it, train_loss=0.4061, val_loss=0.4341, best_val_f1=0.6800, lr=3.00e-04, 2025-06-05 16:10:25 - INFO - \n",
    "#...\n",
    "# (other run)\n",
    "# Epochs:  32%|▎| 32/100 [18:51<3:20:29, 176.90s/it, train_loss=0.3984, val_loss=0.4484, best_val_f1=0.6800, lr=3.00e-04, 2025-06-05 18:22:51 - INFO - \n",
    "# ...\n",
    "# Epochs:  35%|▎| 35/100 [52:42<3:31:23, 195.13s/it, train_loss=0.3835, val_loss=0.4302, best_val_f1=0.6800, lr=3.00e-04, 2025-06-05 16:23:36 - INFO - \n",
    "# Epochs:  37%|▎| 37/100 [32:44<2:56:23, 168.00s/it, train_loss=0.3619, val_loss=0.4276, best_val_f1=0.6800, lr=3.00e-04, 2025-06-05 18:36:45 - INFO - \n",
    "# NOTE: BEST MODEL SO FAR!!!\n",
    "# SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_best_model_epochs_.pt\"\n",
    "best_model = LSTM_GNN_Model(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25,\n",
    "    lstm_hidden_dim = 128, # 96 original\n",
    "    lstm_out_dim = 128,  # This will be the time_encoder_output_dim for the GCN\n",
    "    lstm_dropout = 0.25,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 128,\n",
    "    num_gcn_layers = 3,\n",
    "    gcn_dropout = 0.5,\n",
    "    num_classes = 1,  # For binary classification (seizure/non-seizure)\n",
    "    num_channels = 19,  # Number of EEG channels\n",
    ")\n",
    "\n",
    "# Epochs:  29%|▎| 29/100 [2:32:36<6:26:13, 326.39s/it, train_loss=0.3669, val_loss=0.4361, best_val_f1=0.6758, lr=3.00e-042025-06-05 21:22:07 - INFO - \n",
    "# Epochs:  30%|▎| 30/100 [2:38:01<6:20:19, 326.00s/it, train_loss=0.3709, val_loss=0.5588, best_val_f1=0.6758, lr=3.00e-042025-06-05 21:27:32 - INFO - \n",
    "# NOTE: Not performing well....\n",
    "# SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_best_model_even_bigger.pt\"\n",
    "new_model = LSTM_GNN_Model(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25,\n",
    "    lstm_hidden_dim = 128, # 96 original\n",
    "    lstm_out_dim = 128,  # This will be the time_encoder_output_dim for the GCN\n",
    "    lstm_dropout = 0.25,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 64,\n",
    "    num_gcn_layers = 3,\n",
    "    gcn_dropout = 0.5,\n",
    "    num_classes = 1,  # For binary classification (seizure/non-seizure)\n",
    "    num_channels = 19,  # Number of EEG channels\n",
    ")\n",
    "\n",
    "# Same setup as best model, with bigger GCN output channels to check if it can learn something\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_best_model_bigger_gcn_output_channels.pt\"\n",
    "# Epochs:  13%|▏| 13/100 [33:58<4:06:35, 170.07s/it, train_loss=0.5545, val_loss=0.5253, best_val_f1=0.5505, lr=3.00e-04, 2025-06-05 22:20:18 - INFO - \n",
    "# Epochs:  28%|▎| 28/100 [1:16:29<3:23:10, 169.32s/it, train_loss=0.4188, val_loss=0.4747, best_val_f1=0.5817, lr=3.00e-042025-06-05 23:02:49 - INFO - \n",
    "# NOTE: THIS MODEL IS NOT PERFORMING WELL + IT IS SLOW TO TRAIN\n",
    "model_improved_bigger = LSTM_GNN_Model(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25,\n",
    "    lstm_hidden_dim = 128, # original 128\n",
    "    lstm_out_dim = 128,  # original 128\n",
    "    lstm_dropout = 0.25,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 192, # original 128\n",
    "    gcn_out_channels = 192, # original 64\n",
    "    num_gcn_layers = 3,\n",
    "    gcn_dropout = 0.5,\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")\n",
    "\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_best_model_35_epochs.pt\"\n",
    "# Epochs:   2%| | 2/100 [04:54<8:01:46, 294.97s/it, train_loss=0.5635, val_loss=0.6869, best_val_f1=0.5291, lr=1.00e-03, b2025-06-06 00:08:26 - INFO - \n",
    "# Epochs:  26%|▎| 26/100 [1:16:00<3:35:35, 174.81s/it, train_loss=0.2740, val_loss=0.4044, best_val_f1=0.7278, lr=6.25e-052025-06-06 01:19:31 - INFO - \n",
    "new_best_model_test = LSTM_GNN_Model(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25,\n",
    "    lstm_hidden_dim = 128, # 96 original\n",
    "    lstm_out_dim = 128,  # This will be the time_encoder_output_dim for the GCN\n",
    "    lstm_dropout = 0.25,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 64,\n",
    "    num_gcn_layers = 4,\n",
    "    gcn_dropout = 0.5,\n",
    "    num_classes = 1,  # For binary classification (seizure/non-seizure)\n",
    "    num_channels = 19,  # Number of EEG channels\n",
    ")\n",
    "\n",
    "# select model to use\n",
    "model = new_best_model_test\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "adjusted_pos_weight = torch.tensor([1.5], dtype=torch.float32).to(device)\n",
    "print(f'pos_weight:{adjusted_pos_weight}')\n",
    "loss = nn.BCEWithLogitsLoss(pos_weight=adjusted_pos_weight)\n",
    "\n",
    "# /home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
    "\n",
    "# train model\n",
    "train_history, val_history = train_model(\n",
    "    wandb_config=None,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=loss,\n",
    "    scheduler=scheduler,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=config[\"epochs\"],\n",
    "    patience=config[\"patience\"],\n",
    "    save_path=SAVE_PATH,\n",
    "    use_gnn=True,\n",
    "    # hidden attribute\n",
    "    try_load_checkpoint=True,\n",
    "    log_wandb=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63329a79",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_training_loss\n\u001b[0;32m----> 3\u001b[0m plot_training_loss(\u001b[43mtrain_history\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], val_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_history' is not defined"
     ]
    }
   ],
   "source": [
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "293bb297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Evaluating model. Loading model from: .checkpoints/lstm_gnn_best_model_35_epochs.pt\n",
      "   - Loading checkpoint from: .checkpoints/lstm_gnn_best_model_35_epochs.pt\n",
      "   - Detected full checkpoint dictionary.\n",
      "   - Model state successfully loaded.\n",
      "🧪 Performing inference on the test set...\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ 13.7818,  12.0088,  10.9369,  ...,  55.7613,  54.8321,  53.2772],\n",
      "        [ 17.4475,  12.0189,   8.5730,  ...,  25.5270,  23.5848,  20.9486],\n",
      "        [ -1.1044,  -2.4138,  -1.8681,  ...,  12.8679,  12.2525,  11.3361],\n",
      "        ...,\n",
      "        [  4.4002,  -1.2584,  -4.3083,  ..., -25.9106, -30.0391, -37.5544],\n",
      "        [ 26.8536,  24.6873,  23.8955,  ..., -17.1835, -21.8779, -27.2960],\n",
      "        [ 37.8393,  39.9656,  40.4172,  ..., -13.8870, -16.6136, -16.9901]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[-15.3751, -15.9740, -13.1877,  ...,  14.2007,  17.7176,  18.9752],\n",
      "        [-15.1118, -22.0024, -24.6107,  ..., -31.2303, -23.7167, -18.5535],\n",
      "        [-48.2384, -47.3045, -40.8518,  ..., -55.7094, -54.6654, -54.2824],\n",
      "        ...,\n",
      "        [-10.1833,  -8.0958,  -6.3664,  ...,   2.9009,   2.9213,   0.5675],\n",
      "        [-19.2513, -17.7443, -15.2040,  ...,  -0.7109,   0.4171,  -0.6318],\n",
      "        [-16.7416, -15.3205, -14.1273,  ...,   1.4004,   5.1552,   6.4765]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[-17.5601, -24.7429, -23.8394,  ...,  -5.9558,  -1.2985,   1.2526],\n",
      "        [-20.1962, -24.3221, -27.1088,  ...,   3.9662,   3.2620,   0.6461],\n",
      "        [ 10.9332,  12.3759,  11.8396,  ...,   2.0548,   1.8313,   5.4410],\n",
      "        ...,\n",
      "        [ 20.2303,  17.1818,  16.4179,  ...,  -2.8626,  -2.3164,  -1.5223],\n",
      "        [-47.1462, -47.5542, -47.1141,  ...,  26.9326,  28.1487,  29.7391],\n",
      "        [-23.1429, -22.9026, -22.6689,  ...,  39.4230,  41.3294,  43.5083]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ 20.0648,   0.3884, -19.5247,  ...,  60.2155,  63.2063,  56.4372],\n",
      "        [-15.7369, -26.2935, -36.4932,  ..., -10.2490, -20.8118, -29.1580],\n",
      "        [ 38.4583,  35.2995,  32.0816,  ...,  -2.3830,  -6.5127,  -9.9188],\n",
      "        ...,\n",
      "        [ 34.9709,  33.7413,  33.3130,  ...,  19.6626,  21.1411,  20.7401],\n",
      "        [-26.2434, -27.3785, -27.0290,  ...,  47.6057,  43.7631,  40.7905],\n",
      "        [ 36.7117,  36.6134,  38.6052,  ...,  96.9737,  90.2067,  84.1035]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[  43.3637,   44.7238,   48.3575,  ...,   18.3549,   17.0198,\n",
      "           22.3151],\n",
      "        [  41.1865,   54.6340,   57.3282,  ..., -458.0980, -450.5905,\n",
      "         -446.1565],\n",
      "        [  41.2048,   43.1422,   47.7613,  ...,   50.6715,   47.0928,\n",
      "           46.6608],\n",
      "        ...,\n",
      "        [ -20.4481,  -22.9799,  -24.9577,  ...,    3.8245,    4.3363,\n",
      "            5.0084],\n",
      "        [   0.8187,    2.8980,    3.4496,  ...,    4.2356,    4.6990,\n",
      "            5.0378],\n",
      "        [  14.8853,   18.3976,   20.5481,  ...,    2.0021,    1.7554,\n",
      "            1.2539]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ 6.1248,  7.3512,  7.7461,  ..., -2.6879, -3.7676, -4.5265],\n",
      "        [-1.0567,  0.7357,  1.4226,  ..., -5.0094, -5.9031, -6.3488],\n",
      "        [13.9702, 12.8054, 11.4575,  ..., -6.9265, -8.0731, -9.3117],\n",
      "        ...,\n",
      "        [ 1.3323,  1.3422,  0.6884,  ..., -1.5118,  0.5600,  2.7218],\n",
      "        [ 3.5061,  3.1946,  2.3408,  ...,  2.4960,  3.9453,  6.3486],\n",
      "        [ 1.3651,  0.9871,  0.8874,  ..., -2.2174, -3.5475, -4.0611]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[-7.8724e+00, -8.0412e+00, -7.0785e+00,  ...,  2.9284e+00,\n",
      "          2.6871e+00,  2.5629e+00],\n",
      "        [-7.7851e+00, -7.4436e+00, -6.1026e+00,  ...,  3.8708e+00,\n",
      "          3.7933e+00,  4.3744e+00],\n",
      "        [ 5.1103e+00,  2.7835e+00,  1.2963e+00,  ...,  1.2592e-01,\n",
      "          1.7761e-01, -6.7741e-01],\n",
      "        ...,\n",
      "        [-2.1990e-02, -2.1859e-02, -2.1721e-02,  ..., -2.2145e+01,\n",
      "         -1.7638e+01, -6.4047e+00],\n",
      "        [ 2.6143e-03,  2.6078e-03,  2.6006e-03,  ...,  5.1807e+00,\n",
      "          6.7172e+00,  7.7730e+00],\n",
      "        [ 1.9971e-02,  1.9888e-02,  1.9799e-02,  ...,  7.7799e+00,\n",
      "          1.1943e+01,  1.4335e+01]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[-11.8446, -19.4161, -23.1567,  ..., 101.4980, 113.3134, 115.2570],\n",
      "        [-42.5267, -51.2791, -53.1964,  ...,  47.1180,  59.2627,  78.4063],\n",
      "        [ 14.2151,  18.9020,  13.7124,  ...,  42.0482,  39.2880,  32.9436],\n",
      "        ...,\n",
      "        [  0.5016,  -1.7849,  -2.6356,  ...,  21.9537,  19.8480,  19.1060],\n",
      "        [ -4.4801,  -7.8305,  -9.4484,  ...,  -0.1201,   0.7990,   2.9581],\n",
      "        [-14.0638, -13.9502, -12.6662,  ..., -13.7269, -13.3187, -11.7541]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ 25.6187,  22.8521,  13.1580,  ..., -11.0850, -12.8552, -12.8772],\n",
      "        [  9.9303,  13.8071,  14.0920,  ...,   2.7544,   6.7464,  13.3476],\n",
      "        [ 16.8178,  19.9968,  21.5713,  ...,   6.5426,   5.2701,   0.8171],\n",
      "        ...,\n",
      "        [ -7.6472,  -7.5910,  -7.2635,  ...,   6.1226,   4.6260,   2.2206],\n",
      "        [ -4.7605,  -4.4915,  -4.3872,  ...,  10.9165,  10.7582,  10.3559],\n",
      "        [  1.7602,   1.0093,   1.1553,  ...,  -2.9418,  -0.4402,   0.8923]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[  0.6853,  -1.2749,  -3.1045,  ...,   5.2428,   5.7166,   4.6043],\n",
      "        [-11.3701, -12.9082, -13.7424,  ...,   3.3313,   4.1589,   3.4658],\n",
      "        [ 14.9650,  13.4881,  11.7081,  ...,   4.0178,   3.3858,   1.1119],\n",
      "        ...,\n",
      "        [-38.3013, -37.9220, -37.1634,  ..., -46.2931, -47.8607, -49.4814],\n",
      "        [  4.9540,   4.5975,   4.3793,  ...,   5.3795,   5.5549,   5.6223],\n",
      "        [ 17.3796,  18.9531,  21.0001,  ...,  37.3023,  39.0455,  39.9931]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[-22.0372, -24.4611, -25.5835,  ...,  -8.4203,  -6.7188,  -5.0386],\n",
      "        [-74.6908, -74.9301, -75.5182,  ...,  -0.1508,   1.4287,   2.6992],\n",
      "        [ 12.2395,  12.5473,  13.2252,  ...,   2.9176,   3.3698,   3.8590],\n",
      "        ...,\n",
      "        [  5.5725,   6.4640,   7.3531,  ...,   1.5105,   1.6067,   2.0121],\n",
      "        [-10.2084,  -9.9696,  -9.8628,  ...,  -2.1976,  -2.0660,  -1.9274],\n",
      "        [-10.7317, -11.1054, -11.0075,  ...,  -2.0244,  -2.1602,  -3.5905]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ 3.2228e+00,  1.1985e+00, -6.2408e-01,  ..., -5.8441e+00,\n",
      "         -5.7092e+00, -5.8473e+00],\n",
      "        [-6.0022e+00, -7.6527e+00, -8.2063e+00,  ..., -7.9983e+00,\n",
      "         -7.8780e+00, -7.9510e+00],\n",
      "        [ 1.9080e+00,  4.2426e-01, -9.1062e-01,  ..., -2.3698e-01,\n",
      "         -3.6628e-01, -1.1414e+00],\n",
      "        ...,\n",
      "        [ 7.3189e+01,  7.6012e+01,  7.1097e+01,  ..., -1.3069e+01,\n",
      "         -1.6303e+01, -1.5294e+01],\n",
      "        [ 3.3906e+02,  3.1914e+02,  2.9727e+02,  ...,  9.6584e+00,\n",
      "          1.2191e+01,  7.4849e+00],\n",
      "        [ 9.9916e+01,  8.2212e+01,  7.2244e+01,  ..., -5.9812e+00,\n",
      "         -1.2111e+00,  3.2444e+00]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ 41.4459,  33.1453,  22.4574,  ...,   5.9966,   6.2686,   8.2730],\n",
      "        [ -6.4440,  -6.4214,  -2.7126,  ...,   0.6301,  -0.8955,  -2.5250],\n",
      "        [ 13.6708,  20.3189,  24.5560,  ..., -18.1477, -18.3427, -19.1964],\n",
      "        ...,\n",
      "        [  6.0361,   8.2208,   9.4903,  ..., -11.8934, -11.4025,  -9.5550],\n",
      "        [ 58.0808,  58.7551,  58.8435,  ...,   6.1701,   5.6528,   6.0191],\n",
      "        [ 18.2367,  16.6630,  17.1859,  ...,  11.0173,   9.5855,   8.8562]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[-15.6210, -13.5878, -12.0405,  ...,  -0.7873,  -2.5487,  -2.4265],\n",
      "        [ -8.3129,  -5.1657,  -3.7311,  ...,  -2.6946,  -2.5588,  -2.1429],\n",
      "        [-13.7036, -11.3302, -13.1857,  ...,  -3.4427,  -2.7525,  -1.8952],\n",
      "        ...,\n",
      "        [  0.3482,   0.2842,   0.1019,  ...,   0.8584,   0.4344,   0.0511],\n",
      "        [ -1.4542,  -1.9184,  -2.3500,  ...,   3.1418,   2.2268,   1.8724],\n",
      "        [ -1.9575,  -2.4187,  -2.9620,  ...,   4.9953,   4.9948,   5.5448]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[-12.9431, -13.1136, -12.7843,  ...,  -6.3535,  -6.7619,  -6.7067],\n",
      "        [-16.1435, -16.9329, -16.3552,  ..., -10.9890, -10.8727, -10.3193],\n",
      "        [  0.3369,   0.1745,   0.0864,  ...,  -4.7268,  -4.8622,  -5.3144],\n",
      "        ...,\n",
      "        [-22.8766, -23.7078, -24.4950,  ...,  41.1238,  37.2184,  33.2719],\n",
      "        [-15.7368, -15.8725, -16.2747,  ...,  -1.7019,  -2.2653,  -3.9651],\n",
      "        [ -3.8701,  -5.2311,  -6.5923,  ..., -34.9587, -35.2011, -35.4081]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[-3.3736e+01, -3.5723e+01, -3.7344e+01,  ...,  3.8804e+01,\n",
      "          3.8933e+01,  3.9354e+01],\n",
      "        [-3.4299e+01, -3.5294e+01, -3.5932e+01,  ...,  3.7439e+01,\n",
      "          3.5226e+01,  3.3166e+01],\n",
      "        [-1.6061e+01, -1.8000e+01, -2.0119e+01,  ...,  3.3718e+01,\n",
      "          3.4703e+01,  3.5448e+01],\n",
      "        ...,\n",
      "        [ 1.2163e+01,  1.4703e+01,  1.7132e+01,  ..., -1.4433e+01,\n",
      "         -1.5002e+01, -1.5951e+01],\n",
      "        [-1.5262e+00,  1.9153e-02,  1.4697e+00,  ..., -4.9926e-02,\n",
      "          5.3018e-01,  1.1556e+00],\n",
      "        [-1.1774e+01, -1.3343e+01, -1.4269e+01,  ...,  2.1315e+01,\n",
      "          2.1609e+01,  2.1438e+01]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[-12.3292, -11.7319,  -9.5587,  ...,  32.5510,  34.0092,  34.3536],\n",
      "        [-22.5897, -22.8600, -21.3172,  ...,  60.3616,  61.7441,  61.7236],\n",
      "        [ -5.9149,  -6.3811,  -5.5672,  ...,   9.2481,  12.4072,  14.3812],\n",
      "        ...,\n",
      "        [-32.3919, -31.9388, -31.5940,  ...,  -4.4505,  -3.3946,  -1.5704],\n",
      "        [  7.3582,   6.8011,   6.6227,  ..., -22.1830, -23.1735, -23.5026],\n",
      "        [ 29.7885,  28.9114,  27.9733,  ...,  -0.4664,  -1.7898,  -3.9555]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[  9.9984,  10.7856,  11.4212,  ...,   5.0519,   8.3906,  11.0457],\n",
      "        [ 21.4846,  21.7229,  21.1373,  ...,  -2.3461,  -1.1700,   0.5896],\n",
      "        [-10.5309,  -8.7684,  -7.1937,  ...,   9.0861,  13.0345,  16.2671],\n",
      "        ...,\n",
      "        [ -6.1736,  -5.6893,  -4.8545,  ...,  15.8254,  14.8659,  13.1523],\n",
      "        [ -1.3262,  -0.8048,  -0.4194,  ...,  24.2147,  21.8233,  19.2496],\n",
      "        [ 10.2553,  10.9477,  10.2689,  ...,  12.1650,   8.6201,   6.6889]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ -9.1154,  -8.7558,  -8.5245,  ..., -14.4497, -15.0230, -15.8969],\n",
      "        [ -7.8439,  -6.8796,  -5.5980,  ..., -12.2706, -12.5910, -12.6900],\n",
      "        [  1.4697,  -0.6091,  -2.9198,  ...,  -8.8417,  -9.1716,  -9.3831],\n",
      "        ...,\n",
      "        [  0.2887,  -0.3286,  -0.8443,  ...,   1.8072,  -0.8061,  -3.1313],\n",
      "        [  0.2034,   2.0053,   3.3495,  ...,  -6.0637,  -8.9377, -12.0133],\n",
      "        [ -2.7499,  -1.0522,  -0.2962,  ...,  -3.7222,  -5.1434,  -7.2547]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[  1.0614,   2.0891,   3.7980,  ...,  17.9942,  18.2185,  18.3257],\n",
      "        [  0.9210,   2.2109,   2.6468,  ...,  17.7512,  18.9201,  20.0823],\n",
      "        [  2.9436,   2.2676,   1.6776,  ...,  -0.4595,  -1.4662,  -2.0805],\n",
      "        ...,\n",
      "        [ -6.0204,  -7.0100,  -7.8143,  ...,  -2.5549,  -4.0302,  -5.6089],\n",
      "        [-12.7263, -13.4451, -13.8884,  ...,   1.0709,  -0.8856,  -2.2091],\n",
      "        [ -3.6914,  -3.9885,  -3.5690,  ...,  11.6944,  11.0546,  11.9310]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ -9.4200, -12.1548, -10.9995,  ..., -20.1036, -19.4624, -18.6717],\n",
      "        [ -4.5091,  -2.2332,  -0.2080,  ..., -13.3732, -13.4323, -14.9079],\n",
      "        [  8.9355,   7.4388,   5.6685,  ...,   9.6893,   7.6544,   4.9023],\n",
      "        ...,\n",
      "        [ 11.3993,  11.7533,  11.2096,  ...,  -2.6234,  -0.8058,   0.9513],\n",
      "        [ -2.6509,  -1.3314,   0.0248,  ..., -15.0764, -13.5338, -11.8007],\n",
      "        [ -1.5682,   0.7759,   2.5942,  ...,  -7.8159,  -7.5223,  -6.0938]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[  12.3379,    8.2334,    6.7470,  ...,    9.0353,    7.3713,\n",
      "            6.8198],\n",
      "        [   6.7654,    7.3502,    7.9486,  ...,   35.3161,   34.0328,\n",
      "           30.4360],\n",
      "        [   9.4394,   10.6982,   12.1471,  ...,   -2.3879,   -4.2816,\n",
      "           -5.2655],\n",
      "        ...,\n",
      "        [  21.9270,   14.8171,    8.4160,  ...,   -9.8059,  -10.2613,\n",
      "          -10.7059],\n",
      "        [  26.6656,   24.6604,   22.0521,  ...,   11.6378,   12.0447,\n",
      "           11.3501],\n",
      "        [  11.0132,    8.9378,   11.3000,  ..., -100.2861, -106.4092,\n",
      "         -107.2032]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[-17.6181, -17.1316, -15.7791,  ...,  -6.7720, -10.2848, -12.1114],\n",
      "        [-21.5288, -19.6325, -17.6519,  ...,  11.8711,  10.2520,   8.4891],\n",
      "        [  8.7518,  10.1817,   7.4278,  ..., -21.4479, -23.0469, -24.0434],\n",
      "        ...,\n",
      "        [  4.1711,   3.6364,   3.1422,  ...,  -6.9456,  -6.7804,  -6.8728],\n",
      "        [  2.2859,   2.8642,   3.3001,  ...,   0.5062,   0.4589,   0.3276],\n",
      "        [-55.2761, -51.2001, -50.6931,  ...,  32.1772,  30.5479,  28.6505]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[-16.1721, -17.6871, -19.3279,  ...,  -4.6289,  -4.7555,  -4.7332],\n",
      "        [-14.8135, -15.4684, -15.9717,  ...,  -4.3703,  -4.0391,  -3.8764],\n",
      "        [ -3.2101,  -3.5305,  -4.2760,  ...,  -2.3538,  -2.9061,  -2.9568],\n",
      "        ...,\n",
      "        [ 15.5258,  15.7482,  16.3407,  ...,   2.6656,   4.7897,   6.6912],\n",
      "        [  0.4045,   0.6218,   0.6091,  ...,   9.7511,   9.5426,   9.2984],\n",
      "        [ -8.8212,  -9.3998, -10.8316,  ...,   6.4295,   3.2682,   0.5357]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ -8.5214,  -8.0536,  -8.4454,  ..., -24.6142, -30.4154, -34.0535],\n",
      "        [-13.9902, -12.1200,  -9.1277,  ..., -25.6612, -28.9393, -32.9959],\n",
      "        [  9.3779,   9.5979,   9.3006,  ...,   3.8820,   5.5373,   7.0974],\n",
      "        ...,\n",
      "        [-10.3672,  -9.5842, -10.5262,  ..., -24.5014, -23.5315, -23.1077],\n",
      "        [ 23.6779,  24.3058,  22.5927,  ..., -12.0375, -12.0150, -12.5102],\n",
      "        [ 30.7070,  28.8308,  26.3846,  ...,   4.9649,   4.7592,   5.0525]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[  6.0621,   6.2956,   6.6344,  ..., -41.7267, -40.6643, -39.9417],\n",
      "        [-40.0401, -38.4849, -36.4719,  ..., -28.9073, -27.1211, -25.5710],\n",
      "        [  4.4618,   1.7680,  -1.0302,  ..., -47.1362, -47.5312, -48.1769],\n",
      "        ...,\n",
      "        [ -2.5946,  -3.0138,  -3.9751,  ...,   2.2230,   2.5319,   0.7903],\n",
      "        [ -0.5954,   0.2064,   0.6251,  ...,   4.9559,   5.8074,   6.1479],\n",
      "        [ -0.1285,   1.0240,   2.6070,  ...,   1.5819,   2.3774,   3.4748]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[-10.4517, -15.5519, -14.0357,  ...,  -5.4586,  -1.9067,  -2.3756],\n",
      "        [ -7.0560,  -6.5026,  -1.0498,  ..., -11.8526,  -7.5748,  -5.9301],\n",
      "        [ -5.7924,  -8.4043,  -7.6975,  ...,   4.6820,   5.9468,   5.9531],\n",
      "        ...,\n",
      "        [ -2.0371,  -3.0740,  -4.4177,  ...,  -4.0863,  -8.9844, -11.9533],\n",
      "        [  6.7117,   8.2966,   9.9828,  ...,   0.4140,  -1.3616,  -1.9238],\n",
      "        [  8.0603,  10.2702,  12.5870,  ...,   6.5056,   6.0220,   4.3566]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[-10.7500, -14.6738, -21.2273,  ..., -21.0420, -26.0004, -29.4369],\n",
      "        [-30.9354, -30.0858, -27.0280,  ..., -24.2032, -27.8823, -34.2052],\n",
      "        [ -1.6634,  -2.5993,  -4.1859,  ...,  -4.8386,  -3.0087,  -2.6014],\n",
      "        ...,\n",
      "        [  4.0043,   3.4193,   2.6841,  ...,  -0.1570,  -0.6696,  -0.3420],\n",
      "        [ -4.0414,  -4.3416,  -4.7843,  ...,  -5.7490,  -6.6904,  -6.7568],\n",
      "        [ -6.0756,  -5.9347,  -5.8992,  ...,  -6.9246,  -7.2706,  -7.8654]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ 6.0274,  7.4799,  8.2367,  ..., -1.1682, -1.5102, -1.4699],\n",
      "        [ 5.6121,  6.6898,  7.0701,  ..., -1.3423, -1.5179, -1.5802],\n",
      "        [ 1.8108,  3.1076,  4.1488,  ...,  0.4844,  0.4466,  0.4801],\n",
      "        ...,\n",
      "        [ 1.0423,  1.1249,  1.4995,  ..., -0.4805, -0.0249,  0.2185],\n",
      "        [-1.6059, -0.8982, -0.2369,  ...,  2.0601,  2.8630,  3.6124],\n",
      "        [-2.9104, -2.8666, -2.1581,  ...,  3.8462,  4.0154,  4.4987]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[-0.8729, -1.9250, -2.3533,  ...,  3.6564,  3.0560,  3.1752],\n",
      "        [-3.4877, -3.9770, -4.4119,  ...,  6.3397,  5.5382,  5.1522],\n",
      "        [ 6.8543,  6.4071,  5.4534,  ...,  2.8273,  2.3267,  2.0061],\n",
      "        ...,\n",
      "        [ 5.6488,  8.5378, 11.5778,  ..., 16.7501, 19.8542, 22.9055],\n",
      "        [ 0.0543,  0.0908,  0.7418,  ..., 11.9964, 12.7185, 13.9302],\n",
      "        [-6.7715, -7.5044, -7.8375,  ..., -6.3045, -6.9238, -7.1751]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[  3.3883,   0.4972,  -3.6290,  ...,  29.7391,  28.4208,  27.8293],\n",
      "        [  8.1944,   3.9978,  -2.0546,  ...,  18.9797,  16.6425,  13.3604],\n",
      "        [ 18.4740,  20.1436,  21.1122,  ...,  33.2971,  33.2259,  31.9837],\n",
      "        ...,\n",
      "        [  1.3963,  -1.2790,  -3.4833,  ...,   7.8588,   8.0932,   8.8132],\n",
      "        [ -3.5414,  -4.4802,  -5.0482,  ...,  -4.8629,  -4.0646,  -2.5472],\n",
      "        [-14.8032, -13.7347, -12.7556,  ..., -26.4215, -25.6992, -24.1817]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ 8.4715e+00,  6.6399e+00,  1.2519e+00,  ...,  1.8699e+01,\n",
      "          1.9603e+01,  1.8074e+01],\n",
      "        [ 2.5937e+01,  2.6414e+01,  2.4902e+01,  ..., -2.5298e+00,\n",
      "         -3.3360e+00, -4.8819e+00],\n",
      "        [ 3.8435e+00,  2.9506e+00,  7.6624e-01,  ...,  2.7447e+01,\n",
      "          2.7728e+01,  2.7280e+01],\n",
      "        ...,\n",
      "        [-3.0519e+00, -1.9274e+00, -1.1773e+00,  ...,  8.1097e-01,\n",
      "          1.4402e-02, -3.0324e-01],\n",
      "        [-7.9466e-01,  4.3905e-01,  1.0855e+00,  ...,  2.6747e+00,\n",
      "          1.5511e+00,  5.1938e-01],\n",
      "        [-1.6075e+00, -9.3297e-01, -7.6211e-01,  ...,  1.5131e+00,\n",
      "          3.8420e-01, -6.4241e-01]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ -9.3122,  -7.1352,  -5.0636,  ...,  13.4692,  11.2665,   8.9032],\n",
      "        [-17.3363, -15.0324, -12.9933,  ...,  18.6995,  17.2080,  16.5819],\n",
      "        [  5.8422,   6.1415,   5.5727,  ...,   0.7678,  -0.4233,  -2.6313],\n",
      "        ...,\n",
      "        [  0.5939,  11.3057,  11.8371,  ...,   0.5929,   0.7648,   6.1298],\n",
      "        [  2.1681,  10.6576,  10.6591,  ...,  -6.0054,  -6.0568,   0.2985],\n",
      "        [ -3.0040,   3.2664,   2.9140,  ...,  -4.7266,  -5.4659,   1.5774]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ 23.9585,  10.7132, -11.5740,  ...,   2.3399,  -0.3487,  -2.9237],\n",
      "        [ 19.1539,  13.3294,  -2.5510,  ...,  12.8463,   9.9558,   6.8056],\n",
      "        [ 14.8948,   3.3793, -15.2206,  ...,   2.0277,   0.6680,  -3.8844],\n",
      "        ...,\n",
      "        [ -1.8086,  -4.5742,  -6.6884,  ...,   7.6075,   7.4178,   6.7841],\n",
      "        [ -2.8238,  -4.5712,  -5.8386,  ...,   5.9189,   5.7874,   5.2218],\n",
      "        [ -2.4330,  -2.7751,  -2.9631,  ...,   2.4881,   1.8423,   1.3913]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[  0.5488,  -0.8977,  -2.3187,  ...,  10.4517,   9.2697,   7.6350],\n",
      "        [  1.3852,   0.5055,  -0.2831,  ...,   6.7389,   6.1563,   5.1712],\n",
      "        [  3.3978,   1.7139,  -0.1754,  ...,  10.5309,   9.3112,   7.4661],\n",
      "        ...,\n",
      "        [ 10.5441,   7.9259,   4.0274,  ...,  30.2292,  37.6315,  34.5014],\n",
      "        [ -0.7780,  -1.8135,  -3.8886,  ...,  19.6302,  18.3498,  18.0183],\n",
      "        [-10.3581,  -8.4533,  -6.9350,  ...,  -2.1338,  -4.6384,  -1.0032]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ 2.8549,  2.9860,  3.1638,  ...,  6.2767,  4.3790,  1.9846],\n",
      "        [ 6.0347,  5.3807,  4.5727,  ..., 11.2486,  9.4845,  6.8513],\n",
      "        [-5.9278, -5.1434, -4.0518,  ...,  0.8019, -0.0475, -1.4308],\n",
      "        ...,\n",
      "        [-2.9631, -3.0534, -2.7846,  ...,  2.7482,  2.4334,  2.0460],\n",
      "        [-0.6921,  0.0764,  0.5601,  ...,  5.3297,  7.0870,  8.5416],\n",
      "        [-0.9692, -1.1523, -1.4025,  ...,  2.9661,  5.1515,  6.6856]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[  6.9424,   2.7244,  -2.0109,  ...,   9.4103,   8.4905,   7.5212],\n",
      "        [  4.6686,   1.8246,  -0.8720,  ...,  13.9499,  14.2898,  14.0775],\n",
      "        [  2.0749,   0.8835,  -1.2527,  ..., -21.3525, -21.8961, -21.8031],\n",
      "        ...,\n",
      "        [ 12.4007,  10.1110,   9.1161,  ..., -11.8778, -13.4627, -14.7777],\n",
      "        [ 21.3332,  15.0341,  10.6612,  ..., -10.1064, -12.5188, -14.5779],\n",
      "        [  5.2405,   0.7632,  -1.7440,  ...,   1.2814,  -0.2825,  -1.4121]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[  5.4805,   6.4991,   7.8821,  ..., -15.3930, -18.3958, -21.1830],\n",
      "        [ -2.4724,  -2.8312,  -1.1988,  ..., -15.5350, -18.1172, -21.1926],\n",
      "        [ -6.4619,  -6.9040,  -6.5157,  ...,  -9.1694,  -7.2119,  -5.7309],\n",
      "        ...,\n",
      "        [  0.9142,   0.1772,  -0.1289,  ...,   2.7913,   0.5092,  -1.8904],\n",
      "        [  2.2215,   1.7094,   1.0857,  ...,   2.7778,  -0.7394,  -4.0413],\n",
      "        [  1.4988,   1.5027,   1.3045,  ...,   0.2221,  -1.9514,  -3.4081]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[  2.2340,   0.0484,  -1.8803,  ...,  -1.5669,  -1.8253,  -1.6056],\n",
      "        [  2.7515,   0.8245,  -1.3270,  ...,  -2.2708,  -2.7685,  -2.8900],\n",
      "        [ -5.0703,  -6.4208,  -6.8723,  ...,  -0.3310,  -0.2140,   0.2399],\n",
      "        ...,\n",
      "        [-40.7002, -38.0809, -37.0698,  ...,   4.1214,   2.1689,   0.8474],\n",
      "        [-38.8631, -37.2034, -35.9529,  ...,  -3.5095,  -6.0328,  -7.9198],\n",
      "        [ -6.8349,  -6.4156,  -4.9592,  ..., -10.6504,  -9.7710,  -7.7946]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ 15.3869,  15.5248,  15.0180,  ...,  -2.7670,  -3.0302,  -1.8069],\n",
      "        [  0.3850,   7.9892,  15.9319,  ...,  26.0320,  17.7672,  12.9677],\n",
      "        [ -3.0692,  -5.1138,  -7.1497,  ...,  18.8115,  26.4746,  30.7487],\n",
      "        ...,\n",
      "        [  9.4340,   9.5250,   9.8238,  ...,  -1.2104,  -0.6704,   2.9957],\n",
      "        [ -2.7323,  -2.2529,  -2.1452,  ...,  -1.5260,  -0.8555,  -1.2736],\n",
      "        [-12.6902, -11.4896, -10.5997,  ...,   7.6464,   6.8639,   1.5644]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[   4.3972,    3.8496,    3.3381,  ...,    2.2067,    2.4540,\n",
      "            2.9378],\n",
      "        [   2.5510,    2.4870,    2.1997,  ...,    4.9974,    5.1069,\n",
      "            5.0891],\n",
      "        [  -2.6653,   -2.7485,   -2.3184,  ...,    3.0374,    3.1973,\n",
      "            3.3046],\n",
      "        ...,\n",
      "        [ -54.1540,   87.2777,  157.5813,  ...,  125.9491,  -50.3839,\n",
      "         -114.0607],\n",
      "        [ -39.1994,   98.6522,  167.4169,  ...,  130.6258,  -46.5653,\n",
      "         -110.7320],\n",
      "        [ -25.6081,  108.4068,  177.3293,  ...,  127.5048,  -51.2403,\n",
      "         -115.7026]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ 26.7158, 265.4986, 381.8437,  ..., 227.5425, 364.2415, 285.1136],\n",
      "        [  6.4397, 231.6322, 339.8583,  ..., 222.8810, 376.3467, 302.9964],\n",
      "        [ 35.3010, 280.5708, 400.9444,  ..., 232.6276, 372.0609, 283.6110],\n",
      "        ...,\n",
      "        [  1.3418,   1.3645,   2.3470,  ...,   6.1728,   6.9191,   7.3904],\n",
      "        [ -5.5215,  -5.3827,  -4.6701,  ...,   4.0058,   5.2494,   6.3178],\n",
      "        [ -7.0903,  -7.1791,  -7.2200,  ...,   7.2044,   9.0760,  10.3453]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ 1.0395e+01,  1.1138e+01,  1.1201e+01,  ..., -1.7003e+01,\n",
      "         -1.7572e+01, -1.8263e+01],\n",
      "        [-1.1984e+00,  3.4470e-01,  3.0370e+00,  ..., -1.2445e+01,\n",
      "         -1.2520e+01, -1.4740e+01],\n",
      "        [ 1.2635e+01,  1.2234e+01,  1.1673e+01,  ..., -9.2849e+00,\n",
      "         -9.6960e+00, -1.0197e+01],\n",
      "        ...,\n",
      "        [-5.6555e+00, -4.6810e+00, -3.5064e+00,  ..., -1.0886e-02,\n",
      "          2.7417e+00,  3.3356e+00],\n",
      "        [-1.4364e-01,  3.5057e-01,  8.2429e-01,  ..., -3.5118e+00,\n",
      "         -4.0436e+00, -4.1149e+00],\n",
      "        [ 6.7055e-01,  3.6319e-01,  4.5761e-01,  ..., -3.3496e+00,\n",
      "         -5.2864e+00, -6.3874e+00]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ -1.7292,  -3.1297,  -4.2992,  ...,   0.5894,   1.7452,   2.4364],\n",
      "        [  5.6106,  -5.8856, -13.2835,  ...,   2.9986,   5.7773,   5.0929],\n",
      "        [ -6.2679,  -6.7320,  -6.6840,  ...,  -1.4768,  -0.9755,  -0.0715],\n",
      "        ...,\n",
      "        [  6.8806,   6.9056,   6.6463,  ...,   6.2484,   5.9710,   5.6515],\n",
      "        [  6.8600,   8.4537,   9.4251,  ...,   4.8495,   4.8749,   4.6291],\n",
      "        [  5.4203,   7.3140,   8.9284,  ...,   1.0865,   1.8324,   1.9987]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[  5.3043,   5.3658,   4.2995,  ..., -17.1548, -18.7924, -19.8465],\n",
      "        [  4.4197,   5.1916,   6.3508,  ..., -17.9360, -19.9587, -20.6461],\n",
      "        [  5.4297,   5.2423,   4.6745,  ...,  -9.9558,  -9.8789,  -9.8443],\n",
      "        ...,\n",
      "        [  0.1212,  -1.7747,  -3.1934,  ...,  -5.4182,  -4.6044,  -3.8398],\n",
      "        [ -1.8860,  -3.3698,  -4.3848,  ...,  -2.9841,  -2.5711,  -2.4196],\n",
      "        [ -3.6379,  -4.0033,  -4.0434,  ...,   0.6006,   0.0579,  -0.4729]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[-0.1326,  0.3688,  1.4190,  ..., -0.4254, -0.3292,  1.9423],\n",
      "        [-0.9588, -2.5112, -3.3047,  ...,  3.1778,  2.7709,  1.7675],\n",
      "        [ 4.9585,  5.1635,  5.4202,  ...,  0.0176, -0.6324, -0.8582],\n",
      "        ...,\n",
      "        [-3.4179, -3.2320, -3.3178,  ...,  0.2269, -0.3826, -1.1068],\n",
      "        [-0.3312, -0.5701, -1.4581,  ..., 14.1232, 14.2169, 14.2997],\n",
      "        [ 2.5224,  1.4236,  0.3088,  ..., 13.5885, 14.3961, 15.0128]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ 4.3622,  4.6123,  4.5340,  ..., 10.6489, 11.0605,  9.7798],\n",
      "        [ 3.1103,  3.2800,  3.5552,  ..., 14.2094, 15.0575, 13.1960],\n",
      "        [ 5.1218,  4.6656,  4.3085,  ...,  5.0607,  5.1890,  5.2755],\n",
      "        ...,\n",
      "        [ 3.3028,  5.2249,  7.2186,  ...,  6.6433,  7.5286,  7.5145],\n",
      "        [ 5.1838,  6.5212,  7.8608,  ...,  0.5077,  1.9413,  2.7843],\n",
      "        [-0.7384, -1.6943, -2.5365,  ...,  0.2395,  0.4028,  0.9575]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[  6.7418,   7.4550,   8.3292,  ...,   2.6074,   1.8781,   0.7634],\n",
      "        [  1.5424,   0.0663,  -0.6036,  ...,   0.6243,  -0.0280,  -1.3092],\n",
      "        [  5.6008,   6.7331,   8.1407,  ...,   3.5671,   3.6470,   3.2729],\n",
      "        ...,\n",
      "        [-16.3104, -15.8940, -14.6425,  ..., -22.4434, -21.3838, -20.7722],\n",
      "        [ -2.0843,  -1.5947,  -1.1245,  ...,  -6.0850,  -5.5044,  -5.0208],\n",
      "        [ -5.9710,  -6.0186,  -6.2029,  ...,  -2.7955,  -3.1710,  -3.3325]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[-6.2506, -6.2765, -6.0195,  ..., 10.3752, 12.3661, 13.7422],\n",
      "        [ 8.2422,  7.5364,  6.5467,  ...,  2.4025,  5.2811,  7.2272],\n",
      "        [-8.2727, -6.8697, -5.4119,  ..., 32.1389, 31.2935, 31.4046],\n",
      "        ...,\n",
      "        [12.3885, 12.8111, 12.8941,  ...,  2.0000,  1.7913,  1.6836],\n",
      "        [ 7.8971,  8.6603,  8.5156,  ..., -1.8849, -3.0799, -4.1619],\n",
      "        [-4.4687, -4.6218, -4.7985,  ..., -0.7658, -1.4271, -3.0558]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ 10.5233,  10.7753,  10.4569,  ...,  10.6144,  10.2708,   9.9682],\n",
      "        [  2.1840,   1.9664,   0.4050,  ...,  -3.3917,  -2.1250,  -3.6630],\n",
      "        [  3.0297,   4.4725,   6.0416,  ...,  13.8507,  14.5732,  13.7880],\n",
      "        ...,\n",
      "        [  1.8602,   1.7494,   2.3701,  ...,  -0.8071,  -0.4507,   0.9676],\n",
      "        [  0.1891,   1.9310,   4.3047,  ...,  -7.9949,  -7.1014,  -5.9771],\n",
      "        [ -1.3837,  -0.0262,   1.6064,  ..., -14.5822, -13.2434, -12.3800]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[13.7416, 16.4252, 18.2331,  ..., 10.5525,  7.2544,  3.6692],\n",
      "        [-4.6951,  0.3129,  4.1132,  ..., -3.0355,  0.5555,  4.7053],\n",
      "        [ 5.5624,  4.1742,  2.1973,  ...,  8.1119,  5.2587,  3.9572],\n",
      "        ...,\n",
      "        [-3.6858, -4.3405, -5.2745,  ...,  3.9646,  4.5074,  5.0393],\n",
      "        [-5.9044, -6.5698, -7.0862,  ...,  2.9961,  3.8534,  3.9996],\n",
      "        [-6.1723, -6.2008, -6.7403,  ...,  3.4920,  3.7068,  3.0742]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ 3.9262,  4.2216,  3.8345,  ..., -2.7239, -2.5167, -2.4372],\n",
      "        [-2.2900, -1.7205, -1.2966,  ..., 10.9643, 11.1376, 11.2396],\n",
      "        [ 7.0873,  7.8341,  7.5202,  ..., -7.7240, -6.5501, -5.8625],\n",
      "        ...,\n",
      "        [-2.7483, -3.2604, -3.4708,  ..., -2.7369, -1.2131,  0.9342],\n",
      "        [ 3.4222,  3.5613,  3.5826,  ..., -4.2639, -4.1143, -3.3863],\n",
      "        [ 2.7527,  3.6634,  4.4959,  ...,  2.9236,  1.8187,  0.5560]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[20.3608, 22.3995, 24.3753,  ...,  4.3151,  3.6773,  2.5898],\n",
      "        [35.1551, 38.6096, 39.4686,  ..., 15.9490, 14.3555, 12.9469],\n",
      "        [-5.3164, -5.4471, -4.9629,  ..., -6.4241, -5.8449, -5.2277],\n",
      "        ...,\n",
      "        [ 2.4898,  1.7290,  0.7809,  ...,  7.4512,  6.5438,  5.2709],\n",
      "        [ 2.6715,  2.1119,  1.5252,  ...,  8.1659,  7.4188,  6.2602],\n",
      "        [-3.3777, -2.7384, -2.2162,  ...,  7.3007,  6.7078,  6.3818]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[  1.7461,   2.1089,   2.7054,  ...,  -2.2070,  -3.7948,  -5.2867],\n",
      "        [  0.1388,   0.2924,   0.9534,  ...,  -3.1542,  -2.2336,  -1.8378],\n",
      "        [  5.9175,   6.7091,   7.2505,  ...,   0.0331,  -2.3853,  -4.6935],\n",
      "        ...,\n",
      "        [-11.1419, -10.3285, -12.7093,  ...,   5.0922,   4.2859,   2.9996],\n",
      "        [ -5.4314,  -5.1381,  -6.9887,  ...,   6.9200,   6.2712,   5.5837],\n",
      "        [-11.7320, -10.7458, -10.1075,  ...,   0.5582,   0.9731,   1.5400]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ 1.4381e-04,  4.2071e-04,  5.5305e-04,  ..., -6.6330e-09,\n",
      "         -6.5326e-09, -6.4302e-09],\n",
      "        [-7.4436e-04, -2.2713e-03, -3.0014e-03,  ...,  4.4006e-09,\n",
      "          4.6789e-09,  4.9593e-09],\n",
      "        [ 1.0387e-03,  3.1420e-03,  4.1475e-03,  ..., -8.0092e-10,\n",
      "         -6.4249e-10, -4.8266e-10],\n",
      "        ...,\n",
      "        [ 9.7205e+00,  1.0051e+01,  9.9626e+00,  ...,  1.1564e+01,\n",
      "          1.1558e+01,  1.1203e+01],\n",
      "        [ 2.1503e+01,  2.0519e+01,  1.9082e+01,  ..., -2.5758e+00,\n",
      "         -2.1800e+00, -1.9374e+00],\n",
      "        [ 1.0556e+01,  1.0395e+01,  1.0189e+01,  ...,  4.1374e+00,\n",
      "          2.9511e+00,  2.0887e+00]])\n",
      "BATCH: DataBatch(x=[1216, 3000], edge_index=[2, 21888], id=[64], batch=[1216], ptr=[65]), feature: tensor([[ 17.8835,  18.1868,  18.5653,  ...,  15.4043,  15.5648,  16.3559],\n",
      "        [ 27.1565,  28.4588,  27.6870,  ...,   3.0940,   2.1634,   1.3166],\n",
      "        [ 14.6279,  15.2627,  16.0485,  ...,  -9.1713,  -8.9306,  -8.1510],\n",
      "        ...,\n",
      "        [ -6.3672,  -6.7937,  -6.9620,  ...,  -4.2160,  -3.2055,  -3.2397],\n",
      "        [-23.1027, -23.1334, -22.7303,  ...,   8.5382,   7.4493,   5.6917],\n",
      "        [-37.4011, -38.4538, -38.6313,  ...,  -9.3590,  -8.9107,  -8.3396]])\n",
      "BATCH: DataBatch(x=[570, 3000], edge_index=[2, 10260], id=[30], batch=[570], ptr=[31]), feature: tensor([[  4.7333,   4.1031,   3.6731,  ..., -11.9302, -11.1443, -10.6706],\n",
      "        [ -7.7185,  -7.1920,  -5.3239,  ..., -11.5672, -11.6717, -10.7765],\n",
      "        [  0.3256,  -0.0666,  -0.4141,  ...,  -6.8303,  -7.1493,  -7.3195],\n",
      "        ...,\n",
      "        [ -0.2630,  -0.6333,  -1.2967,  ..., -14.9450, -15.0554, -15.2061],\n",
      "        [-17.3858, -17.5110, -17.7429,  ..., -29.0252, -29.9219, -30.3591],\n",
      "        [-13.6386, -13.2790, -12.8792,  ..., -15.6676, -15.8151, -15.9300]])\n",
      "   Generated 3614 predictions for 3614 IDs.\n",
      "📄 Saved submission (3614 rows) → .submissions/lstm_gnn_submission_new_best_model.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pqejgcvm_s001_t000_0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pqejgcvm_s001_t000_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pqejgcvm_s001_t000_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pqejgcvm_s001_t000_11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pqejgcvm_s001_t000_12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3609</th>\n",
       "      <td>pqejgvej_s001_t000_95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3610</th>\n",
       "      <td>pqejgvej_s001_t000_96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3611</th>\n",
       "      <td>pqejgvej_s001_t000_97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3612</th>\n",
       "      <td>pqejgvej_s001_t000_98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3613</th>\n",
       "      <td>pqejgvej_s001_t000_99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3614 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  label\n",
       "0      pqejgcvm_s001_t000_0      0\n",
       "1      pqejgcvm_s001_t000_1      0\n",
       "2     pqejgcvm_s001_t000_10      1\n",
       "3     pqejgcvm_s001_t000_11      1\n",
       "4     pqejgcvm_s001_t000_12      1\n",
       "...                     ...    ...\n",
       "3609  pqejgvej_s001_t000_95      0\n",
       "3610  pqejgvej_s001_t000_96      0\n",
       "3611  pqejgvej_s001_t000_97      0\n",
       "3612  pqejgvej_s001_t000_98      0\n",
       "3613  pqejgvej_s001_t000_99      0\n",
       "\n",
       "[3614 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%aimport src.utils.train\n",
    "from src.utils.train import evaluate_model\n",
    "\n",
    "evaluate_model(\n",
    "    model=model,\n",
    "    test_loader=te_loader,\n",
    "    device=device,\n",
    "    checkpoint_path=SAVE_PATH,\n",
    "    submission_path=SUBMISSION_ROOT / \"lstm_gnn_submission_new_best_model.csv\",\n",
    "    use_gnn=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3887a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "import wandb\n",
    "from src.utils.general_funcs import confusion_matrix_plot\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_val_f1 = 0\n",
    "best_val_f1_epoch = 0\n",
    "patience = 10\n",
    "counter = 0\n",
    "num_epochs = 100\n",
    "print(\"Training started\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # ------- Training ------- #\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_targets = batch.y.reshape(-1, 1)\n",
    "        out = model(batch.x, batch.edge_index, batch.batch)\n",
    "        loss = loss_fn(\n",
    "            out, y_targets\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_loader)  # Average loss per batch\n",
    "\n",
    "    # ------- Validation ------- #\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            print(\"Batch batch:\", batch.batch)\n",
    "            out = model(\n",
    "                batch.x, batch.edge_index, batch.batch\n",
    "            )\n",
    "            loss = loss_fn(out, batch.y.reshape(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "            probs = torch.sigmoid(out).squeeze()  # [batch_size, 1] -> [batch_size]\n",
    "            preds = (probs > 0.5).int()\n",
    "            all_preds.extend(preds.cpu().numpy().ravel())\n",
    "            all_labels.extend(\n",
    "                batch.y.int().cpu().numpy().ravel()\n",
    "            )\n",
    "            \n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)  # Average loss per batch\n",
    "    #scheduler.step(avg_val_loss)\n",
    "    val_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    all_labels = np.array(all_labels).astype(int)\n",
    "    all_preds = np.array(all_preds).astype(int)\n",
    "\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.grad is not None:\n",
    "    #         print(f\"{name} grad mean: {param.grad.abs().mean()}\")\n",
    "    \n",
    "    # Monitor progress\n",
    "    print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Additional metrics\n",
    "\n",
    "    # Confusion matrix\n",
    "    confusion_matrix_plot(all_preds, all_labels)\n",
    "    # Compute metrics per class (0 and 1)\n",
    "    precision = precision_score(all_labels, all_preds, average=None)\n",
    "    recall = recall_score(all_labels, all_preds, average=None)\n",
    "    f1 = f1_score(all_labels, all_preds, average=None)\n",
    "\n",
    "    # Print only for class 1\n",
    "    print(f\"Class 1 — Precision: {precision[1]:.2f}, Recall: {recall[1]:.2f}, F1: {f1[1]:.2f}\")\n",
    "    \n",
    "    # W&B\n",
    "    # wandb.log(\n",
    "    #     {\n",
    "    #         \"epoch\": epoch,\n",
    "    #         \"train_loss\": avg_train_loss,\n",
    "    #         \"val_loss\": avg_val_loss,\n",
    "    #         \"val_f1\": val_f1,\n",
    "    #         \"val_f1_class_1\":f1[1],\n",
    "    #             \"val_f1_class_0\":f1[0]\n",
    "    #     }\n",
    "    # )\n",
    "    print(f\"Epoch {epoch} — Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val F1: {val_f1:.4f}\", flush=True)\n",
    "    # ------- Record best F1 score ------- #\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_val_f1_epoch = epoch\n",
    "        best_preds = all_preds.copy()\n",
    "        best_labels = all_labels.copy()\n",
    "        # Load best stats in wandb\n",
    "        wandb.summary[\"best_f1_score\"] = val_f1\n",
    "        wandb.summary[\"f1_score_epoch\"] = epoch\n",
    "    # ------- Early Stopping ------- #\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        # Save best statistics and model\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        best_state_dict = model.state_dict().copy()  # Save the best model state\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(f\"Best validation F1: {best_val_f1:.4f} at epoch {best_val_f1_epoch}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
