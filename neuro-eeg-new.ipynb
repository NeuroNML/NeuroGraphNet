{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b998b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from src.data.geodataloader import GeoDataLoader\n",
    "from torch.utils.data import Subset, WeightedRandomSampler\n",
    "# from torch.utils.data import DataLoader\n",
    "from src.utils.seeder import seed_everything\n",
    "\n",
    "# set seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# create useful constants\n",
    "RANDOM_SEED = 42\n",
    "IS_SCITAS = True # set to True if running on SCITAS cluster\n",
    "LOCAL_DATA_ROOT = Path(\"./data\")\n",
    "DATA_ROOT = Path(\"/home/ogut/data\") if IS_SCITAS else LOCAL_DATA_ROOT\n",
    "CHECKPOINT_ROOT = Path(\"./.checkpoints\")\n",
    "SUBMISSION_ROOT = Path(\"./.submissions\")\n",
    "\n",
    "# create directories if they do not exist\n",
    "CHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# set dataset root\n",
    "seed_everything(RANDOM_SEED)\n",
    "\n",
    "# setup torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d28586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# # execute feature extraction script\n",
    "# process = None\n",
    "# try:\n",
    "#     process = subprocess.Popen([\"python3\", \"scripts/feature_extractor.py\"])\n",
    "#     process.wait()\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"Process interrupted, terminating...\")\n",
    "#     if process:\n",
    "#         process.terminate()\n",
    "#         process.wait()\n",
    "# except Exception as e:\n",
    "#     print(f\"Error occurred: {e}\")\n",
    "#     if process:\n",
    "#         process.terminate()\n",
    "#         process.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45999291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacial distance matrix between sensors\n",
    "spatial_distance_file = LOCAL_DATA_ROOT / \"distances_3d.csv\"\n",
    "\n",
    "# training data\n",
    "train_dir = DATA_ROOT / \"train\"\n",
    "train_dir_metadata = train_dir / \"segments.parquet\"\n",
    "train_dataset_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_correlation_train\"\n",
    "train_dataset_spatial_dir = LOCAL_DATA_ROOT / \"graph_dataset_spatial_train\"\n",
    "\n",
    "# test data\n",
    "test_dir = DATA_ROOT / \"test\"\n",
    "test_dir_metadata = test_dir / \"segments.parquet\"\n",
    "test_dataset_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_correlation_test\"\n",
    "test_dataset_spatial_dir = LOCAL_DATA_ROOT / \"graph_dataset_spatial_test\"\n",
    "\n",
    "# additional features\n",
    "extracted_features_dir = LOCAL_DATA_ROOT / \"extracted_features\"\n",
    "embeddings_dir =  LOCAL_DATA_ROOT / \"embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83d93851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.index import ensure_eeg_multiindex \n",
    "\n",
    "# Load clips from datasets\n",
    "clips_tr = pd.read_parquet(train_dir_metadata)\n",
    "clips_tr = ensure_eeg_multiindex(clips_tr)\n",
    "clips_tr['id'] = clips_tr.index.map(lambda x: '_'.join(str(i) for i in x))\n",
    "assert clips_tr.id.nunique() == len(clips_tr), \"There are duplicate IDs\"\n",
    "clips_tr = clips_tr[~clips_tr.label.isna()].reset_index()\n",
    "\n",
    "# Load clips from datasets\n",
    "clips_te = pd.read_parquet(test_dir_metadata)\n",
    "clips_te = ensure_eeg_multiindex(clips_te)\n",
    "clips_te['id'] = clips_te.index.map(lambda x: '_'.join(str(i) for i in x))\n",
    "assert clips_te.id.nunique() == len(clips_te), \"There are duplicate IDs\"\n",
    "clips_te = clips_te.reset_index()\n",
    "\n",
    "# sort in order to maintain the same submission order\n",
    "clips_te = clips_te.sort_values(by=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ea225",
   "metadata": {},
   "source": [
    "## Create + load spatial graph datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d983da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.dataset_graph:Initializing GraphEEGDataset...\n",
      "INFO:src.data.dataset_graph:Dataset parameters:\n",
      "INFO:src.data.dataset_graph:  - Root directory: data/graph_dataset_spatial_train\n",
      "INFO:src.data.dataset_graph:  - Edge strategy: spatial\n",
      "INFO:src.data.dataset_graph:  - Top-k neighbors: None\n",
      "INFO:src.data.dataset_graph:  - Correlation threshold: 0.7\n",
      "INFO:src.data.dataset_graph:  - Force reprocess: False\n",
      "INFO:src.data.dataset_graph:  - Bandpass frequencies: (0.5, 50)\n",
      "INFO:src.data.dataset_graph:  - Segment length: 3000\n",
      "INFO:src.data.dataset_graph:  - Apply filtering: True\n",
      "INFO:src.data.dataset_graph:  - Apply rereferencing: True\n",
      "INFO:src.data.dataset_graph:  - Apply normalization: False\n",
      "INFO:src.data.dataset_graph:  - Sampling rate: 250\n",
      "INFO:src.data.dataset_graph:  - Test mode: False\n",
      "INFO:src.data.dataset_graph:  - Extract graph features: True\n",
      "INFO:src.data.dataset_graph:Initializing graph feature extractor...\n",
      "2025-06-08 22:06:00,008 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.utils.graph_features:GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Number of EEG channels: 19\n",
      "INFO:src.data.dataset_graph:Setting up signal filters...\n",
      "INFO:src.data.dataset_graph:Loading spatial distances from data/distances_3d.csv\n",
      "INFO:src.data.dataset_graph:Loading spatial distances from data/distances_3d.csv\n",
      "INFO:src.data.dataset_graph:Loaded 361 spatial distances in 0.02s\n",
      "INFO:src.data.dataset_graph:Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_dataset: 12993\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# dataset settings\n",
    "low_bandpass_frequency = 0.5\n",
    "high_bandpass_frequency = 50\n",
    "\n",
    "# additional settings\n",
    "oversampling_power = 1.0\n",
    "\n",
    "# load training dataset\n",
    "dataset_spatial_tr = GraphEEGDataset(\n",
    "    root=train_dataset_spatial_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"spatial\",\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file\n",
    "    ),\n",
    "    top_k=None,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=False,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None # collect all graph features\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of train_dataset: {len(dataset_spatial_tr)}\")\n",
    "print(f' Eliminated IDs: {dataset_spatial_tr.ids_to_eliminate}')\n",
    "clips_spatial_tr = clips_tr[~clips_tr.index.isin(dataset_spatial_tr.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f11824a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.dataset_graph:Initializing GraphEEGDataset...\n",
      "INFO:src.data.dataset_graph:Dataset parameters:\n",
      "INFO:src.data.dataset_graph:  - Root directory: data/graph_dataset_correlation_test\n",
      "INFO:src.data.dataset_graph:  - Edge strategy: spatial\n",
      "INFO:src.data.dataset_graph:  - Top-k neighbors: None\n",
      "INFO:src.data.dataset_graph:  - Correlation threshold: 0.7\n",
      "INFO:src.data.dataset_graph:  - Force reprocess: False\n",
      "INFO:src.data.dataset_graph:  - Bandpass frequencies: (0.5, 50)\n",
      "INFO:src.data.dataset_graph:  - Segment length: 3000\n",
      "INFO:src.data.dataset_graph:  - Apply filtering: True\n",
      "INFO:src.data.dataset_graph:  - Apply rereferencing: True\n",
      "INFO:src.data.dataset_graph:  - Apply normalization: True\n",
      "INFO:src.data.dataset_graph:  - Sampling rate: 250\n",
      "INFO:src.data.dataset_graph:  - Test mode: True\n",
      "INFO:src.data.dataset_graph:  - Extract graph features: True\n",
      "INFO:src.data.dataset_graph:Initializing graph feature extractor...\n",
      "2025-06-08 22:06:00,147 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.utils.graph_features:GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Number of EEG channels: 19\n",
      "INFO:src.data.dataset_graph:Setting up signal filters...\n",
      "INFO:src.data.dataset_graph:Loading spatial distances from data/distances_3d.csv\n",
      "INFO:src.data.dataset_graph:Loading spatial distances from data/distances_3d.csv\n",
      "INFO:src.data.dataset_graph:Loaded 361 spatial distances in 0.01s\n",
      "INFO:src.data.dataset_graph:Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "Length of test_dataset: 3614\n",
      " Eliminated IDs:[]\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_corr_te = GraphEEGDataset(\n",
    "    root=test_dataset_correlation_dir,\n",
    "    clips=clips_te,\n",
    "    signal_folder=test_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"spatial\",\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file\n",
    "    ),\n",
    "    top_k=None,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    is_test=True, # NOTE: needed to let the dataset know that is okay to now have labels!\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None # collect all graph features\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of test_dataset: {len(dataset_corr_te)}\")\n",
    "print(f' Eliminated IDs:{dataset_corr_te.ids_to_eliminate}')\n",
    "clips_spatial_te = clips_te[~clips_te.index.isin(dataset_corr_te.ids_to_eliminate)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac9eebb",
   "metadata": {},
   "source": [
    "## Create + load correlation-based graph datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd45b398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.dataset_graph:Initializing GraphEEGDataset...\n",
      "INFO:src.data.dataset_graph:Dataset parameters:\n",
      "INFO:src.data.dataset_graph:  - Root directory: data/graph_dataset_correlation_train\n",
      "INFO:src.data.dataset_graph:  - Edge strategy: correlation\n",
      "INFO:src.data.dataset_graph:  - Top-k neighbors: 5\n",
      "INFO:src.data.dataset_graph:  - Correlation threshold: 0.7\n",
      "INFO:src.data.dataset_graph:  - Force reprocess: False\n",
      "INFO:src.data.dataset_graph:  - Bandpass frequencies: (0.5, 50)\n",
      "INFO:src.data.dataset_graph:  - Segment length: 3000\n",
      "INFO:src.data.dataset_graph:  - Apply filtering: True\n",
      "INFO:src.data.dataset_graph:  - Apply rereferencing: True\n",
      "INFO:src.data.dataset_graph:  - Apply normalization: True\n",
      "INFO:src.data.dataset_graph:  - Sampling rate: 250\n",
      "INFO:src.data.dataset_graph:  - Test mode: False\n",
      "INFO:src.data.dataset_graph:  - Extract graph features: True\n",
      "INFO:src.data.dataset_graph:Initializing graph feature extractor...\n",
      "2025-06-08 22:06:00,231 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.utils.graph_features:GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Number of EEG channels: 19\n",
      "INFO:src.data.dataset_graph:Setting up signal filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "Length of train_dataset: 12986\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# dataset settings\n",
    "top_k = 5\n",
    "low_bandpass_frequency = 0.5\n",
    "high_bandpass_frequency = 50\n",
    "\n",
    "# additional settings\n",
    "oversampling_power = 1.0\n",
    "\n",
    "# load training dataset\n",
    "dataset_corr_tr = GraphEEGDataset(\n",
    "    root=train_dataset_correlation_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"correlation\",\n",
    "    spatial_distance_file=None,\n",
    "    top_k=top_k,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None # collect all graph features\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of train_dataset: {len(dataset_corr_tr)}\")\n",
    "print(f' Eliminated IDs: {dataset_corr_tr.ids_to_eliminate}')\n",
    "clips_corr_tr = clips_tr[~clips_tr.index.isin(dataset_corr_tr.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0198413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.dataset_graph:Initializing GraphEEGDataset...\n",
      "INFO:src.data.dataset_graph:Dataset parameters:\n",
      "INFO:src.data.dataset_graph:  - Root directory: data/graph_dataset_correlation_test\n",
      "INFO:src.data.dataset_graph:  - Edge strategy: correlation\n",
      "INFO:src.data.dataset_graph:  - Top-k neighbors: 5\n",
      "INFO:src.data.dataset_graph:  - Correlation threshold: 0.7\n",
      "INFO:src.data.dataset_graph:  - Force reprocess: False\n",
      "INFO:src.data.dataset_graph:  - Bandpass frequencies: (0.5, 50)\n",
      "INFO:src.data.dataset_graph:  - Segment length: 3000\n",
      "INFO:src.data.dataset_graph:  - Apply filtering: True\n",
      "INFO:src.data.dataset_graph:  - Apply rereferencing: True\n",
      "INFO:src.data.dataset_graph:  - Apply normalization: True\n",
      "INFO:src.data.dataset_graph:  - Sampling rate: 250\n",
      "INFO:src.data.dataset_graph:  - Test mode: True\n",
      "INFO:src.data.dataset_graph:  - Extract graph features: True\n",
      "INFO:src.data.dataset_graph:Initializing graph feature extractor...\n",
      "2025-06-08 22:06:00,312 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.utils.graph_features:GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Number of EEG channels: 19\n",
      "INFO:src.data.dataset_graph:Setting up signal filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "Length of test_dataset: 3614\n",
      " Eliminated IDs:[]\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_corr_te = GraphEEGDataset(\n",
    "    root=test_dataset_correlation_dir,\n",
    "    clips=clips_te,\n",
    "    signal_folder=test_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"correlation\",\n",
    "    spatial_distance_file=None,\n",
    "    top_k=top_k,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    is_test=True, # NOTE: needed to let the dataset know that is okay to now have labels!\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None # collect all graph features\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of test_dataset: {len(dataset_corr_te)}\")\n",
    "print(f' Eliminated IDs:{dataset_corr_te.ids_to_eliminate}')\n",
    "clips_corr_te = clips_te[~clips_te.index.isin(dataset_corr_te.ids_to_eliminate)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0db2758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the original clips from memory\n",
    "del clips_tr, clips_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05d80bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "=== SPATIAL DATASET SPLITTING ===\n",
      "Spatial dataset - Total: 12993, Train: 10394, Val: 2599\n",
      "Spatial dataset labels distribution before split:\n",
      "[22:06:00] Train labels: 0 -> 8375, 1 -> 2019\n",
      "[22:06:00] Val labels:   0 -> 2101, 1 -> 498\n",
      "\n",
      "Spatial dataset - Class weights: [0.0001194  0.00049529]\n",
      "Spatial dataset - Class distribution in train: [8375 2019]\n",
      "\n",
      "=== CORRELATION DATASET SPLITTING ===\n",
      "Correlation dataset - Total: 12986, Train: 10388, Val: 2598\n",
      "Correlation dataset labels distribution before split:\n",
      "[22:06:00] Train labels: 0 -> 8401, 1 -> 1987\n",
      "[22:06:00] Val labels:   0 -> 2074, 1 -> 524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "INFO:src.data.geodataloader:Found 22 graph-level features: ['graph_avg_degree', 'graph_degree_std', 'graph_transitivity', 'graph_max_degree', 'graph_radius', 'graph_diameter', 'graph_spectral_radius', 'graph_clustering', 'graph_local_efficiency', 'graph_num_nodes', 'graph_modularity', 'graph_is_connected', 'graph_degeneracy', 'graph_avg_path_length', 'graph_assortativity', 'graph_num_communities', 'graph_num_edges', 'graph_global_efficiency', 'graph_algebraic_connectivity', 'graph_min_degree', 'graph_density', 'graph_num_components']\n",
      "INFO:src.data.geodataloader:Found 22 graph-level features: ['graph_avg_degree', 'graph_degree_std', 'graph_transitivity', 'graph_max_degree', 'graph_radius', 'graph_diameter', 'graph_spectral_radius', 'graph_clustering', 'graph_local_efficiency', 'graph_num_nodes', 'graph_modularity', 'graph_is_connected', 'graph_degeneracy', 'graph_avg_path_length', 'graph_assortativity', 'graph_num_communities', 'graph_num_edges', 'graph_global_efficiency', 'graph_algebraic_connectivity', 'graph_min_degree', 'graph_density', 'graph_num_components']\n",
      "INFO:src.data.geodataloader:Found 22 graph-level features: ['graph_avg_degree', 'graph_degree_std', 'graph_transitivity', 'graph_max_degree', 'graph_radius', 'graph_diameter', 'graph_spectral_radius', 'graph_clustering', 'graph_local_efficiency', 'graph_num_nodes', 'graph_modularity', 'graph_is_connected', 'graph_degeneracy', 'graph_avg_path_length', 'graph_assortativity', 'graph_num_communities', 'graph_num_edges', 'graph_global_efficiency', 'graph_algebraic_connectivity', 'graph_min_degree', 'graph_density', 'graph_num_components']\n",
      "INFO:src.data.geodataloader:Found 22 graph-level features: ['graph_avg_degree', 'graph_degree_std', 'graph_transitivity', 'graph_max_degree', 'graph_radius', 'graph_diameter', 'graph_spectral_radius', 'graph_clustering', 'graph_local_efficiency', 'graph_num_nodes', 'graph_modularity', 'graph_is_connected', 'graph_degeneracy', 'graph_avg_path_length', 'graph_assortativity', 'graph_num_communities', 'graph_num_edges', 'graph_global_efficiency', 'graph_algebraic_connectivity', 'graph_min_degree', 'graph_density', 'graph_num_components']\n",
      "INFO:src.data.geodataloader:Found 22 graph-level features: ['graph_avg_degree', 'graph_degree_std', 'graph_transitivity', 'graph_max_degree', 'graph_radius', 'graph_diameter', 'graph_spectral_radius', 'graph_clustering', 'graph_local_efficiency', 'graph_num_nodes', 'graph_modularity', 'graph_is_connected', 'graph_degeneracy', 'graph_avg_path_length', 'graph_assortativity', 'graph_num_communities', 'graph_num_edges', 'graph_global_efficiency', 'graph_algebraic_connectivity', 'graph_min_degree', 'graph_density', 'graph_num_components']\n",
      "INFO:src.data.geodataloader:Found 22 graph-level features: ['graph_avg_degree', 'graph_degree_std', 'graph_transitivity', 'graph_max_degree', 'graph_radius', 'graph_diameter', 'graph_spectral_radius', 'graph_clustering', 'graph_local_efficiency', 'graph_num_nodes', 'graph_modularity', 'graph_is_connected', 'graph_degeneracy', 'graph_avg_path_length', 'graph_assortativity', 'graph_num_communities', 'graph_num_edges', 'graph_global_efficiency', 'graph_algebraic_connectivity', 'graph_min_degree', 'graph_density', 'graph_num_components']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correlation dataset - Class weights: [0.00011903 0.00050327]\n",
      "Correlation dataset - Class distribution in train: [8401 1987]\n",
      "\n",
      "=== SUMMARY ===\n",
      "Spatial: 10394 train, 2599 val\n",
      "Correlation: 10388 train, 2598 val\n",
      "\n",
      "=== DATA LOADERS CREATED ===\n",
      "Spatial - Train: 162 batches, Val: 41 batches, Test: 57 batches\n",
      "Correlation - Train: 162 batches, Val: 41 batches, Test: 203 batches\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "import numpy as np\n",
    "from src.utils.general_funcs import labels_stats\n",
    "\n",
    "# Split settings\n",
    "TRAIN_RATIO = 0.8\n",
    "oversampling_power = 1.0\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(\"=== SPATIAL DATASET SPLITTING ===\")\n",
    "# Get total samples and split sizes for spatial dataset\n",
    "total_samples_spatial = len(dataset_spatial_tr)\n",
    "train_size_spatial = int(TRAIN_RATIO * total_samples_spatial)\n",
    "val_size_spatial = total_samples_spatial - train_size_spatial\n",
    "\n",
    "print(f\"Spatial dataset - Total: {total_samples_spatial}, Train: {train_size_spatial}, Val: {val_size_spatial}\")\n",
    "\n",
    "# Get labels for spatial dataset split\n",
    "y_spatial = clips_spatial_tr[\"label\"].values\n",
    "\n",
    "# Create initial train/val split using random permutation\n",
    "indices_spatial = torch.randperm(total_samples_spatial)\n",
    "train_indices_spatial = indices_spatial[:train_size_spatial].numpy()\n",
    "val_indices_spatial = indices_spatial[train_size_spatial:].numpy()\n",
    "\n",
    "print('Spatial dataset labels distribution before split:')\n",
    "labels_stats(y_spatial, train_indices_spatial, val_indices_spatial)\n",
    "\n",
    "# Create train and val datasets for spatial\n",
    "train_dataset_spatial = Subset(dataset_spatial_tr, train_indices_spatial)\n",
    "val_dataset_spatial = Subset(dataset_spatial_tr, val_indices_spatial)\n",
    "\n",
    "# Compute sample weights for oversampling - spatial\n",
    "train_labels_spatial = [clips_spatial_tr.iloc[i][\"label\"] for i in train_indices_spatial]\n",
    "class_counts_spatial = np.bincount(train_labels_spatial)\n",
    "class_weights_spatial = (1. / class_counts_spatial) ** oversampling_power\n",
    "sample_weights_spatial = [class_weights_spatial[label] for label in train_labels_spatial]\n",
    "\n",
    "# Define sampler for spatial\n",
    "sampler_spatial = WeightedRandomSampler(sample_weights_spatial, num_samples=len(sample_weights_spatial), replacement=True)\n",
    "\n",
    "print(f\"\\nSpatial dataset - Class weights: {class_weights_spatial}\")\n",
    "print(f\"Spatial dataset - Class distribution in train: {np.bincount(train_labels_spatial)}\")\n",
    "\n",
    "print(\"\\n=== CORRELATION DATASET SPLITTING ===\")\n",
    "\n",
    "# Get total samples and split sizes for correlation dataset\n",
    "total_samples_corr = len(dataset_corr_tr)\n",
    "train_size_corr = int(TRAIN_RATIO * total_samples_corr)\n",
    "val_size_corr = total_samples_corr - train_size_corr\n",
    "\n",
    "print(f\"Correlation dataset - Total: {total_samples_corr}, Train: {train_size_corr}, Val: {val_size_corr}\")\n",
    "\n",
    "# Get labels for correlation dataset split (should be same as spatial, but let's be explicit)\n",
    "y_corr = clips_corr_tr[\"label\"].values\n",
    "\n",
    "# Create initial train/val split using random permutation\n",
    "indices_corr = torch.randperm(total_samples_corr)\n",
    "train_indices_corr = indices_corr[:train_size_corr].numpy()\n",
    "val_indices_corr = indices_corr[train_size_corr:].numpy()\n",
    "\n",
    "print('Correlation dataset labels distribution before split:')\n",
    "labels_stats(y_corr, train_indices_corr, val_indices_corr)\n",
    "\n",
    "# Create train and val datasets for correlation\n",
    "train_dataset_corr = Subset(dataset_corr_tr, train_indices_corr)\n",
    "val_dataset_corr = Subset(dataset_corr_tr, val_indices_corr)\n",
    "\n",
    "# Compute sample weights for oversampling - correlation\n",
    "train_labels_corr = [clips_corr_tr.iloc[i][\"label\"] for i in train_indices_corr]\n",
    "class_counts_corr = np.bincount(train_labels_corr)\n",
    "class_weights_corr = (1. / class_counts_corr) ** oversampling_power\n",
    "sample_weights_corr = [class_weights_corr[label] for label in train_labels_corr]\n",
    "\n",
    "# Define sampler for correlation\n",
    "sampler_corr = WeightedRandomSampler(sample_weights_corr, num_samples=len(sample_weights_corr), replacement=True)\n",
    "\n",
    "print(f\"\\nCorrelation dataset - Class weights: {class_weights_corr}\")\n",
    "print(f\"Correlation dataset - Class distribution in train: {np.bincount(train_labels_corr)}\")\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"Spatial: {len(train_dataset_spatial)} train, {len(val_dataset_spatial)} val\")\n",
    "print(f\"Correlation: {len(train_dataset_corr)} train, {len(val_dataset_corr)} val\")\n",
    "\n",
    "# Create GeoDataLoaders for spatial dataset\n",
    "train_loader_spatial = GeoDataLoader(\n",
    "    train_dataset_spatial,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler_spatial,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader_spatial = GeoDataLoader(\n",
    "    val_dataset_spatial,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "te_loader_spatial = GeoDataLoader(\n",
    "    dataset_corr_te, # Use full spatial test dataset\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# Create GeoDataLoaders for correlation dataset\n",
    "train_loader_corr = GeoDataLoader(\n",
    "    train_dataset_corr,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler_corr,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader_corr = GeoDataLoader(\n",
    "    val_dataset_corr,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "te_loader_corr = GeoDataLoader(\n",
    "    dataset_corr_tr,  # Use full correlation test dataset\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print(\"\\n=== DATA LOADERS CREATED ===\")\n",
    "print(f\"Spatial - Train: {len(train_loader_spatial)} batches, Val: {len(val_loader_spatial)} batches, Test: {len(te_loader_spatial)} batches\")\n",
    "print(f\"Correlation - Train: {len(train_loader_corr)} batches, Val: {len(val_loader_corr)} batches, Test: {len(te_loader_corr)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6943f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Selected CORRELATION dataset for training\n",
      "âœ… Using correlation dataset:\n",
      "   Train batches: 162\n",
      "   Val batches: 41\n",
      "   Test batches: 203\n",
      "   Total samples in dataset: 12986\n",
      "   First batch - Nodes: torch.Size([1216, 3000]), Edges: torch.Size([2, 12160])\n",
      "   First batch - Labels: torch.Size([64]), Batch size: 64\n",
      "\n",
      "ðŸš€ Ready to train with correlation dataset!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ==============================================================================\n",
    "# DATASET SELECTION FOR TRAINING\n",
    "# ==============================================================================\n",
    "# Choose which dataset type to use for training:\n",
    "# - 'spatial': Uses spatial distance-based graph connections\n",
    "# - 'correlation': Uses correlation-based graph connections\n",
    "\n",
    "DATASET_TYPE = 'correlation'\n",
    "\n",
    "if DATASET_TYPE == 'spatial':\n",
    "    print(\"ðŸŒ Selected SPATIAL dataset for training\")\n",
    "    train_loader = train_loader_spatial\n",
    "    val_loader = val_loader_spatial\n",
    "    te_loader = te_loader_spatial\n",
    "    current_dataset = dataset_spatial_tr\n",
    "elif DATASET_TYPE == 'correlation':\n",
    "    print(\"ðŸ”— Selected CORRELATION dataset for training\")\n",
    "    train_loader = train_loader_corr\n",
    "    val_loader = val_loader_corr\n",
    "    te_loader = te_loader_corr\n",
    "    current_dataset = dataset_corr_tr\n",
    "else:\n",
    "    raise ValueError(f\"Unknown dataset type: {DATASET_TYPE}. Choose 'spatial' or 'correlation'\")\n",
    "\n",
    "print(f\"âœ… Using {DATASET_TYPE} dataset:\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches: {len(te_loader)}\")\n",
    "print(f\"   Total samples in dataset: {len(current_dataset)}\")\n",
    "\n",
    "# Optional: Print first batch info to verify data loading\n",
    "try:\n",
    "    first_batch = next(iter(train_loader))\n",
    "    print(f\"   First batch - Nodes: {first_batch.x.shape}, Edges: {first_batch.edge_index.shape}\")\n",
    "    print(f\"   First batch - Labels: {first_batch.y.shape}, Batch size: {first_batch.num_graphs}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Could not inspect first batch: {e}\")\n",
    "\n",
    "print(f\"\\nðŸš€ Ready to train with {DATASET_TYPE} dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e073d9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[19, 3000], edge_index=[2, 342], y=[1], id='pqejgcff_s001_t000_0', graph_num_nodes=[1], graph_num_edges=[1], graph_density=[1], graph_is_connected=[1], graph_num_components=[1], graph_clustering=[1], graph_transitivity=[1], graph_avg_path_length=[1], graph_diameter=[1], graph_radius=[1], graph_global_efficiency=[1], graph_local_efficiency=[1], graph_avg_degree=[1], graph_degree_std=[1], graph_max_degree=[1], graph_min_degree=[1], graph_assortativity=[1], graph_num_communities=[1], graph_modularity=[1], graph_algebraic_connectivity=[1], graph_spectral_radius=[1], graph_degeneracy=[1])\n",
      "Graph feature names: ['graph_avg_degree', 'graph_degree_std', 'graph_transitivity', 'graph_max_degree', 'graph_radius', 'graph_diameter', 'graph_spectral_radius', 'graph_clustering', 'graph_local_efficiency', 'graph_num_nodes', 'graph_modularity', 'graph_is_connected', 'graph_degeneracy', 'graph_avg_path_length', 'graph_assortativity', 'graph_num_communities', 'graph_num_edges', 'graph_global_efficiency', 'graph_algebraic_connectivity', 'graph_min_degree', 'graph_density', 'graph_num_components']\n",
      "Graph feature dimension: 22\n"
     ]
    }
   ],
   "source": [
    "sample = current_dataset[0]\n",
    "print(sample)\n",
    "graph_feature_names = [attr for attr in sample.keys() if attr.startswith('graph_')] # type: ignore\n",
    "graph_feature_dim = len(graph_feature_names)\n",
    "\n",
    "print(f\"Graph feature names: {graph_feature_names}\")\n",
    "print(f\"Graph feature dimension: {graph_feature_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67042055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from src.layers.hybrid.cnn_bilstm_gcn import EEGCNNBiLSTMGCN\n",
    "from src.utils.train import train_model\n",
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"patience\": 10,\n",
    "    \"epochs\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60bc6d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_traditional_train(model, save_path):\n",
    "    model = model.to(device)\n",
    "\n",
    "    # if multiple GPUs are available, use DataParallel\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for training\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5)\n",
    "    loss = nn.BCEWithLogitsLoss()  # Not weighted as we use a balanced sampler!\n",
    "\n",
    "    # train model\n",
    "    train_history, val_history = train_model(\n",
    "        wandb_config=None,\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=loss,\n",
    "        scheduler=scheduler,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=config[\"epochs\"],\n",
    "        patience=config[\"patience\"],\n",
    "        save_path=save_path,\n",
    "        use_gnn=False,\n",
    "        # hidden attribute\n",
    "        try_load_checkpoint=True,\n",
    "    )\n",
    "    plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "177fedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.utils.train\n",
    "def wrap_gnn_train(model, save_path):\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "    # optimizer = Lion(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5)\n",
    "    loss = nn.BCEWithLogitsLoss()  # Not weighted as we use a balanced sampler!\n",
    "\n",
    "    # train model\n",
    "    train_history, val_history = train_model(\n",
    "        wandb_config=None,\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=loss,\n",
    "        scheduler=scheduler,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=config[\"epochs\"],\n",
    "        patience=config[\"patience\"],\n",
    "        save_path=save_path,\n",
    "        use_gnn=True,\n",
    "        # hidden attribute\n",
    "        try_load_checkpoint=True,\n",
    "    )\n",
    "    plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720925b0",
   "metadata": {},
   "source": [
    "### Test 3 - First breakthrough model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "036835f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm .checkpoints/cnn_bilstm_gcn_test_3_correlation_test_mean_pooling.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b640647a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.layers.hybrid.cnn_bilstm_gcn:EEGCNNBiLSTMGCN initialized:\n",
      "INFO:src.layers.hybrid.cnn_bilstm_gcn:  - Node input dim: 3000\n",
      "INFO:src.layers.hybrid.cnn_bilstm_gcn:  - Node feature dim (LSTM output): 128\n",
      "INFO:src.layers.hybrid.cnn_bilstm_gcn:  - GCN hidden dim: 128\n",
      "INFO:src.layers.hybrid.cnn_bilstm_gcn:  - Graph feature dim: 22\n",
      "INFO:src.layers.hybrid.cnn_bilstm_gcn:  - Use graph features: True\n",
      "INFO:src.layers.hybrid.cnn_bilstm_gcn:  - Classifier input dim: 118\n",
      "INFO:src.layers.hybrid.cnn_bilstm_gcn:  - Num classes: 1\n",
      "INFO:src.layers.hybrid.cnn_bilstm_gcn:  - Num channels: 19\n",
      "INFO:src.utils.train:Starting training setup...\n",
      "INFO:src.utils.train:Model type: GNN\n",
      "INFO:src.utils.train:Device: cuda\n",
      "INFO:src.utils.train:Batch size: 64\n",
      "INFO:src.utils.train:Number of epochs: 100\n",
      "INFO:src.utils.train:Patience: 10\n",
      "INFO:src.utils.train:Monitor metric: val_f1\n",
      "INFO:src.utils.train:Initializing wandb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "src.utils.train\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cnn_bilstm_gcn_test_3_correlation_test_mean_pooling</strong> at: <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/d88s4ddf' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/d88s4ddf</a><br> View project at: <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250608_222409-d88s4ddf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ldibello/NeuroGraphNet/wandb/run-20250608_222447-h6w6sgjw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/h6w6sgjw' target=\"_blank\">cnn_bilstm_gcn_test_3_correlation_test_mean_pooling</a></strong> to <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/h6w6sgjw' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/h6w6sgjw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:ðŸ”— Wandb run initialized: cnn_bilstm_gcn_test_3_correlation_test_mean_pooling\n",
      "INFO:src.utils.train:Total training batches per epoch: 162\n",
      "INFO:src.utils.train:Starting training from epoch 1 to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Wandb initialized: cnn_bilstm_gcn_test_3_correlation_test_mean_pooling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]INFO:src.utils.train:\n",
      "Epoch 1/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 12160]), y: torch.Size([64, 1])\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.9115 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.8219 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.8630 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.7090 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.7327 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.8328 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.8000 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.6962 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.7779 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.7561 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.6470 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.6836 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.6083 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.6141 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.6266 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.6017 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.6982 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:\n",
      "Epoch 1 training completed in 237.62s\n",
      "INFO:src.utils.train:Average training loss: 0.7480\n",
      "Epochs:   2%| | 2/100 [05:12<8:30:44, 312.69s/it, train_loss=0.7480, val_loss=0.4761, best_val_f1=0.0650, lr=1.00e-04, bINFO:src.utils.train:\n",
      "Epoch 2/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 12160]), y: torch.Size([64, 1])\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.6268 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.6953 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.6453 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.6337 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.5894 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.5491 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.5932 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.6552 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.5188 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.5494 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.6377 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.6176 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.6237 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.6571 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.4599 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.5558 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.4649 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:\n",
      "Epoch 2 training completed in 188.87s\n",
      "INFO:src.utils.train:Average training loss: 0.5880\n",
      "Epochs:   3%| | 3/100 [08:54<6:58:39, 258.97s/it, train_loss=0.5880, val_loss=0.4879, best_val_f1=0.0650, lr=1.00e-04, bINFO:src.utils.train:\n",
      "Epoch 3/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 12160]), y: torch.Size([64, 1])\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.4303 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.6542 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.5593 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.4765 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.5249 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.4058 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.5254 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.6066 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.4713 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.6344 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.4643 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.6031 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.5074 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.6010 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.5845 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.4560 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.3924 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:\n",
      "Epoch 3 training completed in 170.51s\n",
      "INFO:src.utils.train:Average training loss: 0.5275\n",
      "Epochs:   4%| | 4/100 [12:16<6:13:02, 233.15s/it, train_loss=0.5275, val_loss=0.4196, best_val_f1=0.3223, lr=1.00e-04, bINFO:src.utils.train:\n",
      "Epoch 4/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 12160]), y: torch.Size([64, 1])\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.5353 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.5897 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.5461 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.5534 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.4294 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.6716 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.4454 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.4505 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.5318 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.5024 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.4675 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.5496 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.6096 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.5775 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.5755 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.5213 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.3909 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:\n",
      "Epoch 4 training completed in 162.62s\n",
      "INFO:src.utils.train:Average training loss: 0.5185\n",
      "Epochs:   5%| | 5/100 [15:31<5:45:15, 218.06s/it, train_loss=0.5185, val_loss=0.4321, best_val_f1=0.4358, lr=1.00e-04, bINFO:src.utils.train:\n",
      "Epoch 5/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 12160]), y: torch.Size([64, 1])\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.5886 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.6334 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.5172 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.5008 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.4949 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.5679 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.4330 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.4690 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.5087 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.4974 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.5106 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.4607 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.5203 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.5270 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.4594 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.4738 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.4378 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:\n",
      "Epoch 5 training completed in 157.72s\n",
      "INFO:src.utils.train:Average training loss: 0.5108\n",
      "Epochs:   6%| | 6/100 [18:42<5:26:24, 208.34s/it, train_loss=0.5108, val_loss=0.4308, best_val_f1=0.4358, lr=1.00e-04, bINFO:src.utils.train:\n",
      "Epoch 6/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 12160]), y: torch.Size([64, 1])\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.4474 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.4209 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.6162 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.4398 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.4730 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.4557 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.5555 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.5192 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.5106 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.4106 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.5850 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.3560 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.4471 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.4012 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.3464 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.4355 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.4887 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:\n",
      "Epoch 6 training completed in 155.15s\n",
      "INFO:src.utils.train:Average training loss: 0.4793\n",
      "Epochs:   7%| | 7/100 [21:48<5:11:24, 200.90s/it, train_loss=0.4793, val_loss=0.4866, best_val_f1=0.4358, lr=1.00e-04, bINFO:src.utils.train:\n",
      "Epoch 7/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 12160]), y: torch.Size([64, 1])\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.3742 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.5582 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.6442 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.5633 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.4858 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.5275 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.4645 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.4620 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.4586 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.4893 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.4632 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.4152 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.3842 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.6089 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.5934 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.4421 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.5617 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:\n",
      "Epoch 7 training completed in 153.99s\n",
      "INFO:src.utils.train:Average training loss: 0.4887\n",
      "Epochs:   8%| | 8/100 [24:54<5:00:24, 195.92s/it, train_loss=0.4887, val_loss=0.4882, best_val_f1=0.4358, lr=1.00e-04, bINFO:src.utils.train:\n",
      "Epoch 8/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 12160]), y: torch.Size([64, 1])\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.4889 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.3938 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.6511 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.5369 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.4786 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.5162 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.4528 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.4906 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.3885 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.4503 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.4495 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.3672 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.6558 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.4582 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.4728 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.4351 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.5854 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:\n",
      "Epoch 8 training completed in 152.92s\n",
      "INFO:src.utils.train:Average training loss: 0.4785\n",
      "Epochs:   9%| | 9/100 [27:59<4:51:44, 192.35s/it, train_loss=0.4785, val_loss=0.4713, best_val_f1=0.4358, lr=5.00e-05, bINFO:src.utils.train:\n",
      "Epoch 9/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 12160]), y: torch.Size([64, 1])\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.4953 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.4138 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.5215 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.3661 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.4066 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.4617 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.4143 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.3227 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.4260 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.4859 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.6150 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.4251 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.5197 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.3427 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.4947 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.3981 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.4277 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:\n",
      "Epoch 9 training completed in 152.59s\n",
      "INFO:src.utils.train:Average training loss: 0.4517\n",
      "Epochs:  10%| | 10/100 [31:04<4:45:10, 190.11s/it, train_loss=0.4517, val_loss=0.4035, best_val_f1=0.4358, lr=5.00e-05, INFO:src.utils.train:\n",
      "Epoch 10/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 12160]), y: torch.Size([64, 1])\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.5076 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.4444 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.4596 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.3773 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.4590 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.4451 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.4666 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.4503 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.4214 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.4413 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.3684 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.3920 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.5445 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.4630 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.4136 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.4432 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.4641 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:\n",
      "Epoch 10 training completed in 154.72s\n",
      "INFO:src.utils.train:Average training loss: 0.4462\n",
      "Epochs:  11%| | 11/100 [34:11<4:40:31, 189.11s/it, train_loss=0.4462, val_loss=0.4383, best_val_f1=0.4770, lr=5.00e-05, INFO:src.utils.train:\n",
      "Epoch 11/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 12160]), y: torch.Size([64, 1])\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.4360 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.4049 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.4036 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.4284 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.4771 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.4167 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.3809 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.3754 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.4888 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.4435 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.4326 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.4797 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.4490 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.4361 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.5475 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.4496 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.4779 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:\n",
      "Epoch 11 training completed in 154.21s\n",
      "INFO:src.utils.train:Average training loss: 0.4484\n",
      "Epochs:  12%| | 12/100 [37:17<4:36:03, 188.23s/it, train_loss=0.4484, val_loss=0.4440, best_val_f1=0.4770, lr=5.00e-05, INFO:src.utils.train:\n",
      "Epoch 12/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 12160]), y: torch.Size([64, 1])\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.3351 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.3680 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.4164 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.3496 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.4544 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.3363 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.4131 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.5298 - Avg batch time: 0.26s\n",
      "Epochs:  12%| | 12/100 [38:28<5:07:48, 209.87s/it, train_loss=0.4484, val_loss=0.4440, best_val_f1=0.4770, lr=5.00e-05, \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 24\u001b[0m\n\u001b[1;32m      2\u001b[0m SAVE_PATH \u001b[38;5;241m=\u001b[39m CHECKPOINT_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn_bilstm_gcn_test_3_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_TYPE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_test_mean_pooling.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m EEGCNNBiLSTMGCN(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     cnn_dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.25\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     use_graph_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m DATASET_TYPE \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspatial\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m \u001b[43mwrap_gnn_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAVE_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 14\u001b[0m, in \u001b[0;36mwrap_gnn_train\u001b[0;34m(model, save_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()  \u001b[38;5;66;03m# Not weighted as we use a balanced sampler!\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hidden attribute\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_load_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m plot_training_loss(train_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], val_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:391\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, wandb_config, wandb_project, wandb_run_name, log_wandb, try_load_checkpoint, original_dataset)\u001b[0m\n\u001b[1;32m    388\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training phase\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    389\u001b[0m batch_times \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, data_batch_item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader_to_iterate):\n\u001b[1;32m    392\u001b[0m     batch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Log every 10 batches\u001b[39;00m\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/data/geodataloader.py:40\u001b[0m, in \u001b[0;36mGeoDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;66;03m# Extract graph-level features and batch them\u001b[39;00m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_feature_names:\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;66;03m# Get individual data objects from the batch\u001b[39;00m\n\u001b[1;32m     44\u001b[0m             batch_data \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto_data_list()\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch_geometric/data/dataset.py:291\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03mpresent).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03mbool, will return a subset of the dataset at the specified indices.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(idx))):\n\u001b[0;32m--> 291\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[idx])\n\u001b[1;32m    292\u001b[0m     data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch_geometric/data/dataset.py:118\u001b[0m, in \u001b[0;36mDataset.indices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mindices\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/data/dataset_graph.py:613\u001b[0m, in \u001b[0;36mGraphEEGDataset.len\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    610\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    Returns the number of examples in the dataset (number of graphs saved)\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 613\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_file_names\u001b[49m)\n\u001b[1;32m    614\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlength\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m length\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/data/dataset_graph.py:647\u001b[0m, in \u001b[0;36mGraphEEGDataset.processed_file_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03mReturns the names of all processed files.\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# return [f\"data_{i}.pt\" for i in range(len(self.clips))]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[0;32m--> 647\u001b[0m     [\n\u001b[1;32m    648\u001b[0m         f\n\u001b[1;32m    649\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir)\n\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Guard against other files\u001b[39;00m\n\u001b[1;32m    652\u001b[0m     ]\n\u001b[1;32m    653\u001b[0m )\n\u001b[1;32m    654\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processed files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m files\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/data/dataset_graph.py:651\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03mReturns the names of all processed files.\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# return [f\"data_{i}.pt\" for i in range(len(self.clips))]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m    647\u001b[0m     [\n\u001b[1;32m    648\u001b[0m         f\n\u001b[1;32m    649\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir)\n\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 651\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Guard against other files\u001b[39;00m\n\u001b[1;32m    652\u001b[0m     ]\n\u001b[1;32m    653\u001b[0m )\n\u001b[1;32m    654\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processed files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m files\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_test_3_{DATASET_TYPE}_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim = 128,\n",
    "    out_channels = 96,\n",
    "    pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_conv_layers = 3,\n",
    "    gcn_dropout = 0.5,\n",
    "    num_channels = 19,\n",
    "    # enable graph features\n",
    "    graph_feature_dim=graph_feature_dim,\n",
    "    # NOTE: using graph level features gives worse results with spatial dataset\n",
    "    use_graph_features=False if DATASET_TYPE == 'spatial' else True,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11afb6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_test_3_{DATASET_TYPE}_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout=0.25,\n",
    "    lstm_hidden_dim=128,\n",
    "    lstm_out_dim=128,\n",
    "    lstm_dropout=0.25,\n",
    "    encoder_use_batch_norm=True,\n",
    "    encoder_use_layer_norm=False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim=128,\n",
    "    out_channels=128,\n",
    "    pooling_type=\"mean\",\n",
    "    gcn_use_batch_norm=True,\n",
    "    num_conv_layers=3,\n",
    "    gcn_dropout=0.5,\n",
    "    num_channels=19,\n",
    "    use_graph_features=True\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424f5b8b",
   "metadata": {},
   "source": [
    "### Test 4 - Smaller CGN output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_4.pt\"\n",
    "\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 64,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 3,\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eaaf2d",
   "metadata": {},
   "source": [
    "### Test 5 - Smaller GCN output channels + increased embedding length + Deeper GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1335c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_5.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 64,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da60626",
   "metadata": {},
   "source": [
    "\n",
    "### Test 6: slighly bigger GCN output channels\n",
    ">[HIGHEST F1 SCORE EVER RECORDED]\n",
    "```\n",
    "âœ… Checkpoint loaded. Resuming from epoch 33. Best 'val_f1' score: 0.7346\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_6.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 96,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad429a",
   "metadata": {},
   "source": [
    "### Test 7B: Alternative architecture to improve generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f882bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_8.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.35, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.35, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 96,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b4f3c",
   "metadata": {},
   "source": [
    "### Test 7C: slightly bigger GCN layers\n",
    "\n",
    "BEST MODEL YET!\n",
    "\n",
    "(SPATIAL FEATURES!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6c3ccd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.layers.hybrid.cnn_bilstm_gcn:EEGCNNBiLSTMGCN initialized:\n",
      "INFO:src.layers.hybrid.cnn_bilstm_gcn:  - Node input dim: 3000\n",
      "INFO:src.layers.hybrid.cnn_bilstm_gcn:  - Node feature dim (LSTM output): 128\n",
      "INFO:src.layers.hybrid.cnn_bilstm_gcn:  - GCN hidden dim: 192\n",
      "INFO:src.layers.hybrid.cnn_bilstm_gcn:  - Graph feature dim: 22\n",
      "INFO:src.layers.hybrid.cnn_bilstm_gcn:  - Use graph features: True\n",
      "INFO:src.layers.hybrid.cnn_bilstm_gcn:  - Classifier input dim: 150\n",
      "INFO:src.layers.hybrid.cnn_bilstm_gcn:  - Num classes: 1\n",
      "INFO:src.layers.hybrid.cnn_bilstm_gcn:  - Num channels: 19\n",
      "INFO:src.utils.train:Starting training setup...\n",
      "INFO:src.utils.train:Model type: GNN\n",
      "INFO:src.utils.train:Device: cuda\n",
      "INFO:src.utils.train:Batch size: 64\n",
      "INFO:src.utils.train:Number of epochs: 100\n",
      "INFO:src.utils.train:Patience: 10\n",
      "INFO:src.utils.train:Monitor metric: val_f1\n",
      "INFO:src.utils.train:Initializing wandb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "src.utils.train\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cnn_bilstm_gcn_test_best_model_correlation_test_mean_pooling</strong> at: <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/ghun2uny' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/ghun2uny</a><br> View project at: <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250608_230505-ghun2uny/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ldibello/NeuroGraphNet/wandb/run-20250608_230637-baqdp6n2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/baqdp6n2' target=\"_blank\">cnn_bilstm_gcn_test_best_model_correlation_test_mean_pooling</a></strong> to <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/baqdp6n2' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/baqdp6n2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:ðŸ”— Wandb run initialized: cnn_bilstm_gcn_test_best_model_correlation_test_mean_pooling\n",
      "INFO:src.utils.train:Total training batches per epoch: 162\n",
      "INFO:src.utils.train:Starting training from epoch 1 to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Wandb initialized: cnn_bilstm_gcn_test_best_model_correlation_test_mean_pooling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]INFO:src.utils.train:\n",
      "Epoch 1/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 12160]), y: torch.Size([64, 1])\n",
      "/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.7665 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.8037 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.7801 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.7762 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.7405 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.6549 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.7210 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.5900 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.6736 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.6067 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.5608 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.6711 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.6453 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.5756 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.6364 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.6690 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.5991 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:\n",
      "Epoch 1 training completed in 152.78s\n",
      "INFO:src.utils.train:Average training loss: 0.7051\n",
      "Epochs:   2%| | 2/100 [03:05<5:02:20, 185.11s/it, train_loss=0.7051, val_loss=0.4741, best_val_f1=0.0373, lr=1.00e-04, bINFO:src.utils.train:\n",
      "Epoch 2/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 12160]), y: torch.Size([64, 1])\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.6525 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.6612 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.6410 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.5714 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.7399 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.6665 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.6181 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.5870 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.5980 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.5870 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.5879 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.6109 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.6683 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.4644 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.5125 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.5443 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.4954 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:\n",
      "Epoch 2 training completed in 153.51s\n",
      "INFO:src.utils.train:Average training loss: 0.6034\n",
      "Epochs:   3%| | 3/100 [06:10<4:59:29, 185.25s/it, train_loss=0.6034, val_loss=0.4780, best_val_f1=0.0373, lr=1.00e-04, bINFO:src.utils.train:\n",
      "Epoch 3/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 12160]), y: torch.Size([64, 1])\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.4704 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.4627 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.5334 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.6084 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.5506 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.4929 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.6026 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.5400 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.6339 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.5481 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.4702 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.6649 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.4535 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.5933 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.5482 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.5739 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.5152 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:\n",
      "Epoch 3 training completed in 155.91s\n",
      "INFO:src.utils.train:Average training loss: 0.5336\n",
      "Epochs:   4%| | 4/100 [09:18<4:58:07, 186.33s/it, train_loss=0.5336, val_loss=0.5062, best_val_f1=0.0373, lr=1.00e-04, bINFO:src.utils.train:\n",
      "Epoch 4/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 12160]), y: torch.Size([64, 1])\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.5638 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.5945 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.4136 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.5319 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.4518 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.5146 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.5044 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.5499 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.4873 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.4355 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.4787 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.5309 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.6289 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.5058 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.6455 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.5849 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.5394 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:\n",
      "Epoch 4 training completed in 156.13s\n",
      "INFO:src.utils.train:Average training loss: 0.5225\n",
      "Epochs:   5%| | 5/100 [12:26<4:56:11, 187.07s/it, train_loss=0.5225, val_loss=0.4996, best_val_f1=0.0373, lr=1.00e-04, bINFO:src.utils.train:\n",
      "Epoch 5/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 12160]), y: torch.Size([64, 1])\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.4542 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.4103 - Avg batch time: 0.26s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.5120 - Avg batch time: 0.26s\n",
      "Epochs:   5%| | 5/100 [12:48<5:04:16, 192.18s/it, train_loss=0.5225, val_loss=0.4996, best_val_f1=0.0373, lr=1.00e-04, b\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 22\u001b[0m\n\u001b[1;32m      2\u001b[0m SAVE_PATH \u001b[38;5;241m=\u001b[39m CHECKPOINT_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn_bilstm_gcn_test_best_model_correlation_test_mean_pooling.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m EEGCNNBiLSTMGCN(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     cnn_dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;66;03m# slightly higher dropout to avoid overfitting\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     use_graph_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m DATASET_TYPE \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspatial\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m \u001b[43mwrap_gnn_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAVE_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 14\u001b[0m, in \u001b[0;36mwrap_gnn_train\u001b[0;34m(model, save_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()  \u001b[38;5;66;03m# Not weighted as we use a balanced sampler!\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hidden attribute\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_load_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m plot_training_loss(train_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], val_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:391\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, wandb_config, wandb_project, wandb_run_name, log_wandb, try_load_checkpoint, original_dataset)\u001b[0m\n\u001b[1;32m    388\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training phase\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    389\u001b[0m batch_times \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, data_batch_item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader_to_iterate):\n\u001b[1;32m    392\u001b[0m     batch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Log every 10 batches\u001b[39;00m\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/data/geodataloader.py:40\u001b[0m, in \u001b[0;36mGeoDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;66;03m# Extract graph-level features and batch them\u001b[39;00m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_feature_names:\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;66;03m# Get individual data objects from the batch\u001b[39;00m\n\u001b[1;32m     44\u001b[0m             batch_data \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto_data_list()\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch_geometric/data/dataset.py:291\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03mpresent).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03mbool, will return a subset of the dataset at the specified indices.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(idx))):\n\u001b[0;32m--> 291\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[idx])\n\u001b[1;32m    292\u001b[0m     data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch_geometric/data/dataset.py:118\u001b[0m, in \u001b[0;36mDataset.indices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mindices\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/data/dataset_graph.py:613\u001b[0m, in \u001b[0;36mGraphEEGDataset.len\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    610\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    Returns the number of examples in the dataset (number of graphs saved)\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 613\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_file_names\u001b[49m)\n\u001b[1;32m    614\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlength\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m length\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/data/dataset_graph.py:649\u001b[0m, in \u001b[0;36mGraphEEGDataset.processed_file_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03mReturns the names of all processed files.\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# return [f\"data_{i}.pt\" for i in range(len(self.clips))]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m    647\u001b[0m     [\n\u001b[1;32m    648\u001b[0m         f\n\u001b[0;32m--> 649\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Guard against other files\u001b[39;00m\n\u001b[1;32m    652\u001b[0m     ]\n\u001b[1;32m    653\u001b[0m )\n\u001b[1;32m    654\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processed files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m files\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_best_model_correlation_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm = True,\n",
    "    encoder_use_layer_norm = False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim = 192,\n",
    "    out_channels = 128,\n",
    "    pooling_type = \"max\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_conv_layers = 4,\n",
    "    gcn_dropout = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    "    graph_feature_dim=graph_feature_dim,\n",
    "    use_graph_features=False if DATASET_TYPE == 'spatial' else True,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6355457",
   "metadata": {},
   "source": [
    "### Test 7D: even bigger GCN layers\n",
    "\n",
    "Comparable performance to best model. We might need to increase the number of GCN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed9d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_even_bigger.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm = True,\n",
    "    encoder_use_layer_norm = False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 224,\n",
    "    gcn_out_channels = 192,\n",
    "    gcn_pooling_type = \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50956294",
   "metadata": {},
   "source": [
    "### Test 7E: increased number of GCN layers\n",
    "\n",
    "Assumption: the previous model was unable to learn enough, maybe the GCN was unable to capture\n",
    "\n",
    "```\n",
    "Epochs:   9%| | 9/100 [17:54<3:23:31, 134.20s/it, train_loss=0.4532, val_loss=0.3489, best_val_f1=0.6695, lr=5.00e-05, b2025-06-07 17:01:05 - INFO - \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_even_more_bigger.pt\"\n",
    "model_generalizable_even_more_bigger = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 224,\n",
    "    gcn_out_channels = 192,\n",
    "    gcn_num_layers = 5,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08442186",
   "metadata": {},
   "source": [
    "### Test 7F: Increased number of BiLSTM layers + Test 7E architecture\n",
    "\n",
    "Assumpion: we saw a drammatical increase in accuracy by increasing the number of GCN layers. This hints that the model was now able to learn the most from the embeddings. To improve the performance even further without having to increase the number of GCN layers even more (overall reduce complexity, improve generalization), we will try to increase the number of BiLSTM layers. \n",
    "\n",
    "Using multiple BiLSTM layers will allow embeddings to be processed in a more complex way, potentially capturing more intricate relationships in the data. The GCN layers will take care of the graph structure, while the BiLSTM layers will enhance the temporal dependencies and relationships in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b963a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_even_more_bigger.pt\"\n",
    "model_generalizable_even_more_bigger = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    lstm_num_layers = 2,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 224,\n",
    "    gcn_out_channels = 192,\n",
    "    gcn_num_layers = 5,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663332c",
   "metadata": {},
   "source": [
    "```\n",
    "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]2025-06-07 18:55:16 - INFO -\n",
    "Epochs:   2%| | 2/100 [04:35<7:29:19, 275.10s/it, train_loss=0.6212, val_loss=0.4619, best_val_f1=0.4055, lr=1.00e-04, b2025-06-07 18:59:51 - INFO -\n",
    "Epochs:   3%| | 3/100 [09:09<7:23:49, 274.53s/it, train_loss=0.5819, val_loss=0.4295, best_val_f1=0.4055, lr=1.00e-04, b2025-06-07 19:04:25 - INFO -\n",
    "Epochs:   4%| | 4/100 [13:42<7:18:31, 274.08s/it, train_loss=0.5628, val_loss=0.4437, best_val_f1=0.4055, lr=1.00e-04, b2025-06-07 19:08:59 - INFO -\n",
    "Epochs:   5%| | 5/100 [18:16<7:13:28, 273.78s/it, train_loss=0.5452, val_loss=0.3942, best_val_f1=0.4858, lr=1.00e-04, b2025-06-07 19:13:32 - INFO -\n",
    "Epochs:   6%| | 6/100 [22:49<7:08:41, 273.63s/it, train_loss=0.5334, val_loss=0.4563, best_val_f1=0.4858, lr=1.00e-04, b2025-06-07 19:18:05 - INFO -\n",
    "Epochs:   7%| | 7/100 [27:22<7:04:01, 273.57s/it, train_loss=0.5319, val_loss=0.3738, best_val_f1=0.5137, lr=1.00e-04, b2025-06-07 19:22:39 - INFO -\n",
    "Epochs:   8%| | 8/100 [31:56<6:59:20, 273.48s/it, train_loss=0.5181, val_loss=0.4369, best_val_f1=0.5695, lr=1.00e-04, b2025-06-07 19:27:12 - INFO -\n",
    "Epochs:   9%| | 9/100 [36:29<6:54:50, 273.52s/it, train_loss=0.5220, val_loss=0.4202, best_val_f1=0.5695, lr=1.00e-04, b2025-06-07 19:31:46 - INFO -\n",
    "Epochs:  10%| | 10/100 [41:03<6:50:17, 273.52s/it, train_loss=0.5286, val_loss=0.4167, best_val_f1=0.5695, lr=1.00e-04, 2025-06-07 19:36:19 - INFO -\n",
    "Epochs:  11%| | 11/100 [45:36<6:45:44, 273.53s/it, train_loss=0.5065, val_loss=0.3864, best_val_f1=0.5695, lr=1.00e-04, 2025-06-07 19:40:53 - INFO -\n",
    "Epochs:  12%| | 12/100 [50:10<6:41:03, 273.45s/it, train_loss=0.5158, val_loss=0.5175, best_val_f1=0.5695, lr=5.00e-05, 2025-06-07 19:45:26 - INFO -\n",
    "Epochs:  13%|â–| 13/100 [54:43<6:36:23, 273.37s/it, train_loss=0.5035, val_loss=0.3785, best_val_f1=0.5940, lr=5.00e-05, 2025-06-07 19:49:59 - INFO -\n",
    "Epochs:  14%|â–| 14/100 [59:16<6:31:50, 273.38s/it, train_loss=0.4842, val_loss=0.3838, best_val_f1=0.5981, lr=5.00e-05, 2025-06-07 19:54:33 - INFO -\n",
    "Epochs:  15%|â–| 15/100 [1:03:50<6:27:17, 273.38s/it, train_loss=0.4644, val_loss=0.3493, best_val_f1=0.6106, lr=5.00e-052025-06-07 19:59:06 - INFO -\n",
    "Epochs:  16%|â–| 16/100 [1:08:23<6:22:46, 273.41s/it, train_loss=0.4887, val_loss=0.3737, best_val_f1=0.6106, lr=5.00e-052025-06-07 20:03:39 - INFO -\n",
    "Epochs:  17%|â–| 17/100 [1:12:57<6:18:12, 273.41s/it, train_loss=0.4775, val_loss=0.3565, best_val_f1=0.6106, lr=5.00e-052025-06-07 20:08:13 - INFO -\n",
    "Epochs:  18%|â–| 18/100 [1:17:30<6:13:42, 273.44s/it, train_loss=0.4635, val_loss=0.3704, best_val_f1=0.6106, lr=2.50e-052025-06-07 20:12:46 - INFO -\n",
    "Epochs:  19%|â–| 19/100 [1:22:04<6:09:15, 273.53s/it, train_loss=0.4501, val_loss=0.3635, best_val_f1=0.6131, lr=2.50e-052025-06-07 20:17:20 - INFO -\n",
    "Epochs:  20%|â–| 20/100 [1:26:37<6:04:39, 273.49s/it, train_loss=0.4379, val_loss=0.3638, best_val_f1=0.6179, lr=2.50e-052025-06-07 20:21:53 - INFO -\n",
    "Epochs:  21%|â–| 21/100 [1:31:10<6:00:01, 273.43s/it, train_loss=0.4494, val_loss=0.3543, best_val_f1=0.6179, lr=2.50e-052025-06-07 20:26:27 - INFO -\n",
    "Epochs:  22%|â–| 22/100 [1:35:44<5:55:26, 273.42s/it, train_loss=0.4616, val_loss=0.3616, best_val_f1=0.6659, lr=2.50e-052025-06-07 20:31:00 - INFO -\n",
    "Epochs:  23%|â–| 23/100 [1:40:17<5:50:54, 273.44s/it, train_loss=0.4381, val_loss=0.3532, best_val_f1=0.6659, lr=2.50e-052025-06-07 20:35:34 - INFO -\n",
    "Epochs:  24%|â–| 24/100 [1:44:51<5:46:22, 273.45s/it, train_loss=0.4423, val_loss=0.3635, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:40:07 - INFO -\n",
    "Epochs:  25%|â–Ž| 25/100 [1:49:24<5:41:52, 273.49s/it, train_loss=0.4291, val_loss=0.3473, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:44:41 - INFO -\n",
    "Epochs:  26%|â–Ž| 26/100 [1:53:58<5:37:12, 273.42s/it, train_loss=0.4403, val_loss=0.3380, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:49:14 - INFO -\n",
    "Epochs:  27%|â–Ž| 27/100 [1:58:31<5:32:38, 273.40s/it, train_loss=0.4312, val_loss=0.3374, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:53:47 - INFO -\n",
    "Epochs:  28%|â–Ž| 28/100 [2:03:05<5:28:07, 273.44s/it, train_loss=0.4393, val_loss=0.3441, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:58:21 - INFO -\n",
    "Epochs:  29%|â–Ž| 29/100 [2:07:38<5:23:35, 273.46s/it, train_loss=0.4226, val_loss=0.3392, best_val_f1=0.6659, lr=1.25e-052025-06-07 21:02:54 - INFO -\n",
    "Epochs:  30%|â–Ž| 30/100 [2:12:11<5:19:02, 273.46s/it, train_loss=0.4240, val_loss=0.3525, best_val_f1=0.6659, lr=6.25e-062025-06-07 21:07:28 - INFO -\n",
    "Epochs:  31%|â–Ž| 31/100 [2:16:45<5:14:28, 273.46s/it, train_loss=0.4249, val_loss=0.3492, best_val_f1=0.6659, lr=6.25e-062025-06-07 21:12:01 - INFO -\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_optimized.pt\"\n",
    "model_generalizable_optimized = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 160,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    lstm_num_layers = 2,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 192,\n",
    "    gcn_out_channels = 128,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.5, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c4b3a",
   "metadata": {},
   "source": [
    "### Test 8: Narrow but Deep GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff01da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_narrow_deep_model.pt\"\n",
    "narrow_deep_model = EEGCNNBiLSTMGCN(\n",
    "    # --- Simplify the Temporal Encoder ---\n",
    "    cnn_dropout_prob = 0.2,\n",
    "    lstm_hidden_dim = 64,  # Reduced\n",
    "    lstm_out_dim = 64,     # Reduced\n",
    "    lstm_dropout_prob = 0.2,\n",
    "    # --- Focus on the GCN ---\n",
    "    gcn_hidden_channels = 128, # Keep GCN capacity high\n",
    "    gcn_out_channels = 64,\n",
    "    gcn_num_layers = 5,      # Try going even deeper\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ef8b3",
   "metadata": {},
   "source": [
    "### Test 9: First best model, with wider + deeper GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_new_old_best_model.pt\"\n",
    "new_old_best_model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 128, # from 64 to 128\n",
    "    gcn_num_layers = 4, # from 3 to 4\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351da028",
   "metadata": {},
   "source": [
    "### Best model + attention BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.hybrid.cnn_bilstm_attention_gcn\n",
    "from src.layers.hybrid.cnn_bilstm_attention_gcn import EEGCNNBiLSTMAttentionGNN\n",
    "\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_attention.pt\"\n",
    "model_first_attention = EEGCNNBiLSTMAttentionGNN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 192,\n",
    "    gcn_out_channels = 128,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a8a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from src.utils.train import train_model\n",
    "\n",
    "model = model_small_gcn_bigger_embedding\n",
    "model = model.to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "# optimizer = Lion(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "loss = nn.BCEWithLogitsLoss() # Not weighted as we use a balanced sampler!\n",
    "\n",
    "# empty cache in order to free up VRAM (if available)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# train model\n",
    "train_history, val_history = train_model(\n",
    "    wandb_config=None,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=loss,\n",
    "    scheduler=scheduler,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=config[\"epochs\"],\n",
    "    patience=config[\"patience\"],\n",
    "    save_path=SAVE_PATH,\n",
    "    use_gnn=True,\n",
    "    # hidden attribute\n",
    "    try_load_checkpoint=True,\n",
    ")\n",
    "\n",
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch cuda clear cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63329a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fdd646",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CREATING DATA LOADERS ===\")\n",
    "\n",
    "# Create data loaders for SPATIAL dataset\n",
    "print(\"Creating spatial data loaders...\")\n",
    "train_loader_spatial = GeoDataLoader(\n",
    "    train_dataset_spatial,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler_spatial,\n",
    "    shuffle=False,  # Don't shuffle when using sampler\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "val_loader_spatial = GeoDataLoader(\n",
    "    val_dataset_spatial,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "te_loader_spatial = GeoDataLoader(\n",
    "    dataset_corr_te,  # Using correlation test dataset for consistency\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "print(f\"Spatial - Train batches: {len(train_loader_spatial)}\")\n",
    "print(f\"Spatial - Val batches: {len(val_loader_spatial)}\")\n",
    "print(f\"Spatial - Test batches: {len(te_loader_spatial)}\")\n",
    "\n",
    "# Create data loaders for CORRELATION dataset\n",
    "print(\"\\nCreating correlation data loaders...\")\n",
    "train_loader_corr = GeoDataLoader(\n",
    "    train_dataset_corr,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler_corr,\n",
    "    shuffle=False,  # Don't shuffle when using sampler\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "val_loader_corr = GeoDataLoader(\n",
    "    val_dataset_corr,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "te_loader_corr = GeoDataLoader(\n",
    "    dataset_corr_te,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "print(f\"Correlation - Train batches: {len(train_loader_corr)}\")\n",
    "print(f\"Correlation - Val batches: {len(val_loader_corr)}\")\n",
    "print(f\"Correlation - Test batches: {len(te_loader_corr)}\")\n",
    "\n",
    "print(\"\\nâœ… All data loaders created successfully!\")\n",
    "print(\"\\nYou can now use:\")\n",
    "print(\"  - train_loader_spatial, val_loader_spatial, te_loader_spatial for spatial graph training\")\n",
    "print(\"  - train_loader_corr, val_loader_corr, te_loader_corr for correlation graph training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a56808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DATASET VERIFICATION AND COMPARISON\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=== DATASET COMPARISON ===\")\n",
    "print(f\"Spatial dataset size: {len(dataset_spatial_tr)} samples\")\n",
    "print(f\"Correlation dataset size: {len(dataset_corr_tr)} samples\")\n",
    "print(f\"Test dataset size: {len(dataset_corr_te)} samples\")\n",
    "\n",
    "# Verify split consistency\n",
    "print(\"\\n=== SPLIT VERIFICATION ===\")\n",
    "print(f\"Spatial splits - Train: {len(train_dataset_spatial)}, Val: {len(val_dataset_spatial)}\")\n",
    "print(f\"Correlation splits - Train: {len(train_dataset_corr)}, Val: {len(val_dataset_corr)}\")\n",
    "\n",
    "# Check split ratios\n",
    "spatial_train_ratio = len(train_dataset_spatial) / len(dataset_spatial_tr)\n",
    "corr_train_ratio = len(train_dataset_corr) / len(dataset_corr_tr)\n",
    "print(f\"\\nTrain ratios - Spatial: {spatial_train_ratio:.3f}, Correlation: {corr_train_ratio:.3f}\")\n",
    "\n",
    "# Verify labels are balanced\n",
    "print(\"\\n=== LABEL BALANCE VERIFICATION ===\")\n",
    "print(\"Spatial train labels:\", np.bincount([clips_tr.iloc[i]['label'] for i in train_indices_spatial]))\n",
    "print(\"Spatial val labels:\", np.bincount([clips_tr.iloc[i]['label'] for i in val_indices_spatial]))\n",
    "print(\"Correlation train labels:\", np.bincount([clips_tr.iloc[i]['label'] for i in train_indices_corr]))\n",
    "print(\"Correlation val labels:\", np.bincount([clips_tr.iloc[i]['label'] for i in val_indices_corr]))\n",
    "\n",
    "print(\"\\nâœ… All splits created successfully and verified!\")\n",
    "print(\"\\nðŸ“ Note: To train with different datasets, change DATASET_TYPE in the cell above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e14ad1f",
   "metadata": {},
   "source": [
    "## Training Instructions\n",
    "\n",
    "### Dataset Selection\n",
    "You can now train with either dataset type by changing the `DATASET_TYPE` variable:\n",
    "\n",
    "- **Spatial**: `DATASET_TYPE = 'spatial'` - Uses spatial distance-based graph connections\n",
    "- **Correlation**: `DATASET_TYPE = 'correlation'` - Uses correlation-based graph connections\n",
    "\n",
    "### Available Data Loaders\n",
    "\n",
    "#### For Spatial Dataset:\n",
    "- `train_loader_spatial` - Training data with weighted sampling for class balance\n",
    "- `val_loader_spatial` - Validation data\n",
    "- `te_loader_spatial` - Test data\n",
    "\n",
    "#### For Correlation Dataset:\n",
    "- `train_loader_corr` - Training data with weighted sampling for class balance\n",
    "- `val_loader_corr` - Validation data\n",
    "- `te_loader_corr` - Test data\n",
    "\n",
    "### Split Details\n",
    "- **Train/Validation ratio**: 80/20\n",
    "- **Random seed**: 42 (for reproducibility)\n",
    "- **Class balancing**: WeightedRandomSampler with oversampling power = 1.0\n",
    "- **Batch size**: 64\n",
    "\n",
    "### Training Tips\n",
    "1. The `train_loader`, `val_loader`, and `te_loader` variables are automatically set based on your `DATASET_TYPE` selection\n",
    "2. Both datasets use the same preprocessing pipeline but different graph construction strategies\n",
    "3. The correlation dataset uses top-k=5 connections, while spatial uses distance-based connections\n",
    "4. All data loaders include proper error handling and batch verification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
