{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d97e92",
   "metadata": {},
   "source": [
    "# NeuroGraphNet\n",
    "\n",
    "*A graph-based deep learning framework for EEG seizure detection, designed to improve accuracy and interpretability by leveraging Graph Neural Networks (GNNs) to capture spatial and temporal brain dynamics.*\n",
    "\n",
    "<hr />\n",
    "\n",
    "This notebook presents different approaches to EEG seizure detection using traditional machine learning and deep learning methods as well as a novel approaches using Graph Neural Networks (GNNs). The dataset used a subset of the TUSZ EEG Seizure dataset, which contains EEG recordings from patients with epilepsy.\n",
    "\n",
    "**Authors**: Luca Di Bello, Guillaume AndrÃ© BÃ©lissent, Abdessalem Ben Ali, Beatriz Izquierdo GonzÃ¡lez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5837c2",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b998b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from src.utils.seeder import seed_everything\n",
    "\n",
    "# set seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# create useful constants\n",
    "RANDOM_SEED = 42\n",
    "IS_SCITAS = True # set to True if running on SCITAS cluster\n",
    "LOCAL_DATA_ROOT = Path(\"./data\")\n",
    "DATA_ROOT = Path(\"/home/ogut/data\") if IS_SCITAS else LOCAL_DATA_ROOT\n",
    "CHECKPOINT_ROOT = Path(\"./.checkpoints\")\n",
    "SUBMISSION_ROOT = Path(\"./.submissions\")\n",
    "\n",
    "# create directories if they do not exist\n",
    "CHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# set dataset root\n",
    "seed_everything(RANDOM_SEED)\n",
    "\n",
    "# setup torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f74a9",
   "metadata": {},
   "source": [
    "## Run feature extraction script\n",
    "\n",
    "In order to run all the models in this notebook, is necessary to run the feature extraction script first. This script extracts features from the EEG signals for both the training and test dataset, saving three files: `X_train.npy`, `y_train.npy`, and `X_test.npy`. The features extracted are the same used in the original paper, which are based on the EEG signals.\n",
    "\n",
    "You can run the script by uncommenting and executing the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d28586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# # execute feature extraction script\n",
    "# process = None\n",
    "# try:\n",
    "#     process = subprocess.Popen([\"python3\", \"scripts/feature_extractor.py\"])\n",
    "#     process.wait()\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"Process interrupted, terminating...\")\n",
    "#     if process:\n",
    "#         process.terminate()\n",
    "#         process.wait()\n",
    "# except Exception as e:\n",
    "#     print(f\"Error occurred: {e}\")\n",
    "#     if process:\n",
    "#         process.terminate()\n",
    "#         process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527547f9",
   "metadata": {},
   "source": [
    "## Define paths\n",
    "\n",
    "The following paths are used to load the required data files and save the results of the models. Make sure to adjust them according to your local setup.\n",
    "\n",
    "The current configuration assumes that the data files are located in a folder named `data` within the current working directory. \n",
    "\n",
    "**NOTE:** to simplify the process on SCITAS cluster, we provide a toggle `IS_SCITAS` to set the paths accordingly (_refer to first cell of the notebook_). If you are running this notebook on your local machine, you can set `IS_SCITAS = False` and adjust the paths as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45999291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacial distance matrix between sensors\n",
    "spatial_distance_file = LOCAL_DATA_ROOT / \"distances_3d.csv\"\n",
    "\n",
    "# absdiff correlation matrix\n",
    "absdiff_correlation_file = LOCAL_DATA_ROOT / \"diff_corr_matrix.csv\"\n",
    "\n",
    "# training data\n",
    "train_dir = DATA_ROOT / \"train\"\n",
    "train_dir_metadata = train_dir / \"segments.parquet\"\n",
    "train_dataset_absdiff_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_absdiff_correlation_train\"\n",
    "train_dataset_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_correlation_train\"\n",
    "train_dataset_spatial_dir = LOCAL_DATA_ROOT / \"graph_dataset_spatial_train\"\n",
    "train_dataset_timeseries_feature_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_features\")\n",
    "train_dataset_timeseries_signal_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_signal\")\n",
    "\n",
    "# test data\n",
    "test_dir = DATA_ROOT / \"test\"\n",
    "test_dir_metadata = test_dir / \"segments.parquet\"\n",
    "test_dataset_absdiff_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_absdiff_correlation_test\"\n",
    "test_dataset_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_correlation_test\"\n",
    "test_dataset_spatial_dir = LOCAL_DATA_ROOT / \"graph_dataset_spatial_test\"\n",
    "test_dataset_timeseries_signal_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_signal\")\n",
    "test_dataset_timeseries_feature_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_features\")\n",
    "\n",
    "# additional features\n",
    "extracted_features_dir = LOCAL_DATA_ROOT / \"extracted_features\"\n",
    "embeddings_dir =  LOCAL_DATA_ROOT / \"embeddings\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f1780",
   "metadata": {},
   "source": [
    "## Loading Train and Test Clips from the Dataset\n",
    "\n",
    "To load patient clips from the dataset, we use the `load_clips` function. This function retrieves EEG signals and labels from the specified paths and returns them as NumPy arrays.\n",
    "\n",
    "Different versions of Pandas may return either a MultiIndex or a single index, even when called with the same parameters. To address this inconsistency, we use the `ensure_eeg_multiindex` function to ensure that the resulting DataFrame has a MultiIndex structure. This is essential for subsequent processing steps.\n",
    "\n",
    "If a MultiIndex is not present, it will be created using the following levels: `patient_id`, `clip_id`, and `channel`. This structure is crucial for organizing the dataset, as EEG signals are grouped by patient, clip, and channel. It also ensures compatibility with existing code that expects this format, such as the `EEGDataset` class from the [seizure-eeg](https://www.piwheels.org/project/seiz-eeg/) package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d93851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.index import ensure_eeg_multiindex \n",
    "\n",
    "# Load clips from datasets\n",
    "clips_tr = pd.read_parquet(train_dir_metadata)\n",
    "clips_tr = ensure_eeg_multiindex(clips_tr)\n",
    "clips_tr['id'] = clips_tr.index.map(lambda x: '_'.join(str(i) for i in x))\n",
    "assert clips_tr.id.nunique() == len(clips_tr), \"There are duplicate IDs\"\n",
    "clips_tr = clips_tr[~clips_tr.label.isna()].reset_index()\n",
    "\n",
    "# Load clips from datasets\n",
    "clips_te = pd.read_parquet(test_dir_metadata)\n",
    "clips_te = ensure_eeg_multiindex(clips_te)\n",
    "clips_te['id'] = clips_te.index.map(lambda x: '_'.join(str(i) for i in x))\n",
    "assert clips_te.id.nunique() == len(clips_te), \"There are duplicate IDs\"\n",
    "clips_te = clips_te.reset_index()\n",
    "\n",
    "# sort in order to maintain the same submission order\n",
    "clips_te = clips_te.sort_values(by=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f7bc0",
   "metadata": {},
   "source": [
    "## Loading Datasets\n",
    "\n",
    "This notebook explores various approaches to EEG seizure detection, requiring multiple dataset variants with distinct preprocessing strategies (e.g., raw EEG signals, extracted features, and diverse graph construction methods). The `GraphEEGDataset` class, a custom implementation of `torch.utils.data.Dataset`, is used to load these datasets based on specified parameters.\n",
    "\n",
    "The `GraphEEGDataset` class is designed to support all preprocessing strategies, including graph-based approaches. It preprocesses EEG data on-the-fly, offering flexibility in data handling and model input preparation. Additionally, it includes a caching mechanism to store preprocessed data on disk. This mechanism ensures that subsequent calls with identical parameters load precomputed data, significantly reducing dataset loading time during repeated runs. This feature has been instrumental in accelerating development and experimentation within this notebook.\n",
    "\n",
    "Specifically, we will load the following datasets:\n",
    "\n",
    "A) **EEG signals**: \n",
    "\n",
    "- **Raw EEG**:\n",
    "\n",
    "    - Feature-based\n",
    "\n",
    "        - Raw EEG signals + signal time-filtering/rereferencing/normalization preprocessing\n",
    "\n",
    "    - Graph-based\n",
    "\n",
    "        - Raw EEG signals + spatial graph construction strategy + signal time-filtering/rereferencing/normalization preprocessing + graph-features\n",
    "\n",
    "        - Raw EEG signals + correlation graph construction strategy (top-k policy with k=10) + signal time-filtering/rereferencing/normalization preprocessing + graph-features\n",
    "\n",
    "        - Raw EEG signals + absolute difference correlation graph construction strategy (top-k policy with k=10) + signal time-filtering/rereferencing/normalization preprocessing + graph-features\n",
    "\n",
    "- **Feature-based EEG**:\n",
    "\n",
    "    - Extracted features + spatial graph construction strategy + signal time-filtering/rereferencing/normalization preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e759ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generael settings\n",
    "\n",
    "# bandpass filter settings (signal time-filtering)\n",
    "low_bandpass_frequency = 0.5\n",
    "high_bandpass_frequency = 50\n",
    "\n",
    "# additional settings\n",
    "# NOTE: the training already fights class imbalance, so this is not used\n",
    "oversampling_power = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef132cd",
   "metadata": {},
   "source": [
    "### Raw-EEG signal datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8110e5",
   "metadata": {},
   "source": [
    "#### A) Spatial graph construction strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a686e9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "2025-06-10 02:14:53 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 02:14:53 - INFO - Dataset parameters:\n",
      "2025-06-10 02:14:53 - INFO -   - Root directory: data/graph_dataset_spatial_train\n",
      "2025-06-10 02:14:53 - INFO -   - Edge strategy: spatial\n",
      "2025-06-10 02:14:53 - INFO -   - Top-k neighbors: None\n",
      "2025-06-10 02:14:53 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 02:14:53 - INFO -   - Force reprocess: False\n",
      "2025-06-10 02:14:53 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 02:14:53 - INFO -   - Segment length: 3000\n",
      "2025-06-10 02:14:53 - INFO -   - Apply filtering: True\n",
      "2025-06-10 02:14:53 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 02:14:53 - INFO -   - Apply normalization: True\n",
      "2025-06-10 02:14:53 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 02:14:53 - INFO -   - Test mode: False\n",
      "2025-06-10 02:14:53 - INFO -   - Extract graph features: False\n",
      "2025-06-10 02:14:53 - INFO -   - Diff Corr Matrix Path: None\n",
      "2025-06-10 02:14:53 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 02:14:53 - INFO - Setting up signal filters...\n",
      "2025-06-10 02:14:53 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 02:14:53 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 02:14:53 - INFO - Loaded 361 spatial distances in 0.01s\n",
      "2025-06-10 02:14:53 - INFO - Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset_spatial_tr: 12993\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_spatial_tr = GraphEEGDataset(\n",
    "    root=train_dataset_spatial_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"spatial\",\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file\n",
    "    ),\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=False,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_spatial_tr: {len(dataset_spatial_tr)}\")\n",
    "print(f' Eliminated IDs: {dataset_spatial_tr.ids_to_eliminate}')\n",
    "clips_spatial_tr = clips_tr[~clips_tr.index.isin(dataset_spatial_tr.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b33fe510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 02:14:53 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 02:14:53 - INFO - Dataset parameters:\n",
      "2025-06-10 02:14:53 - INFO -   - Root directory: data/graph_dataset_spatial_test\n",
      "2025-06-10 02:14:53 - INFO -   - Edge strategy: spatial\n",
      "2025-06-10 02:14:53 - INFO -   - Top-k neighbors: None\n",
      "2025-06-10 02:14:53 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 02:14:53 - INFO -   - Force reprocess: False\n",
      "2025-06-10 02:14:53 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 02:14:53 - INFO -   - Segment length: 3000\n",
      "2025-06-10 02:14:53 - INFO -   - Apply filtering: True\n",
      "2025-06-10 02:14:53 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 02:14:53 - INFO -   - Apply normalization: True\n",
      "2025-06-10 02:14:53 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 02:14:53 - INFO -   - Test mode: True\n",
      "2025-06-10 02:14:53 - INFO -   - Extract graph features: False\n",
      "2025-06-10 02:14:53 - INFO -   - Diff Corr Matrix Path: None\n",
      "2025-06-10 02:14:53 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 02:14:53 - INFO - Setting up signal filters...\n",
      "2025-06-10 02:14:53 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 02:14:53 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 02:14:53 - INFO - Loaded 361 spatial distances in 0.01s\n",
      "2025-06-10 02:14:53 - INFO - Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset_spatial_te: 3614\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "# load training dataset\n",
    "dataset_spatial_te = GraphEEGDataset(\n",
    "    root=test_dataset_spatial_dir,\n",
    "    clips=clips_te,\n",
    "    signal_folder=test_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"spatial\",\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file\n",
    "    ),\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    is_test=True,\n",
    "    # extract graph features\n",
    "    extract_graph_features=False,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_spatial_te: {len(dataset_spatial_te)}\")\n",
    "print(f' Eliminated IDs: {dataset_spatial_te.ids_to_eliminate}')\n",
    "clips_spatial_te = clips_te[~clips_te.index.isin(dataset_spatial_te.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8993f7",
   "metadata": {},
   "source": [
    "#### B) Correlation graph construction strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1472121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "top_k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c3f93ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 02:14:53 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 02:14:53 - INFO - Dataset parameters:\n",
      "2025-06-10 02:14:53 - INFO -   - Root directory: data/graph_dataset_correlation_train\n",
      "2025-06-10 02:14:53 - INFO -   - Edge strategy: correlation\n",
      "2025-06-10 02:14:53 - INFO -   - Top-k neighbors: 5\n",
      "2025-06-10 02:14:53 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 02:14:53 - INFO -   - Force reprocess: False\n",
      "2025-06-10 02:14:53 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 02:14:53 - INFO -   - Segment length: 3000\n",
      "2025-06-10 02:14:53 - INFO -   - Apply filtering: True\n",
      "2025-06-10 02:14:53 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 02:14:53 - INFO -   - Apply normalization: True\n",
      "2025-06-10 02:14:53 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 02:14:53 - INFO -   - Test mode: False\n",
      "2025-06-10 02:14:53 - INFO -   - Extract graph features: True\n",
      "2025-06-10 02:14:53 - INFO -   - Diff Corr Matrix Path: None\n",
      "2025-06-10 02:14:53 - INFO - Initializing graph feature extractor...\n",
      "2025-06-10 02:14:53,636 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:14:53 - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:14:53 - INFO - Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:14:53 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 02:14:53 - INFO - Setting up signal filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "Length of dataset_corr_tr: 12986\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_corr_tr = GraphEEGDataset(\n",
    "    root=train_dataset_correlation_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"correlation\",\n",
    "    spatial_distance_file=None,\n",
    "    top_k=top_k,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None # collect all graph features\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_corr_tr: {len(dataset_corr_tr)}\")\n",
    "print(f' Eliminated IDs: {dataset_corr_tr.ids_to_eliminate}')\n",
    "clips_corr_tr = clips_tr[~clips_tr.index.isin(dataset_corr_tr.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "233887f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 02:14:53 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 02:14:53 - INFO - Dataset parameters:\n",
      "2025-06-10 02:14:53 - INFO -   - Root directory: data/graph_dataset_correlation_test\n",
      "2025-06-10 02:14:53 - INFO -   - Edge strategy: spatial\n",
      "2025-06-10 02:14:53 - INFO -   - Top-k neighbors: None\n",
      "2025-06-10 02:14:53 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 02:14:53 - INFO -   - Force reprocess: False\n",
      "2025-06-10 02:14:53 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 02:14:53 - INFO -   - Segment length: 3000\n",
      "2025-06-10 02:14:53 - INFO -   - Apply filtering: True\n",
      "2025-06-10 02:14:53 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 02:14:53 - INFO -   - Apply normalization: True\n",
      "2025-06-10 02:14:53 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 02:14:53 - INFO -   - Test mode: True\n",
      "2025-06-10 02:14:53 - INFO -   - Extract graph features: True\n",
      "2025-06-10 02:14:53 - INFO -   - Diff Corr Matrix Path: None\n",
      "2025-06-10 02:14:53 - INFO - Initializing graph feature extractor...\n",
      "2025-06-10 02:14:53,702 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:14:53 - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:14:53 - INFO - Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:14:53 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 02:14:53 - INFO - Setting up signal filters...\n",
      "2025-06-10 02:14:53 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 02:14:53 - INFO - Loading spatial distances from data/distances_3d.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 02:14:53 - INFO - Loaded 361 spatial distances in 0.01s\n",
      "2025-06-10 02:14:53 - INFO - Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset_corr_te: 3614\n",
      " Eliminated IDs:[]\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_corr_te = GraphEEGDataset(\n",
    "    root=test_dataset_correlation_dir,\n",
    "    clips=clips_te,\n",
    "    signal_folder=test_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"spatial\",\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file\n",
    "    ),\n",
    "    top_k=None,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    is_test=True, # NOTE: needed to let the dataset know that is okay to now have labels!\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None # collect all graph features\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_corr_te: {len(dataset_corr_te)}\")\n",
    "print(f' Eliminated IDs:{dataset_corr_te.ids_to_eliminate}')\n",
    "clips_spatial_te = clips_te[~clips_te.index.isin(dataset_corr_te.ids_to_eliminate)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4461766e",
   "metadata": {},
   "source": [
    "#### C) Absolute difference correlation graph construction strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2328a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "top_k = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bf782ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 02:14:53 - INFO - Initializing GraphEEGDataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 02:14:53 - INFO - Dataset parameters:\n",
      "2025-06-10 02:14:53 - INFO -   - Root directory: data/graph_dataset_absdiff_correlation_train\n",
      "2025-06-10 02:14:53 - INFO -   - Edge strategy: relevance_diff_correlation\n",
      "2025-06-10 02:14:53 - INFO -   - Top-k neighbors: 8\n",
      "2025-06-10 02:14:53 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 02:14:53 - INFO -   - Force reprocess: False\n",
      "2025-06-10 02:14:53 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 02:14:53 - INFO -   - Segment length: 3000\n",
      "2025-06-10 02:14:53 - INFO -   - Apply filtering: True\n",
      "2025-06-10 02:14:53 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 02:14:53 - INFO -   - Apply normalization: True\n",
      "2025-06-10 02:14:53 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 02:14:53 - INFO -   - Test mode: False\n",
      "2025-06-10 02:14:53 - INFO -   - Extract graph features: True\n",
      "2025-06-10 02:14:53 - INFO -   - Diff Corr Matrix Path: data/diff_corr_matrix.csv\n",
      "2025-06-10 02:14:53 - INFO - Edge strategy: relevance_diff_correlation. Loading average correlation matrices.\n",
      "2025-06-10 02:14:53 - INFO - Loaded absolute difference correlation matrix from data/diff_corr_matrix.csv, shape: (19, 19)\n",
      "2025-06-10 02:14:53 - INFO - Initializing graph feature extractor...\n",
      "2025-06-10 02:14:53,844 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:14:53 - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:14:53 - INFO - Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:14:53 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 02:14:53 - INFO - Setting up signal filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset_absdiff_corr_tr: 4646\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_absdiff_corr_tr = GraphEEGDataset(\n",
    "    root=train_dataset_absdiff_correlation_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"relevance_diff_correlation\",\n",
    "    spatial_distance_file=None,\n",
    "    top_k=top_k,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None, # collect all graph features\n",
    "    # settings for absolute difference correlation\n",
    "    diff_corr_matrix_path=absdiff_correlation_file,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_absdiff_corr_tr: {len(dataset_absdiff_corr_tr)}\")\n",
    "print(f' Eliminated IDs: {dataset_absdiff_corr_tr.ids_to_eliminate}')\n",
    "clips_absdiff_corr_tr = clips_tr[~clips_tr.index.isin(dataset_absdiff_corr_tr.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18240a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 02:14:53 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 02:14:53 - INFO - Dataset parameters:\n",
      "2025-06-10 02:14:53 - INFO -   - Root directory: data/graph_dataset_absdiff_correlation_test\n",
      "2025-06-10 02:14:53 - INFO -   - Edge strategy: relevance_diff_correlation\n",
      "2025-06-10 02:14:53 - INFO -   - Top-k neighbors: 8\n",
      "2025-06-10 02:14:53 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 02:14:53 - INFO -   - Force reprocess: False\n",
      "2025-06-10 02:14:53 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 02:14:53 - INFO -   - Segment length: 3000\n",
      "2025-06-10 02:14:53 - INFO -   - Apply filtering: True\n",
      "2025-06-10 02:14:53 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 02:14:53 - INFO -   - Apply normalization: True\n",
      "2025-06-10 02:14:53 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 02:14:53 - INFO -   - Test mode: False\n",
      "2025-06-10 02:14:53 - INFO -   - Extract graph features: True\n",
      "2025-06-10 02:14:53 - INFO -   - Diff Corr Matrix Path: data/diff_corr_matrix.csv\n",
      "2025-06-10 02:14:53 - INFO - Edge strategy: relevance_diff_correlation. Loading average correlation matrices.\n",
      "2025-06-10 02:14:53 - INFO - Loaded absolute difference correlation matrix from data/diff_corr_matrix.csv, shape: (19, 19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 02:14:53 - INFO - Initializing graph feature extractor...\n",
      "2025-06-10 02:14:53,925 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:14:53 - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:14:53 - INFO - Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:14:53 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 02:14:53 - INFO - Setting up signal filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset_absdiff_corr_te: 0\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load test dataset\n",
    "dataset_absdiff_corr_te = GraphEEGDataset(\n",
    "    root=test_dataset_absdiff_correlation_dir,\n",
    "    clips=clips_te,\n",
    "    signal_folder=test_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"relevance_diff_correlation\",\n",
    "    spatial_distance_file=None,\n",
    "    top_k=top_k,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None, # collect all graph features\n",
    "    # settings for absolute difference correlation\n",
    "    diff_corr_matrix_path=absdiff_correlation_file,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_absdiff_corr_te: {len(dataset_absdiff_corr_te)}\")\n",
    "print(f' Eliminated IDs: {dataset_absdiff_corr_te.ids_to_eliminate}')\n",
    "clips_absdiff_corr_te = clips_te[~clips_te.index.isin(dataset_absdiff_corr_te.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa474f0b",
   "metadata": {},
   "source": [
    "### Timeseries datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4a44e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "ðŸš€ Initializing TimeseriesEEGDataset in SIGNAL mode.\n",
      "   - Sampling rate: 250 Hz\n",
      "   - Derived segment length: 3000 timesteps.\n",
      "   - Segment length: 3000 timesteps.\n",
      "   âœ… Using existing cached data from data/timeseries_dataset_train_signal/processed\n",
      "ðŸ TimeseriesEEGDataset initialization complete. Loaded 12993 samples.\n",
      "ðŸš€ Initializing TimeseriesEEGDataset in SIGNAL mode.\n",
      "   - Sampling rate: 250 Hz\n",
      "   - Derived segment length: 3000 timesteps.\n",
      "   - Segment length: 3000 timesteps.\n",
      "   âš ï¸ Info: Column 'label' not found in clips_df. Processing without labels (e.g., test set).\n",
      "   âœ… Using existing cached data from data/timeseries_dataset_test_signal/processed\n",
      "ðŸ TimeseriesEEGDataset initialization complete. Loaded 3614 samples.\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.utils.timeseries_eeg_dataset import TimeseriesEEGDataset\n",
    "\n",
    "dataset_timeseries_signal_tr = TimeseriesEEGDataset(\n",
    "    root=str(train_dataset_timeseries_signal_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_tr,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    force_reprocess=False,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='signal', # Use raw EEG signal data\n",
    ")\n",
    "dataset_timeseries_signal_te = TimeseriesEEGDataset(\n",
    "    root=str(test_dataset_timeseries_signal_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_te,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='signal', # Use raw EEG signal data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fe1f9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "ðŸš€ Initializing TimeseriesEEGDataset in FEATURE mode.\n",
      "   âœ… Using existing cached data from data/timeseries_dataset_train_features/processed\n",
      "ðŸ TimeseriesEEGDataset initialization complete. Loaded 12993 samples.\n",
      "ðŸš€ Initializing TimeseriesEEGDataset in FEATURE mode.\n",
      "   âš ï¸ Info: Column 'label' not found in clips_df. Processing without labels (e.g., test set).\n",
      "   âœ… Using existing cached data from data/timeseries_dataset_test_features/processed\n",
      "ðŸ TimeseriesEEGDataset initialization complete. Loaded 3614 samples.\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.utils.timeseries_eeg_dataset import TimeseriesEEGDataset\n",
    "\n",
    "dataset_timeseries_feature_tr = TimeseriesEEGDataset(\n",
    "    root=str(train_dataset_timeseries_feature_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_tr,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='feature',\n",
    "    feature_file_path=str(extracted_features_dir / \"X_train.npy\"),\n",
    ")\n",
    "dataset_timeseries_feature_te = TimeseriesEEGDataset(\n",
    "    root=str(test_dataset_timeseries_feature_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_te,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='feature',\n",
    "    feature_file_path=str(extracted_features_dir / \"X_test.npy\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f38f791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "âœ… TrainingContext initialized. Use .switch_to('dataset_type') to begin.\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.utils.lazy import LazyDataLoaderManager, TrainingContext\n",
    "\n",
    "datasets = {\n",
    "    # timeseries datasets\n",
    "    \"signal\": {\n",
    "        \"dataset_tr\": dataset_timeseries_signal_tr,\n",
    "        \"dataset_te\": dataset_timeseries_signal_te,\n",
    "        \"clips_tr\": clips_tr\n",
    "    },\n",
    "    \"feature\": {\n",
    "        \"dataset_tr\": dataset_timeseries_feature_tr,\n",
    "        \"dataset_te\": dataset_timeseries_feature_te,\n",
    "        \"clips_tr\": clips_tr\n",
    "    },\n",
    "    # graph datasets\n",
    "    \"spatial\": {\n",
    "        \"dataset_tr\": dataset_spatial_tr,\n",
    "        \"dataset_te\": dataset_spatial_te,\n",
    "        \"clips_tr\": clips_spatial_tr,\n",
    "    },\n",
    "    \"correlation\": {\n",
    "        \"dataset_tr\": dataset_corr_tr,\n",
    "        \"dataset_te\": dataset_corr_te,\n",
    "        \"clips_tr\": clips_corr_tr,\n",
    "    },\n",
    "    \"absolute_difference\": {\n",
    "        \"dataset_tr\": dataset_absdiff_corr_tr,\n",
    "        \"dataset_te\": dataset_absdiff_corr_te,\n",
    "        \"clips_tr\": clips_absdiff_corr_tr,\n",
    "    }\n",
    "}\n",
    "\n",
    "# create loaders for both datasets\n",
    "loader_manager = LazyDataLoaderManager(\n",
    "    datasets,\n",
    "    oversampling_power=oversampling_power,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# create train context\n",
    "training_context = TrainingContext(loader_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef9bd7",
   "metadata": {},
   "source": [
    "## Dataset Selection and Training Configuration\n",
    "\n",
    "This section allows you to select which dataset type to use for training by modifying the `DATASET_TYPE` variable in the next cell. The notebook supports multiple dataset types, each with different preprocessing strategies and model architectures.\n",
    "\n",
    "### Available Dataset Types\n",
    "\n",
    "#### Graph-Based Datasets (for GNN models):\n",
    "- **`'spatial'`** - Uses spatial distance-based graph connections between EEG electrodes\n",
    "- **`'correlation'`** - Uses correlation-based graph connections (top-k=5)\n",
    "- **`'absdiff_correlation'`** - Uses absolute difference correlation graph connections (top-k=8)\n",
    "\n",
    "#### Timeseries Datasets (for traditional deep learning models):\n",
    "- **`'signal'`** - Raw EEG signal data with temporal processing\n",
    "- **`'features'`** - Pre-extracted feature representations\n",
    "\n",
    "### Data Loaders Structure\n",
    "\n",
    "All datasets are automatically split into train/validation/test sets with the following configuration:\n",
    "- **Train/Validation ratio**: 80/20\n",
    "- **Random seed**: 42 (for reproducibility)  \n",
    "- **Class balancing**: WeightedRandomSampler with oversampling power = 1.0\n",
    "- **Batch size**: 64\n",
    "\n",
    "The data loaders are organized in dictionaries for easy access:\n",
    "- `graph_loaders` - Contains loaders for all graph-based datasets\n",
    "- `timeseries_loaders` - Contains loaders for all timeseries datasets\n",
    "\n",
    "### Usage Instructions\n",
    "\n",
    "1. Set the `DATASET_TYPE` variable to your desired dataset type\n",
    "2. The notebook will automatically configure the appropriate data loaders\n",
    "3. Use the selected `train_loader`, `val_loader`, and `te_loader` for model training\n",
    "4. Choose the corresponding model architecture (GNN for graph datasets, traditional models for timeseries datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67042055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from src.layers.hybrid.cnn_bilstm_gcn import EEGCNNBiLSTMGCN\n",
    "from src.utils.train import train_model\n",
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"patience\": 10,\n",
    "    \"epochs\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5446a5",
   "metadata": {},
   "source": [
    "## Training and Evaluation - Timeseries Models\n",
    "\n",
    "In this section, we will train and evaluate traditional deep learning models on the selected timeseries dataset. The models will be trained using the `train_loader` and evaluated on the `val_loader` and `te_loader` (the latter being used for final evaluation after training. Labels are not available for the test set, so we will not compute metrics on it)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d491209e",
   "metadata": {},
   "source": [
    "### Training / k-Fold Cross-Validation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60bc6d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.utils.train\n",
    "from src.utils.train import train_k_fold\n",
    "\n",
    "def wrap_traditional_train(model, save_path):\n",
    "    global train_context\n",
    "    if 'train_context' not in globals():\n",
    "        raise ValueError(\"Timeseries training context is not initialized. Please initialize it before calling this function.\")\n",
    "    if not isinstance(train_context, TrainingContext):\n",
    "        raise ValueError(\"train_context must be an instance of TrainingContext.\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # if multiple GPUs are available, use DataParallel\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for training\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5)\n",
    "    loss = nn.BCEWithLogitsLoss()  # Not weighted as we use a balanced sampler!\n",
    "\n",
    "    # train model\n",
    "    train_history, val_history = train_model(\n",
    "        wandb_config=None,\n",
    "        model=model,\n",
    "        train_loader=train_context.train_loader,\n",
    "        val_loader=train_context.val_loader,\n",
    "        criterion=loss,\n",
    "        scheduler=scheduler,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=config[\"epochs\"],\n",
    "        patience=config[\"patience\"],\n",
    "        save_path=save_path,\n",
    "        use_gnn=False,\n",
    "        # hidden attribute\n",
    "        try_load_checkpoint=True,\n",
    "        # FIXME: remove this before submission\n",
    "        log_wandb=False,\n",
    "    )\n",
    "    plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])\n",
    "\n",
    "def k_fold_train_shorthand(model_class, model_parameters, save_path, use_gnn=False, log_wandb=False, batch_size=64):\n",
    "    global train_context\n",
    "\n",
    "    if 'train_context' not in globals():\n",
    "        raise ValueError(\"Training context is not initialized. Please initialize it before calling this function.\")\n",
    "    if not isinstance(train_context, TrainingContext):\n",
    "        raise ValueError(\"train_context must be an instance of TrainingContext.\")\n",
    "\n",
    "    import torch.nn as nn # force import to avoid bug\n",
    "\n",
    "    # train model\n",
    "    aggregated_train_history, aggregated_val_history, fold_results = train_k_fold(\n",
    "        # dataset to use\n",
    "        dataset=datasets[train_context.dataset_type][\"dataset_tr\"],\n",
    "        labels=train_context.clips[\"label\"].values,\n",
    "        # train models\n",
    "        model_class=model_class,\n",
    "        model_kwargs=model_parameters,\n",
    "        # optimizer\n",
    "        criterion=nn.BCEWithLogitsLoss(),\n",
    "        optimizer_class=torch.optim.AdamW,\n",
    "        optimizer_kwargs={\n",
    "            \"lr\": config[\"learning_rate\"],\n",
    "            \"weight_decay\": config[\"weight_decay\"],\n",
    "            \"betas\": (0.9, 0.999)\n",
    "        },\n",
    "        # scheduler\n",
    "        scheduler_class=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        scheduler_kwargs={\n",
    "            \"mode\": 'min',\n",
    "            \"factor\": 0.5,\n",
    "            \"patience\": 5\n",
    "        },\n",
    "        batch_size=batch_size,\n",
    "        wandb_config=None,\n",
    "        device=device,\n",
    "        num_epochs=config[\"epochs\"],\n",
    "        patience=config[\"patience\"],\n",
    "        use_gnn=use_gnn,\n",
    "        # hidden attribute\n",
    "        log_wandb=log_wandb,\n",
    "        save_dir=save_path,\n",
    "    )\n",
    "    plot_training_loss(aggregated_train_history[\"loss\"], aggregated_val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06a1e4",
   "metadata": {},
   "source": [
    "### LSTM (signal-based model)\n",
    "\n",
    "The LSTM model is a recurrent neural network (RNN) architecture designed to handle sequential data, making it suitable for time-series analysis like EEG signals. It captures temporal dependencies in the data, allowing it to learn patterns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.temporal.lstm\n",
    "from src.layers.temporal.lstm import EEGLSTMClassifier\n",
    "\n",
    "# create loader manager\n",
    "train_context = training_context.switch_to('signal')\n",
    "\n",
    "# build model with current parameters\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"timeseries_signal_lstm_baseline.pt\"\n",
    "\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGLSTMClassifier,\n",
    "    model_parameters={\n",
    "        \"input_dim\": 19,\n",
    "        \"hidden_dim\": 64,\n",
    "        \"num_layers\": 4,\n",
    "        \"dropout\": 0.3,\n",
    "        \"bidirectional\": False,\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_lstm_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89807a",
   "metadata": {},
   "source": [
    "### BiLSTM (signal-based model)\n",
    "\n",
    "The BiLSTM (Bidirectional Long Short-Term Memory) model extends the LSTM by processing the input sequence in both forward and backward directions. This bidirectional approach allows the model to capture context from both past and future time steps, enhancing its ability to understand complex temporal relationships in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ce139",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.temporal.lstm\n",
    "from src.layers.temporal.lstm import EEGLSTMClassifier\n",
    "\n",
    "# create loader manager\n",
    "train_context = training_context.switch_to('signal')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGLSTMClassifier,\n",
    "    model_parameters={\n",
    "        \"input_dim\": 19,\n",
    "        \"hidden_dim\": 64,\n",
    "        \"num_layers\": 4,\n",
    "        \"dropout\": 0.3,\n",
    "        \"bidirectional\": True, # use bidirectional LSTM\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_bilstm_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4a846",
   "metadata": {},
   "source": [
    "### MLP (feature-based model)\n",
    "\n",
    "The MLP (Multi-Layer Perceptron) is a feedforward neural network architecture consisting of multiple layers of neurons. It is designed to learn complex mappings from input features to output labels, making it suitable for tasks like EEG seizure detection when using pre-extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.mlp\n",
    "from src.layers.mlp import EEGMLPClassifier\n",
    "\n",
    "train_context = training_context.switch_to('feature')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGMLPClassifier,\n",
    "    model_parameters={\n",
    "        \"input_dim\": 228, # extracted features dimension\n",
    "        \"hidden_dims\": [1024, 512, 256],\n",
    "        \"output_dim\": 1,\n",
    "        \"dropout_prob\": 0.3,\n",
    "        \"use_batch_norm\": True,\n",
    "        \"use_residual\": False,\n",
    "        \"activation\": \"relu\"\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_feature_mlp_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8f8223",
   "metadata": {},
   "source": [
    "### MLP (signal-based model, flattened EEG signals)\n",
    "\n",
    "The same MLP architecture as above, but trained on raw EEG signals instead of pre-extracted features to evaluate the performance of the model on raw data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.mlp\n",
    "from src.layers.mlp import EEGMLPClassifier\n",
    "\n",
    "# switch to signal dataset\n",
    "train_context = training_context.switch_to('signal')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGMLPClassifier,\n",
    "    model_parameters={\n",
    "        \"input_channels\": 19,\n",
    "        \"input_time_steps\": 3000,\n",
    "        \"hidden_dims\": [4096, 2048, 1024, 512, 256],\n",
    "        \"output_dim\": 1,\n",
    "        \"dropout_prob\": 0.3,\n",
    "        \"use_batch_norm\": True,\n",
    "        \"use_residual\": False,\n",
    "        \"activation\": \"leaky_relu\"\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_mlp_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d45648",
   "metadata": {},
   "source": [
    "### CNN-MLP (signal-based model)\n",
    "\n",
    "We have proven that the MLP model alone is not sufficient to capture the temporal dependencies in the EEG signals. Therefore, we will use a CNN-MLP model that combines convolutional layers to extract spatial features from the EEG signals and MLP layers to learn the mapping from these features to the output labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.cnn.cnn\n",
    "from src.layers.cnn.cnn import EEGCNNClassifier\n",
    "\n",
    "train_context = training_context.switch_to('signal')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGCNNClassifier,\n",
    "    model_parameters={\n",
    "        \"input_channels\": 19,\n",
    "        \"cnn_out_dim\": 128,\n",
    "        \"mlp_hidden_dims\": [256, 128],\n",
    "        \"output_dim\": 1,\n",
    "        \"cnn_dropout_prob\": 0.3,\n",
    "        \"mlp_dropout_prob\": 0.3,\n",
    "        \"activation_mlp\": \"leaky_relu\",\n",
    "        \"activation_cnn\": \"leaky_relu\",\n",
    "        \"cnn_use_batch_norm\": True,\n",
    "        \"use_batch_norm_mlp\": True,\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_cnn_mlp_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febab4cf",
   "metadata": {},
   "source": [
    "### CNN-BiLSTM-MLP (signal-based model)\n",
    "\n",
    "In this section we test the CNN-BiLSTM model, which combines convolutional layers to extract spatial features from the EEG signals and LSTM layers to learn the temporal dependencies in the data. This architecture is particularly effective for EEG seizure detection, as it captures both spatial and temporal patterns in the signals. A final fully connected layer is used to map the extracted features to the output labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322efc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.cuda import clean_cuda_memory_usage\n",
    "clean_cuda_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ffe62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.cnn.cnn_lstm\n",
    "from src.layers.cnn.cnn_lstm import EEGCNNBiLSTMClassifier \n",
    "\n",
    "train_context = training_context.switch_to('signal')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGCNNBiLSTMClassifier,\n",
    "    model_parameters={\n",
    "        \"input_channels\": 19,\n",
    "        \"output_dim\": 1,\n",
    "        \"cnn_dropout_prob\": 0.3,\n",
    "        \"mlp_dropout_prob\": 0.3,\n",
    "        \"activation_mlp\": \"leaky_relu\",\n",
    "        \"cnn_use_batch_norm\": True,\n",
    "        \"use_batch_norm_mlp\": True,\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_cnn_bilstm_k_fold\",\n",
    "    batch_size=128 # slightly higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae02a362",
   "metadata": {},
   "source": [
    "## Training and Evaluation - Graph-Based Models\n",
    "\n",
    "This section focuses on training and evaluating Graph Neural Network (GNN) models on the selected graph-based datasets. These models leverage the spatial and functional relationships between EEG electrodes to improve seizure detection accuracy.\n",
    "\n",
    "### Available Graph-Based Architectures\n",
    "\n",
    "The notebook implements several hybrid architectures that combine temporal and graph processing:\n",
    "\n",
    "- **CNN-BiLSTM-GCN**: Combines Convolutional Neural Networks for feature extraction, Bidirectional LSTM for temporal modeling, and Graph Convolutional Networks for spatial relationships\n",
    "- **CNN-BiLSTM-GAT**: Similar to above but uses Graph Attention Networks instead of GCN for learning adaptive attention weights between electrodes\n",
    "- **CNN-BiLSTM-Attention-GNN**: Enhanced version with attention mechanisms in both temporal and graph components\n",
    "\n",
    "### Graph Construction Strategies\n",
    "\n",
    "The models can be trained on different graph construction approaches:\n",
    "- **Spatial graphs**: Based on physical electrode distances (19 channels)\n",
    "- **Correlation graphs**: Dynamic graphs based on signal correlations (top-k=5)\n",
    "- **Absolute difference correlation**: Advanced correlation-based graphs (top-k=8)\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "All graph models use:\n",
    "- **Optimizer**: AdamW with learning rate 1e-4 and weight decay 0.01\n",
    "- **Loss function**: BCEWithLogitsLoss (unweighted due to balanced sampling)\n",
    "- **Scheduler**: ReduceLROnPlateau with factor 0.5 and patience 5\n",
    "- **Early stopping**: Patience of 10 epochs based on validation F1 score\n",
    "- **Data handling**: GeoDataLoader for efficient graph batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8277022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure to clean up CUDA memory before this big model\n",
    "from src.utils.cuda import clean_cuda_memory_usage\n",
    "clean_cuda_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b336bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒ Switching context to 'SPATIAL' dataset...\n",
      "ðŸ§¹ Cleared CUDA memory. Previous usage: 0.00 MB. Current usage: 0.00 MB\n",
      "\n",
      "--- Lazily creating loaders for 'spatial' ---\n",
      "\n",
      "--- Creating train/val subsets for 'spatial' dataset ---\n",
      "Splitting 12993 samples -> Train: 10394, Val: 2599\n",
      "[02:16:06] Train labels: 0 -> 8375, 1 -> 2019\n",
      "[02:16:06] Val labels:   0 -> 2101, 1 -> 498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "2025-06-10 02:16:06 - INFO - Graph-level features are not included in this loader.\n",
      "2025-06-10 02:16:06 - INFO - Graph-level features are not included in this loader.\n",
      "2025-06-10 02:16:06 - INFO - Graph-level features are not included in this loader.\n",
      "2025-06-10 02:16:06 - INFO - Starting 5-fold cross-validation\n",
      "2025-06-10 02:16:06 - INFO - Dataset size: 12993\n",
      "2025-06-10 02:16:06 - INFO - Stratified: True\n",
      "2025-06-10 02:16:06 - INFO - Batch size: 8\n",
      "2025-06-10 02:16:06 - INFO - Using stratified k-fold with label distribution\n",
      "2025-06-10 02:16:06 - INFO - Created 5 folds\n",
      "2025-06-10 02:16:06 - INFO - Folds: [(array([    0,     2,     3, ..., 12990, 12991, 12992]), array([    1,     5,     6, ..., 12979, 12983, 12985])), (array([    0,     1,     2, ..., 12988, 12989, 12990]), array([    3,    19,    27, ..., 12977, 12991, 12992])), (array([    0,     1,     3, ..., 12987, 12991, 12992]), array([    2,     7,    17, ..., 12988, 12989, 12990])), (array([    0,     1,     2, ..., 12990, 12991, 12992]), array([    4,     9,    12, ..., 12975, 12978, 12987])), (array([    1,     2,     3, ..., 12990, 12991, 12992]), array([    0,    10,    16, ..., 12976, 12981, 12986]))]\n",
      "2025-06-10 02:16:06 - INFO - \n",
      "============================================================\n",
      "2025-06-10 02:16:06 - INFO - FOLD 1/5\n",
      "2025-06-10 02:16:06 - INFO - ============================================================\n",
      "2025-06-10 02:16:06 - INFO - Train samples: 10394\n",
      "2025-06-10 02:16:06 - INFO - Val samples: 2599\n",
      "2025-06-10 02:16:06 - INFO - Train positive ratio: 0.194\n",
      "2025-06-10 02:16:06 - INFO - Val positive ratio: 0.194\n",
      "/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "2025-06-10 02:16:06 - INFO - Graph-level features are not included in this loader.\n",
      "2025-06-10 02:16:06 - INFO - Graph-level features are not included in this loader.\n",
      "2025-06-10 02:16:06 - INFO - EEGCNNBiLSTMGCN initialized:\n",
      "2025-06-10 02:16:06 - INFO -   - Node input dim: 3000\n",
      "2025-06-10 02:16:06 - INFO -   - Node feature dim (LSTM output): 128\n",
      "2025-06-10 02:16:06 - INFO -   - GCN hidden dim: 128\n",
      "2025-06-10 02:16:06 - INFO -   - Graph feature dim: 0\n",
      "2025-06-10 02:16:06 - INFO -   - Use graph features: False\n",
      "2025-06-10 02:16:06 - INFO -   - Classifier input dim: 96\n",
      "2025-06-10 02:16:06 - INFO -   - Num classes: 1\n",
      "2025-06-10 02:16:06 - INFO -   - Num channels: 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [0.0001194  0.00049529]\n",
      "Train set class distribution: [8375 2019]\n",
      "ðŸš€ Context ready for 'spatial'.\n",
      "   Memory usage: 0.00 MB\n",
      "   Train batches: 162, Val batches: 41\n",
      "   Type: spatial\n",
      "   Total Train Samples: 12993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 02:16:06 - INFO - Initialized model: EEGCNNBiLSTMGCN\n",
      "2025-06-10 02:16:06 - INFO - Initialized optimizer: AdamW\n",
      "2025-06-10 02:16:06 - INFO - Initialized scheduler: ReduceLROnPlateau\n",
      "2025-06-10 02:16:06 - INFO - Starting training for fold 1\n",
      "2025-06-10 02:16:06 - INFO - Starting training setup...\n",
      "2025-06-10 02:16:06 - INFO - Model type: GNN\n",
      "2025-06-10 02:16:06 - INFO - Device: cuda\n",
      "2025-06-10 02:16:06 - INFO - Batch size: 8\n",
      "2025-06-10 02:16:06 - INFO - Number of epochs: 100\n",
      "2025-06-10 02:16:06 - INFO - Patience: 10\n",
      "2025-06-10 02:16:06 - INFO - Monitor metric: val_f1\n",
      "2025-06-10 02:16:06 - INFO - Initializing wandb...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucadibello\u001b[0m (\u001b[33mlucadibello-epfl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ldibello/NeuroGraphNet/wandb/run-20250610_021606-qvhq3fma</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/qvhq3fma' target=\"_blank\">fold_1</a></strong> to <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/qvhq3fma' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/qvhq3fma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 02:16:07 - INFO - ðŸ”— Wandb run initialized: fold_1\n",
      "2025-06-10 02:16:07 - INFO - Total training batches per epoch: 1300\n",
      "2025-06-10 02:16:07 - INFO - Starting training from epoch 1 to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Wandb initialized: fold_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]2025-06-10 02:16:07 - INFO - \n",
      "Epoch 1/100 - Training phase\n",
      "2025-06-10 02:16:08 - INFO - Processing batch 1/1300\n",
      "2025-06-10 02:16:08 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "2025-06-10 02:16:09 - INFO - Batch 1/1300 - Loss: 0.7436 - Avg batch time: 1.20s\n",
      "2025-06-10 02:16:10 - INFO - Processing batch 11/1300\n",
      "2025-06-10 02:16:10 - INFO - Batch 11/1300 - Loss: 0.6880 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:12 - INFO - Processing batch 21/1300\n",
      "2025-06-10 02:16:12 - INFO - Batch 21/1300 - Loss: 0.6118 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:13 - INFO - Processing batch 31/1300\n",
      "2025-06-10 02:16:13 - INFO - Batch 31/1300 - Loss: 0.6326 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:15 - INFO - Processing batch 41/1300\n",
      "2025-06-10 02:16:15 - INFO - Batch 41/1300 - Loss: 0.4225 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:16 - INFO - Processing batch 51/1300\n",
      "2025-06-10 02:16:16 - INFO - Batch 51/1300 - Loss: 0.2700 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:18 - INFO - Processing batch 61/1300\n",
      "2025-06-10 02:16:18 - INFO - Batch 61/1300 - Loss: 0.5711 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:19 - INFO - Processing batch 71/1300\n",
      "2025-06-10 02:16:20 - INFO - Batch 71/1300 - Loss: 0.4019 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:21 - INFO - Processing batch 81/1300\n",
      "2025-06-10 02:16:21 - INFO - Batch 81/1300 - Loss: 0.5504 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:22 - INFO - Processing batch 91/1300\n",
      "2025-06-10 02:16:23 - INFO - Batch 91/1300 - Loss: 0.4022 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:24 - INFO - Processing batch 101/1300\n",
      "2025-06-10 02:16:24 - INFO - Batch 101/1300 - Loss: 0.3905 - Avg batch time: 0.16s\n",
      "2025-06-10 02:16:26 - INFO - Processing batch 111/1300\n",
      "2025-06-10 02:16:26 - INFO - Batch 111/1300 - Loss: 0.3824 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:27 - INFO - Processing batch 121/1300\n",
      "2025-06-10 02:16:27 - INFO - Batch 121/1300 - Loss: 0.3617 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:29 - INFO - Processing batch 131/1300\n",
      "2025-06-10 02:16:29 - INFO - Batch 131/1300 - Loss: 0.5789 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:30 - INFO - Processing batch 141/1300\n",
      "2025-06-10 02:16:30 - INFO - Batch 141/1300 - Loss: 0.4974 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:32 - INFO - Processing batch 151/1300\n",
      "2025-06-10 02:16:32 - INFO - Batch 151/1300 - Loss: 0.6240 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:33 - INFO - Processing batch 161/1300\n",
      "2025-06-10 02:16:33 - INFO - Batch 161/1300 - Loss: 0.1866 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:35 - INFO - Processing batch 171/1300\n",
      "2025-06-10 02:16:35 - INFO - Batch 171/1300 - Loss: 0.4901 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:36 - INFO - Processing batch 181/1300\n",
      "2025-06-10 02:16:37 - INFO - Batch 181/1300 - Loss: 0.5849 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:38 - INFO - Processing batch 191/1300\n",
      "2025-06-10 02:16:38 - INFO - Batch 191/1300 - Loss: 0.6789 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:40 - INFO - Processing batch 201/1300\n",
      "2025-06-10 02:16:40 - INFO - Batch 201/1300 - Loss: 0.4578 - Avg batch time: 0.16s\n",
      "2025-06-10 02:16:41 - INFO - Processing batch 211/1300\n",
      "2025-06-10 02:16:41 - INFO - Batch 211/1300 - Loss: 0.3264 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:43 - INFO - Processing batch 221/1300\n",
      "2025-06-10 02:16:43 - INFO - Batch 221/1300 - Loss: 0.1561 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:44 - INFO - Processing batch 231/1300\n",
      "2025-06-10 02:16:44 - INFO - Batch 231/1300 - Loss: 0.3240 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:46 - INFO - Processing batch 241/1300\n",
      "2025-06-10 02:16:46 - INFO - Batch 241/1300 - Loss: 0.8409 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:47 - INFO - Processing batch 251/1300\n",
      "2025-06-10 02:16:47 - INFO - Batch 251/1300 - Loss: 0.2416 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:49 - INFO - Processing batch 261/1300\n",
      "2025-06-10 02:16:49 - INFO - Batch 261/1300 - Loss: 0.2275 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:50 - INFO - Processing batch 271/1300\n",
      "2025-06-10 02:16:50 - INFO - Batch 271/1300 - Loss: 0.9489 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:52 - INFO - Processing batch 281/1300\n",
      "2025-06-10 02:16:52 - INFO - Batch 281/1300 - Loss: 0.1851 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:53 - INFO - Processing batch 291/1300\n",
      "2025-06-10 02:16:54 - INFO - Batch 291/1300 - Loss: 0.5684 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:55 - INFO - Processing batch 301/1300\n",
      "2025-06-10 02:16:55 - INFO - Batch 301/1300 - Loss: 1.2919 - Avg batch time: 0.16s\n",
      "2025-06-10 02:16:56 - INFO - Processing batch 311/1300\n",
      "2025-06-10 02:16:57 - INFO - Batch 311/1300 - Loss: 0.3059 - Avg batch time: 0.15s\n",
      "2025-06-10 02:16:58 - INFO - Processing batch 321/1300\n",
      "2025-06-10 02:16:58 - INFO - Batch 321/1300 - Loss: 0.5326 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:00 - INFO - Processing batch 331/1300\n",
      "2025-06-10 02:17:00 - INFO - Batch 331/1300 - Loss: 0.5983 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:01 - INFO - Processing batch 341/1300\n",
      "2025-06-10 02:17:01 - INFO - Batch 341/1300 - Loss: 0.4475 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:03 - INFO - Processing batch 351/1300\n",
      "2025-06-10 02:17:03 - INFO - Batch 351/1300 - Loss: 0.6015 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:04 - INFO - Processing batch 361/1300\n",
      "2025-06-10 02:17:04 - INFO - Batch 361/1300 - Loss: 0.3351 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:06 - INFO - Processing batch 371/1300\n",
      "2025-06-10 02:17:06 - INFO - Batch 371/1300 - Loss: 0.5484 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:07 - INFO - Processing batch 381/1300\n",
      "2025-06-10 02:17:07 - INFO - Batch 381/1300 - Loss: 0.7274 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:09 - INFO - Processing batch 391/1300\n",
      "2025-06-10 02:17:09 - INFO - Batch 391/1300 - Loss: 0.5900 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:10 - INFO - Processing batch 401/1300\n",
      "2025-06-10 02:17:11 - INFO - Batch 401/1300 - Loss: 0.4530 - Avg batch time: 0.16s\n",
      "2025-06-10 02:17:12 - INFO - Processing batch 411/1300\n",
      "2025-06-10 02:17:12 - INFO - Batch 411/1300 - Loss: 1.0094 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:14 - INFO - Processing batch 421/1300\n",
      "2025-06-10 02:17:14 - INFO - Batch 421/1300 - Loss: 0.5554 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:15 - INFO - Processing batch 431/1300\n",
      "2025-06-10 02:17:15 - INFO - Batch 431/1300 - Loss: 0.8327 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:17 - INFO - Processing batch 441/1300\n",
      "2025-06-10 02:17:17 - INFO - Batch 441/1300 - Loss: 0.0515 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:18 - INFO - Processing batch 451/1300\n",
      "2025-06-10 02:17:18 - INFO - Batch 451/1300 - Loss: 1.0993 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:20 - INFO - Processing batch 461/1300\n",
      "2025-06-10 02:17:20 - INFO - Batch 461/1300 - Loss: 0.1894 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:21 - INFO - Processing batch 471/1300\n",
      "2025-06-10 02:17:21 - INFO - Batch 471/1300 - Loss: 0.1884 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:23 - INFO - Processing batch 481/1300\n",
      "2025-06-10 02:17:23 - INFO - Batch 481/1300 - Loss: 0.1497 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:24 - INFO - Processing batch 491/1300\n",
      "2025-06-10 02:17:24 - INFO - Batch 491/1300 - Loss: 0.2412 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:26 - INFO - Processing batch 501/1300\n",
      "2025-06-10 02:17:26 - INFO - Batch 501/1300 - Loss: 0.4952 - Avg batch time: 0.16s\n",
      "2025-06-10 02:17:27 - INFO - Processing batch 511/1300\n",
      "2025-06-10 02:17:28 - INFO - Batch 511/1300 - Loss: 0.5045 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:29 - INFO - Processing batch 521/1300\n",
      "2025-06-10 02:17:29 - INFO - Batch 521/1300 - Loss: 0.2800 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:30 - INFO - Processing batch 531/1300\n",
      "2025-06-10 02:17:31 - INFO - Batch 531/1300 - Loss: 0.7698 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:32 - INFO - Processing batch 541/1300\n",
      "2025-06-10 02:17:32 - INFO - Batch 541/1300 - Loss: 0.3363 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:34 - INFO - Processing batch 551/1300\n",
      "2025-06-10 02:17:34 - INFO - Batch 551/1300 - Loss: 0.5765 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:35 - INFO - Processing batch 561/1300\n",
      "2025-06-10 02:17:35 - INFO - Batch 561/1300 - Loss: 0.1529 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:37 - INFO - Processing batch 571/1300\n",
      "2025-06-10 02:17:37 - INFO - Batch 571/1300 - Loss: 0.4370 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:38 - INFO - Processing batch 581/1300\n",
      "2025-06-10 02:17:38 - INFO - Batch 581/1300 - Loss: 0.3030 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:40 - INFO - Processing batch 591/1300\n",
      "2025-06-10 02:17:40 - INFO - Batch 591/1300 - Loss: 0.5416 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:41 - INFO - Processing batch 601/1300\n",
      "2025-06-10 02:17:41 - INFO - Batch 601/1300 - Loss: 0.5697 - Avg batch time: 0.16s\n",
      "2025-06-10 02:17:43 - INFO - Processing batch 611/1300\n",
      "2025-06-10 02:17:43 - INFO - Batch 611/1300 - Loss: 0.2424 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:44 - INFO - Processing batch 621/1300\n",
      "2025-06-10 02:17:45 - INFO - Batch 621/1300 - Loss: 0.6590 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:46 - INFO - Processing batch 631/1300\n",
      "2025-06-10 02:17:46 - INFO - Batch 631/1300 - Loss: 0.5866 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:47 - INFO - Processing batch 641/1300\n",
      "2025-06-10 02:17:48 - INFO - Batch 641/1300 - Loss: 0.5371 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:49 - INFO - Processing batch 651/1300\n",
      "2025-06-10 02:17:49 - INFO - Batch 651/1300 - Loss: 0.3914 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:51 - INFO - Processing batch 661/1300\n",
      "2025-06-10 02:17:51 - INFO - Batch 661/1300 - Loss: 0.6586 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:52 - INFO - Processing batch 671/1300\n",
      "2025-06-10 02:17:52 - INFO - Batch 671/1300 - Loss: 0.2620 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:54 - INFO - Processing batch 681/1300\n",
      "2025-06-10 02:17:54 - INFO - Batch 681/1300 - Loss: 0.4298 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:55 - INFO - Processing batch 691/1300\n",
      "2025-06-10 02:17:55 - INFO - Batch 691/1300 - Loss: 0.6150 - Avg batch time: 0.15s\n",
      "2025-06-10 02:17:57 - INFO - Processing batch 701/1300\n",
      "2025-06-10 02:17:57 - INFO - Batch 701/1300 - Loss: 0.4005 - Avg batch time: 0.16s\n",
      "2025-06-10 02:17:58 - INFO - Processing batch 711/1300\n",
      "2025-06-10 02:17:58 - INFO - Batch 711/1300 - Loss: 0.2674 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:00 - INFO - Processing batch 721/1300\n",
      "2025-06-10 02:18:00 - INFO - Batch 721/1300 - Loss: 1.0603 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:01 - INFO - Processing batch 731/1300\n",
      "2025-06-10 02:18:02 - INFO - Batch 731/1300 - Loss: 0.7939 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:03 - INFO - Processing batch 741/1300\n",
      "2025-06-10 02:18:03 - INFO - Batch 741/1300 - Loss: 0.6134 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:04 - INFO - Processing batch 751/1300\n",
      "2025-06-10 02:18:05 - INFO - Batch 751/1300 - Loss: 0.3157 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:06 - INFO - Processing batch 761/1300\n",
      "2025-06-10 02:18:06 - INFO - Batch 761/1300 - Loss: 0.4393 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:08 - INFO - Processing batch 771/1300\n",
      "2025-06-10 02:18:08 - INFO - Batch 771/1300 - Loss: 1.0434 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:09 - INFO - Processing batch 781/1300\n",
      "2025-06-10 02:18:09 - INFO - Batch 781/1300 - Loss: 0.5386 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:11 - INFO - Processing batch 791/1300\n",
      "2025-06-10 02:18:11 - INFO - Batch 791/1300 - Loss: 0.5426 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:12 - INFO - Processing batch 801/1300\n",
      "2025-06-10 02:18:12 - INFO - Batch 801/1300 - Loss: 0.3401 - Avg batch time: 0.16s\n",
      "2025-06-10 02:18:14 - INFO - Processing batch 811/1300\n",
      "2025-06-10 02:18:14 - INFO - Batch 811/1300 - Loss: 0.3431 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:15 - INFO - Processing batch 821/1300\n",
      "2025-06-10 02:18:15 - INFO - Batch 821/1300 - Loss: 0.5810 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:17 - INFO - Processing batch 831/1300\n",
      "2025-06-10 02:18:17 - INFO - Batch 831/1300 - Loss: 0.4961 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:18 - INFO - Processing batch 841/1300\n",
      "2025-06-10 02:18:18 - INFO - Batch 841/1300 - Loss: 0.4006 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:20 - INFO - Processing batch 851/1300\n",
      "2025-06-10 02:18:20 - INFO - Batch 851/1300 - Loss: 0.9620 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:21 - INFO - Processing batch 861/1300\n",
      "2025-06-10 02:18:22 - INFO - Batch 861/1300 - Loss: 0.1168 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:23 - INFO - Processing batch 871/1300\n",
      "2025-06-10 02:18:23 - INFO - Batch 871/1300 - Loss: 0.3043 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:24 - INFO - Processing batch 881/1300\n",
      "2025-06-10 02:18:25 - INFO - Batch 881/1300 - Loss: 1.2031 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:26 - INFO - Processing batch 891/1300\n",
      "2025-06-10 02:18:26 - INFO - Batch 891/1300 - Loss: 0.8183 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:28 - INFO - Processing batch 901/1300\n",
      "2025-06-10 02:18:28 - INFO - Batch 901/1300 - Loss: 0.3730 - Avg batch time: 0.16s\n",
      "2025-06-10 02:18:29 - INFO - Processing batch 911/1300\n",
      "2025-06-10 02:18:29 - INFO - Batch 911/1300 - Loss: 0.5357 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:31 - INFO - Processing batch 921/1300\n",
      "2025-06-10 02:18:31 - INFO - Batch 921/1300 - Loss: 0.2373 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:32 - INFO - Processing batch 931/1300\n",
      "2025-06-10 02:18:32 - INFO - Batch 931/1300 - Loss: 1.5358 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:34 - INFO - Processing batch 941/1300\n",
      "2025-06-10 02:18:34 - INFO - Batch 941/1300 - Loss: 0.1316 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:35 - INFO - Processing batch 951/1300\n",
      "2025-06-10 02:18:36 - INFO - Batch 951/1300 - Loss: 0.6645 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:37 - INFO - Processing batch 961/1300\n",
      "2025-06-10 02:18:37 - INFO - Batch 961/1300 - Loss: 0.5105 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:38 - INFO - Processing batch 971/1300\n",
      "2025-06-10 02:18:39 - INFO - Batch 971/1300 - Loss: 0.6647 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:40 - INFO - Processing batch 981/1300\n",
      "2025-06-10 02:18:40 - INFO - Batch 981/1300 - Loss: 1.0961 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:42 - INFO - Processing batch 991/1300\n",
      "2025-06-10 02:18:42 - INFO - Batch 991/1300 - Loss: 0.8804 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:43 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 02:18:43 - INFO - Batch 1001/1300 - Loss: 0.6910 - Avg batch time: 0.16s\n",
      "2025-06-10 02:18:45 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 02:18:45 - INFO - Batch 1011/1300 - Loss: 0.1606 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:46 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 02:18:46 - INFO - Batch 1021/1300 - Loss: 0.2837 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:48 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 02:18:48 - INFO - Batch 1031/1300 - Loss: 0.0612 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:49 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 02:18:49 - INFO - Batch 1041/1300 - Loss: 0.1528 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:51 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 02:18:51 - INFO - Batch 1051/1300 - Loss: 1.2319 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:52 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 02:18:53 - INFO - Batch 1061/1300 - Loss: 0.3152 - Avg batch time: 0.15s\n",
      "2025-06-10 02:18:54 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 02:18:54 - INFO - Batch 1071/1300 - Loss: 0.3754 - Avg batch time: 0.15s\n",
      "Exception in thread Thread-8 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/ssoft/spack/syrah/v2/opt/spack/linux-rhel9-skylake_avx512/gcc-11.3.0/python-3.10.4-4cxk6upcdoypsygwvcoa7e5vfj4amry2/lib/python3.10/threading.py\", line 1009, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ldibello/venvs/neuro/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/ssoft/spack/syrah/v2/opt/spack/linux-rhel9-skylake_avx512/gcc-11.3.0/python-3.10.4-4cxk6upcdoypsygwvcoa7e5vfj4amry2/lib/python3.10/threading.py\", line 946, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 59, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 35, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/ssoft/spack/syrah/v2/opt/spack/linux-rhel9-skylake_avx512/gcc-11.3.0/python-3.10.4-4cxk6upcdoypsygwvcoa7e5vfj4amry2/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/ssoft/spack/syrah/v2/opt/spack/linux-rhel9-skylake_avx512/gcc-11.3.0/python-3.10.4-4cxk6upcdoypsygwvcoa7e5vfj4amry2/lib/python3.10/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/ssoft/spack/syrah/v2/opt/spack/linux-rhel9-skylake_avx512/gcc-11.3.0/python-3.10.4-4cxk6upcdoypsygwvcoa7e5vfj4amry2/lib/python3.10/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/ssoft/spack/syrah/v2/opt/spack/linux-rhel9-skylake_avx512/gcc-11.3.0/python-3.10.4-4cxk6upcdoypsygwvcoa7e5vfj4amry2/lib/python3.10/multiprocessing/connection.py\", line 513, in Client\n",
      "    answer_challenge(c, authkey)\n",
      "  File \"/ssoft/spack/syrah/v2/opt/spack/linux-rhel9-skylake_avx512/gcc-11.3.0/python-3.10.4-4cxk6upcdoypsygwvcoa7e5vfj4amry2/lib/python3.10/multiprocessing/connection.py\", line 757, in answer_challenge\n",
      "    message = connection.recv_bytes(256)         # reject large message\n",
      "  File \"/ssoft/spack/syrah/v2/opt/spack/linux-rhel9-skylake_avx512/gcc-11.3.0/python-3.10.4-4cxk6upcdoypsygwvcoa7e5vfj4amry2/lib/python3.10/multiprocessing/connection.py\", line 221, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/ssoft/spack/syrah/v2/opt/spack/linux-rhel9-skylake_avx512/gcc-11.3.0/python-3.10.4-4cxk6upcdoypsygwvcoa7e5vfj4amry2/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/ssoft/spack/syrah/v2/opt/spack/linux-rhel9-skylake_avx512/gcc-11.3.0/python-3.10.4-4cxk6upcdoypsygwvcoa7e5vfj4amry2/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Epochs:   1%|â–Š                                                                                  | 1/100 [02:47<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhybrid\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcnn_bilstm_gcn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EEGCNNBiLSTMGCN\n\u001b[1;32m      3\u001b[0m train_context \u001b[38;5;241m=\u001b[39m training_context\u001b[38;5;241m.\u001b[39mswitch_to(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspatial\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mk_fold_train_shorthand\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEEGCNNBiLSTMGCN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcnn_dropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# slightly higher dropout to avoid overfitting\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlstm_hidden_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlstm_out_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlstm_dropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# slightly higher dropout to avoid overfitting\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder_use_batch_norm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder_use_layer_norm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Parameters for the EEGGCN (graph neural network)\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mout_channels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpooling_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgcn_use_batch_norm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_conv_layers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgcn_dropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# slightly higher dropout to avoid overfitting\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_channels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m19\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_graph_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHECKPOINT_ROOT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcnn_bilstm_gcn_signal_k_fold_.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# use training loop for GNN models\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# lower batch size (GPU poor)\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# log to wandb\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 55\u001b[0m, in \u001b[0;36mk_fold_train_shorthand\u001b[0;34m(model_class, model_parameters, save_path, use_gnn, log_wandb, batch_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;66;03m# force import to avoid bug\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m aggregated_train_history, aggregated_val_history, fold_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_k_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# dataset to use\u001b[39;49;00m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_tr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclips\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# train models\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# optimizer\u001b[39;49;00m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBCEWithLogitsLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbetas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# scheduler\u001b[39;49;00m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfactor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hidden attribute\u001b[39;49;00m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m plot_training_loss(aggregated_train_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], aggregated_val_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:1220\u001b[0m, in \u001b[0;36mtrain_k_fold\u001b[0;34m(dataset, labels, model_class, model_kwargs, criterion, optimizer_class, optimizer_kwargs, device, save_dir, k_folds, stratified, scheduler_class, scheduler_kwargs, monitor, patience, num_epochs, grad_clip, batch_size, use_gnn, wandb_config, wandb_project, log_wandb, random_state)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# Train this fold\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1220\u001b[0m     train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_clip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Always overwrite for k-fold\u001b[39;49;00m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_wandb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_run_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_wandb_run_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtry_load_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Don't load checkpoint for k-fold\u001b[39;49;00m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;66;03m# Get best scores for this fold\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m     best_train_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(train_history\u001b[38;5;241m.\u001b[39mget(monitor\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m), [\u001b[38;5;241m0\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m monitor \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(train_history\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)]))\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:490\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, wandb_config, wandb_project, wandb_run_name, log_wandb, try_load_checkpoint, original_dataset)\u001b[0m\n\u001b[1;32m    487\u001b[0m probs \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39msigmoid()\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m# [batch_size, 1] -> [batch_size]\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# Validate probability ranges\u001b[39;00m\n\u001b[0;32m--> 490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39many(probs \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    491\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Probabilities outside [0,1] range detected. Clamping values.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    492\u001b[0m     probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(probs, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%aimport src.layers.hybrid.cnn_bilstm_gcn\n",
    "from src.layers.hybrid.cnn_bilstm_gcn import EEGCNNBiLSTMGCN\n",
    "train_context = training_context.switch_to('spatial')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGCNNBiLSTMGCN,\n",
    "    model_parameters={\n",
    "        # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "        \"cnn_dropout\": 0.25, # slightly higher dropout to avoid overfitting\n",
    "        \"lstm_hidden_dim\": 128,\n",
    "        \"lstm_out_dim\": 128,\n",
    "        \"lstm_dropout\": 0.25, # slightly higher dropout to avoid overfitting\n",
    "        \"encoder_use_batch_norm\": True,\n",
    "        \"encoder_use_layer_norm\": False,\n",
    "        # Parameters for the EEGGCN (graph neural network)\n",
    "        \"hidden_dim\": 128,\n",
    "        \"out_channels\": 96,\n",
    "        \"pooling_type\": \"max\",\n",
    "        \"gcn_use_batch_norm\": True,\n",
    "        \"num_conv_layers\": 4,\n",
    "        \"gcn_dropout\": 0.6, # slightly higher dropout to avoid overfitting\n",
    "        \"num_channels\": 19,\n",
    "        \"use_graph_features\": False\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_signal_k_fold_.pt\",\n",
    "    use_gnn=True, # use training loop for GNN models\n",
    "    batch_size=8, # lower batch size (GPU poor)\n",
    "    log_wandb=True, # log to wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "177fedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.utils.train\n",
    "def wrap_gnn_train(model, save_path):\n",
    "    global graph_training_context\n",
    "    if 'graph_training_context' not in globals():\n",
    "        raise ValueError(\"Graph training context is not initialized. Please initialize it before calling this function.\")\n",
    "    if not isinstance(graph_training_context, TrainingContext):\n",
    "        raise ValueError(\"graph_training_context must be an instance of TrainingContext.\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "    # optimizer = Lion(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5)\n",
    "    loss = nn.BCEWithLogitsLoss()  # Not weighted as we use a balanced sampler!\n",
    "\n",
    "    # train model\n",
    "    train_history, val_history = train_model(\n",
    "        wandb_config=None,\n",
    "        model=model,\n",
    "        train_loader=graph_training_context.train_loader,\n",
    "        val_loader=graph_training_context.val_loader,\n",
    "        criterion=loss,\n",
    "        scheduler=scheduler,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=config[\"epochs\"],\n",
    "        patience=config[\"patience\"],\n",
    "        save_path=save_path,\n",
    "        use_gnn=True,\n",
    "        # hidden attribute\n",
    "        try_load_checkpoint=True,\n",
    "    )\n",
    "    plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720925b0",
   "metadata": {},
   "source": [
    "### Test 3 - First breakthrough model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036835f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm .checkpoints/cnn_bilstm_gcn_test_3_correlation_test_mean_pooling.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a43eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_test_best_model_{DATASET_TYPE}_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm = True,\n",
    "    encoder_use_layer_norm = False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim = 192,\n",
    "    out_channels = 128,\n",
    "    pooling_type = \"max\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_conv_layers = 4,\n",
    "    gcn_dropout = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    "    use_graph_features=False\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a5bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "from src.layers.hybrid.cnn_bilstm_gat import EEGCNNBiLSTMGAT\n",
    "\n",
    "\n",
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gat_test_3_{DATASET_TYPE}_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGAT(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout=0.25,\n",
    "    lstm_hidden_dim=128,\n",
    "    lstm_out_dim=128,\n",
    "    lstm_dropout=0.25,\n",
    "    encoder_use_batch_norm=True,\n",
    "    encoder_use_layer_norm=False,\n",
    "    # Parameters for the GAT (graph attention network)\n",
    "    hidden_dim=128,\n",
    "    out_channels=96,\n",
    "    pooling_type=\"mean\",\n",
    "    gat_use_batch_norm=True,\n",
    "    num_conv_layers=3,\n",
    "    gat_dropout=0.5,\n",
    "    gat_heads=4,  # Number of attention heads\n",
    "    num_channels=19,\n",
    "    # Graph features configuration\n",
    "    graph_feature_dim=graph_feature_dim,\n",
    "    use_graph_features=False if DATASET_TYPE == 'spatial' else True,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_test_3_{DATASET_TYPE}_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim = 128,\n",
    "    out_channels = 96,\n",
    "    pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_conv_layers = 3,\n",
    "    gcn_dropout = 0.5,\n",
    "    num_channels = 19,\n",
    "    # enable graph features\n",
    "    # NOTE: using graph level features gives worse results with spatial dataset\n",
    "    use_graph_features=False if DATASET_TYPE == 'spatial' else True,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11afb6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_test_3_{DATASET_TYPE}_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout=0.25,\n",
    "    lstm_hidden_dim=128,\n",
    "    lstm_out_dim=128,\n",
    "    lstm_dropout=0.25,\n",
    "    encoder_use_batch_norm=True,\n",
    "    encoder_use_layer_norm=False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim=128,\n",
    "    out_channels=128,\n",
    "    pooling_type=\"mean\",\n",
    "    gcn_use_batch_norm=True,\n",
    "    num_conv_layers=3,\n",
    "    gcn_dropout=0.5,\n",
    "    num_channels=19,\n",
    "    use_graph_features=True\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424f5b8b",
   "metadata": {},
   "source": [
    "### Test 4 - Smaller CGN output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_4.pt\"\n",
    "\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 64,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 3,\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eaaf2d",
   "metadata": {},
   "source": [
    "### Test 5 - Smaller GCN output channels + increased embedding length + Deeper GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1335c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_5.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 64,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da60626",
   "metadata": {},
   "source": [
    "\n",
    "### Test 6: slighly bigger GCN output channels\n",
    ">[HIGHEST F1 SCORE EVER RECORDED]\n",
    "```\n",
    "âœ… Checkpoint loaded. Resuming from epoch 33. Best 'val_f1' score: 0.7346\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_6.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 96,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad429a",
   "metadata": {},
   "source": [
    "### Test 7B: Alternative architecture to improve generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f882bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_8.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.35, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.35, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 96,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b4f3c",
   "metadata": {},
   "source": [
    "### Test 7C: slightly bigger GCN layers\n",
    "\n",
    "BEST MODEL YET!\n",
    "\n",
    "(SPATIAL FEATURES!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_best_model_correlation_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm = True,\n",
    "    encoder_use_layer_norm = False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim = 192,\n",
    "    out_channels = 128,\n",
    "    pooling_type = \"max\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_conv_layers = 4,\n",
    "    gcn_dropout = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    "    graph_feature_dim=graph_feature_dim,\n",
    "    use_graph_features=False if DATASET_TYPE == 'spatial' else True,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6355457",
   "metadata": {},
   "source": [
    "### Test 7D: even bigger GCN layers\n",
    "\n",
    "Comparable performance to best model. We might need to increase the number of GCN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed9d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_even_bigger.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm = True,\n",
    "    encoder_use_layer_norm = False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 224,\n",
    "    gcn_out_channels = 192,\n",
    "    gcn_pooling_type = \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50956294",
   "metadata": {},
   "source": [
    "### Test 7E: increased number of GCN layers\n",
    "\n",
    "Assumption: the previous model was unable to learn enough, maybe the GCN was unable to capture\n",
    "\n",
    "```\n",
    "Epochs:   9%| | 9/100 [17:54<3:23:31, 134.20s/it, train_loss=0.4532, val_loss=0.3489, best_val_f1=0.6695, lr=5.00e-05, b2025-06-07 17:01:05 - INFO - \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_even_more_bigger.pt\"\n",
    "model_generalizable_even_more_bigger = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 224,\n",
    "    gcn_out_channels = 192,\n",
    "    gcn_num_layers = 5,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08442186",
   "metadata": {},
   "source": [
    "### Test 7F: Increased number of BiLSTM layers + Test 7E architecture\n",
    "\n",
    "Assumpion: we saw a drammatical increase in accuracy by increasing the number of GCN layers. This hints that the model was now able to learn the most from the embeddings. To improve the performance even further without having to increase the number of GCN layers even more (overall reduce complexity, improve generalization), we will try to increase the number of BiLSTM layers. \n",
    "\n",
    "Using multiple BiLSTM layers will allow embeddings to be processed in a more complex way, potentially capturing more intricate relationships in the data. The GCN layers will take care of the graph structure, while the BiLSTM layers will enhance the temporal dependencies and relationships in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b963a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_even_more_bigger.pt\"\n",
    "model_generalizable_even_more_bigger = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    lstm_num_layers = 2,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 224,\n",
    "    gcn_out_channels = 192,\n",
    "    gcn_num_layers = 5,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663332c",
   "metadata": {},
   "source": [
    "```\n",
    "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]2025-06-07 18:55:16 - INFO -\n",
    "Epochs:   2%| | 2/100 [04:35<7:29:19, 275.10s/it, train_loss=0.6212, val_loss=0.4619, best_val_f1=0.4055, lr=1.00e-04, b2025-06-07 18:59:51 - INFO -\n",
    "Epochs:   3%| | 3/100 [09:09<7:23:49, 274.53s/it, train_loss=0.5819, val_loss=0.4295, best_val_f1=0.4055, lr=1.00e-04, b2025-06-07 19:04:25 - INFO -\n",
    "Epochs:   4%| | 4/100 [13:42<7:18:31, 274.08s/it, train_loss=0.5628, val_loss=0.4437, best_val_f1=0.4055, lr=1.00e-04, b2025-06-07 19:08:59 - INFO -\n",
    "Epochs:   5%| | 5/100 [18:16<7:13:28, 273.78s/it, train_loss=0.5452, val_loss=0.3942, best_val_f1=0.4858, lr=1.00e-04, b2025-06-07 19:13:32 - INFO -\n",
    "Epochs:   6%| | 6/100 [22:49<7:08:41, 273.63s/it, train_loss=0.5334, val_loss=0.4563, best_val_f1=0.4858, lr=1.00e-04, b2025-06-07 19:18:05 - INFO -\n",
    "Epochs:   7%| | 7/100 [27:22<7:04:01, 273.57s/it, train_loss=0.5319, val_loss=0.3738, best_val_f1=0.5137, lr=1.00e-04, b2025-06-07 19:22:39 - INFO -\n",
    "Epochs:   8%| | 8/100 [31:56<6:59:20, 273.48s/it, train_loss=0.5181, val_loss=0.4369, best_val_f1=0.5695, lr=1.00e-04, b2025-06-07 19:27:12 - INFO -\n",
    "Epochs:   9%| | 9/100 [36:29<6:54:50, 273.52s/it, train_loss=0.5220, val_loss=0.4202, best_val_f1=0.5695, lr=1.00e-04, b2025-06-07 19:31:46 - INFO -\n",
    "Epochs:  10%| | 10/100 [41:03<6:50:17, 273.52s/it, train_loss=0.5286, val_loss=0.4167, best_val_f1=0.5695, lr=1.00e-04, 2025-06-07 19:36:19 - INFO -\n",
    "Epochs:  11%| | 11/100 [45:36<6:45:44, 273.53s/it, train_loss=0.5065, val_loss=0.3864, best_val_f1=0.5695, lr=1.00e-04, 2025-06-07 19:40:53 - INFO -\n",
    "Epochs:  12%| | 12/100 [50:10<6:41:03, 273.45s/it, train_loss=0.5158, val_loss=0.5175, best_val_f1=0.5695, lr=5.00e-05, 2025-06-07 19:45:26 - INFO -\n",
    "Epochs:  13%|â–| 13/100 [54:43<6:36:23, 273.37s/it, train_loss=0.5035, val_loss=0.3785, best_val_f1=0.5940, lr=5.00e-05, 2025-06-07 19:49:59 - INFO -\n",
    "Epochs:  14%|â–| 14/100 [59:16<6:31:50, 273.38s/it, train_loss=0.4842, val_loss=0.3838, best_val_f1=0.5981, lr=5.00e-05, 2025-06-07 19:54:33 - INFO -\n",
    "Epochs:  15%|â–| 15/100 [1:03:50<6:27:17, 273.38s/it, train_loss=0.4644, val_loss=0.3493, best_val_f1=0.6106, lr=5.00e-052025-06-07 19:59:06 - INFO -\n",
    "Epochs:  16%|â–| 16/100 [1:08:23<6:22:46, 273.41s/it, train_loss=0.4887, val_loss=0.3737, best_val_f1=0.6106, lr=5.00e-052025-06-07 20:03:39 - INFO -\n",
    "Epochs:  17%|â–| 17/100 [1:12:57<6:18:12, 273.41s/it, train_loss=0.4775, val_loss=0.3565, best_val_f1=0.6106, lr=5.00e-052025-06-07 20:08:13 - INFO -\n",
    "Epochs:  18%|â–| 18/100 [1:17:30<6:13:42, 273.44s/it, train_loss=0.4635, val_loss=0.3704, best_val_f1=0.6106, lr=2.50e-052025-06-07 20:12:46 - INFO -\n",
    "Epochs:  19%|â–| 19/100 [1:22:04<6:09:15, 273.53s/it, train_loss=0.4501, val_loss=0.3635, best_val_f1=0.6131, lr=2.50e-052025-06-07 20:17:20 - INFO -\n",
    "Epochs:  20%|â–| 20/100 [1:26:37<6:04:39, 273.49s/it, train_loss=0.4379, val_loss=0.3638, best_val_f1=0.6179, lr=2.50e-052025-06-07 20:21:53 - INFO -\n",
    "Epochs:  21%|â–| 21/100 [1:31:10<6:00:01, 273.43s/it, train_loss=0.4494, val_loss=0.3543, best_val_f1=0.6179, lr=2.50e-052025-06-07 20:26:27 - INFO -\n",
    "Epochs:  22%|â–| 22/100 [1:35:44<5:55:26, 273.42s/it, train_loss=0.4616, val_loss=0.3616, best_val_f1=0.6659, lr=2.50e-052025-06-07 20:31:00 - INFO -\n",
    "Epochs:  23%|â–| 23/100 [1:40:17<5:50:54, 273.44s/it, train_loss=0.4381, val_loss=0.3532, best_val_f1=0.6659, lr=2.50e-052025-06-07 20:35:34 - INFO -\n",
    "Epochs:  24%|â–| 24/100 [1:44:51<5:46:22, 273.45s/it, train_loss=0.4423, val_loss=0.3635, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:40:07 - INFO -\n",
    "Epochs:  25%|â–Ž| 25/100 [1:49:24<5:41:52, 273.49s/it, train_loss=0.4291, val_loss=0.3473, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:44:41 - INFO -\n",
    "Epochs:  26%|â–Ž| 26/100 [1:53:58<5:37:12, 273.42s/it, train_loss=0.4403, val_loss=0.3380, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:49:14 - INFO -\n",
    "Epochs:  27%|â–Ž| 27/100 [1:58:31<5:32:38, 273.40s/it, train_loss=0.4312, val_loss=0.3374, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:53:47 - INFO -\n",
    "Epochs:  28%|â–Ž| 28/100 [2:03:05<5:28:07, 273.44s/it, train_loss=0.4393, val_loss=0.3441, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:58:21 - INFO -\n",
    "Epochs:  29%|â–Ž| 29/100 [2:07:38<5:23:35, 273.46s/it, train_loss=0.4226, val_loss=0.3392, best_val_f1=0.6659, lr=1.25e-052025-06-07 21:02:54 - INFO -\n",
    "Epochs:  30%|â–Ž| 30/100 [2:12:11<5:19:02, 273.46s/it, train_loss=0.4240, val_loss=0.3525, best_val_f1=0.6659, lr=6.25e-062025-06-07 21:07:28 - INFO -\n",
    "Epochs:  31%|â–Ž| 31/100 [2:16:45<5:14:28, 273.46s/it, train_loss=0.4249, val_loss=0.3492, best_val_f1=0.6659, lr=6.25e-062025-06-07 21:12:01 - INFO -\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_optimized.pt\"\n",
    "model_generalizable_optimized = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 160,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    lstm_num_layers = 2,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 192,\n",
    "    gcn_out_channels = 128,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.5, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c4b3a",
   "metadata": {},
   "source": [
    "### Test 8: Narrow but Deep GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff01da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_narrow_deep_model.pt\"\n",
    "narrow_deep_model = EEGCNNBiLSTMGCN(\n",
    "    # --- Simplify the Temporal Encoder ---\n",
    "    cnn_dropout_prob = 0.2,\n",
    "    lstm_hidden_dim = 64,  # Reduced\n",
    "    lstm_out_dim = 64,     # Reduced\n",
    "    lstm_dropout_prob = 0.2,\n",
    "    # --- Focus on the GCN ---\n",
    "    gcn_hidden_channels = 128, # Keep GCN capacity high\n",
    "    gcn_out_channels = 64,\n",
    "    gcn_num_layers = 5,      # Try going even deeper\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ef8b3",
   "metadata": {},
   "source": [
    "### Test 9: First best model, with wider + deeper GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_new_old_best_model.pt\"\n",
    "new_old_best_model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 128, # from 64 to 128\n",
    "    gcn_num_layers = 4, # from 3 to 4\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351da028",
   "metadata": {},
   "source": [
    "### Best model + attention BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.hybrid.cnn_bilstm_attention_gcn\n",
    "from src.layers.hybrid.cnn_bilstm_attention_gcn import EEGCNNBiLSTMAttentionGNN\n",
    "\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_attention.pt\"\n",
    "model_first_attention = EEGCNNBiLSTMAttentionGNN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 192,\n",
    "    gcn_out_channels = 128,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a8a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from src.utils.train import train_model\n",
    "\n",
    "model = model_small_gcn_bigger_embedding\n",
    "model = model.to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "# optimizer = Lion(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "loss = nn.BCEWithLogitsLoss() # Not weighted as we use a balanced sampler!\n",
    "\n",
    "# empty cache in order to free up VRAM (if available)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# train model\n",
    "train_history, val_history = train_model(\n",
    "    wandb_config=None,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=loss,\n",
    "    scheduler=scheduler,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=config[\"epochs\"],\n",
    "    patience=config[\"patience\"],\n",
    "    save_path=SAVE_PATH,\n",
    "    use_gnn=True,\n",
    "    # hidden attribute\n",
    "    try_load_checkpoint=True,\n",
    ")\n",
    "\n",
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch cuda clear cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63329a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
