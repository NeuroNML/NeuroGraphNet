{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b998b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from torch.utils.data import Subset, WeightedRandomSampler\n",
    "# from torch.utils.data import DataLoader\n",
    "from src.utils.seeder import seed_everything\n",
    "\n",
    "# set seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# create useful constants\n",
    "RANDOM_SEED = 42\n",
    "IS_SCITAS = False # set to True if running on SCITAS cluster\n",
    "LOCAL_DATA_ROOT = Path(\"./data\")\n",
    "DATA_ROOT = Path(\"/home/ogut/data\") if IS_SCITAS else LOCAL_DATA_ROOT\n",
    "CHECKPOINT_ROOT = Path(\"./.checkpoints\")\n",
    "SUBMISSION_ROOT = Path(\"./.submissions\")\n",
    "\n",
    "# create directories if they do not exist\n",
    "CHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# set dataset root\n",
    "seed_everything(RANDOM_SEED)\n",
    "\n",
    "# setup torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45999291",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = DATA_ROOT / \"train\"\n",
    "train_dir_metadata = train_dir / \"segments.parquet\"\n",
    "train_dataset_dir = LOCAL_DATA_ROOT / \"graph_dataset_train\"\n",
    "spatial_distance_file = LOCAL_DATA_ROOT / \"distances_3d.csv\"\n",
    "extracted_features_dir = LOCAL_DATA_ROOT / \"extracted_features\"\n",
    "embeddings_dir =  LOCAL_DATA_ROOT / \"embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "638b7e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd45b398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:19:31] Processing sessions\n",
      "Length of train_dataset: 12993\n",
      " Eliminated IDs:[]\n"
     ]
    }
   ],
   "source": [
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "from src.utils.index import ensure_eeg_multiindex \n",
    "\n",
    "# ----------------- Prepare training data -----------------#\n",
    "clips_tr = pd.read_parquet(train_dir_metadata)\n",
    "\n",
    "# Ensure multiindex is correct\n",
    "clips_tr = ensure_eeg_multiindex(clips_tr)\n",
    "\n",
    "clips_tr = clips_tr[~clips_tr.label.isna()].reset_index()  # Filter NaN values out of clips_tr\n",
    "\n",
    "# dataset settings\n",
    "batch_size = 512\n",
    "selected_features = []\n",
    "embeddings = []\n",
    "edge_strategy = \"spatial\"\n",
    "correlation_threshold = 0.5\n",
    "top_k = None\n",
    "low_bandpass_frequency = 0.5\n",
    "high_bandpass_frequency = 50\n",
    "\n",
    "# additional settings\n",
    "oversampling_power = 1.0\n",
    "\n",
    "# -------------- Dataset definition -------------- #\n",
    "dataset = GraphEEGDataset(\n",
    "    root=train_dataset_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    selected_features_train=selected_features,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    embeddings_train=embeddings,\n",
    "    edge_strategy=edge_strategy,\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file if edge_strategy == \"spatial\" else None\n",
    "    ),\n",
    "    top_k=top_k,\n",
    "    correlation_threshold=correlation_threshold,\n",
    "    force_reprocess=True,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=False,\n",
    "    apply_normalization=False,\n",
    "    sampling_rate=250,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of train_dataset: {len(dataset)}\")\n",
    "print(f' Eliminated IDs:{dataset.ids_to_eliminate}')\n",
    "\n",
    "# Eliminate ids that did not have electrodes above correlation threshols\n",
    "clips_tr = clips_tr[~clips_tr.index.isin(dataset.ids_to_eliminate)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f368519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels before Kfold\n",
      "[1 1 1 ... 1 1 0]\n",
      "[09:20:38] Train labels: 0 -> 8389, 1 -> 2093\n",
      "[09:20:38] Val labels:   0 -> 2087, 1 -> 424\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from src.utils.general_funcs import labels_stats\n",
    "\n",
    "cv = GroupKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "groups = clips_tr.patient.values\n",
    "y = clips_tr[\"label\"].values\n",
    "X = np.zeros(len(y))  # Dummy X (not used); just placeholder for the Kfold\n",
    "train_ids, val_ids = next(cv.split(X, y, groups=groups))  # Just select one split\n",
    "print('Labels before Kfold', flush=True)\n",
    "print(y,flush=True)\n",
    "\n",
    "# Print stats for class 0 and 1\n",
    "labels_stats(y, train_ids, val_ids)\n",
    "\n",
    "# 2. From dataset generate train and val datasets\n",
    "train_dataset = Subset(dataset, train_ids)\n",
    "val_dataset = Subset(dataset, val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6943f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 21\n",
      "Val batches: 5\n"
     ]
    }
   ],
   "source": [
    "# 3. Compute sample weights for oversampling\n",
    "train_labels = [clips_tr.iloc[i][\"label\"] for i in train_ids]\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = (1. / class_counts) ** oversampling_power # Higher weights for not frequent classes\n",
    "sample_weights = [class_weights[label] for label in train_labels] # Assign weight to each sample based on its class\n",
    "\n",
    "# 4. Define sampler\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True) # Still train on N samples per epoch, but instead of sampling uniformly takes more from minority class\n",
    "\n",
    "# Define dataloaders\n",
    "train_loader = GeoDataLoader(train_dataset, batch_size=batch_size, sampler=sampler, shuffle=False)\n",
    "val_loader = GeoDataLoader(val_dataset, batch_size=batch_size)\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1427c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight:tensor([1.5000])\n"
     ]
    }
   ],
   "source": [
    "from src.layers.gat import EEGGAT\n",
    "\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_best_model.pt\"\n",
    "SUBMISSION_PATH = SUBMISSION_ROOT / \"lstm_gnn_submission.csv\"\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"patience\": 5,\n",
    "    \"epochs\": 100,\n",
    "}\n",
    "\n",
    "# build model with current parameters\n",
    "model = EEGGAT(\n",
    "    in_channels = 3000,\n",
    ")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=config[\"patience\"], factor=0.5)\n",
    "\n",
    "adjusted_pos_weight = torch.tensor([1.5], dtype=torch.float32).to(device)\n",
    "print(f'pos_weight:{adjusted_pos_weight}')\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=adjusted_pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41a924c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.utils.train import train_model\n",
    "\n",
    "# train_model(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     criterion=loss_fn,\n",
    "#     optimizer=optimizer,\n",
    "#     device=device,\n",
    "#     num_epochs=config[\"epochs\"],\n",
    "#     patience=config[\"patience\"],\n",
    "#     save_path=SAVE_PATH\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3887a39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "After GAT 1\n",
      "mean: 0.4146, std: 348.7146\n",
      "After GAT 2\n",
      "mean: -0.5427, std: 305.2911\n",
      "After pooling\n",
      "mean: 17.4345, std: 198.3475\n",
      "After GAT 1\n",
      "mean: 0.0724, std: 166.8059\n",
      "After GAT 2\n",
      "mean: -0.3880, std: 100.4480\n",
      "After pooling\n",
      "mean: 8.7432, std: 59.6474\n",
      "After GAT 1\n",
      "mean: 1.1868, std: 202.3258\n",
      "After GAT 2\n",
      "mean: -0.8765, std: 215.1833\n",
      "After pooling\n",
      "mean: 11.8740, std: 130.2765\n",
      "After GAT 1\n",
      "mean: 2.2345, std: 275.3886\n",
      "After GAT 2\n",
      "mean: 0.3252, std: 170.1325\n",
      "After pooling\n",
      "mean: 10.3878, std: 104.4349\n",
      "After GAT 1\n",
      "mean: -0.1660, std: 204.3885\n",
      "After GAT 2\n",
      "mean: -0.8690, std: 196.0014\n",
      "After pooling\n",
      "mean: 11.6791, std: 116.5223\n",
      "After GAT 1\n",
      "mean: 1.9601, std: 252.0050\n",
      "After GAT 2\n",
      "mean: -0.5715, std: 130.4063\n",
      "After pooling\n",
      "mean: 10.0979, std: 83.4311\n",
      "After GAT 1\n",
      "mean: 0.1008, std: 139.0171\n",
      "After GAT 2\n",
      "mean: -0.2727, std: 47.3037\n",
      "After pooling\n",
      "mean: 5.6907, std: 30.1175\n",
      "After GAT 1\n",
      "mean: 0.4784, std: 310.9085\n",
      "After GAT 2\n",
      "mean: -1.8209, std: 272.1334\n",
      "After pooling\n",
      "mean: 13.5985, std: 158.7823\n",
      "After GAT 1\n",
      "mean: -3.5259, std: 374.6566\n",
      "After GAT 2\n",
      "mean: -1.2538, std: 250.3154\n",
      "After pooling\n",
      "mean: 11.3751, std: 160.1573\n",
      "After GAT 1\n",
      "mean: 1.5542, std: 200.6370\n",
      "After GAT 2\n",
      "mean: -0.4954, std: 52.5955\n",
      "After pooling\n",
      "mean: 5.2687, std: 34.7501\n",
      "After GAT 1\n",
      "mean: 0.5659, std: 128.6993\n",
      "After GAT 2\n",
      "mean: -0.8638, std: 112.0235\n",
      "After pooling\n",
      "mean: 5.9481, std: 68.9341\n",
      "After GAT 1\n",
      "mean: 0.6848, std: 124.6068\n",
      "After GAT 2\n",
      "mean: -0.6231, std: 106.9505\n",
      "After pooling\n",
      "mean: 6.6343, std: 64.5040\n",
      "After GAT 1\n",
      "mean: 0.1469, std: 131.0108\n",
      "After GAT 2\n",
      "mean: -1.5157, std: 123.9831\n",
      "After pooling\n",
      "mean: 6.6229, std: 68.0096\n",
      "After GAT 1\n",
      "mean: -0.1453, std: 99.9243\n",
      "After GAT 2\n",
      "mean: -0.4360, std: 29.5657\n",
      "After pooling\n",
      "mean: 3.7225, std: 18.4683\n",
      "After GAT 1\n",
      "mean: 1.1733, std: 129.9348\n",
      "After GAT 2\n",
      "mean: -1.7136, std: 127.9896\n",
      "After pooling\n",
      "mean: 6.3348, std: 80.5040\n",
      "After GAT 1\n",
      "mean: 0.4200, std: 181.0410\n",
      "After GAT 2\n",
      "mean: -1.2467, std: 122.3870\n",
      "After pooling\n",
      "mean: 5.5426, std: 76.8058\n",
      "After GAT 1\n",
      "mean: 1.6462, std: 296.1606\n",
      "After GAT 2\n",
      "mean: -1.7149, std: 117.4931\n",
      "After pooling\n",
      "mean: 6.4178, std: 69.3799\n",
      "After GAT 1\n",
      "mean: 0.3947, std: 242.1456\n",
      "After GAT 2\n",
      "mean: -1.0596, std: 73.2807\n",
      "After pooling\n",
      "mean: 4.0132, std: 44.1688\n",
      "After GAT 1\n",
      "mean: 0.2777, std: 232.8250\n",
      "After GAT 2\n",
      "mean: -0.4825, std: 22.7051\n",
      "After pooling\n",
      "mean: 3.2441, std: 13.8383\n",
      "After GAT 1\n",
      "mean: 1.4291, std: 304.0503\n",
      "After GAT 2\n",
      "mean: -0.6978, std: 50.4414\n",
      "After pooling\n",
      "mean: 3.5713, std: 29.0622\n",
      "After GAT 1\n",
      "mean: -0.2177, std: 193.7739\n",
      "After GAT 2\n",
      "mean: -0.7771, std: 113.9566\n",
      "After pooling\n",
      "mean: 5.8598, std: 77.7789\n",
      "After GAT 1\n",
      "mean: 0.6264, std: 54.5062\n",
      "After GAT 2\n",
      "mean: 1.2222, std: 40.6462\n",
      "After pooling\n",
      "mean: 10.7566, std: 27.8117\n",
      "After GAT 1\n",
      "mean: -0.1582, std: 94.7273\n",
      "After GAT 2\n",
      "mean: 1.2351, std: 70.2694\n",
      "After pooling\n",
      "mean: 13.4232, std: 50.0739\n",
      "After GAT 1\n",
      "mean: 0.7149, std: 64.2891\n",
      "After GAT 2\n",
      "mean: 0.4245, std: 48.7620\n",
      "After pooling\n",
      "mean: 7.3330, std: 35.6913\n",
      "After GAT 1\n",
      "mean: 0.4272, std: 71.0718\n",
      "After GAT 2\n",
      "mean: 0.7990, std: 51.8554\n",
      "After pooling\n",
      "mean: 10.0818, std: 36.0056\n",
      "After GAT 1\n",
      "mean: -0.1853, std: 32.9433\n",
      "After GAT 2\n",
      "mean: 0.4662, std: 21.9464\n",
      "After pooling\n",
      "mean: 5.8968, std: 14.8998\n",
      "Epoch 1 | Train Loss: 1.4263 | Val Loss: 2.0375 | Val F1: 0.2767\n",
      "\n",
      "Confusion Matrix:\n",
      "               Predicted\n",
      "              0       1\n",
      "Actual 0      359    1728\n",
      "Actual 1       88     336\n",
      "Class 1 — Precision: 0.16, Recall: 0.79, F1: 0.27\n",
      "Epoch 1 — Train Loss: 1.4263, Val Loss: 2.0375, Val F1: 0.2767\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.summary['best_f1_score']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mError\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 109\u001b[39m\n\u001b[32m    107\u001b[39m     best_labels = all_labels.copy()\n\u001b[32m    108\u001b[39m     \u001b[38;5;66;03m# Load best stats in wandb\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbest_f1_score\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m = val_f1\n\u001b[32m    110\u001b[39m     wandb.summary[\u001b[33m\"\u001b[39m\u001b[33mf1_score_epoch\u001b[39m\u001b[33m\"\u001b[39m] = epoch\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# ------- Early Stopping ------- #\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/NeuroGraphNet/.venv/lib/python3.13/site-packages/wandb/sdk/lib/preinit.py:17\u001b[39m, in \u001b[36mPreInitObject.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m, value: Any) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m wandb.Error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou must call wandb.init() before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mError\u001b[39m: You must call wandb.init() before wandb.summary['best_f1_score']"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "import wandb\n",
    "\n",
    "from src.utils.general_funcs import confusion_matrix_plot\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_val_f1 = 0\n",
    "best_val_f1_epoch = 0\n",
    "patience = 10\n",
    "counter = 0\n",
    "num_epochs = 100\n",
    "print(\"Training started\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # ------- Training ------- #\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    # for batch in tqdm(train_loader,desc=f\"Epoch {epoch} — Training\" ):\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)  # Move batch to GPU\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index, batch.batch)\n",
    "        loss = loss_fn(\n",
    "            out, batch.y.reshape(-1, 1)\n",
    "        )  # y: [batch_size] ->[batch_size, 1]\n",
    "        loss.backward()\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.grad is not None:\n",
    "        #         print(f\"{name}: grad norm = {param.grad.norm().item():.2e}\")\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_loader)  # Average loss per batch\n",
    "\n",
    "    # ------- Validation ------- #\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)  # Move batch to GPU\n",
    "            out = model(\n",
    "                batch.x, batch.edge_index, batch.batch\n",
    "            )  # batch.batch: [num_nodes_batch] = 19*batch_size -> tells the model which graph each node belongs to\n",
    "            loss = loss_fn(out, batch.y.reshape(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "\n",
    "            probs = torch.sigmoid(out).squeeze()  # [batch_size, 1] -> [batch_size]\n",
    "            preds = (probs > 0.5).int()\n",
    "            all_preds.extend(preds.cpu().numpy().ravel())\n",
    "            all_labels.extend(\n",
    "                batch.y.int().cpu().numpy().ravel()\n",
    "            )  # Labels: stored as float in dataset\n",
    "            # print(f\"Val logits stats — min: {out.min().item():.4f}, max: {out.max().item():.4f}, mean: {out.mean().item():.4f}, std: {out.std().item():.4f}\")\n",
    "            # print(f\"Predictions:{preds.cpu().numpy()}\")\n",
    "            # print(f\"Sigmoid outputs: { torch.sigmoid(out).detach().cpu().numpy()}\")\n",
    "            # print(f\"Labels:{batch.y}\")\n",
    "            \n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)  # Average loss per batch\n",
    "    #scheduler.step(avg_val_loss)\n",
    "    val_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    all_labels = np.array(all_labels).astype(int)\n",
    "    all_preds = np.array(all_preds).astype(int)\n",
    "\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.grad is not None:\n",
    "    #         print(f\"{name} grad mean: {param.grad.abs().mean()}\")\n",
    "    \n",
    "    # Monitor progress\n",
    "    print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Additional metrics\n",
    "\n",
    "    # Confusion matrix\n",
    "    confusion_matrix_plot(all_preds, all_labels)\n",
    "    # Compute metrics per class (0 and 1)\n",
    "    precision = precision_score(all_labels, all_preds, average=None)\n",
    "    recall = recall_score(all_labels, all_preds, average=None)\n",
    "    f1 = f1_score(all_labels, all_preds, average=None)\n",
    "\n",
    "    # Print only for class 1\n",
    "    print(f\"Class 1 — Precision: {precision[1]:.2f}, Recall: {recall[1]:.2f}, F1: {f1[1]:.2f}\")\n",
    "    \n",
    "\n",
    "    # W&B\n",
    "    # wandb.log(\n",
    "    #     {\n",
    "    #         \"epoch\": epoch,\n",
    "    #         \"train_loss\": avg_train_loss,\n",
    "    #         \"val_loss\": avg_val_loss,\n",
    "    #         \"val_f1\": val_f1,\n",
    "    #         \"val_f1_class_1\":f1[1],\n",
    "    #             \"val_f1_class_0\":f1[0]\n",
    "    #     }\n",
    "    # )\n",
    "    print(f\"Epoch {epoch} — Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val F1: {val_f1:.4f}\", flush=True)\n",
    "    # ------- Record best F1 score ------- #\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_val_f1_epoch = epoch\n",
    "        best_preds = all_preds.copy()\n",
    "        best_labels = all_labels.copy()\n",
    "        # Load best stats in wandb\n",
    "        wandb.summary[\"best_f1_score\"] = val_f1\n",
    "        wandb.summary[\"f1_score_epoch\"] = epoch\n",
    "    # ------- Early Stopping ------- #\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        # Save best statistics and model\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        best_state_dict = model.state_dict().copy()  # Save the best model state\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(f\"Best validation F1: {best_val_f1:.4f} at epoch {best_val_f1_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c08a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
