{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d97e92",
   "metadata": {},
   "source": [
    "# NeuroGraphNet\n",
    "\n",
    "*A graph-based deep learning framework for EEG seizure detection, designed to improve accuracy and interpretability by leveraging Graph Neural Networks (GNNs) to capture spatial and temporal brain dynamics.*\n",
    "\n",
    "<hr />\n",
    "\n",
    "This notebook presents different approaches to EEG seizure detection using traditional machine learning and deep learning methods as well as a novel approaches using Graph Neural Networks (GNNs). The dataset used a subset of the TUSZ EEG Seizure dataset, which contains EEG recordings from patients with epilepsy.\n",
    "\n",
    "**Authors**: Luca Di Bello, Guillaume AndrÃ© BÃ©lissent, Abdessalem Ben Ali, Beatriz Izquierdo GonzÃ¡lez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5837c2",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b998b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from src.utils.seeder import seed_everything\n",
    "\n",
    "# set seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# create useful constants\n",
    "RANDOM_SEED = 42\n",
    "IS_SCITAS = True # set to True if running on SCITAS cluster\n",
    "LOCAL_DATA_ROOT = Path(\"./data\")\n",
    "DATA_ROOT = Path(\"/home/ogut/data\") if IS_SCITAS else LOCAL_DATA_ROOT\n",
    "CHECKPOINT_ROOT = Path(\"./.checkpoints\")\n",
    "SUBMISSION_ROOT = Path(\"./.submissions\")\n",
    "\n",
    "# create directories if they do not exist\n",
    "CHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# set dataset root\n",
    "seed_everything(RANDOM_SEED)\n",
    "\n",
    "# setup torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f74a9",
   "metadata": {},
   "source": [
    "## Run feature extraction script\n",
    "\n",
    "In order to run all the models in this notebook, is necessary to run the feature extraction script first. This script extracts features from the EEG signals for both the training and test dataset, saving three files: `X_train.npy`, `y_train.npy`, and `X_test.npy`. The features extracted are the same used in the original paper, which are based on the EEG signals.\n",
    "\n",
    "You can run the script by uncommenting and executing the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d28586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# # execute feature extraction script\n",
    "# process = None\n",
    "# try:\n",
    "#     process = subprocess.Popen([\"python3\", \"scripts/feature_extractor.py\"])\n",
    "#     process.wait()\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"Process interrupted, terminating...\")\n",
    "#     if process:\n",
    "#         process.terminate()\n",
    "#         process.wait()\n",
    "# except Exception as e:\n",
    "#     print(f\"Error occurred: {e}\")\n",
    "#     if process:\n",
    "#         process.terminate()\n",
    "#         process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527547f9",
   "metadata": {},
   "source": [
    "## Define paths\n",
    "\n",
    "The following paths are used to load the required data files and save the results of the models. Make sure to adjust them according to your local setup.\n",
    "\n",
    "The current configuration assumes that the data files are located in a folder named `data` within the current working directory. \n",
    "\n",
    "**NOTE:** to simplify the process on SCITAS cluster, we provide a toggle `IS_SCITAS` to set the paths accordingly (_refer to first cell of the notebook_). If you are running this notebook on your local machine, you can set `IS_SCITAS = False` and adjust the paths as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45999291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacial distance matrix between sensors\n",
    "spatial_distance_file = LOCAL_DATA_ROOT / \"distances_3d.csv\"\n",
    "\n",
    "# absdiff correlation matrix\n",
    "absdiff_correlation_file = LOCAL_DATA_ROOT / \"diff_corr_matrix.csv\"\n",
    "\n",
    "# training data\n",
    "train_dir = DATA_ROOT / \"train\"\n",
    "train_dir_metadata = train_dir / \"segments.parquet\"\n",
    "train_dataset_absdiff_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_absdiff_correlation_train\"\n",
    "train_dataset_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_correlation_train\"\n",
    "train_dataset_spatial_dir = LOCAL_DATA_ROOT / \"graph_dataset_spatial_train\"\n",
    "train_dataset_timeseries_feature_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_features\")\n",
    "train_dataset_timeseries_signal_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_signal\")\n",
    "\n",
    "# test data\n",
    "test_dir = DATA_ROOT / \"test\"\n",
    "test_dir_metadata = test_dir / \"segments.parquet\"\n",
    "test_dataset_absdiff_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_absdiff_correlation_test\"\n",
    "test_dataset_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_correlation_test\"\n",
    "test_dataset_spatial_dir = LOCAL_DATA_ROOT / \"graph_dataset_spatial_test\"\n",
    "test_dataset_timeseries_signal_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_signal\")\n",
    "test_dataset_timeseries_feature_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_features\")\n",
    "\n",
    "# additional features\n",
    "extracted_features_dir = LOCAL_DATA_ROOT / \"extracted_features\"\n",
    "embeddings_dir =  LOCAL_DATA_ROOT / \"embeddings\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f1780",
   "metadata": {},
   "source": [
    "## Loading Train and Test Clips from the Dataset\n",
    "\n",
    "To load patient clips from the dataset, we use the `load_clips` function. This function retrieves EEG signals and labels from the specified paths and returns them as NumPy arrays.\n",
    "\n",
    "Different versions of Pandas may return either a MultiIndex or a single index, even when called with the same parameters. To address this inconsistency, we use the `ensure_eeg_multiindex` function to ensure that the resulting DataFrame has a MultiIndex structure. This is essential for subsequent processing steps.\n",
    "\n",
    "If a MultiIndex is not present, it will be created using the following levels: `patient_id`, `clip_id`, and `channel`. This structure is crucial for organizing the dataset, as EEG signals are grouped by patient, clip, and channel. It also ensures compatibility with existing code that expects this format, such as the `EEGDataset` class from the [seizure-eeg](https://www.piwheels.org/project/seiz-eeg/) package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d93851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.index import ensure_eeg_multiindex \n",
    "\n",
    "# Load clips from datasets\n",
    "clips_tr = pd.read_parquet(train_dir_metadata)\n",
    "clips_tr = ensure_eeg_multiindex(clips_tr)\n",
    "clips_tr['id'] = clips_tr.index.map(lambda x: '_'.join(str(i) for i in x))\n",
    "assert clips_tr.id.nunique() == len(clips_tr), \"There are duplicate IDs\"\n",
    "clips_tr = clips_tr[~clips_tr.label.isna()].reset_index()\n",
    "\n",
    "# Load clips from datasets\n",
    "clips_te = pd.read_parquet(test_dir_metadata)\n",
    "clips_te = ensure_eeg_multiindex(clips_te)\n",
    "clips_te['id'] = clips_te.index.map(lambda x: '_'.join(str(i) for i in x))\n",
    "assert clips_te.id.nunique() == len(clips_te), \"There are duplicate IDs\"\n",
    "clips_te = clips_te.reset_index()\n",
    "\n",
    "# sort in order to maintain the same submission order\n",
    "clips_te = clips_te.sort_values(by=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f7bc0",
   "metadata": {},
   "source": [
    "## Loading Datasets\n",
    "\n",
    "This notebook explores various approaches to EEG seizure detection, requiring multiple dataset variants with distinct preprocessing strategies (e.g., raw EEG signals, extracted features, and diverse graph construction methods). The `GraphEEGDataset` class, a custom implementation of `torch.utils.data.Dataset`, is used to load these datasets based on specified parameters.\n",
    "\n",
    "The `GraphEEGDataset` class is designed to support all preprocessing strategies, including graph-based approaches. It preprocesses EEG data on-the-fly, offering flexibility in data handling and model input preparation. Additionally, it includes a caching mechanism to store preprocessed data on disk. This mechanism ensures that subsequent calls with identical parameters load precomputed data, significantly reducing dataset loading time during repeated runs. This feature has been instrumental in accelerating development and experimentation within this notebook.\n",
    "\n",
    "Specifically, we will load the following datasets:\n",
    "\n",
    "A) **EEG signals**: \n",
    "\n",
    "- **Raw EEG**:\n",
    "\n",
    "    - Feature-based\n",
    "\n",
    "        - Raw EEG signals + signal time-filtering/rereferencing/normalization preprocessing\n",
    "\n",
    "    - Graph-based\n",
    "\n",
    "        - Raw EEG signals + spatial graph construction strategy + signal time-filtering/rereferencing/normalization preprocessing + graph-features\n",
    "\n",
    "        - Raw EEG signals + correlation graph construction strategy (top-k policy with k=10) + signal time-filtering/rereferencing/normalization preprocessing + graph-features\n",
    "\n",
    "        - Raw EEG signals + absolute difference correlation graph construction strategy (top-k policy with k=10) + signal time-filtering/rereferencing/normalization preprocessing + graph-features\n",
    "\n",
    "- **Feature-based EEG**:\n",
    "\n",
    "    - Extracted features + spatial graph construction strategy + signal time-filtering/rereferencing/normalization preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e759ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generael settings\n",
    "\n",
    "# bandpass filter settings (signal time-filtering)\n",
    "low_bandpass_frequency = 0.5\n",
    "high_bandpass_frequency = 50\n",
    "\n",
    "# additional settings\n",
    "# NOTE: the training already fights class imbalance, so this is not used\n",
    "oversampling_power = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef132cd",
   "metadata": {},
   "source": [
    "### Raw-EEG signal datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8110e5",
   "metadata": {},
   "source": [
    "#### A) Spatial graph construction strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a686e9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "2025-06-10 00:53:06 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 00:53:06 - INFO - Dataset parameters:\n",
      "2025-06-10 00:53:06 - INFO -   - Root directory: data/graph_dataset_spatial_train\n",
      "2025-06-10 00:53:06 - INFO -   - Edge strategy: spatial\n",
      "2025-06-10 00:53:06 - INFO -   - Top-k neighbors: None\n",
      "2025-06-10 00:53:06 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 00:53:06 - INFO -   - Force reprocess: False\n",
      "2025-06-10 00:53:06 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 00:53:06 - INFO -   - Segment length: 3000\n",
      "2025-06-10 00:53:06 - INFO -   - Apply filtering: True\n",
      "2025-06-10 00:53:06 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 00:53:06 - INFO -   - Apply normalization: True\n",
      "2025-06-10 00:53:06 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 00:53:06 - INFO -   - Test mode: False\n",
      "2025-06-10 00:53:06 - INFO -   - Extract graph features: False\n",
      "2025-06-10 00:53:06 - INFO -   - Diff Corr Matrix Path: None\n",
      "2025-06-10 00:53:06 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 00:53:06 - INFO - Setting up signal filters...\n",
      "2025-06-10 00:53:06 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 00:53:06 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 00:53:06 - INFO - Loaded 361 spatial distances in 0.01s\n",
      "2025-06-10 00:53:06 - INFO - Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset_spatial_tr: 12993\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_spatial_tr = GraphEEGDataset(\n",
    "    root=train_dataset_spatial_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"spatial\",\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file\n",
    "    ),\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=False,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_spatial_tr: {len(dataset_spatial_tr)}\")\n",
    "print(f' Eliminated IDs: {dataset_spatial_tr.ids_to_eliminate}')\n",
    "clips_spatial_tr = clips_tr[~clips_tr.index.isin(dataset_spatial_tr.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b33fe510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 00:53:06 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 00:53:06 - INFO - Dataset parameters:\n",
      "2025-06-10 00:53:06 - INFO -   - Root directory: data/graph_dataset_spatial_test\n",
      "2025-06-10 00:53:06 - INFO -   - Edge strategy: spatial\n",
      "2025-06-10 00:53:06 - INFO -   - Top-k neighbors: None\n",
      "2025-06-10 00:53:06 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 00:53:06 - INFO -   - Force reprocess: False\n",
      "2025-06-10 00:53:06 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 00:53:06 - INFO -   - Segment length: 3000\n",
      "2025-06-10 00:53:06 - INFO -   - Apply filtering: True\n",
      "2025-06-10 00:53:06 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 00:53:06 - INFO -   - Apply normalization: True\n",
      "2025-06-10 00:53:06 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 00:53:06 - INFO -   - Test mode: True\n",
      "2025-06-10 00:53:06 - INFO -   - Extract graph features: False\n",
      "2025-06-10 00:53:06 - INFO -   - Diff Corr Matrix Path: None\n",
      "2025-06-10 00:53:06 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 00:53:06 - INFO - Setting up signal filters...\n",
      "2025-06-10 00:53:06 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 00:53:06 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 00:53:06 - INFO - Loaded 361 spatial distances in 0.01s\n",
      "2025-06-10 00:53:06 - INFO - Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset_spatial_te: 3614\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "# load training dataset\n",
    "dataset_spatial_te = GraphEEGDataset(\n",
    "    root=test_dataset_spatial_dir,\n",
    "    clips=clips_te,\n",
    "    signal_folder=test_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"spatial\",\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file\n",
    "    ),\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    is_test=True,\n",
    "    # extract graph features\n",
    "    extract_graph_features=False,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_spatial_te: {len(dataset_spatial_te)}\")\n",
    "print(f' Eliminated IDs: {dataset_spatial_te.ids_to_eliminate}')\n",
    "clips_spatial_te = clips_te[~clips_te.index.isin(dataset_spatial_te.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8993f7",
   "metadata": {},
   "source": [
    "#### B) Correlation graph construction strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1472121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "top_k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c3f93ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 00:53:07 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 00:53:07 - INFO - Dataset parameters:\n",
      "2025-06-10 00:53:07 - INFO -   - Root directory: data/graph_dataset_correlation_train\n",
      "2025-06-10 00:53:07 - INFO -   - Edge strategy: correlation\n",
      "2025-06-10 00:53:07 - INFO -   - Top-k neighbors: 5\n",
      "2025-06-10 00:53:07 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 00:53:07 - INFO -   - Force reprocess: False\n",
      "2025-06-10 00:53:07 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 00:53:07 - INFO -   - Segment length: 3000\n",
      "2025-06-10 00:53:07 - INFO -   - Apply filtering: True\n",
      "2025-06-10 00:53:07 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 00:53:07 - INFO -   - Apply normalization: True\n",
      "2025-06-10 00:53:07 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 00:53:07 - INFO -   - Test mode: False\n",
      "2025-06-10 00:53:07 - INFO -   - Extract graph features: True\n",
      "2025-06-10 00:53:07 - INFO -   - Diff Corr Matrix Path: None\n",
      "2025-06-10 00:53:07 - INFO - Initializing graph feature extractor...\n",
      "2025-06-10 00:53:07,110 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 00:53:07 - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 00:53:07 - INFO - Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 00:53:07 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 00:53:07 - INFO - Setting up signal filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "Length of dataset_corr_tr: 12986\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_corr_tr = GraphEEGDataset(\n",
    "    root=train_dataset_correlation_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"correlation\",\n",
    "    spatial_distance_file=None,\n",
    "    top_k=top_k,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None # collect all graph features\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_corr_tr: {len(dataset_corr_tr)}\")\n",
    "print(f' Eliminated IDs: {dataset_corr_tr.ids_to_eliminate}')\n",
    "clips_corr_tr = clips_tr[~clips_tr.index.isin(dataset_corr_tr.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "233887f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 00:53:07 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 00:53:07 - INFO - Dataset parameters:\n",
      "2025-06-10 00:53:07 - INFO -   - Root directory: data/graph_dataset_correlation_test\n",
      "2025-06-10 00:53:07 - INFO -   - Edge strategy: spatial\n",
      "2025-06-10 00:53:07 - INFO -   - Top-k neighbors: None\n",
      "2025-06-10 00:53:07 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 00:53:07 - INFO -   - Force reprocess: False\n",
      "2025-06-10 00:53:07 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 00:53:07 - INFO -   - Segment length: 3000\n",
      "2025-06-10 00:53:07 - INFO -   - Apply filtering: True\n",
      "2025-06-10 00:53:07 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 00:53:07 - INFO -   - Apply normalization: True\n",
      "2025-06-10 00:53:07 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 00:53:07 - INFO -   - Test mode: True\n",
      "2025-06-10 00:53:07 - INFO -   - Extract graph features: True\n",
      "2025-06-10 00:53:07 - INFO -   - Diff Corr Matrix Path: None\n",
      "2025-06-10 00:53:07 - INFO - Initializing graph feature extractor...\n",
      "2025-06-10 00:53:07,191 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 00:53:07 - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 00:53:07 - INFO - Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 00:53:07 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 00:53:07 - INFO - Setting up signal filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 00:53:07 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 00:53:07 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 00:53:07 - INFO - Loaded 361 spatial distances in 0.01s\n",
      "2025-06-10 00:53:07 - INFO - Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset_corr_te: 3614\n",
      " Eliminated IDs:[]\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_corr_te = GraphEEGDataset(\n",
    "    root=test_dataset_correlation_dir,\n",
    "    clips=clips_te,\n",
    "    signal_folder=test_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"spatial\",\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file\n",
    "    ),\n",
    "    top_k=None,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    is_test=True, # NOTE: needed to let the dataset know that is okay to now have labels!\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None # collect all graph features\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_corr_te: {len(dataset_corr_te)}\")\n",
    "print(f' Eliminated IDs:{dataset_corr_te.ids_to_eliminate}')\n",
    "clips_spatial_te = clips_te[~clips_te.index.isin(dataset_corr_te.ids_to_eliminate)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4461766e",
   "metadata": {},
   "source": [
    "#### C) Absolute difference correlation graph construction strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2328a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "top_k = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bf782ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 00:53:07 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 00:53:07 - INFO - Dataset parameters:\n",
      "2025-06-10 00:53:07 - INFO -   - Root directory: data/graph_dataset_absdiff_correlation_train\n",
      "2025-06-10 00:53:07 - INFO -   - Edge strategy: relevance_diff_correlation\n",
      "2025-06-10 00:53:07 - INFO -   - Top-k neighbors: 8\n",
      "2025-06-10 00:53:07 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 00:53:07 - INFO -   - Force reprocess: False\n",
      "2025-06-10 00:53:07 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 00:53:07 - INFO -   - Segment length: 3000\n",
      "2025-06-10 00:53:07 - INFO -   - Apply filtering: True\n",
      "2025-06-10 00:53:07 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 00:53:07 - INFO -   - Apply normalization: True\n",
      "2025-06-10 00:53:07 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 00:53:07 - INFO -   - Test mode: False\n",
      "2025-06-10 00:53:07 - INFO -   - Extract graph features: True\n",
      "2025-06-10 00:53:07 - INFO -   - Diff Corr Matrix Path: data/diff_corr_matrix.csv\n",
      "2025-06-10 00:53:07 - INFO - Edge strategy: relevance_diff_correlation. Loading average correlation matrices.\n",
      "2025-06-10 00:53:07 - INFO - Loaded absolute difference correlation matrix from data/diff_corr_matrix.csv, shape: (19, 19)\n",
      "2025-06-10 00:53:07 - INFO - Initializing graph feature extractor...\n",
      "2025-06-10 00:53:07,343 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 00:53:07 - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 00:53:07 - INFO - Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 00:53:07 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 00:53:07 - INFO - Setting up signal filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "Length of dataset_absdiff_corr_tr: 4646\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_absdiff_corr_tr = GraphEEGDataset(\n",
    "    root=train_dataset_absdiff_correlation_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"relevance_diff_correlation\",\n",
    "    spatial_distance_file=None,\n",
    "    top_k=top_k,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None, # collect all graph features\n",
    "    # settings for absolute difference correlation\n",
    "    diff_corr_matrix_path=absdiff_correlation_file,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_absdiff_corr_tr: {len(dataset_absdiff_corr_tr)}\")\n",
    "print(f' Eliminated IDs: {dataset_absdiff_corr_tr.ids_to_eliminate}')\n",
    "clips_absdiff_corr_tr = clips_tr[~clips_tr.index.isin(dataset_absdiff_corr_tr.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18240a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 00:53:07 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 00:53:07 - INFO - Dataset parameters:\n",
      "2025-06-10 00:53:07 - INFO -   - Root directory: data/graph_dataset_absdiff_correlation_test\n",
      "2025-06-10 00:53:07 - INFO -   - Edge strategy: relevance_diff_correlation\n",
      "2025-06-10 00:53:07 - INFO -   - Top-k neighbors: 8\n",
      "2025-06-10 00:53:07 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 00:53:07 - INFO -   - Force reprocess: False\n",
      "2025-06-10 00:53:07 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 00:53:07 - INFO -   - Segment length: 3000\n",
      "2025-06-10 00:53:07 - INFO -   - Apply filtering: True\n",
      "2025-06-10 00:53:07 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 00:53:07 - INFO -   - Apply normalization: True\n",
      "2025-06-10 00:53:07 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 00:53:07 - INFO -   - Test mode: False\n",
      "2025-06-10 00:53:07 - INFO -   - Extract graph features: True\n",
      "2025-06-10 00:53:07 - INFO -   - Diff Corr Matrix Path: data/diff_corr_matrix.csv\n",
      "2025-06-10 00:53:07 - INFO - Edge strategy: relevance_diff_correlation. Loading average correlation matrices.\n",
      "2025-06-10 00:53:07 - INFO - Loaded absolute difference correlation matrix from data/diff_corr_matrix.csv, shape: (19, 19)\n",
      "2025-06-10 00:53:07 - INFO - Initializing graph feature extractor...\n",
      "2025-06-10 00:53:07,399 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 00:53:07 - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 00:53:07 - INFO - Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 00:53:07 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 00:53:07 - INFO - Setting up signal filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "Length of dataset_absdiff_corr_te: 0\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load test dataset\n",
    "dataset_absdiff_corr_te = GraphEEGDataset(\n",
    "    root=test_dataset_absdiff_correlation_dir,\n",
    "    clips=clips_te,\n",
    "    signal_folder=test_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"relevance_diff_correlation\",\n",
    "    spatial_distance_file=None,\n",
    "    top_k=top_k,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None, # collect all graph features\n",
    "    # settings for absolute difference correlation\n",
    "    diff_corr_matrix_path=absdiff_correlation_file,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_absdiff_corr_te: {len(dataset_absdiff_corr_te)}\")\n",
    "print(f' Eliminated IDs: {dataset_absdiff_corr_te.ids_to_eliminate}')\n",
    "clips_absdiff_corr_te = clips_te[~clips_te.index.isin(dataset_absdiff_corr_te.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa474f0b",
   "metadata": {},
   "source": [
    "### Timeseries datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4a44e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "ðŸš€ Initializing TimeseriesEEGDataset in SIGNAL mode.\n",
      "   - Sampling rate: 250 Hz\n",
      "   - Derived segment length: 3000 timesteps.\n",
      "   - Segment length: 3000 timesteps.\n",
      "   âœ… Using existing cached data from data/timeseries_dataset_train_signal/processed\n",
      "ðŸ TimeseriesEEGDataset initialization complete. Loaded 12993 samples.\n",
      "ðŸš€ Initializing TimeseriesEEGDataset in SIGNAL mode.\n",
      "   - Sampling rate: 250 Hz\n",
      "   - Derived segment length: 3000 timesteps.\n",
      "   - Segment length: 3000 timesteps.\n",
      "   âš ï¸ Info: Column 'label' not found in clips_df. Processing without labels (e.g., test set).\n",
      "   âœ… Using existing cached data from data/timeseries_dataset_test_signal/processed\n",
      "ðŸ TimeseriesEEGDataset initialization complete. Loaded 3614 samples.\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.utils.timeseries_eeg_dataset import TimeseriesEEGDataset\n",
    "\n",
    "dataset_timeseries_signal_tr = TimeseriesEEGDataset(\n",
    "    root=str(train_dataset_timeseries_signal_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_tr,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    force_reprocess=False,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='signal', # Use raw EEG signal data\n",
    ")\n",
    "dataset_timeseries_signal_te = TimeseriesEEGDataset(\n",
    "    root=str(test_dataset_timeseries_signal_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_te,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='signal', # Use raw EEG signal data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fe1f9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "ðŸš€ Initializing TimeseriesEEGDataset in FEATURE mode.\n",
      "   âœ… Using existing cached data from data/timeseries_dataset_train_features/processed\n",
      "ðŸ TimeseriesEEGDataset initialization complete. Loaded 12993 samples.\n",
      "ðŸš€ Initializing TimeseriesEEGDataset in FEATURE mode.\n",
      "   âš ï¸ Info: Column 'label' not found in clips_df. Processing without labels (e.g., test set).\n",
      "   âœ… Using existing cached data from data/timeseries_dataset_test_features/processed\n",
      "ðŸ TimeseriesEEGDataset initialization complete. Loaded 3614 samples.\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.utils.timeseries_eeg_dataset import TimeseriesEEGDataset\n",
    "\n",
    "dataset_timeseries_feature_tr = TimeseriesEEGDataset(\n",
    "    root=str(train_dataset_timeseries_feature_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_tr,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='feature',\n",
    "    feature_file_path=str(extracted_features_dir / \"X_train.npy\"),\n",
    ")\n",
    "dataset_timeseries_feature_te = TimeseriesEEGDataset(\n",
    "    root=str(test_dataset_timeseries_feature_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_te,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='feature',\n",
    "    feature_file_path=str(extracted_features_dir / \"X_test.npy\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f38f791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "âœ… TrainingContext initialized. Use .switch_to('dataset_type') to begin.\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.utils.lazy import LazyDataLoaderManager, TrainingContext\n",
    "\n",
    "datasets = {\n",
    "    # timeseries datasets\n",
    "    \"signal\": {\n",
    "        \"dataset_tr\": dataset_timeseries_signal_tr,\n",
    "        \"dataset_te\": dataset_timeseries_signal_te,\n",
    "        \"clips_tr\": clips_tr\n",
    "    },\n",
    "    \"feature\": {\n",
    "        \"dataset_tr\": dataset_timeseries_feature_tr,\n",
    "        \"dataset_te\": dataset_timeseries_feature_te,\n",
    "        \"clips_tr\": clips_tr\n",
    "    },\n",
    "    # graph datasets\n",
    "    \"spatial\": {\n",
    "        \"dataset_tr\": dataset_spatial_tr,\n",
    "        \"dataset_te\": dataset_spatial_te,\n",
    "        \"clips_tr\": clips_spatial_tr,\n",
    "    },\n",
    "    \"correlation\": {\n",
    "        \"dataset_tr\": dataset_corr_tr,\n",
    "        \"dataset_te\": dataset_corr_te,\n",
    "        \"clips_tr\": clips_corr_tr,\n",
    "    },\n",
    "    \"absolute_difference\": {\n",
    "        \"dataset_tr\": dataset_absdiff_corr_tr,\n",
    "        \"dataset_te\": dataset_absdiff_corr_te,\n",
    "        \"clips_tr\": clips_absdiff_corr_tr,\n",
    "    }\n",
    "}\n",
    "\n",
    "# create loaders for both datasets\n",
    "loader_manager = LazyDataLoaderManager(\n",
    "    datasets,\n",
    "    oversampling_power=oversampling_power,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# create train context\n",
    "training_context = TrainingContext(loader_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef9bd7",
   "metadata": {},
   "source": [
    "## Dataset Selection and Training Configuration\n",
    "\n",
    "This section allows you to select which dataset type to use for training by modifying the `DATASET_TYPE` variable in the next cell. The notebook supports multiple dataset types, each with different preprocessing strategies and model architectures.\n",
    "\n",
    "### Available Dataset Types\n",
    "\n",
    "#### Graph-Based Datasets (for GNN models):\n",
    "- **`'spatial'`** - Uses spatial distance-based graph connections between EEG electrodes\n",
    "- **`'correlation'`** - Uses correlation-based graph connections (top-k=5)\n",
    "- **`'absdiff_correlation'`** - Uses absolute difference correlation graph connections (top-k=8)\n",
    "\n",
    "#### Timeseries Datasets (for traditional deep learning models):\n",
    "- **`'signal'`** - Raw EEG signal data with temporal processing\n",
    "- **`'features'`** - Pre-extracted feature representations\n",
    "\n",
    "### Data Loaders Structure\n",
    "\n",
    "All datasets are automatically split into train/validation/test sets with the following configuration:\n",
    "- **Train/Validation ratio**: 80/20\n",
    "- **Random seed**: 42 (for reproducibility)  \n",
    "- **Class balancing**: WeightedRandomSampler with oversampling power = 1.0\n",
    "- **Batch size**: 64\n",
    "\n",
    "The data loaders are organized in dictionaries for easy access:\n",
    "- `graph_loaders` - Contains loaders for all graph-based datasets\n",
    "- `timeseries_loaders` - Contains loaders for all timeseries datasets\n",
    "\n",
    "### Usage Instructions\n",
    "\n",
    "1. Set the `DATASET_TYPE` variable to your desired dataset type\n",
    "2. The notebook will automatically configure the appropriate data loaders\n",
    "3. Use the selected `train_loader`, `val_loader`, and `te_loader` for model training\n",
    "4. Choose the corresponding model architecture (GNN for graph datasets, traditional models for timeseries datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67042055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from src.layers.hybrid.cnn_bilstm_gcn import EEGCNNBiLSTMGCN\n",
    "from src.utils.train import train_model\n",
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"patience\": 10,\n",
    "    \"epochs\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5446a5",
   "metadata": {},
   "source": [
    "## Training and Evaluation - Timeseries Models\n",
    "\n",
    "In this section, we will train and evaluate traditional deep learning models on the selected timeseries dataset. The models will be trained using the `train_loader` and evaluated on the `val_loader` and `te_loader` (the latter being used for final evaluation after training. Labels are not available for the test set, so we will not compute metrics on it)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d491209e",
   "metadata": {},
   "source": [
    "### Training / k-Fold Cross-Validation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60bc6d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.utils.train\n",
    "from src.utils.train import train_k_fold\n",
    "\n",
    "def wrap_traditional_train(model, save_path):\n",
    "    global train_context\n",
    "    if 'train_context' not in globals():\n",
    "        raise ValueError(\"Timeseries training context is not initialized. Please initialize it before calling this function.\")\n",
    "    if not isinstance(train_context, TrainingContext):\n",
    "        raise ValueError(\"train_context must be an instance of TrainingContext.\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # if multiple GPUs are available, use DataParallel\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for training\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5)\n",
    "    loss = nn.BCEWithLogitsLoss()  # Not weighted as we use a balanced sampler!\n",
    "\n",
    "    # train model\n",
    "    train_history, val_history = train_model(\n",
    "        wandb_config=None,\n",
    "        model=model,\n",
    "        train_loader=train_context.train_loader,\n",
    "        val_loader=train_context.val_loader,\n",
    "        criterion=loss,\n",
    "        scheduler=scheduler,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=config[\"epochs\"],\n",
    "        patience=config[\"patience\"],\n",
    "        save_path=save_path,\n",
    "        use_gnn=False,\n",
    "        # hidden attribute\n",
    "        try_load_checkpoint=True,\n",
    "        # FIXME: remove this before submission\n",
    "        log_wandb=False,\n",
    "    )\n",
    "    plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])\n",
    "\n",
    "def k_fold_train_shorthand(model_class, model_parameters, save_path, use_gnn=False, log_wandb=False, batch_size=64):\n",
    "    global train_context\n",
    "\n",
    "    if 'train_context' not in globals():\n",
    "        raise ValueError(\"Training context is not initialized. Please initialize it before calling this function.\")\n",
    "    if not isinstance(train_context, TrainingContext):\n",
    "        raise ValueError(\"train_context must be an instance of TrainingContext.\")\n",
    "\n",
    "    import torch.nn as nn # force import to avoid bug\n",
    "\n",
    "    # train model\n",
    "    aggregated_train_history, aggregated_val_history, fold_results = train_k_fold(\n",
    "        # dataset to use\n",
    "        dataset=datasets[train_context.dataset_type][\"dataset_tr\"],\n",
    "        labels=train_context.clips[\"label\"].values,\n",
    "        # train models\n",
    "        model_class=model_class,\n",
    "        model_kwargs=model_parameters,\n",
    "        # optimizer\n",
    "        criterion=nn.BCEWithLogitsLoss(),\n",
    "        optimizer_class=torch.optim.AdamW,\n",
    "        optimizer_kwargs={\n",
    "            \"lr\": config[\"learning_rate\"],\n",
    "            \"weight_decay\": config[\"weight_decay\"],\n",
    "            \"betas\": (0.9, 0.999)\n",
    "        },\n",
    "        # scheduler\n",
    "        scheduler_class=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        scheduler_kwargs={\n",
    "            \"mode\": 'min',\n",
    "            \"factor\": 0.5,\n",
    "            \"patience\": 5\n",
    "        },\n",
    "        batch_size=batch_size,\n",
    "        wandb_config=None,\n",
    "        device=device,\n",
    "        num_epochs=config[\"epochs\"],\n",
    "        patience=config[\"patience\"],\n",
    "        use_gnn=use_gnn,\n",
    "        # hidden attribute\n",
    "        log_wandb=log_wandb,\n",
    "        save_dir=save_path,\n",
    "    )\n",
    "    plot_training_loss(aggregated_train_history[\"loss\"], aggregated_val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06a1e4",
   "metadata": {},
   "source": [
    "### LSTM (signal-based model)\n",
    "\n",
    "The LSTM model is a recurrent neural network (RNN) architecture designed to handle sequential data, making it suitable for time-series analysis like EEG signals. It captures temporal dependencies in the data, allowing it to learn patterns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cc3bc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒ Switching context to 'SIGNAL' dataset...\n",
      "\n",
      "--- Lazily creating loaders for 'signal' ---\n",
      "\n",
      "--- Creating train/val subsets for 'signal' dataset ---\n",
      "Splitting 12993 samples -> Train: 10394, Val: 2599\n",
      "[00:36:36] Train labels: 0 -> 8375, 1 -> 2019\n",
      "[00:36:36] Val labels:   0 -> 2101, 1 -> 498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 00:36:36 - INFO - Starting 5-fold cross-validation\n",
      "2025-06-10 00:36:36 - INFO - Dataset size: 12993\n",
      "2025-06-10 00:36:36 - INFO - Stratified: True\n",
      "2025-06-10 00:36:36 - INFO - Batch size: 512\n",
      "2025-06-10 00:36:36 - INFO - Using stratified k-fold with label distribution\n",
      "2025-06-10 00:36:36 - INFO - Created 5 folds\n",
      "2025-06-10 00:36:36 - INFO - Folds: [(array([    0,     2,     3, ..., 12990, 12991, 12992]), array([    1,     5,     6, ..., 12979, 12983, 12985])), (array([    0,     1,     2, ..., 12988, 12989, 12990]), array([    3,    19,    27, ..., 12977, 12991, 12992])), (array([    0,     1,     3, ..., 12987, 12991, 12992]), array([    2,     7,    17, ..., 12988, 12989, 12990])), (array([    0,     1,     2, ..., 12990, 12991, 12992]), array([    4,     9,    12, ..., 12975, 12978, 12987])), (array([    1,     2,     3, ..., 12990, 12991, 12992]), array([    0,    10,    16, ..., 12976, 12981, 12986]))]\n",
      "2025-06-10 00:36:36 - INFO - \n",
      "============================================================\n",
      "2025-06-10 00:36:36 - INFO - FOLD 1/5\n",
      "2025-06-10 00:36:36 - INFO - ============================================================\n",
      "2025-06-10 00:36:36 - INFO - Train samples: 10394\n",
      "2025-06-10 00:36:36 - INFO - Val samples: 2599\n",
      "2025-06-10 00:36:36 - INFO - Train positive ratio: 0.194\n",
      "2025-06-10 00:36:36 - INFO - Val positive ratio: 0.194\n",
      "2025-06-10 00:36:36 - INFO - Initialized model: EEGLSTMClassifier\n",
      "2025-06-10 00:36:36 - INFO - Initialized optimizer: AdamW\n",
      "2025-06-10 00:36:36 - INFO - Initialized scheduler: ReduceLROnPlateau\n",
      "2025-06-10 00:36:36 - INFO - Starting training for fold 1\n",
      "2025-06-10 00:36:36 - INFO - Starting training setup...\n",
      "2025-06-10 00:36:36 - INFO - Model type: Standard\n",
      "2025-06-10 00:36:36 - INFO - Device: cuda\n",
      "2025-06-10 00:36:36 - INFO - Batch size: 512\n",
      "2025-06-10 00:36:36 - INFO - Number of epochs: 100\n",
      "2025-06-10 00:36:36 - INFO - Patience: 10\n",
      "2025-06-10 00:36:36 - INFO - Monitor metric: val_f1\n",
      "2025-06-10 00:36:36 - INFO - Total training batches per epoch: 21\n",
      "2025-06-10 00:36:36 - INFO - Starting training from epoch 1 to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [0.0001194  0.00049529]\n",
      "Train set class distribution: [8375 2019]\n",
      "ðŸš€ Context ready for 'signal'.\n",
      "   Train batches: 162, Val batches: 41\n",
      "   Type: signal\n",
      "   Total Train Samples: 12993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]2025-06-10 00:36:36 - INFO - \n",
      "Epoch 1/100 - Training phase\n",
      "2025-06-10 00:36:37 - INFO - Processing batch 1/21\n",
      "2025-06-10 00:36:37 - INFO - Batch 1/21 - Loss: 0.7403 - Avg batch time: 0.49s\n",
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# build model with current parameters\u001b[39;00m\n\u001b[1;32m      8\u001b[0m SAVE_PATH \u001b[38;5;241m=\u001b[39m CHECKPOINT_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeseries_signal_lstm_baseline.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mk_fold_train_shorthand\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEEGLSTMClassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m19\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_layers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbidirectional\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHECKPOINT_ROOT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimeseries_signal_lstm_k_fold.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# higher batch size for faster training\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 55\u001b[0m, in \u001b[0;36mk_fold_train_shorthand\u001b[0;34m(model_class, model_parameters, save_path, use_gnn, log_wandb, batch_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;66;03m# force import to avoid bug\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m aggregated_train_history, aggregated_val_history, fold_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_k_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# dataset to use\u001b[39;49;00m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_tr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclips\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# train models\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# optimizer\u001b[39;49;00m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBCEWithLogitsLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbetas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# scheduler\u001b[39;49;00m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfactor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hidden attribute\u001b[39;49;00m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m plot_training_loss(aggregated_train_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], aggregated_val_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:1220\u001b[0m, in \u001b[0;36mtrain_k_fold\u001b[0;34m(dataset, labels, model_class, model_kwargs, criterion, optimizer_class, optimizer_kwargs, device, save_dir, k_folds, stratified, scheduler_class, scheduler_kwargs, monitor, patience, num_epochs, grad_clip, batch_size, use_gnn, wandb_config, wandb_project, log_wandb, random_state)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# Train this fold\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1220\u001b[0m     train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_clip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Always overwrite for k-fold\u001b[39;49;00m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_wandb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_run_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_wandb_run_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtry_load_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Don't load checkpoint for k-fold\u001b[39;49;00m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;66;03m# Get best scores for this fold\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m     best_train_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(train_history\u001b[38;5;241m.\u001b[39mget(monitor\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m), [\u001b[38;5;241m0\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m monitor \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(train_history\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)]))\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:399\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, wandb_config, wandb_project, wandb_run_name, log_wandb, try_load_checkpoint, original_dataset)\u001b[0m\n\u001b[1;32m    396\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training phase\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    397\u001b[0m batch_times \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 399\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, data_batch_item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader_to_iterate):\n\u001b[1;32m    400\u001b[0m     batch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Log every 10 batches\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/timeseries_eeg_dataset.py:421\u001b[0m, in \u001b[0;36mTimeseriesEEGDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(idx) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processed_items_count:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of range [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processed_items_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cached_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[idx]\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/timeseries_eeg_dataset.py:449\u001b[0m, in \u001b[0;36mTimeseriesEEGDataset._get_cached_item\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    447\u001b[0m file_path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_file_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:1426\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m-> 1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_is_zipfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   1431\u001b[0m         overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:433\u001b[0m, in \u001b[0;36m_is_zipfile\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Read the first few bytes and match against the ZIP file signature\u001b[39;00m\n\u001b[1;32m    432\u001b[0m local_header_magic_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 433\u001b[0m read_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlocal_header_magic_number\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m f\u001b[38;5;241m.\u001b[39mseek(start)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m read_bytes \u001b[38;5;241m==\u001b[39m local_header_magic_number\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%aimport src.layers.temporal.lstm\n",
    "from src.layers.temporal.lstm import EEGLSTMClassifier\n",
    "\n",
    "# create loader manager\n",
    "train_context = training_context.switch_to('signal')\n",
    "\n",
    "# build model with current parameters\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"timeseries_signal_lstm_baseline.pt\"\n",
    "\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGLSTMClassifier,\n",
    "    model_parameters={\n",
    "        \"input_dim\": 19,\n",
    "        \"hidden_dim\": 64,\n",
    "        \"num_layers\": 4,\n",
    "        \"dropout\": 0.3,\n",
    "        \"bidirectional\": False,\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_lstm_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89807a",
   "metadata": {},
   "source": [
    "### BiLSTM (signal-based model)\n",
    "\n",
    "The BiLSTM (Bidirectional Long Short-Term Memory) model extends the LSTM by processing the input sequence in both forward and backward directions. This bidirectional approach allows the model to capture context from both past and future time steps, enhancing its ability to understand complex temporal relationships in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ce139",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.temporal.lstm\n",
    "from src.layers.temporal.lstm import EEGLSTMClassifier\n",
    "\n",
    "# create loader manager\n",
    "train_context = training_context.switch_to('signal')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGLSTMClassifier,\n",
    "    model_parameters={\n",
    "        \"input_dim\": 19,\n",
    "        \"hidden_dim\": 64,\n",
    "        \"num_layers\": 4,\n",
    "        \"dropout\": 0.3,\n",
    "        \"bidirectional\": True, # use bidirectional LSTM\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_bilstm_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4a846",
   "metadata": {},
   "source": [
    "### MLP (feature-based model)\n",
    "\n",
    "The MLP (Multi-Layer Perceptron) is a feedforward neural network architecture consisting of multiple layers of neurons. It is designed to learn complex mappings from input features to output labels, making it suitable for tasks like EEG seizure detection when using pre-extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.mlp\n",
    "from src.layers.mlp import EEGMLPClassifier\n",
    "\n",
    "train_context = training_context.switch_to('feature')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGMLPClassifier,\n",
    "    model_parameters={\n",
    "        \"input_dim\": 228, # extracted features dimension\n",
    "        \"hidden_dims\": [1024, 512, 256],\n",
    "        \"output_dim\": 1,\n",
    "        \"dropout_prob\": 0.3,\n",
    "        \"use_batch_norm\": True,\n",
    "        \"use_residual\": False,\n",
    "        \"activation\": \"relu\"\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_feature_mlp_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8f8223",
   "metadata": {},
   "source": [
    "### MLP (signal-based model, flattened EEG signals)\n",
    "\n",
    "The same MLP architecture as above, but trained on raw EEG signals instead of pre-extracted features to evaluate the performance of the model on raw data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307a8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 23:02:14 - INFO - Starting 5-fold cross-validation\n",
      "2025-06-09 23:02:14 - INFO - Dataset size: 12993\n",
      "2025-06-09 23:02:14 - INFO - Stratified: True\n",
      "2025-06-09 23:02:14 - INFO - Batch size: 512\n",
      "2025-06-09 23:02:14 - INFO - Using stratified k-fold with label distribution\n",
      "2025-06-09 23:02:14 - INFO - Created 5 folds\n",
      "2025-06-09 23:02:14 - INFO - Folds: [(array([    0,     2,     3, ..., 12990, 12991, 12992]), array([    1,     5,     6, ..., 12979, 12983, 12985])), (array([    0,     1,     2, ..., 12988, 12989, 12990]), array([    3,    19,    27, ..., 12977, 12991, 12992])), (array([    0,     1,     3, ..., 12987, 12991, 12992]), array([    2,     7,    17, ..., 12988, 12989, 12990])), (array([    0,     1,     2, ..., 12990, 12991, 12992]), array([    4,     9,    12, ..., 12975, 12978, 12987])), (array([    1,     2,     3, ..., 12990, 12991, 12992]), array([    0,    10,    16, ..., 12976, 12981, 12986]))]\n",
      "2025-06-09 23:02:14 - INFO - \n",
      "============================================================\n",
      "2025-06-09 23:02:14 - INFO - FOLD 1/5\n",
      "2025-06-09 23:02:14 - INFO - ============================================================\n",
      "2025-06-09 23:02:14 - INFO - Train samples: 10394\n",
      "2025-06-09 23:02:14 - INFO - Val samples: 2599\n",
      "2025-06-09 23:02:14 - INFO - Train positive ratio: 0.194\n",
      "2025-06-09 23:02:14 - INFO - Val positive ratio: 0.194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒ Switching context to 'SIGNAL' dataset...\n",
      "ðŸš€ Context ready for 'signal'.\n",
      "   Train batches: 162, Val batches: 41\n",
      "   Type: signal\n",
      "   Total Train Samples: 12993\n",
      "   Channels: 19\n",
      "   Sequence Length: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 23:02:16 - INFO - Initialized model: EEGMLPClassifier\n",
      "2025-06-09 23:02:16 - INFO - Initialized optimizer: AdamW\n",
      "2025-06-09 23:02:16 - INFO - Initialized scheduler: ReduceLROnPlateau\n",
      "2025-06-09 23:02:16 - INFO - Starting training for fold 1\n",
      "2025-06-09 23:02:16 - INFO - Starting training setup...\n",
      "2025-06-09 23:02:16 - INFO - Model type: Standard\n",
      "2025-06-09 23:02:16 - INFO - Device: cuda\n",
      "2025-06-09 23:02:16 - INFO - Batch size: 512\n",
      "2025-06-09 23:02:16 - INFO - Number of epochs: 100\n",
      "2025-06-09 23:02:16 - INFO - Patience: 10\n",
      "2025-06-09 23:02:16 - INFO - Monitor metric: val_f1\n",
      "2025-06-09 23:02:16 - INFO - Total training batches per epoch: 21\n",
      "2025-06-09 23:02:16 - INFO - Starting training from epoch 1 to 100\n",
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]2025-06-09 23:02:16 - INFO - \n",
      "Epoch 1/100 - Training phase\n",
      "2025-06-09 23:02:26 - INFO - Processing batch 1/21\n",
      "2025-06-09 23:02:27 - INFO - Batch 1/21 - Loss: 0.7378 - Avg batch time: 0.66s\n",
      "2025-06-09 23:03:51 - INFO - Processing batch 11/21\n",
      "2025-06-09 23:03:51 - INFO - Batch 11/21 - Loss: 0.7310 - Avg batch time: 0.10s\n",
      "Epochs:   1%|â–Š                                                                                  | 1/100 [01:47<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# switch to signal dataset\u001b[39;00m\n\u001b[1;32m      5\u001b[0m train_context \u001b[38;5;241m=\u001b[39m training_context\u001b[38;5;241m.\u001b[39mswitch_to(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtraditional_k_fold_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEEGMLPClassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_channels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m19\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_time_steps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_dims\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdropout_prob\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_batch_norm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_residual\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactivation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleaky_relu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHECKPOINT_ROOT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimeseries_signal_mlp_k_fold.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# higher batch size for faster training\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 55\u001b[0m, in \u001b[0;36mtraditional_k_fold_train\u001b[0;34m(model_class, model_parameters, save_path, batch_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;66;03m# force import to avoid bug\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m aggregated_train_history, aggregated_val_history, fold_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_k_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# dataset to use\u001b[39;49;00m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_tr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclips\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# train models\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# optimizer\u001b[39;49;00m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBCEWithLogitsLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbetas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# scheduler\u001b[39;49;00m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfactor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hidden attribute\u001b[39;49;00m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m plot_training_loss(aggregated_train_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], aggregated_val_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:1220\u001b[0m, in \u001b[0;36mtrain_k_fold\u001b[0;34m(dataset, labels, model_class, model_kwargs, criterion, optimizer_class, optimizer_kwargs, device, save_dir, k_folds, stratified, scheduler_class, scheduler_kwargs, monitor, patience, num_epochs, grad_clip, batch_size, use_gnn, wandb_config, wandb_project, log_wandb, random_state)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# Train this fold\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1220\u001b[0m     train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_clip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Always overwrite for k-fold\u001b[39;49;00m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_wandb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_run_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_wandb_run_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtry_load_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Don't load checkpoint for k-fold\u001b[39;49;00m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;66;03m# Get best scores for this fold\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m     best_train_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(train_history\u001b[38;5;241m.\u001b[39mget(monitor\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m), [\u001b[38;5;241m0\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m monitor \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(train_history\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)]))\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:399\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, wandb_config, wandb_project, wandb_run_name, log_wandb, try_load_checkpoint, original_dataset)\u001b[0m\n\u001b[1;32m    396\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training phase\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    397\u001b[0m batch_times \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 399\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, data_batch_item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader_to_iterate):\n\u001b[1;32m    400\u001b[0m     batch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Log every 10 batches\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/timeseries_eeg_dataset.py:421\u001b[0m, in \u001b[0;36mTimeseriesEEGDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(idx) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processed_items_count:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of range [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processed_items_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cached_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[idx]\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/timeseries_eeg_dataset.py:449\u001b[0m, in \u001b[0;36mTimeseriesEEGDataset._get_cached_item\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    447\u001b[0m file_path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_file_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:1426\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m-> 1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_is_zipfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   1431\u001b[0m         overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:433\u001b[0m, in \u001b[0;36m_is_zipfile\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Read the first few bytes and match against the ZIP file signature\u001b[39;00m\n\u001b[1;32m    432\u001b[0m local_header_magic_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 433\u001b[0m read_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlocal_header_magic_number\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m f\u001b[38;5;241m.\u001b[39mseek(start)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m read_bytes \u001b[38;5;241m==\u001b[39m local_header_magic_number\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%aimport src.layers.mlp\n",
    "from src.layers.mlp import EEGMLPClassifier\n",
    "\n",
    "# switch to signal dataset\n",
    "train_context = training_context.switch_to('signal')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGMLPClassifier,\n",
    "    model_parameters={\n",
    "        \"input_channels\": 19,\n",
    "        \"input_time_steps\": 3000,\n",
    "        \"hidden_dims\": [4096, 2048, 1024, 512, 256],\n",
    "        \"output_dim\": 1,\n",
    "        \"dropout_prob\": 0.3,\n",
    "        \"use_batch_norm\": True,\n",
    "        \"use_residual\": False,\n",
    "        \"activation\": \"leaky_relu\"\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_mlp_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d45648",
   "metadata": {},
   "source": [
    "### CNN-MLP (signal-based model)\n",
    "\n",
    "We have proven that the MLP model alone is not sufficient to capture the temporal dependencies in the EEG signals. Therefore, we will use a CNN-MLP model that combines convolutional layers to extract spatial features from the EEG signals and MLP layers to learn the mapping from these features to the output labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc832a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 23:06:20 - INFO - Starting 5-fold cross-validation\n",
      "2025-06-09 23:06:20 - INFO - Dataset size: 12993\n",
      "2025-06-09 23:06:20 - INFO - Stratified: True\n",
      "2025-06-09 23:06:20 - INFO - Batch size: 512\n",
      "2025-06-09 23:06:20 - INFO - Using stratified k-fold with label distribution\n",
      "2025-06-09 23:06:20 - INFO - Created 5 folds\n",
      "2025-06-09 23:06:20 - INFO - Folds: [(array([    0,     2,     3, ..., 12990, 12991, 12992]), array([    1,     5,     6, ..., 12979, 12983, 12985])), (array([    0,     1,     2, ..., 12988, 12989, 12990]), array([    3,    19,    27, ..., 12977, 12991, 12992])), (array([    0,     1,     3, ..., 12987, 12991, 12992]), array([    2,     7,    17, ..., 12988, 12989, 12990])), (array([    0,     1,     2, ..., 12990, 12991, 12992]), array([    4,     9,    12, ..., 12975, 12978, 12987])), (array([    1,     2,     3, ..., 12990, 12991, 12992]), array([    0,    10,    16, ..., 12976, 12981, 12986]))]\n",
      "2025-06-09 23:06:20 - INFO - \n",
      "============================================================\n",
      "2025-06-09 23:06:20 - INFO - FOLD 1/5\n",
      "2025-06-09 23:06:20 - INFO - ============================================================\n",
      "2025-06-09 23:06:20 - INFO - Train samples: 10394\n",
      "2025-06-09 23:06:20 - INFO - Val samples: 2599\n",
      "2025-06-09 23:06:20 - INFO - Train positive ratio: 0.194\n",
      "2025-06-09 23:06:20 - INFO - Val positive ratio: 0.194\n",
      "2025-06-09 23:06:20 - INFO - Initialized model: EEGCNNClassifier\n",
      "2025-06-09 23:06:20 - INFO - Initialized optimizer: AdamW\n",
      "2025-06-09 23:06:20 - INFO - Initialized scheduler: ReduceLROnPlateau\n",
      "2025-06-09 23:06:20 - INFO - Starting training for fold 1\n",
      "2025-06-09 23:06:20 - INFO - Starting training setup...\n",
      "2025-06-09 23:06:20 - INFO - Model type: Standard\n",
      "2025-06-09 23:06:20 - INFO - Device: cuda\n",
      "2025-06-09 23:06:20 - INFO - Batch size: 512\n",
      "2025-06-09 23:06:20 - INFO - Number of epochs: 100\n",
      "2025-06-09 23:06:20 - INFO - Patience: 10\n",
      "2025-06-09 23:06:20 - INFO - Monitor metric: val_f1\n",
      "2025-06-09 23:06:20 - INFO - Total training batches per epoch: 21\n",
      "2025-06-09 23:06:20 - INFO - Starting training from epoch 1 to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒ Switching context to 'SIGNAL' dataset...\n",
      "ðŸš€ Context ready for 'signal'.\n",
      "   Train batches: 162, Val batches: 41\n",
      "   Type: signal\n",
      "   Total Train Samples: 12993\n",
      "   Channels: 19\n",
      "   Sequence Length: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]2025-06-09 23:06:20 - INFO - \n",
      "Epoch 1/100 - Training phase\n",
      "2025-06-09 23:06:25 - INFO - Processing batch 1/21\n",
      "2025-06-09 23:06:28 - INFO - Batch 1/21 - Loss: 0.7647 - Avg batch time: 2.64s\n",
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:28<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EEGCNNClassifier\n\u001b[1;32m      4\u001b[0m train_context \u001b[38;5;241m=\u001b[39m training_context\u001b[38;5;241m.\u001b[39mswitch_to(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtraditional_k_fold_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEEGCNNClassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_channels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m19\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcnn_out_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmlp_hidden_dims\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcnn_dropout_prob\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmlp_dropout_prob\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactivation_mlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleaky_relu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactivation_cnn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleaky_relu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcnn_use_batch_norm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_batch_norm_mlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHECKPOINT_ROOT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimeseries_signal_cnn_mlp_k_fold.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# higher batch size for faster training\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 55\u001b[0m, in \u001b[0;36mtraditional_k_fold_train\u001b[0;34m(model_class, model_parameters, save_path, batch_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;66;03m# force import to avoid bug\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m aggregated_train_history, aggregated_val_history, fold_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_k_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# dataset to use\u001b[39;49;00m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_tr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclips\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# train models\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# optimizer\u001b[39;49;00m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBCEWithLogitsLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbetas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# scheduler\u001b[39;49;00m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfactor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hidden attribute\u001b[39;49;00m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m plot_training_loss(aggregated_train_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], aggregated_val_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:1220\u001b[0m, in \u001b[0;36mtrain_k_fold\u001b[0;34m(dataset, labels, model_class, model_kwargs, criterion, optimizer_class, optimizer_kwargs, device, save_dir, k_folds, stratified, scheduler_class, scheduler_kwargs, monitor, patience, num_epochs, grad_clip, batch_size, use_gnn, wandb_config, wandb_project, log_wandb, random_state)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# Train this fold\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1220\u001b[0m     train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_clip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Always overwrite for k-fold\u001b[39;49;00m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_wandb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_run_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_wandb_run_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtry_load_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Don't load checkpoint for k-fold\u001b[39;49;00m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;66;03m# Get best scores for this fold\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m     best_train_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(train_history\u001b[38;5;241m.\u001b[39mget(monitor\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m), [\u001b[38;5;241m0\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m monitor \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(train_history\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)]))\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:399\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, wandb_config, wandb_project, wandb_run_name, log_wandb, try_load_checkpoint, original_dataset)\u001b[0m\n\u001b[1;32m    396\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training phase\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    397\u001b[0m batch_times \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 399\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, data_batch_item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader_to_iterate):\n\u001b[1;32m    400\u001b[0m     batch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Log every 10 batches\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/timeseries_eeg_dataset.py:421\u001b[0m, in \u001b[0;36mTimeseriesEEGDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(idx) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processed_items_count:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of range [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processed_items_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cached_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[idx]\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/timeseries_eeg_dataset.py:449\u001b[0m, in \u001b[0;36mTimeseriesEEGDataset._get_cached_item\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    447\u001b[0m file_path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_file_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:1426\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m-> 1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_is_zipfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   1431\u001b[0m         overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:433\u001b[0m, in \u001b[0;36m_is_zipfile\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Read the first few bytes and match against the ZIP file signature\u001b[39;00m\n\u001b[1;32m    432\u001b[0m local_header_magic_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 433\u001b[0m read_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlocal_header_magic_number\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m f\u001b[38;5;241m.\u001b[39mseek(start)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m read_bytes \u001b[38;5;241m==\u001b[39m local_header_magic_number\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%aimport src.layers.cnn.cnn\n",
    "from src.layers.cnn.cnn import EEGCNNClassifier\n",
    "\n",
    "train_context = training_context.switch_to('signal')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGCNNClassifier,\n",
    "    model_parameters={\n",
    "        \"input_channels\": 19,\n",
    "        \"cnn_out_dim\": 128,\n",
    "        \"mlp_hidden_dims\": [256, 128],\n",
    "        \"output_dim\": 1,\n",
    "        \"cnn_dropout_prob\": 0.3,\n",
    "        \"mlp_dropout_prob\": 0.3,\n",
    "        \"activation_mlp\": \"leaky_relu\",\n",
    "        \"activation_cnn\": \"leaky_relu\",\n",
    "        \"cnn_use_batch_norm\": True,\n",
    "        \"use_batch_norm_mlp\": True,\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_cnn_mlp_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febab4cf",
   "metadata": {},
   "source": [
    "### CNN-BiLSTM-MLP (signal-based model)\n",
    "\n",
    "In this section we test the CNN-BiLSTM model, which combines convolutional layers to extract spatial features from the EEG signals and LSTM layers to learn the temporal dependencies in the data. This architecture is particularly effective for EEG seizure detection, as it captures both spatial and temporal patterns in the signals. A final fully connected layer is used to map the extracted features to the output labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322efc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.cuda import clean_cuda_memory_usage\n",
    "clean_cuda_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ffe62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "src.layers.cnn.cnn_lstm src.utils.train\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%aimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae7288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 23:51:51 - INFO - Starting 5-fold cross-validation\n",
      "2025-06-09 23:51:51 - INFO - Dataset size: 12993\n",
      "2025-06-09 23:51:51 - INFO - Stratified: True\n",
      "2025-06-09 23:51:51 - INFO - Batch size: 128\n",
      "2025-06-09 23:51:51 - INFO - Using stratified k-fold with label distribution\n",
      "2025-06-09 23:51:51 - INFO - Created 5 folds\n",
      "2025-06-09 23:51:51 - INFO - Folds: [(array([    0,     2,     3, ..., 12990, 12991, 12992]), array([    1,     5,     6, ..., 12979, 12983, 12985])), (array([    0,     1,     2, ..., 12988, 12989, 12990]), array([    3,    19,    27, ..., 12977, 12991, 12992])), (array([    0,     1,     3, ..., 12987, 12991, 12992]), array([    2,     7,    17, ..., 12988, 12989, 12990])), (array([    0,     1,     2, ..., 12990, 12991, 12992]), array([    4,     9,    12, ..., 12975, 12978, 12987])), (array([    1,     2,     3, ..., 12990, 12991, 12992]), array([    0,    10,    16, ..., 12976, 12981, 12986]))]\n",
      "2025-06-09 23:51:51 - INFO - \n",
      "============================================================\n",
      "2025-06-09 23:51:51 - INFO - FOLD 1/5\n",
      "2025-06-09 23:51:51 - INFO - ============================================================\n",
      "2025-06-09 23:51:51 - INFO - Train samples: 10394\n",
      "2025-06-09 23:51:51 - INFO - Val samples: 2599\n",
      "2025-06-09 23:51:51 - INFO - Train positive ratio: 0.194\n",
      "2025-06-09 23:51:51 - INFO - Val positive ratio: 0.194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒ Switching context to 'SIGNAL' dataset...\n",
      "ðŸš€ Context ready for 'signal'.\n",
      "   Train batches: 162, Val batches: 41\n",
      "   Type: signal\n",
      "   Total Train Samples: 12993\n",
      "   Channels: 19\n",
      "   Sequence Length: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 23:51:51 - INFO - Initialized model: EEGCNNBiLSTMClassifier\n",
      "2025-06-09 23:51:51 - INFO - Initialized optimizer: AdamW\n",
      "2025-06-09 23:51:51 - INFO - Initialized scheduler: ReduceLROnPlateau\n",
      "2025-06-09 23:51:51 - INFO - Starting training for fold 1\n",
      "2025-06-09 23:51:51 - INFO - Starting training setup...\n",
      "2025-06-09 23:51:51 - INFO - Model type: Standard\n",
      "2025-06-09 23:51:51 - INFO - Device: cuda\n",
      "2025-06-09 23:51:51 - INFO - Batch size: 128\n",
      "2025-06-09 23:51:51 - INFO - Number of epochs: 100\n",
      "2025-06-09 23:51:51 - INFO - Patience: 10\n",
      "2025-06-09 23:51:51 - INFO - Monitor metric: val_f1\n",
      "2025-06-09 23:51:51 - INFO - Total training batches per epoch: 82\n",
      "2025-06-09 23:51:51 - INFO - Starting training from epoch 1 to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ðŸ—‘ï¸ Overwrite enabled: Removed existing checkpoint at .checkpoints/timeseries_signal_cnn_bilstm_k_fold/fold_1_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]2025-06-09 23:51:51 - INFO - \n",
      "Epoch 1/100 - Training phase\n",
      "2025-06-09 23:51:52 - INFO - Processing batch 1/82\n",
      "2025-06-09 23:51:52 - INFO - Batch 1/82 - Loss: 0.7391 - Avg batch time: 0.69s\n",
      "2025-06-09 23:51:57 - INFO - Processing batch 11/82\n",
      "2025-06-09 23:51:57 - INFO - Batch 11/82 - Loss: 0.7442 - Avg batch time: 0.28s\n",
      "2025-06-09 23:52:02 - INFO - Processing batch 21/82\n",
      "2025-06-09 23:52:02 - INFO - Batch 21/82 - Loss: 0.7470 - Avg batch time: 0.28s\n",
      "2025-06-09 23:52:06 - INFO - Processing batch 31/82\n",
      "2025-06-09 23:52:07 - INFO - Batch 31/82 - Loss: 0.7395 - Avg batch time: 0.28s\n",
      "2025-06-09 23:52:11 - INFO - Processing batch 41/82\n",
      "2025-06-09 23:52:12 - INFO - Batch 41/82 - Loss: 0.7310 - Avg batch time: 0.28s\n",
      "2025-06-09 23:52:17 - INFO - Processing batch 51/82\n",
      "2025-06-09 23:52:17 - INFO - Batch 51/82 - Loss: 0.7295 - Avg batch time: 0.28s\n",
      "2025-06-09 23:52:22 - INFO - Processing batch 61/82\n",
      "2025-06-09 23:52:22 - INFO - Batch 61/82 - Loss: 0.7239 - Avg batch time: 0.28s\n",
      "2025-06-09 23:52:28 - INFO - Processing batch 71/82\n",
      "2025-06-09 23:52:29 - INFO - Batch 71/82 - Loss: 0.7047 - Avg batch time: 0.28s\n",
      "2025-06-09 23:52:33 - INFO - Processing batch 81/82\n",
      "2025-06-09 23:52:34 - INFO - Batch 81/82 - Loss: 0.7280 - Avg batch time: 0.28s\n",
      "2025-06-09 23:52:34 - INFO - \n",
      "Epoch 1 training completed in 42.45s\n",
      "2025-06-09 23:52:34 - INFO - Average training loss: 0.7331\n",
      "Epochs:   2%| | 2/100 [00:48<1:19:06, 48.43s/it, train_loss=0.7331, val_loss=0.6744, best_val_f1=0.2529, lr=1.00e-04, ba2025-06-09 23:52:40 - INFO - \n",
      "Epoch 2/100 - Training phase\n",
      "2025-06-09 23:52:40 - INFO - Processing batch 1/82\n",
      "2025-06-09 23:52:40 - INFO - Batch 1/82 - Loss: 0.6823 - Avg batch time: 0.28s\n",
      "2025-06-09 23:52:45 - INFO - Processing batch 11/82\n",
      "2025-06-09 23:52:45 - INFO - Batch 11/82 - Loss: 0.7321 - Avg batch time: 0.28s\n",
      "2025-06-09 23:52:49 - INFO - Processing batch 21/82\n",
      "2025-06-09 23:52:50 - INFO - Batch 21/82 - Loss: 0.7161 - Avg batch time: 0.28s\n",
      "2025-06-09 23:52:54 - INFO - Processing batch 31/82\n",
      "2025-06-09 23:52:55 - INFO - Batch 31/82 - Loss: 0.7086 - Avg batch time: 0.28s\n",
      "2025-06-09 23:52:59 - INFO - Processing batch 41/82\n",
      "2025-06-09 23:52:59 - INFO - Batch 41/82 - Loss: 0.7013 - Avg batch time: 0.28s\n",
      "2025-06-09 23:53:04 - INFO - Processing batch 51/82\n",
      "2025-06-09 23:53:05 - INFO - Batch 51/82 - Loss: 0.6919 - Avg batch time: 0.28s\n",
      "2025-06-09 23:53:09 - INFO - Processing batch 61/82\n",
      "2025-06-09 23:53:09 - INFO - Batch 61/82 - Loss: 0.6862 - Avg batch time: 0.28s\n",
      "2025-06-09 23:53:15 - INFO - Processing batch 71/82\n",
      "2025-06-09 23:53:15 - INFO - Batch 71/82 - Loss: 0.6902 - Avg batch time: 0.28s\n",
      "2025-06-09 23:53:20 - INFO - Processing batch 81/82\n",
      "2025-06-09 23:53:20 - INFO - Batch 81/82 - Loss: 0.7257 - Avg batch time: 0.28s\n",
      "2025-06-09 23:53:20 - INFO - \n",
      "Epoch 2 training completed in 40.26s\n",
      "2025-06-09 23:53:20 - INFO - Average training loss: 0.7076\n",
      "Epochs:   3%| | 3/100 [01:34<1:15:44, 46.85s/it, train_loss=0.7076, val_loss=0.6618, best_val_f1=0.2529, lr=1.00e-04, ba2025-06-09 23:53:26 - INFO - \n",
      "Epoch 3/100 - Training phase\n",
      "2025-06-09 23:53:26 - INFO - Processing batch 1/82\n",
      "2025-06-09 23:53:26 - INFO - Batch 1/82 - Loss: 0.6679 - Avg batch time: 0.28s\n",
      "2025-06-09 23:53:30 - INFO - Processing batch 11/82\n",
      "2025-06-09 23:53:31 - INFO - Batch 11/82 - Loss: 0.6905 - Avg batch time: 0.28s\n",
      "2025-06-09 23:53:35 - INFO - Processing batch 21/82\n",
      "2025-06-09 23:53:35 - INFO - Batch 21/82 - Loss: 0.6741 - Avg batch time: 0.28s\n",
      "2025-06-09 23:53:39 - INFO - Processing batch 31/82\n",
      "2025-06-09 23:53:39 - INFO - Batch 31/82 - Loss: 0.6988 - Avg batch time: 0.28s\n",
      "2025-06-09 23:53:43 - INFO - Processing batch 41/82\n",
      "2025-06-09 23:53:43 - INFO - Batch 41/82 - Loss: 0.6857 - Avg batch time: 0.28s\n",
      "2025-06-09 23:53:48 - INFO - Processing batch 51/82\n",
      "2025-06-09 23:53:48 - INFO - Batch 51/82 - Loss: 0.6747 - Avg batch time: 0.28s\n",
      "2025-06-09 23:53:53 - INFO - Processing batch 61/82\n",
      "2025-06-09 23:53:53 - INFO - Batch 61/82 - Loss: 0.6542 - Avg batch time: 0.28s\n",
      "2025-06-09 23:53:57 - INFO - Processing batch 71/82\n",
      "2025-06-09 23:53:57 - INFO - Batch 71/82 - Loss: 0.7083 - Avg batch time: 0.28s\n",
      "2025-06-09 23:54:02 - INFO - Processing batch 81/82\n",
      "2025-06-09 23:54:02 - INFO - Batch 81/82 - Loss: 0.6817 - Avg batch time: 0.28s\n",
      "2025-06-09 23:54:02 - INFO - \n",
      "Epoch 3 training completed in 36.95s\n",
      "2025-06-09 23:54:02 - INFO - Average training loss: 0.6887\n",
      "Epochs:   4%| | 4/100 [02:17<1:12:17, 45.18s/it, train_loss=0.6887, val_loss=0.6709, best_val_f1=0.2529, lr=1.00e-04, ba2025-06-09 23:54:09 - INFO - \n",
      "Epoch 4/100 - Training phase\n",
      "2025-06-09 23:54:09 - INFO - Processing batch 1/82\n",
      "2025-06-09 23:54:09 - INFO - Batch 1/82 - Loss: 0.6616 - Avg batch time: 0.28s\n",
      "2025-06-09 23:54:13 - INFO - Processing batch 11/82\n",
      "2025-06-09 23:54:13 - INFO - Batch 11/82 - Loss: 0.6613 - Avg batch time: 0.28s\n",
      "2025-06-09 23:54:19 - INFO - Processing batch 21/82\n",
      "2025-06-09 23:54:19 - INFO - Batch 21/82 - Loss: 0.6858 - Avg batch time: 0.28s\n",
      "2025-06-09 23:54:24 - INFO - Processing batch 31/82\n",
      "2025-06-09 23:54:25 - INFO - Batch 31/82 - Loss: 0.6688 - Avg batch time: 0.28s\n",
      "2025-06-09 23:54:30 - INFO - Processing batch 41/82\n",
      "2025-06-09 23:54:30 - INFO - Batch 41/82 - Loss: 0.6897 - Avg batch time: 0.28s\n",
      "2025-06-09 23:54:34 - INFO - Processing batch 51/82\n",
      "2025-06-09 23:54:35 - INFO - Batch 51/82 - Loss: 0.6868 - Avg batch time: 0.28s\n",
      "2025-06-09 23:54:39 - INFO - Processing batch 61/82\n",
      "2025-06-09 23:54:39 - INFO - Batch 61/82 - Loss: 0.6735 - Avg batch time: 0.28s\n",
      "2025-06-09 23:54:43 - INFO - Processing batch 71/82\n",
      "2025-06-09 23:54:44 - INFO - Batch 71/82 - Loss: 0.6542 - Avg batch time: 0.28s\n",
      "2025-06-09 23:54:48 - INFO - Processing batch 81/82\n",
      "2025-06-09 23:54:48 - INFO - Batch 81/82 - Loss: 0.6708 - Avg batch time: 0.28s\n",
      "2025-06-09 23:54:48 - INFO - \n",
      "Epoch 4 training completed in 39.60s\n",
      "2025-06-09 23:54:48 - INFO - Average training loss: 0.6734\n",
      "Epochs:   5%| | 5/100 [03:02<1:11:18, 45.03s/it, train_loss=0.6734, val_loss=0.6548, best_val_f1=0.2529, lr=1.00e-04, ba2025-06-09 23:54:54 - INFO - \n",
      "Epoch 5/100 - Training phase\n",
      "2025-06-09 23:54:54 - INFO - Processing batch 1/82\n",
      "2025-06-09 23:54:54 - INFO - Batch 1/82 - Loss: 0.6801 - Avg batch time: 0.28s\n",
      "2025-06-09 23:54:58 - INFO - Processing batch 11/82\n",
      "2025-06-09 23:54:59 - INFO - Batch 11/82 - Loss: 0.6789 - Avg batch time: 0.28s\n",
      "2025-06-09 23:55:03 - INFO - Processing batch 21/82\n",
      "2025-06-09 23:55:03 - INFO - Batch 21/82 - Loss: 0.6722 - Avg batch time: 0.28s\n",
      "2025-06-09 23:55:07 - INFO - Processing batch 31/82\n",
      "2025-06-09 23:55:07 - INFO - Batch 31/82 - Loss: 0.6251 - Avg batch time: 0.28s\n",
      "2025-06-09 23:55:12 - INFO - Processing batch 41/82\n",
      "2025-06-09 23:55:12 - INFO - Batch 41/82 - Loss: 0.6345 - Avg batch time: 0.28s\n",
      "2025-06-09 23:55:16 - INFO - Processing batch 51/82\n",
      "2025-06-09 23:55:16 - INFO - Batch 51/82 - Loss: 0.6540 - Avg batch time: 0.28s\n",
      "2025-06-09 23:55:20 - INFO - Processing batch 61/82\n",
      "2025-06-09 23:55:21 - INFO - Batch 61/82 - Loss: 0.6513 - Avg batch time: 0.28s\n",
      "2025-06-09 23:55:25 - INFO - Processing batch 71/82\n",
      "2025-06-09 23:55:25 - INFO - Batch 71/82 - Loss: 0.6449 - Avg batch time: 0.28s\n",
      "2025-06-09 23:55:29 - INFO - Processing batch 81/82\n",
      "2025-06-09 23:55:29 - INFO - Batch 81/82 - Loss: 0.6327 - Avg batch time: 0.28s\n",
      "2025-06-09 23:55:29 - INFO - \n",
      "Epoch 5 training completed in 35.95s\n",
      "2025-06-09 23:55:29 - INFO - Average training loss: 0.6544\n",
      "Epochs:   6%| | 6/100 [03:44<1:09:18, 44.24s/it, train_loss=0.6544, val_loss=0.6412, best_val_f1=0.2529, lr=1.00e-04, ba2025-06-09 23:55:36 - INFO - \n",
      "Epoch 6/100 - Training phase\n",
      "2025-06-09 23:55:37 - INFO - Processing batch 1/82\n",
      "2025-06-09 23:55:37 - INFO - Batch 1/82 - Loss: 0.6375 - Avg batch time: 0.29s\n",
      "2025-06-09 23:55:42 - INFO - Processing batch 11/82\n",
      "2025-06-09 23:55:42 - INFO - Batch 11/82 - Loss: 0.6114 - Avg batch time: 0.28s\n",
      "2025-06-09 23:55:46 - INFO - Processing batch 21/82\n",
      "2025-06-09 23:55:46 - INFO - Batch 21/82 - Loss: 0.6371 - Avg batch time: 0.28s\n",
      "2025-06-09 23:55:51 - INFO - Processing batch 31/82\n",
      "2025-06-09 23:55:51 - INFO - Batch 31/82 - Loss: 0.6479 - Avg batch time: 0.28s\n",
      "2025-06-09 23:55:55 - INFO - Processing batch 41/82\n",
      "2025-06-09 23:55:56 - INFO - Batch 41/82 - Loss: 0.6476 - Avg batch time: 0.28s\n",
      "2025-06-09 23:56:00 - INFO - Processing batch 51/82\n",
      "2025-06-09 23:56:00 - INFO - Batch 51/82 - Loss: 0.6517 - Avg batch time: 0.28s\n",
      "2025-06-09 23:56:04 - INFO - Processing batch 61/82\n",
      "2025-06-09 23:56:05 - INFO - Batch 61/82 - Loss: 0.6232 - Avg batch time: 0.28s\n",
      "2025-06-09 23:56:10 - INFO - Processing batch 71/82\n",
      "2025-06-09 23:56:10 - INFO - Batch 71/82 - Loss: 0.6228 - Avg batch time: 0.28s\n",
      "2025-06-09 23:56:15 - INFO - Processing batch 81/82\n",
      "2025-06-09 23:56:16 - INFO - Batch 81/82 - Loss: 0.6525 - Avg batch time: 0.28s\n",
      "2025-06-09 23:56:16 - INFO - \n",
      "Epoch 6 training completed in 39.47s\n",
      "2025-06-09 23:56:16 - INFO - Average training loss: 0.6404\n",
      "Epochs:   7%| | 7/100 [04:29<1:08:50, 44.42s/it, train_loss=0.6404, val_loss=0.6253, best_val_f1=0.2529, lr=1.00e-04, ba2025-06-09 23:56:21 - INFO - \n",
      "Epoch 7/100 - Training phase\n",
      "2025-06-09 23:56:21 - INFO - Processing batch 1/82\n",
      "2025-06-09 23:56:22 - INFO - Batch 1/82 - Loss: 0.6410 - Avg batch time: 0.28s\n",
      "2025-06-09 23:56:27 - INFO - Processing batch 11/82\n",
      "2025-06-09 23:56:27 - INFO - Batch 11/82 - Loss: 0.6614 - Avg batch time: 0.28s\n",
      "2025-06-09 23:56:31 - INFO - Processing batch 21/82\n",
      "2025-06-09 23:56:31 - INFO - Batch 21/82 - Loss: 0.6384 - Avg batch time: 0.28s\n",
      "2025-06-09 23:56:36 - INFO - Processing batch 31/82\n",
      "2025-06-09 23:56:36 - INFO - Batch 31/82 - Loss: 0.6361 - Avg batch time: 0.28s\n",
      "2025-06-09 23:56:40 - INFO - Processing batch 41/82\n",
      "2025-06-09 23:56:41 - INFO - Batch 41/82 - Loss: 0.6364 - Avg batch time: 0.28s\n",
      "2025-06-09 23:56:45 - INFO - Processing batch 51/82\n",
      "2025-06-09 23:56:45 - INFO - Batch 51/82 - Loss: 0.6172 - Avg batch time: 0.28s\n",
      "2025-06-09 23:56:49 - INFO - Processing batch 61/82\n",
      "2025-06-09 23:56:49 - INFO - Batch 61/82 - Loss: 0.6014 - Avg batch time: 0.28s\n",
      "2025-06-09 23:56:54 - INFO - Processing batch 71/82\n",
      "2025-06-09 23:56:54 - INFO - Batch 71/82 - Loss: 0.5883 - Avg batch time: 0.28s\n",
      "2025-06-09 23:56:58 - INFO - Processing batch 81/82\n",
      "2025-06-09 23:56:58 - INFO - Batch 81/82 - Loss: 0.6044 - Avg batch time: 0.28s\n",
      "2025-06-09 23:56:59 - INFO - \n",
      "Epoch 7 training completed in 37.40s\n",
      "2025-06-09 23:56:59 - INFO - Average training loss: 0.6290\n",
      "Epochs:   8%| | 8/100 [05:12<1:07:18, 43.90s/it, train_loss=0.6290, val_loss=0.6175, best_val_f1=0.2529, lr=1.00e-04, ba2025-06-09 23:57:04 - INFO - \n",
      "Epoch 8/100 - Training phase\n",
      "2025-06-09 23:57:04 - INFO - Processing batch 1/82\n",
      "2025-06-09 23:57:04 - INFO - Batch 1/82 - Loss: 0.6059 - Avg batch time: 0.29s\n",
      "2025-06-09 23:57:08 - INFO - Processing batch 11/82\n",
      "2025-06-09 23:57:09 - INFO - Batch 11/82 - Loss: 0.6413 - Avg batch time: 0.28s\n",
      "2025-06-09 23:57:13 - INFO - Processing batch 21/82\n",
      "2025-06-09 23:57:13 - INFO - Batch 21/82 - Loss: 0.6002 - Avg batch time: 0.28s\n",
      "2025-06-09 23:57:17 - INFO - Processing batch 31/82\n",
      "2025-06-09 23:57:17 - INFO - Batch 31/82 - Loss: 0.6181 - Avg batch time: 0.28s\n",
      "2025-06-09 23:57:21 - INFO - Processing batch 41/82\n",
      "2025-06-09 23:57:22 - INFO - Batch 41/82 - Loss: 0.5875 - Avg batch time: 0.28s\n",
      "2025-06-09 23:57:26 - INFO - Processing batch 51/82\n",
      "2025-06-09 23:57:26 - INFO - Batch 51/82 - Loss: 0.5641 - Avg batch time: 0.28s\n",
      "2025-06-09 23:57:30 - INFO - Processing batch 61/82\n",
      "2025-06-09 23:57:30 - INFO - Batch 61/82 - Loss: 0.6071 - Avg batch time: 0.28s\n",
      "2025-06-09 23:57:35 - INFO - Processing batch 71/82\n",
      "2025-06-09 23:57:35 - INFO - Batch 71/82 - Loss: 0.5829 - Avg batch time: 0.28s\n",
      "2025-06-09 23:57:39 - INFO - Processing batch 81/82\n",
      "2025-06-09 23:57:40 - INFO - Batch 81/82 - Loss: 0.5920 - Avg batch time: 0.28s\n",
      "2025-06-09 23:57:40 - INFO - \n",
      "Epoch 8 training completed in 35.88s\n",
      "2025-06-09 23:57:40 - INFO - Average training loss: 0.6115\n",
      "Epochs:   9%| | 9/100 [05:53<1:05:10, 42.97s/it, train_loss=0.6115, val_loss=0.5954, best_val_f1=0.2529, lr=1.00e-04, ba2025-06-09 23:57:45 - INFO - \n",
      "Epoch 9/100 - Training phase\n",
      "2025-06-09 23:57:45 - INFO - Processing batch 1/82\n",
      "2025-06-09 23:57:45 - INFO - Batch 1/82 - Loss: 0.6070 - Avg batch time: 0.29s\n",
      "2025-06-09 23:57:49 - INFO - Processing batch 11/82\n",
      "2025-06-09 23:57:50 - INFO - Batch 11/82 - Loss: 0.6137 - Avg batch time: 0.28s\n",
      "2025-06-09 23:57:53 - INFO - Processing batch 21/82\n",
      "2025-06-09 23:57:54 - INFO - Batch 21/82 - Loss: 0.6318 - Avg batch time: 0.28s\n",
      "2025-06-09 23:57:58 - INFO - Processing batch 31/82\n",
      "2025-06-09 23:57:58 - INFO - Batch 31/82 - Loss: 0.6036 - Avg batch time: 0.28s\n",
      "2025-06-09 23:58:03 - INFO - Processing batch 41/82\n",
      "2025-06-09 23:58:03 - INFO - Batch 41/82 - Loss: 0.5880 - Avg batch time: 0.28s\n",
      "2025-06-09 23:58:08 - INFO - Processing batch 51/82\n",
      "2025-06-09 23:58:08 - INFO - Batch 51/82 - Loss: 0.5488 - Avg batch time: 0.28s\n",
      "2025-06-09 23:58:12 - INFO - Processing batch 61/82\n",
      "2025-06-09 23:58:13 - INFO - Batch 61/82 - Loss: 0.5713 - Avg batch time: 0.28s\n",
      "2025-06-09 23:58:17 - INFO - Processing batch 71/82\n",
      "2025-06-09 23:58:17 - INFO - Batch 71/82 - Loss: 0.5938 - Avg batch time: 0.28s\n",
      "2025-06-09 23:58:21 - INFO - Processing batch 81/82\n",
      "2025-06-09 23:58:22 - INFO - Batch 81/82 - Loss: 0.5828 - Avg batch time: 0.28s\n",
      "2025-06-09 23:58:22 - INFO - \n",
      "Epoch 9 training completed in 36.88s\n",
      "2025-06-09 23:58:22 - INFO - Average training loss: 0.5929\n",
      "Epochs:  10%| | 10/100 [06:36<1:04:13, 42.82s/it, train_loss=0.5929, val_loss=0.5807, best_val_f1=0.2529, lr=1.00e-04, b2025-06-09 23:58:27 - INFO - \n",
      "Epoch 10/100 - Training phase\n",
      "2025-06-09 23:58:28 - INFO - Processing batch 1/82\n",
      "2025-06-09 23:58:28 - INFO - Batch 1/82 - Loss: 0.5709 - Avg batch time: 0.28s\n",
      "2025-06-09 23:58:32 - INFO - Processing batch 11/82\n",
      "2025-06-09 23:58:32 - INFO - Batch 11/82 - Loss: 0.5906 - Avg batch time: 0.28s\n",
      "2025-06-09 23:58:36 - INFO - Processing batch 21/82\n",
      "2025-06-09 23:58:36 - INFO - Batch 21/82 - Loss: 0.5732 - Avg batch time: 0.28s\n",
      "2025-06-09 23:58:40 - INFO - Processing batch 31/82\n",
      "2025-06-09 23:58:41 - INFO - Batch 31/82 - Loss: 0.5944 - Avg batch time: 0.28s\n",
      "2025-06-09 23:58:45 - INFO - Processing batch 41/82\n",
      "2025-06-09 23:58:46 - INFO - Batch 41/82 - Loss: 0.5772 - Avg batch time: 0.28s\n",
      "2025-06-09 23:58:50 - INFO - Processing batch 51/82\n",
      "2025-06-09 23:58:50 - INFO - Batch 51/82 - Loss: 0.5870 - Avg batch time: 0.28s\n",
      "2025-06-09 23:58:55 - INFO - Processing batch 61/82\n",
      "2025-06-09 23:58:55 - INFO - Batch 61/82 - Loss: 0.5677 - Avg batch time: 0.28s\n",
      "2025-06-09 23:58:59 - INFO - Processing batch 71/82\n",
      "2025-06-09 23:58:59 - INFO - Batch 71/82 - Loss: 0.5586 - Avg batch time: 0.28s\n",
      "2025-06-09 23:59:04 - INFO - Processing batch 81/82\n",
      "2025-06-09 23:59:04 - INFO - Batch 81/82 - Loss: 0.5965 - Avg batch time: 0.28s\n",
      "2025-06-09 23:59:04 - INFO - \n",
      "Epoch 10 training completed in 36.49s\n",
      "2025-06-09 23:59:04 - INFO - Average training loss: 0.5887\n",
      "Epochs:  11%| | 11/100 [07:18<1:03:13, 42.62s/it, train_loss=0.5887, val_loss=0.5707, best_val_f1=0.2529, lr=1.00e-04, b2025-06-09 23:59:10 - INFO - \n",
      "Epoch 11/100 - Training phase\n",
      "2025-06-09 23:59:10 - INFO - Processing batch 1/82\n",
      "2025-06-09 23:59:10 - INFO - Batch 1/82 - Loss: 0.5928 - Avg batch time: 0.29s\n",
      "2025-06-09 23:59:14 - INFO - Processing batch 11/82\n",
      "2025-06-09 23:59:14 - INFO - Batch 11/82 - Loss: 0.5734 - Avg batch time: 0.28s\n",
      "2025-06-09 23:59:18 - INFO - Processing batch 21/82\n",
      "2025-06-09 23:59:19 - INFO - Batch 21/82 - Loss: 0.5765 - Avg batch time: 0.28s\n",
      "2025-06-09 23:59:23 - INFO - Processing batch 31/82\n",
      "2025-06-09 23:59:23 - INFO - Batch 31/82 - Loss: 0.5643 - Avg batch time: 0.28s\n",
      "2025-06-09 23:59:28 - INFO - Processing batch 41/82\n",
      "2025-06-09 23:59:29 - INFO - Batch 41/82 - Loss: 0.5645 - Avg batch time: 0.28s\n",
      "2025-06-09 23:59:33 - INFO - Processing batch 51/82\n",
      "2025-06-09 23:59:33 - INFO - Batch 51/82 - Loss: 0.5400 - Avg batch time: 0.28s\n",
      "2025-06-09 23:59:37 - INFO - Processing batch 61/82\n",
      "2025-06-09 23:59:37 - INFO - Batch 61/82 - Loss: 0.5828 - Avg batch time: 0.28s\n",
      "2025-06-09 23:59:42 - INFO - Processing batch 71/82\n",
      "2025-06-09 23:59:42 - INFO - Batch 71/82 - Loss: 0.6071 - Avg batch time: 0.28s\n",
      "2025-06-09 23:59:47 - INFO - Processing batch 81/82\n",
      "2025-06-09 23:59:47 - INFO - Batch 81/82 - Loss: 0.5820 - Avg batch time: 0.28s\n",
      "2025-06-09 23:59:47 - INFO - \n",
      "Epoch 11 training completed in 37.41s\n",
      "2025-06-09 23:59:47 - INFO - Average training loss: 0.5724\n",
      "Epochs:  11%| | 11/100 [08:01<1:03:13, 42.62s/it, train_loss=0.5724, val_loss=0.5530, best_val_f1=0.2529, lr=1.00e-04, b2025-06-09 23:59:53 - INFO - Early stopping triggered: no 'val_f1' improvement in 10 epochs\n",
      "Epochs:  11%| | 11/100 [08:01<1:11:25, 48.15s/it, train_loss=0.5724, val_loss=0.5530, best_val_f1=0.2529, lr=1.00e-04, b\n",
      "2025-06-09 23:59:53 - INFO - Training completed successfully!\n",
      "2025-06-09 23:59:53 - INFO - Final results: {'final_best_score': 0.2528940439224243, 'final_epoch': 11, 'total_bad_epochs': 10}\n",
      "2025-06-09 23:59:53 - INFO - Fold 1 completed successfully\n",
      "2025-06-09 23:59:53 - INFO - Best val_f1: 0.2529\n",
      "2025-06-09 23:59:53 - INFO - \n",
      "============================================================\n",
      "2025-06-09 23:59:53 - INFO - FOLD 2/5\n",
      "2025-06-09 23:59:53 - INFO - ============================================================\n",
      "2025-06-09 23:59:53 - INFO - Train samples: 10394\n",
      "2025-06-09 23:59:53 - INFO - Val samples: 2599\n",
      "2025-06-09 23:59:53 - INFO - Train positive ratio: 0.194\n",
      "2025-06-09 23:59:53 - INFO - Val positive ratio: 0.194\n",
      "2025-06-09 23:59:53 - INFO - Initialized model: EEGCNNBiLSTMClassifier\n",
      "2025-06-09 23:59:53 - INFO - Initialized optimizer: AdamW\n",
      "2025-06-09 23:59:53 - INFO - Initialized scheduler: ReduceLROnPlateau\n",
      "2025-06-09 23:59:53 - INFO - Starting training for fold 2\n",
      "2025-06-09 23:59:53 - INFO - Starting training setup...\n",
      "2025-06-09 23:59:53 - INFO - Model type: Standard\n",
      "2025-06-09 23:59:53 - INFO - Device: cuda\n",
      "2025-06-09 23:59:53 - INFO - Batch size: 128\n",
      "2025-06-09 23:59:53 - INFO - Number of epochs: 100\n",
      "2025-06-09 23:59:53 - INFO - Patience: 10\n",
      "2025-06-09 23:59:53 - INFO - Monitor metric: val_f1\n",
      "2025-06-09 23:59:53 - INFO - Total training batches per epoch: 82\n",
      "2025-06-09 23:59:53 - INFO - Starting training from epoch 1 to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ðŸ—‘ï¸ Overwrite enabled: Removed existing checkpoint at .checkpoints/timeseries_signal_cnn_bilstm_k_fold/fold_2_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]2025-06-09 23:59:53 - INFO - \n",
      "Epoch 1/100 - Training phase\n",
      "2025-06-09 23:59:53 - INFO - Processing batch 1/82\n",
      "2025-06-09 23:59:53 - INFO - Batch 1/82 - Loss: 0.7360 - Avg batch time: 0.28s\n",
      "2025-06-09 23:59:58 - INFO - Processing batch 11/82\n",
      "2025-06-09 23:59:58 - INFO - Batch 11/82 - Loss: 0.7454 - Avg batch time: 0.28s\n",
      "2025-06-10 00:00:03 - INFO - Processing batch 21/82\n",
      "2025-06-10 00:00:03 - INFO - Batch 21/82 - Loss: 0.7713 - Avg batch time: 0.28s\n",
      "2025-06-10 00:00:08 - INFO - Processing batch 31/82\n",
      "2025-06-10 00:00:08 - INFO - Batch 31/82 - Loss: 0.7425 - Avg batch time: 0.28s\n",
      "2025-06-10 00:00:12 - INFO - Processing batch 41/82\n",
      "2025-06-10 00:00:12 - INFO - Batch 41/82 - Loss: 0.7498 - Avg batch time: 0.28s\n",
      "2025-06-10 00:00:16 - INFO - Processing batch 51/82\n",
      "2025-06-10 00:00:17 - INFO - Batch 51/82 - Loss: 0.7268 - Avg batch time: 0.28s\n",
      "2025-06-10 00:00:21 - INFO - Processing batch 61/82\n",
      "2025-06-10 00:00:21 - INFO - Batch 61/82 - Loss: 0.7641 - Avg batch time: 0.28s\n",
      "2025-06-10 00:00:26 - INFO - Processing batch 71/82\n",
      "2025-06-10 00:00:27 - INFO - Batch 71/82 - Loss: 0.7005 - Avg batch time: 0.28s\n",
      "2025-06-10 00:00:31 - INFO - Processing batch 81/82\n",
      "2025-06-10 00:00:31 - INFO - Batch 81/82 - Loss: 0.7361 - Avg batch time: 0.28s\n",
      "2025-06-10 00:00:31 - INFO - \n",
      "Epoch 1 training completed in 38.52s\n",
      "2025-06-10 00:00:31 - INFO - Average training loss: 0.7373\n",
      "Epochs:   2%| | 2/100 [00:44<1:11:56, 44.05s/it, train_loss=0.7373, val_loss=0.8022, best_val_f1=0.3264, lr=1.00e-04, ba2025-06-10 00:00:37 - INFO - \n",
      "Epoch 2/100 - Training phase\n",
      "2025-06-10 00:00:37 - INFO - Processing batch 1/82\n",
      "2025-06-10 00:00:37 - INFO - Batch 1/82 - Loss: 0.7210 - Avg batch time: 0.28s\n",
      "2025-06-10 00:00:41 - INFO - Processing batch 11/82\n",
      "2025-06-10 00:00:42 - INFO - Batch 11/82 - Loss: 0.7338 - Avg batch time: 0.28s\n",
      "2025-06-10 00:00:46 - INFO - Processing batch 21/82\n",
      "2025-06-10 00:00:46 - INFO - Batch 21/82 - Loss: 0.7137 - Avg batch time: 0.28s\n",
      "2025-06-10 00:00:50 - INFO - Processing batch 31/82\n",
      "2025-06-10 00:00:50 - INFO - Batch 31/82 - Loss: 0.7250 - Avg batch time: 0.28s\n",
      "2025-06-10 00:00:55 - INFO - Processing batch 41/82\n",
      "2025-06-10 00:00:55 - INFO - Batch 41/82 - Loss: 0.7135 - Avg batch time: 0.28s\n",
      "2025-06-10 00:00:59 - INFO - Processing batch 51/82\n",
      "2025-06-10 00:00:59 - INFO - Batch 51/82 - Loss: 0.7109 - Avg batch time: 0.28s\n",
      "2025-06-10 00:01:03 - INFO - Processing batch 61/82\n",
      "2025-06-10 00:01:03 - INFO - Batch 61/82 - Loss: 0.7167 - Avg batch time: 0.28s\n",
      "2025-06-10 00:01:08 - INFO - Processing batch 71/82\n",
      "2025-06-10 00:01:08 - INFO - Batch 71/82 - Loss: 0.7084 - Avg batch time: 0.28s\n",
      "2025-06-10 00:01:12 - INFO - Processing batch 81/82\n",
      "2025-06-10 00:01:12 - INFO - Batch 81/82 - Loss: 0.7025 - Avg batch time: 0.28s\n",
      "2025-06-10 00:01:13 - INFO - \n",
      "Epoch 2 training completed in 35.54s\n",
      "2025-06-10 00:01:13 - INFO - Average training loss: 0.7126\n",
      "Epochs:   3%| | 3/100 [01:25<1:08:28, 42.35s/it, train_loss=0.7126, val_loss=0.7713, best_val_f1=0.3264, lr=1.00e-04, ba2025-06-10 00:01:18 - INFO - \n",
      "Epoch 3/100 - Training phase\n",
      "2025-06-10 00:01:18 - INFO - Processing batch 1/82\n",
      "2025-06-10 00:01:19 - INFO - Batch 1/82 - Loss: 0.7007 - Avg batch time: 0.28s\n",
      "2025-06-10 00:01:22 - INFO - Processing batch 11/82\n",
      "2025-06-10 00:01:23 - INFO - Batch 11/82 - Loss: 0.7096 - Avg batch time: 0.28s\n",
      "2025-06-10 00:01:27 - INFO - Processing batch 21/82\n",
      "2025-06-10 00:01:27 - INFO - Batch 21/82 - Loss: 0.6696 - Avg batch time: 0.28s\n",
      "2025-06-10 00:01:32 - INFO - Processing batch 31/82\n",
      "2025-06-10 00:01:32 - INFO - Batch 31/82 - Loss: 0.7042 - Avg batch time: 0.28s\n",
      "2025-06-10 00:01:36 - INFO - Processing batch 41/82\n",
      "2025-06-10 00:01:36 - INFO - Batch 41/82 - Loss: 0.7113 - Avg batch time: 0.28s\n",
      "2025-06-10 00:01:42 - INFO - Processing batch 51/82\n",
      "2025-06-10 00:01:42 - INFO - Batch 51/82 - Loss: 0.7171 - Avg batch time: 0.28s\n",
      "2025-06-10 00:01:46 - INFO - Processing batch 61/82\n",
      "2025-06-10 00:01:46 - INFO - Batch 61/82 - Loss: 0.7165 - Avg batch time: 0.28s\n",
      "2025-06-10 00:01:51 - INFO - Processing batch 71/82\n",
      "2025-06-10 00:01:51 - INFO - Batch 71/82 - Loss: 0.7032 - Avg batch time: 0.28s\n",
      "2025-06-10 00:01:55 - INFO - Processing batch 81/82\n",
      "2025-06-10 00:01:55 - INFO - Batch 81/82 - Loss: 0.6674 - Avg batch time: 0.28s\n",
      "2025-06-10 00:01:56 - INFO - \n",
      "Epoch 3 training completed in 37.41s\n",
      "2025-06-10 00:01:56 - INFO - Average training loss: 0.6965\n",
      "Epochs:   4%| | 4/100 [02:08<1:08:24, 42.76s/it, train_loss=0.6965, val_loss=0.6973, best_val_f1=0.3264, lr=1.00e-04, ba2025-06-10 00:02:01 - INFO - \n",
      "Epoch 4/100 - Training phase\n",
      "2025-06-10 00:02:02 - INFO - Processing batch 1/82\n",
      "2025-06-10 00:02:02 - INFO - Batch 1/82 - Loss: 0.6955 - Avg batch time: 0.29s\n",
      "2025-06-10 00:02:06 - INFO - Processing batch 11/82\n",
      "2025-06-10 00:02:06 - INFO - Batch 11/82 - Loss: 0.6831 - Avg batch time: 0.28s\n",
      "2025-06-10 00:02:10 - INFO - Processing batch 21/82\n",
      "2025-06-10 00:02:10 - INFO - Batch 21/82 - Loss: 0.6666 - Avg batch time: 0.28s\n",
      "2025-06-10 00:02:15 - INFO - Processing batch 31/82\n",
      "2025-06-10 00:02:15 - INFO - Batch 31/82 - Loss: 0.6465 - Avg batch time: 0.28s\n",
      "2025-06-10 00:02:21 - INFO - Processing batch 41/82\n",
      "2025-06-10 00:02:21 - INFO - Batch 41/82 - Loss: 0.6846 - Avg batch time: 0.28s\n",
      "2025-06-10 00:02:26 - INFO - Processing batch 51/82\n",
      "2025-06-10 00:02:26 - INFO - Batch 51/82 - Loss: 0.6448 - Avg batch time: 0.28s\n",
      "2025-06-10 00:02:30 - INFO - Processing batch 61/82\n",
      "2025-06-10 00:02:31 - INFO - Batch 61/82 - Loss: 0.6812 - Avg batch time: 0.28s\n",
      "2025-06-10 00:02:35 - INFO - Processing batch 71/82\n",
      "2025-06-10 00:02:36 - INFO - Batch 71/82 - Loss: 0.6735 - Avg batch time: 0.28s\n",
      "2025-06-10 00:02:40 - INFO - Processing batch 81/82\n",
      "2025-06-10 00:02:40 - INFO - Batch 81/82 - Loss: 0.6579 - Avg batch time: 0.28s\n",
      "2025-06-10 00:02:40 - INFO - \n",
      "Epoch 4 training completed in 38.86s\n",
      "2025-06-10 00:02:40 - INFO - Average training loss: 0.6770\n",
      "Epochs:   5%| | 5/100 [02:52<1:08:43, 43.41s/it, train_loss=0.6770, val_loss=0.6789, best_val_f1=0.3264, lr=1.00e-04, ba2025-06-10 00:02:46 - INFO - \n",
      "Epoch 5/100 - Training phase\n",
      "2025-06-10 00:02:46 - INFO - Processing batch 1/82\n",
      "2025-06-10 00:02:46 - INFO - Batch 1/82 - Loss: 0.6709 - Avg batch time: 0.28s\n",
      "2025-06-10 00:02:50 - INFO - Processing batch 11/82\n",
      "2025-06-10 00:02:51 - INFO - Batch 11/82 - Loss: 0.6706 - Avg batch time: 0.28s\n",
      "2025-06-10 00:02:55 - INFO - Processing batch 21/82\n",
      "2025-06-10 00:02:55 - INFO - Batch 21/82 - Loss: 0.6739 - Avg batch time: 0.28s\n",
      "2025-06-10 00:02:59 - INFO - Processing batch 31/82\n",
      "2025-06-10 00:03:00 - INFO - Batch 31/82 - Loss: 0.6797 - Avg batch time: 0.28s\n",
      "2025-06-10 00:03:04 - INFO - Processing batch 41/82\n",
      "2025-06-10 00:03:04 - INFO - Batch 41/82 - Loss: 0.6494 - Avg batch time: 0.28s\n",
      "2025-06-10 00:03:08 - INFO - Processing batch 51/82\n",
      "2025-06-10 00:03:09 - INFO - Batch 51/82 - Loss: 0.6780 - Avg batch time: 0.28s\n",
      "2025-06-10 00:03:13 - INFO - Processing batch 61/82\n",
      "2025-06-10 00:03:13 - INFO - Batch 61/82 - Loss: 0.6406 - Avg batch time: 0.28s\n",
      "2025-06-10 00:03:18 - INFO - Processing batch 71/82\n",
      "2025-06-10 00:03:18 - INFO - Batch 71/82 - Loss: 0.6688 - Avg batch time: 0.28s\n",
      "2025-06-10 00:03:23 - INFO - Processing batch 81/82\n",
      "2025-06-10 00:03:23 - INFO - Batch 81/82 - Loss: 0.6512 - Avg batch time: 0.28s\n",
      "2025-06-10 00:03:23 - INFO - \n",
      "Epoch 5 training completed in 37.23s\n",
      "2025-06-10 00:03:23 - INFO - Average training loss: 0.6604\n",
      "Epochs:   6%| | 6/100 [03:36<1:08:09, 43.51s/it, train_loss=0.6604, val_loss=0.6621, best_val_f1=0.3264, lr=1.00e-04, ba2025-06-10 00:03:29 - INFO - \n",
      "Epoch 6/100 - Training phase\n",
      "2025-06-10 00:03:30 - INFO - Processing batch 1/82\n",
      "2025-06-10 00:03:30 - INFO - Batch 1/82 - Loss: 0.6530 - Avg batch time: 0.28s\n",
      "2025-06-10 00:03:35 - INFO - Processing batch 11/82\n",
      "2025-06-10 00:03:35 - INFO - Batch 11/82 - Loss: 0.6354 - Avg batch time: 0.28s\n",
      "2025-06-10 00:03:39 - INFO - Processing batch 21/82\n",
      "2025-06-10 00:03:40 - INFO - Batch 21/82 - Loss: 0.6429 - Avg batch time: 0.28s\n",
      "2025-06-10 00:03:44 - INFO - Processing batch 31/82\n",
      "2025-06-10 00:03:44 - INFO - Batch 31/82 - Loss: 0.6735 - Avg batch time: 0.28s\n",
      "2025-06-10 00:03:49 - INFO - Processing batch 41/82\n",
      "2025-06-10 00:03:49 - INFO - Batch 41/82 - Loss: 0.6616 - Avg batch time: 0.28s\n",
      "2025-06-10 00:03:54 - INFO - Processing batch 51/82\n",
      "2025-06-10 00:03:54 - INFO - Batch 51/82 - Loss: 0.6300 - Avg batch time: 0.28s\n",
      "2025-06-10 00:03:58 - INFO - Processing batch 61/82\n",
      "2025-06-10 00:03:59 - INFO - Batch 61/82 - Loss: 0.6377 - Avg batch time: 0.28s\n",
      "2025-06-10 00:04:03 - INFO - Processing batch 71/82\n",
      "2025-06-10 00:04:04 - INFO - Batch 71/82 - Loss: 0.6574 - Avg batch time: 0.28s\n",
      "2025-06-10 00:04:08 - INFO - Processing batch 81/82\n",
      "2025-06-10 00:04:09 - INFO - Batch 81/82 - Loss: 0.6160 - Avg batch time: 0.28s\n",
      "2025-06-10 00:04:09 - INFO - \n",
      "Epoch 6 training completed in 39.43s\n",
      "2025-06-10 00:04:09 - INFO - Average training loss: 0.6447\n",
      "Epochs:   7%| | 7/100 [04:21<1:08:09, 43.97s/it, train_loss=0.6447, val_loss=0.6435, best_val_f1=0.3264, lr=1.00e-04, ba2025-06-10 00:04:14 - INFO - \n",
      "Epoch 7/100 - Training phase\n",
      "2025-06-10 00:04:14 - INFO - Processing batch 1/82\n",
      "2025-06-10 00:04:15 - INFO - Batch 1/82 - Loss: 0.6404 - Avg batch time: 0.28s\n",
      "2025-06-10 00:04:19 - INFO - Processing batch 11/82\n",
      "2025-06-10 00:04:19 - INFO - Batch 11/82 - Loss: 0.6331 - Avg batch time: 0.28s\n",
      "2025-06-10 00:04:24 - INFO - Processing batch 21/82\n",
      "2025-06-10 00:04:24 - INFO - Batch 21/82 - Loss: 0.6188 - Avg batch time: 0.28s\n",
      "2025-06-10 00:04:29 - INFO - Processing batch 31/82\n",
      "2025-06-10 00:04:30 - INFO - Batch 31/82 - Loss: 0.6639 - Avg batch time: 0.28s\n",
      "2025-06-10 00:04:34 - INFO - Processing batch 41/82\n",
      "2025-06-10 00:04:35 - INFO - Batch 41/82 - Loss: 0.6362 - Avg batch time: 0.28s\n",
      "2025-06-10 00:04:39 - INFO - Processing batch 51/82\n",
      "2025-06-10 00:04:39 - INFO - Batch 51/82 - Loss: 0.5990 - Avg batch time: 0.28s\n",
      "2025-06-10 00:04:43 - INFO - Processing batch 61/82\n",
      "2025-06-10 00:04:44 - INFO - Batch 61/82 - Loss: 0.6422 - Avg batch time: 0.28s\n",
      "2025-06-10 00:04:48 - INFO - Processing batch 71/82\n",
      "2025-06-10 00:04:48 - INFO - Batch 71/82 - Loss: 0.6351 - Avg batch time: 0.28s\n",
      "2025-06-10 00:04:53 - INFO - Processing batch 81/82\n",
      "2025-06-10 00:04:53 - INFO - Batch 81/82 - Loss: 0.6428 - Avg batch time: 0.28s\n",
      "2025-06-10 00:04:53 - INFO - \n",
      "Epoch 7 training completed in 38.71s\n",
      "2025-06-10 00:04:53 - INFO - Average training loss: 0.6281\n",
      "Epochs:   8%| | 8/100 [05:06<1:07:50, 44.24s/it, train_loss=0.6281, val_loss=0.6161, best_val_f1=0.3264, lr=1.00e-04, ba2025-06-10 00:04:59 - INFO - \n",
      "Epoch 8/100 - Training phase\n",
      "2025-06-10 00:04:59 - INFO - Processing batch 1/82\n",
      "2025-06-10 00:05:00 - INFO - Batch 1/82 - Loss: 0.6346 - Avg batch time: 0.28s\n",
      "2025-06-10 00:05:04 - INFO - Processing batch 11/82\n",
      "2025-06-10 00:05:04 - INFO - Batch 11/82 - Loss: 0.6242 - Avg batch time: 0.28s\n",
      "2025-06-10 00:05:08 - INFO - Processing batch 21/82\n",
      "2025-06-10 00:05:08 - INFO - Batch 21/82 - Loss: 0.6069 - Avg batch time: 0.28s\n",
      "2025-06-10 00:05:12 - INFO - Processing batch 31/82\n",
      "2025-06-10 00:05:13 - INFO - Batch 31/82 - Loss: 0.6465 - Avg batch time: 0.28s\n",
      "2025-06-10 00:05:17 - INFO - Processing batch 41/82\n",
      "2025-06-10 00:05:17 - INFO - Batch 41/82 - Loss: 0.6247 - Avg batch time: 0.28s\n",
      "2025-06-10 00:05:21 - INFO - Processing batch 51/82\n",
      "2025-06-10 00:05:22 - INFO - Batch 51/82 - Loss: 0.5961 - Avg batch time: 0.28s\n",
      "2025-06-10 00:05:28 - INFO - Processing batch 61/82\n",
      "2025-06-10 00:05:28 - INFO - Batch 61/82 - Loss: 0.5910 - Avg batch time: 0.28s\n",
      "2025-06-10 00:05:32 - INFO - Processing batch 71/82\n",
      "2025-06-10 00:05:33 - INFO - Batch 71/82 - Loss: 0.6102 - Avg batch time: 0.28s\n",
      "2025-06-10 00:05:39 - INFO - Processing batch 81/82\n",
      "2025-06-10 00:05:39 - INFO - Batch 81/82 - Loss: 0.5986 - Avg batch time: 0.28s\n",
      "2025-06-10 00:05:39 - INFO - \n",
      "Epoch 8 training completed in 39.83s\n",
      "2025-06-10 00:05:39 - INFO - Average training loss: 0.6157\n",
      "Epochs:   9%| | 9/100 [05:51<1:07:26, 44.46s/it, train_loss=0.6157, val_loss=0.5945, best_val_f1=0.3264, lr=1.00e-04, ba2025-06-10 00:05:44 - INFO - \n",
      "Epoch 9/100 - Training phase\n",
      "2025-06-10 00:05:44 - INFO - Processing batch 1/82\n",
      "2025-06-10 00:05:44 - INFO - Batch 1/82 - Loss: 0.6075 - Avg batch time: 0.28s\n",
      "2025-06-10 00:05:48 - INFO - Processing batch 11/82\n",
      "2025-06-10 00:05:49 - INFO - Batch 11/82 - Loss: 0.6151 - Avg batch time: 0.28s\n",
      "2025-06-10 00:05:53 - INFO - Processing batch 21/82\n",
      "2025-06-10 00:05:53 - INFO - Batch 21/82 - Loss: 0.5893 - Avg batch time: 0.28s\n",
      "2025-06-10 00:05:57 - INFO - Processing batch 31/82\n",
      "2025-06-10 00:05:58 - INFO - Batch 31/82 - Loss: 0.5697 - Avg batch time: 0.28s\n",
      "2025-06-10 00:06:02 - INFO - Processing batch 41/82\n",
      "2025-06-10 00:06:02 - INFO - Batch 41/82 - Loss: 0.6062 - Avg batch time: 0.28s\n",
      "2025-06-10 00:06:06 - INFO - Processing batch 51/82\n",
      "2025-06-10 00:06:07 - INFO - Batch 51/82 - Loss: 0.6260 - Avg batch time: 0.28s\n",
      "2025-06-10 00:06:11 - INFO - Processing batch 61/82\n",
      "2025-06-10 00:06:11 - INFO - Batch 61/82 - Loss: 0.6195 - Avg batch time: 0.28s\n",
      "2025-06-10 00:06:16 - INFO - Processing batch 71/82\n",
      "2025-06-10 00:06:16 - INFO - Batch 71/82 - Loss: 0.5797 - Avg batch time: 0.28s\n",
      "2025-06-10 00:06:20 - INFO - Processing batch 81/82\n",
      "2025-06-10 00:06:21 - INFO - Batch 81/82 - Loss: 0.6192 - Avg batch time: 0.28s\n",
      "2025-06-10 00:06:21 - INFO - \n",
      "Epoch 9 training completed in 36.65s\n",
      "2025-06-10 00:06:21 - INFO - Average training loss: 0.6020\n",
      "Epochs:  10%| | 10/100 [06:33<1:05:45, 43.84s/it, train_loss=0.6020, val_loss=0.5827, best_val_f1=0.3264, lr=1.00e-04, b2025-06-10 00:06:27 - INFO - \n",
      "Epoch 10/100 - Training phase\n",
      "2025-06-10 00:06:27 - INFO - Processing batch 1/82\n",
      "2025-06-10 00:06:27 - INFO - Batch 1/82 - Loss: 0.5718 - Avg batch time: 0.28s\n",
      "2025-06-10 00:06:31 - INFO - Processing batch 11/82\n",
      "2025-06-10 00:06:31 - INFO - Batch 11/82 - Loss: 0.6040 - Avg batch time: 0.28s\n",
      "2025-06-10 00:06:35 - INFO - Processing batch 21/82\n",
      "2025-06-10 00:06:36 - INFO - Batch 21/82 - Loss: 0.5850 - Avg batch time: 0.28s\n",
      "2025-06-10 00:06:40 - INFO - Processing batch 31/82\n",
      "2025-06-10 00:06:40 - INFO - Batch 31/82 - Loss: 0.5770 - Avg batch time: 0.28s\n",
      "2025-06-10 00:06:44 - INFO - Processing batch 41/82\n",
      "2025-06-10 00:06:45 - INFO - Batch 41/82 - Loss: 0.5975 - Avg batch time: 0.28s\n",
      "2025-06-10 00:06:49 - INFO - Processing batch 51/82\n",
      "2025-06-10 00:06:49 - INFO - Batch 51/82 - Loss: 0.5730 - Avg batch time: 0.28s\n",
      "2025-06-10 00:06:53 - INFO - Processing batch 61/82\n",
      "2025-06-10 00:06:53 - INFO - Batch 61/82 - Loss: 0.5858 - Avg batch time: 0.28s\n",
      "2025-06-10 00:06:58 - INFO - Processing batch 71/82\n",
      "2025-06-10 00:06:58 - INFO - Batch 71/82 - Loss: 0.6109 - Avg batch time: 0.28s\n",
      "2025-06-10 00:07:02 - INFO - Processing batch 81/82\n",
      "2025-06-10 00:07:03 - INFO - Batch 81/82 - Loss: 0.5840 - Avg batch time: 0.28s\n",
      "2025-06-10 00:07:03 - INFO - \n",
      "Epoch 10 training completed in 36.15s\n",
      "2025-06-10 00:07:03 - INFO - Average training loss: 0.5866\n",
      "Epochs:  11%| | 11/100 [07:14<1:03:47, 43.01s/it, train_loss=0.5866, val_loss=0.5697, best_val_f1=0.3264, lr=1.00e-04, b2025-06-10 00:07:08 - INFO - \n",
      "Epoch 11/100 - Training phase\n",
      "2025-06-10 00:07:08 - INFO - Processing batch 1/82\n",
      "2025-06-10 00:07:08 - INFO - Batch 1/82 - Loss: 0.5779 - Avg batch time: 0.28s\n",
      "2025-06-10 00:07:12 - INFO - Processing batch 11/82\n",
      "2025-06-10 00:07:13 - INFO - Batch 11/82 - Loss: 0.5870 - Avg batch time: 0.28s\n",
      "2025-06-10 00:07:17 - INFO - Processing batch 21/82\n",
      "2025-06-10 00:07:17 - INFO - Batch 21/82 - Loss: 0.5492 - Avg batch time: 0.28s\n",
      "2025-06-10 00:07:22 - INFO - Processing batch 31/82\n",
      "2025-06-10 00:07:22 - INFO - Batch 31/82 - Loss: 0.5643 - Avg batch time: 0.28s\n",
      "2025-06-10 00:07:26 - INFO - Processing batch 41/82\n",
      "2025-06-10 00:07:27 - INFO - Batch 41/82 - Loss: 0.5680 - Avg batch time: 0.28s\n",
      "2025-06-10 00:07:32 - INFO - Processing batch 51/82\n",
      "2025-06-10 00:07:32 - INFO - Batch 51/82 - Loss: 0.6037 - Avg batch time: 0.28s\n",
      "2025-06-10 00:07:37 - INFO - Processing batch 61/82\n",
      "2025-06-10 00:07:38 - INFO - Batch 61/82 - Loss: 0.5740 - Avg batch time: 0.28s\n",
      "2025-06-10 00:07:42 - INFO - Processing batch 71/82\n",
      "2025-06-10 00:07:42 - INFO - Batch 71/82 - Loss: 0.5599 - Avg batch time: 0.28s\n",
      "2025-06-10 00:07:46 - INFO - Processing batch 81/82\n",
      "2025-06-10 00:07:47 - INFO - Batch 81/82 - Loss: 0.5307 - Avg batch time: 0.28s\n",
      "2025-06-10 00:07:47 - INFO - \n",
      "Epoch 11 training completed in 39.09s\n",
      "2025-06-10 00:07:47 - INFO - Average training loss: 0.5731\n",
      "Epochs:  11%| | 11/100 [08:00<1:03:47, 43.01s/it, train_loss=0.5731, val_loss=0.5494, best_val_f1=0.3264, lr=1.00e-04, b2025-06-10 00:07:53 - INFO - Early stopping triggered: no 'val_f1' improvement in 10 epochs\n",
      "Epochs:  11%| | 11/100 [08:00<1:11:15, 48.04s/it, train_loss=0.5731, val_loss=0.5494, best_val_f1=0.3264, lr=1.00e-04, b\n",
      "2025-06-10 00:07:53 - INFO - Training completed successfully!\n",
      "2025-06-10 00:07:53 - INFO - Final results: {'final_best_score': 0.32642486691474915, 'final_epoch': 11, 'total_bad_epochs': 10}\n",
      "2025-06-10 00:07:53 - INFO - Fold 2 completed successfully\n",
      "2025-06-10 00:07:53 - INFO - Best val_f1: 0.3264\n",
      "2025-06-10 00:07:53 - INFO - \n",
      "============================================================\n",
      "2025-06-10 00:07:53 - INFO - FOLD 3/5\n",
      "2025-06-10 00:07:53 - INFO - ============================================================\n",
      "2025-06-10 00:07:53 - INFO - Train samples: 10394\n",
      "2025-06-10 00:07:53 - INFO - Val samples: 2599\n",
      "2025-06-10 00:07:53 - INFO - Train positive ratio: 0.194\n",
      "2025-06-10 00:07:53 - INFO - Val positive ratio: 0.194\n",
      "2025-06-10 00:07:53 - INFO - Initialized model: EEGCNNBiLSTMClassifier\n",
      "2025-06-10 00:07:53 - INFO - Initialized optimizer: AdamW\n",
      "2025-06-10 00:07:53 - INFO - Initialized scheduler: ReduceLROnPlateau\n",
      "2025-06-10 00:07:53 - INFO - Starting training for fold 3\n",
      "2025-06-10 00:07:53 - INFO - Starting training setup...\n",
      "2025-06-10 00:07:53 - INFO - Model type: Standard\n",
      "2025-06-10 00:07:53 - INFO - Device: cuda\n",
      "2025-06-10 00:07:53 - INFO - Batch size: 128\n",
      "2025-06-10 00:07:53 - INFO - Number of epochs: 100\n",
      "2025-06-10 00:07:53 - INFO - Patience: 10\n",
      "2025-06-10 00:07:53 - INFO - Monitor metric: val_f1\n",
      "2025-06-10 00:07:53 - INFO - Total training batches per epoch: 82\n",
      "2025-06-10 00:07:53 - INFO - Starting training from epoch 1 to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ðŸ—‘ï¸ Overwrite enabled: Removed existing checkpoint at .checkpoints/timeseries_signal_cnn_bilstm_k_fold/fold_3_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]2025-06-10 00:07:53 - INFO - \n",
      "Epoch 1/100 - Training phase\n",
      "2025-06-10 00:07:53 - INFO - Processing batch 1/82\n",
      "2025-06-10 00:07:54 - INFO - Batch 1/82 - Loss: 0.7194 - Avg batch time: 0.29s\n",
      "2025-06-10 00:07:57 - INFO - Processing batch 11/82\n",
      "2025-06-10 00:07:58 - INFO - Batch 11/82 - Loss: 0.7393 - Avg batch time: 0.28s\n",
      "2025-06-10 00:08:02 - INFO - Processing batch 21/82\n",
      "2025-06-10 00:08:02 - INFO - Batch 21/82 - Loss: 0.7129 - Avg batch time: 0.28s\n",
      "2025-06-10 00:08:07 - INFO - Processing batch 31/82\n",
      "2025-06-10 00:08:07 - INFO - Batch 31/82 - Loss: 0.6835 - Avg batch time: 0.28s\n",
      "2025-06-10 00:08:11 - INFO - Processing batch 41/82\n",
      "2025-06-10 00:08:11 - INFO - Batch 41/82 - Loss: 0.6991 - Avg batch time: 0.28s\n",
      "2025-06-10 00:08:15 - INFO - Processing batch 51/82\n",
      "2025-06-10 00:08:15 - INFO - Batch 51/82 - Loss: 0.7173 - Avg batch time: 0.28s\n",
      "2025-06-10 00:08:20 - INFO - Processing batch 61/82\n",
      "2025-06-10 00:08:20 - INFO - Batch 61/82 - Loss: 0.6752 - Avg batch time: 0.28s\n",
      "2025-06-10 00:08:26 - INFO - Processing batch 71/82\n",
      "2025-06-10 00:08:26 - INFO - Batch 71/82 - Loss: 0.6762 - Avg batch time: 0.28s\n",
      "2025-06-10 00:08:30 - INFO - Processing batch 81/82\n",
      "2025-06-10 00:08:30 - INFO - Batch 81/82 - Loss: 0.6934 - Avg batch time: 0.28s\n",
      "2025-06-10 00:08:30 - INFO - \n",
      "Epoch 1 training completed in 36.92s\n",
      "2025-06-10 00:08:30 - INFO - Average training loss: 0.6946\n",
      "Epochs:   2%| | 2/100 [00:42<1:10:10, 42.97s/it, train_loss=0.6946, val_loss=0.6531, best_val_f1=0.1239, lr=1.00e-04, ba2025-06-10 00:08:36 - INFO - \n",
      "Epoch 2/100 - Training phase\n",
      "2025-06-10 00:08:36 - INFO - Processing batch 1/82\n",
      "2025-06-10 00:08:37 - INFO - Batch 1/82 - Loss: 0.6805 - Avg batch time: 0.29s\n",
      "2025-06-10 00:08:41 - INFO - Processing batch 11/82\n",
      "2025-06-10 00:08:41 - INFO - Batch 11/82 - Loss: 0.6813 - Avg batch time: 0.28s\n",
      "2025-06-10 00:08:45 - INFO - Processing batch 21/82\n",
      "2025-06-10 00:08:45 - INFO - Batch 21/82 - Loss: 0.6938 - Avg batch time: 0.28s\n",
      "2025-06-10 00:08:49 - INFO - Processing batch 31/82\n",
      "2025-06-10 00:08:50 - INFO - Batch 31/82 - Loss: 0.6523 - Avg batch time: 0.28s\n",
      "2025-06-10 00:08:54 - INFO - Processing batch 41/82\n",
      "2025-06-10 00:08:54 - INFO - Batch 41/82 - Loss: 0.6521 - Avg batch time: 0.28s\n",
      "2025-06-10 00:08:58 - INFO - Processing batch 51/82\n",
      "2025-06-10 00:08:59 - INFO - Batch 51/82 - Loss: 0.6845 - Avg batch time: 0.28s\n",
      "2025-06-10 00:09:03 - INFO - Processing batch 61/82\n",
      "2025-06-10 00:09:03 - INFO - Batch 61/82 - Loss: 0.6582 - Avg batch time: 0.28s\n",
      "2025-06-10 00:09:08 - INFO - Processing batch 71/82\n",
      "2025-06-10 00:09:08 - INFO - Batch 71/82 - Loss: 0.6644 - Avg batch time: 0.28s\n",
      "2025-06-10 00:09:12 - INFO - Processing batch 81/82\n",
      "2025-06-10 00:09:12 - INFO - Batch 81/82 - Loss: 0.6789 - Avg batch time: 0.28s\n",
      "2025-06-10 00:09:12 - INFO - \n",
      "Epoch 2 training completed in 36.02s\n",
      "2025-06-10 00:09:12 - INFO - Average training loss: 0.6718\n",
      "Epochs:   3%| | 3/100 [01:25<1:08:40, 42.48s/it, train_loss=0.6718, val_loss=0.6566, best_val_f1=0.1239, lr=1.00e-04, ba2025-06-10 00:09:18 - INFO - \n",
      "Epoch 3/100 - Training phase\n",
      "2025-06-10 00:09:19 - INFO - Processing batch 1/82\n",
      "2025-06-10 00:09:19 - INFO - Batch 1/82 - Loss: 0.6586 - Avg batch time: 0.29s\n",
      "2025-06-10 00:09:23 - INFO - Processing batch 11/82\n",
      "2025-06-10 00:09:23 - INFO - Batch 11/82 - Loss: 0.6479 - Avg batch time: 0.28s\n",
      "2025-06-10 00:09:27 - INFO - Processing batch 21/82\n",
      "2025-06-10 00:09:27 - INFO - Batch 21/82 - Loss: 0.6394 - Avg batch time: 0.28s\n",
      "2025-06-10 00:09:32 - INFO - Processing batch 31/82\n",
      "2025-06-10 00:09:32 - INFO - Batch 31/82 - Loss: 0.6694 - Avg batch time: 0.28s\n",
      "2025-06-10 00:09:36 - INFO - Processing batch 41/82\n",
      "2025-06-10 00:09:36 - INFO - Batch 41/82 - Loss: 0.6362 - Avg batch time: 0.28s\n",
      "2025-06-10 00:09:40 - INFO - Processing batch 51/82\n",
      "2025-06-10 00:09:41 - INFO - Batch 51/82 - Loss: 0.6734 - Avg batch time: 0.28s\n",
      "2025-06-10 00:09:45 - INFO - Processing batch 61/82\n",
      "2025-06-10 00:09:45 - INFO - Batch 61/82 - Loss: 0.6690 - Avg batch time: 0.28s\n",
      "2025-06-10 00:09:51 - INFO - Processing batch 71/82\n",
      "2025-06-10 00:09:51 - INFO - Batch 71/82 - Loss: 0.6430 - Avg batch time: 0.28s\n",
      "2025-06-10 00:09:56 - INFO - Processing batch 81/82\n",
      "2025-06-10 00:09:56 - INFO - Batch 81/82 - Loss: 0.6556 - Avg batch time: 0.28s\n",
      "2025-06-10 00:09:56 - INFO - \n",
      "Epoch 3 training completed in 37.46s\n",
      "2025-06-10 00:09:56 - INFO - Average training loss: 0.6598\n",
      "Epochs:   4%| | 4/100 [02:08<1:08:21, 42.73s/it, train_loss=0.6598, val_loss=0.6469, best_val_f1=0.1239, lr=1.00e-04, ba2025-06-10 00:10:01 - INFO - \n",
      "Epoch 4/100 - Training phase\n",
      "2025-06-10 00:10:02 - INFO - Processing batch 1/82\n",
      "2025-06-10 00:10:02 - INFO - Batch 1/82 - Loss: 0.6398 - Avg batch time: 0.29s\n",
      "2025-06-10 00:10:06 - INFO - Processing batch 11/82\n",
      "2025-06-10 00:10:06 - INFO - Batch 11/82 - Loss: 0.6792 - Avg batch time: 0.28s\n",
      "2025-06-10 00:10:10 - INFO - Processing batch 21/82\n",
      "2025-06-10 00:10:11 - INFO - Batch 21/82 - Loss: 0.6393 - Avg batch time: 0.28s\n",
      "2025-06-10 00:10:15 - INFO - Processing batch 31/82\n",
      "2025-06-10 00:10:16 - INFO - Batch 31/82 - Loss: 0.6192 - Avg batch time: 0.28s\n",
      "2025-06-10 00:10:20 - INFO - Processing batch 41/82\n",
      "2025-06-10 00:10:20 - INFO - Batch 41/82 - Loss: 0.6186 - Avg batch time: 0.28s\n",
      "2025-06-10 00:10:24 - INFO - Processing batch 51/82\n",
      "2025-06-10 00:10:24 - INFO - Batch 51/82 - Loss: 0.6401 - Avg batch time: 0.28s\n",
      "2025-06-10 00:10:29 - INFO - Processing batch 61/82\n",
      "2025-06-10 00:10:29 - INFO - Batch 61/82 - Loss: 0.6135 - Avg batch time: 0.28s\n",
      "2025-06-10 00:10:33 - INFO - Processing batch 71/82\n",
      "2025-06-10 00:10:33 - INFO - Batch 71/82 - Loss: 0.6381 - Avg batch time: 0.28s\n",
      "2025-06-10 00:10:38 - INFO - Processing batch 81/82\n",
      "2025-06-10 00:10:38 - INFO - Batch 81/82 - Loss: 0.6411 - Avg batch time: 0.28s\n",
      "2025-06-10 00:10:38 - INFO - \n",
      "Epoch 4 training completed in 36.53s\n",
      "2025-06-10 00:10:38 - INFO - Average training loss: 0.6452\n",
      "Epochs:   5%| | 5/100 [02:50<1:07:19, 42.52s/it, train_loss=0.6452, val_loss=0.6278, best_val_f1=0.1239, lr=1.00e-04, ba2025-06-10 00:10:44 - INFO - \n",
      "Epoch 5/100 - Training phase\n",
      "2025-06-10 00:10:44 - INFO - Processing batch 1/82\n",
      "2025-06-10 00:10:44 - INFO - Batch 1/82 - Loss: 0.6253 - Avg batch time: 0.29s\n",
      "2025-06-10 00:10:48 - INFO - Processing batch 11/82\n",
      "2025-06-10 00:10:48 - INFO - Batch 11/82 - Loss: 0.6383 - Avg batch time: 0.28s\n",
      "2025-06-10 00:10:52 - INFO - Processing batch 21/82\n",
      "2025-06-10 00:10:53 - INFO - Batch 21/82 - Loss: 0.6194 - Avg batch time: 0.28s\n",
      "2025-06-10 00:10:56 - INFO - Processing batch 31/82\n",
      "2025-06-10 00:10:57 - INFO - Batch 31/82 - Loss: 0.6297 - Avg batch time: 0.28s\n",
      "2025-06-10 00:11:01 - INFO - Processing batch 41/82\n",
      "2025-06-10 00:11:01 - INFO - Batch 41/82 - Loss: 0.6298 - Avg batch time: 0.28s\n",
      "2025-06-10 00:11:05 - INFO - Processing batch 51/82\n",
      "2025-06-10 00:11:05 - INFO - Batch 51/82 - Loss: 0.6093 - Avg batch time: 0.28s\n",
      "2025-06-10 00:11:09 - INFO - Processing batch 61/82\n",
      "2025-06-10 00:11:10 - INFO - Batch 61/82 - Loss: 0.6243 - Avg batch time: 0.28s\n",
      "2025-06-10 00:11:14 - INFO - Processing batch 71/82\n",
      "2025-06-10 00:11:14 - INFO - Batch 71/82 - Loss: 0.6018 - Avg batch time: 0.28s\n",
      "2025-06-10 00:11:18 - INFO - Processing batch 81/82\n",
      "2025-06-10 00:11:18 - INFO - Batch 81/82 - Loss: 0.6227 - Avg batch time: 0.28s\n",
      "2025-06-10 00:11:19 - INFO - \n",
      "Epoch 5 training completed in 34.85s\n",
      "2025-06-10 00:11:19 - INFO - Average training loss: 0.6279\n",
      "Epochs:   6%| | 6/100 [03:31<1:06:00, 42.13s/it, train_loss=0.6279, val_loss=0.6077, best_val_f1=0.1239, lr=1.00e-04, ba2025-06-10 00:11:25 - INFO - \n",
      "Epoch 6/100 - Training phase\n",
      "2025-06-10 00:11:25 - INFO - Processing batch 1/82\n",
      "2025-06-10 00:11:26 - INFO - Batch 1/82 - Loss: 0.6304 - Avg batch time: 0.28s\n",
      "2025-06-10 00:11:29 - INFO - Processing batch 11/82\n",
      "2025-06-10 00:11:30 - INFO - Batch 11/82 - Loss: 0.6522 - Avg batch time: 0.28s\n",
      "2025-06-10 00:11:35 - INFO - Processing batch 21/82\n",
      "2025-06-10 00:11:35 - INFO - Batch 21/82 - Loss: 0.6143 - Avg batch time: 0.28s\n",
      "2025-06-10 00:11:39 - INFO - Processing batch 31/82\n",
      "2025-06-10 00:11:39 - INFO - Batch 31/82 - Loss: 0.6302 - Avg batch time: 0.28s\n",
      "2025-06-10 00:11:43 - INFO - Processing batch 41/82\n",
      "2025-06-10 00:11:43 - INFO - Batch 41/82 - Loss: 0.5943 - Avg batch time: 0.28s\n",
      "2025-06-10 00:11:48 - INFO - Processing batch 51/82\n",
      "2025-06-10 00:11:48 - INFO - Batch 51/82 - Loss: 0.6253 - Avg batch time: 0.28s\n",
      "2025-06-10 00:11:52 - INFO - Processing batch 61/82\n",
      "2025-06-10 00:11:52 - INFO - Batch 61/82 - Loss: 0.6442 - Avg batch time: 0.28s\n",
      "2025-06-10 00:11:57 - INFO - Processing batch 71/82\n",
      "2025-06-10 00:11:57 - INFO - Batch 71/82 - Loss: 0.6163 - Avg batch time: 0.28s\n",
      "2025-06-10 00:12:02 - INFO - Processing batch 81/82\n",
      "2025-06-10 00:12:02 - INFO - Batch 81/82 - Loss: 0.6070 - Avg batch time: 0.28s\n",
      "2025-06-10 00:12:02 - INFO - \n",
      "Epoch 6 training completed in 36.83s\n",
      "2025-06-10 00:12:02 - INFO - Average training loss: 0.6183\n",
      "Epochs:   7%| | 7/100 [04:14<1:05:33, 42.30s/it, train_loss=0.6183, val_loss=0.5917, best_val_f1=0.1239, lr=1.00e-04, ba2025-06-10 00:12:08 - INFO - \n",
      "Epoch 7/100 - Training phase\n",
      "2025-06-10 00:12:08 - INFO - Processing batch 1/82\n",
      "2025-06-10 00:12:08 - INFO - Batch 1/82 - Loss: 0.5966 - Avg batch time: 0.28s\n",
      "2025-06-10 00:12:12 - INFO - Processing batch 11/82\n",
      "2025-06-10 00:12:12 - INFO - Batch 11/82 - Loss: 0.6030 - Avg batch time: 0.28s\n",
      "2025-06-10 00:12:16 - INFO - Processing batch 21/82\n",
      "2025-06-10 00:12:17 - INFO - Batch 21/82 - Loss: 0.5903 - Avg batch time: 0.28s\n",
      "2025-06-10 00:12:21 - INFO - Processing batch 31/82\n",
      "2025-06-10 00:12:21 - INFO - Batch 31/82 - Loss: 0.6092 - Avg batch time: 0.28s\n",
      "2025-06-10 00:12:25 - INFO - Processing batch 41/82\n",
      "2025-06-10 00:12:25 - INFO - Batch 41/82 - Loss: 0.6325 - Avg batch time: 0.28s\n",
      "2025-06-10 00:12:30 - INFO - Processing batch 51/82\n",
      "2025-06-10 00:12:30 - INFO - Batch 51/82 - Loss: 0.6192 - Avg batch time: 0.28s\n",
      "2025-06-10 00:12:34 - INFO - Processing batch 61/82\n",
      "2025-06-10 00:12:34 - INFO - Batch 61/82 - Loss: 0.6024 - Avg batch time: 0.28s\n",
      "2025-06-10 00:12:39 - INFO - Processing batch 71/82\n",
      "2025-06-10 00:12:39 - INFO - Batch 71/82 - Loss: 0.5654 - Avg batch time: 0.28s\n",
      "2025-06-10 00:12:43 - INFO - Processing batch 81/82\n",
      "2025-06-10 00:12:43 - INFO - Batch 81/82 - Loss: 0.6085 - Avg batch time: 0.28s\n",
      "2025-06-10 00:12:44 - INFO - \n",
      "Epoch 7 training completed in 35.78s\n",
      "2025-06-10 00:12:44 - INFO - Average training loss: 0.6061\n",
      "Epochs:   7%| | 7/100 [04:54<1:16:03, 49.07s/it, train_loss=0.6183, val_loss=0.5917, best_val_f1=0.1239, lr=1.00e-04, ba\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcnn_lstm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EEGCNNBiLSTMClassifier \n\u001b[1;32m      4\u001b[0m train_context \u001b[38;5;241m=\u001b[39m training_context\u001b[38;5;241m.\u001b[39mswitch_to(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtraditional_k_fold_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEEGCNNBiLSTMClassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_channels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m19\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcnn_dropout_prob\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmlp_dropout_prob\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactivation_mlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleaky_relu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcnn_use_batch_norm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_batch_norm_mlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHECKPOINT_ROOT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimeseries_signal_cnn_bilstm_k_fold\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# slightly higher batch size for faster training\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 55\u001b[0m, in \u001b[0;36mtraditional_k_fold_train\u001b[0;34m(model_class, model_parameters, save_path, batch_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;66;03m# force import to avoid bug\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m aggregated_train_history, aggregated_val_history, fold_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_k_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# dataset to use\u001b[39;49;00m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_tr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclips\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# train models\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# optimizer\u001b[39;49;00m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBCEWithLogitsLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbetas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# scheduler\u001b[39;49;00m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfactor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hidden attribute\u001b[39;49;00m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m plot_training_loss(aggregated_train_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], aggregated_val_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:1220\u001b[0m, in \u001b[0;36mtrain_k_fold\u001b[0;34m(dataset, labels, model_class, model_kwargs, criterion, optimizer_class, optimizer_kwargs, device, save_dir, k_folds, stratified, scheduler_class, scheduler_kwargs, monitor, patience, num_epochs, grad_clip, batch_size, use_gnn, wandb_config, wandb_project, log_wandb, random_state)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# Train this fold\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1220\u001b[0m     train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_clip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Always overwrite for k-fold\u001b[39;49;00m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_wandb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_run_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_wandb_run_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtry_load_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Don't load checkpoint for k-fold\u001b[39;49;00m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;66;03m# Get best scores for this fold\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m     best_train_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(train_history\u001b[38;5;241m.\u001b[39mget(monitor\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m), [\u001b[38;5;241m0\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m monitor \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(train_history\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)]))\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:580\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, wandb_config, wandb_project, wandb_run_name, log_wandb, try_load_checkpoint, original_dataset)\u001b[0m\n\u001b[1;32m    577\u001b[0m patient_targets \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 580\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_batch_item_val \u001b[38;5;129;01min\u001b[39;00m val_loader: \u001b[38;5;66;03m# Use a different variable name\u001b[39;00m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m use_gnn:\n\u001b[1;32m    582\u001b[0m             curr_batch \u001b[38;5;241m=\u001b[39m data_batch_item_val\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/timeseries_eeg_dataset.py:421\u001b[0m, in \u001b[0;36mTimeseriesEEGDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(idx) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processed_items_count:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of range [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processed_items_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cached_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[idx]\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/timeseries_eeg_dataset.py:449\u001b[0m, in \u001b[0;36mTimeseriesEEGDataset._get_cached_item\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    447\u001b[0m file_path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_file_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:1426\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m-> 1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_is_zipfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   1431\u001b[0m         overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:433\u001b[0m, in \u001b[0;36m_is_zipfile\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Read the first few bytes and match against the ZIP file signature\u001b[39;00m\n\u001b[1;32m    432\u001b[0m local_header_magic_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 433\u001b[0m read_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlocal_header_magic_number\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m f\u001b[38;5;241m.\u001b[39mseek(start)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m read_bytes \u001b[38;5;241m==\u001b[39m local_header_magic_number\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%aimport src.layers.cnn.cnn_lstm\n",
    "from src.layers.cnn.cnn_lstm import EEGCNNBiLSTMClassifier \n",
    "\n",
    "train_context = training_context.switch_to('signal')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGCNNBiLSTMClassifier,\n",
    "    model_parameters={\n",
    "        \"input_channels\": 19,\n",
    "        \"output_dim\": 1,\n",
    "        \"cnn_dropout_prob\": 0.3,\n",
    "        \"mlp_dropout_prob\": 0.3,\n",
    "        \"activation_mlp\": \"leaky_relu\",\n",
    "        \"cnn_use_batch_norm\": True,\n",
    "        \"use_batch_norm_mlp\": True,\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_cnn_bilstm_k_fold\",\n",
    "    batch_size=128 # slightly higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae02a362",
   "metadata": {},
   "source": [
    "## Training and Evaluation - Graph-Based Models\n",
    "\n",
    "This section focuses on training and evaluating Graph Neural Network (GNN) models on the selected graph-based datasets. These models leverage the spatial and functional relationships between EEG electrodes to improve seizure detection accuracy.\n",
    "\n",
    "### Available Graph-Based Architectures\n",
    "\n",
    "The notebook implements several hybrid architectures that combine temporal and graph processing:\n",
    "\n",
    "- **CNN-BiLSTM-GCN**: Combines Convolutional Neural Networks for feature extraction, Bidirectional LSTM for temporal modeling, and Graph Convolutional Networks for spatial relationships\n",
    "- **CNN-BiLSTM-GAT**: Similar to above but uses Graph Attention Networks instead of GCN for learning adaptive attention weights between electrodes\n",
    "- **CNN-BiLSTM-Attention-GNN**: Enhanced version with attention mechanisms in both temporal and graph components\n",
    "\n",
    "### Graph Construction Strategies\n",
    "\n",
    "The models can be trained on different graph construction approaches:\n",
    "- **Spatial graphs**: Based on physical electrode distances (19 channels)\n",
    "- **Correlation graphs**: Dynamic graphs based on signal correlations (top-k=5)\n",
    "- **Absolute difference correlation**: Advanced correlation-based graphs (top-k=8)\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "All graph models use:\n",
    "- **Optimizer**: AdamW with learning rate 1e-4 and weight decay 0.01\n",
    "- **Loss function**: BCEWithLogitsLoss (unweighted due to balanced sampling)\n",
    "- **Scheduler**: ReduceLROnPlateau with factor 0.5 and patience 5\n",
    "- **Early stopping**: Patience of 10 epochs based on validation F1 score\n",
    "- **Data handling**: GeoDataLoader for efficient graph batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8277022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure to clean up CUDA memory before this big model\n",
    "from src.utils.cuda import clean_cuda_memory_usage\n",
    "clean_cuda_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b336bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒ Switching context to 'SPATIAL' dataset...\n",
      "\n",
      "--- Lazily creating loaders for 'spatial' ---\n",
      "\n",
      "--- Creating train/val subsets for 'spatial' dataset ---\n",
      "Splitting 12993 samples -> Train: 10394, Val: 2599\n",
      "[00:53:44] Train labels: 0 -> 8375, 1 -> 2019\n",
      "[00:53:44] Val labels:   0 -> 2101, 1 -> 498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "2025-06-10 00:53:44 - INFO - Found 0 graph-level features: []\n",
      "2025-06-10 00:53:44 - INFO - Found 0 graph-level features: []\n",
      "2025-06-10 00:53:44 - INFO - Found 0 graph-level features: []\n",
      "2025-06-10 00:53:44 - INFO - Starting 5-fold cross-validation\n",
      "2025-06-10 00:53:44 - INFO - Dataset size: 12993\n",
      "2025-06-10 00:53:44 - INFO - Stratified: True\n",
      "2025-06-10 00:53:44 - INFO - Batch size: 32\n",
      "2025-06-10 00:53:44 - INFO - Using stratified k-fold with label distribution\n",
      "2025-06-10 00:53:44 - INFO - Created 5 folds\n",
      "2025-06-10 00:53:44 - INFO - Folds: [(array([    0,     2,     3, ..., 12990, 12991, 12992]), array([    1,     5,     6, ..., 12979, 12983, 12985])), (array([    0,     1,     2, ..., 12988, 12989, 12990]), array([    3,    19,    27, ..., 12977, 12991, 12992])), (array([    0,     1,     3, ..., 12987, 12991, 12992]), array([    2,     7,    17, ..., 12988, 12989, 12990])), (array([    0,     1,     2, ..., 12990, 12991, 12992]), array([    4,     9,    12, ..., 12975, 12978, 12987])), (array([    1,     2,     3, ..., 12990, 12991, 12992]), array([    0,    10,    16, ..., 12976, 12981, 12986]))]\n",
      "2025-06-10 00:53:44 - INFO - \n",
      "============================================================\n",
      "2025-06-10 00:53:44 - INFO - FOLD 1/5\n",
      "2025-06-10 00:53:44 - INFO - ============================================================\n",
      "2025-06-10 00:53:44 - INFO - Train samples: 10394\n",
      "2025-06-10 00:53:44 - INFO - Val samples: 2599\n",
      "2025-06-10 00:53:44 - INFO - Train positive ratio: 0.194\n",
      "2025-06-10 00:53:44 - INFO - Val positive ratio: 0.194\n",
      "/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "2025-06-10 00:53:44 - INFO - Found 0 graph-level features: []\n",
      "2025-06-10 00:53:44 - INFO - Found 0 graph-level features: []\n",
      "2025-06-10 00:53:44 - INFO - EEGCNNBiLSTMGCN initialized:\n",
      "2025-06-10 00:53:44 - INFO -   - Node input dim: 3000\n",
      "2025-06-10 00:53:44 - INFO -   - Node feature dim (LSTM output): 128\n",
      "2025-06-10 00:53:44 - INFO -   - GCN hidden dim: 192\n",
      "2025-06-10 00:53:44 - INFO -   - Graph feature dim: 0\n",
      "2025-06-10 00:53:44 - INFO -   - Use graph features: False\n",
      "2025-06-10 00:53:44 - INFO -   - Classifier input dim: 128\n",
      "2025-06-10 00:53:44 - INFO -   - Num classes: 1\n",
      "2025-06-10 00:53:44 - INFO -   - Num channels: 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [0.0001194  0.00049529]\n",
      "Train set class distribution: [8375 2019]\n",
      "ðŸš€ Context ready for 'spatial'.\n",
      "   Train batches: 162, Val batches: 41\n",
      "   Type: spatial\n",
      "   Total Train Samples: 12993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 00:53:45 - INFO - Initialized model: EEGCNNBiLSTMGCN\n",
      "2025-06-10 00:53:45 - INFO - Initialized optimizer: AdamW\n",
      "2025-06-10 00:53:45 - INFO - Initialized scheduler: ReduceLROnPlateau\n",
      "2025-06-10 00:53:45 - INFO - Starting training for fold 1\n",
      "2025-06-10 00:53:45 - INFO - Starting training setup...\n",
      "2025-06-10 00:53:45 - INFO - Model type: GNN\n",
      "2025-06-10 00:53:45 - INFO - Device: cuda\n",
      "2025-06-10 00:53:45 - INFO - Batch size: 32\n",
      "2025-06-10 00:53:45 - INFO - Number of epochs: 100\n",
      "2025-06-10 00:53:45 - INFO - Patience: 10\n",
      "2025-06-10 00:53:45 - INFO - Monitor metric: val_f1\n",
      "2025-06-10 00:53:45 - INFO - Total training batches per epoch: 325\n",
      "2025-06-10 00:53:45 - INFO - Starting training from epoch 1 to 100\n",
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]2025-06-10 00:53:45 - INFO - \n",
      "Epoch 1/100 - Training phase\n",
      "2025-06-10 00:53:45 - INFO - Processing batch 1/325\n",
      "2025-06-10 00:53:45 - INFO - Batch shapes - x: torch.Size([608, 3000]), edge_index: torch.Size([2, 10944]), y: torch.Size([32, 1])\n",
      "2025-06-10 00:53:45 - ERROR - Error in forward pass for batch 0: CUDA out of memory. Tried to allocate 26.98 GiB. GPU 0 has a total capacity of 31.74 GiB of which 23.46 GiB is free. Including non-PyTorch memory, this process has 8.28 GiB memory in use. Of the allocated memory 7.57 GiB is allocated by PyTorch, and 358.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "2025-06-10 00:53:45 - ERROR - Edge index shape: torch.Size([2, 10944])\n",
      "2025-06-10 00:53:45 - ERROR - Edge index content: tensor([[  0,   0,   0,  ..., 607, 607, 607],\n",
      "        [  1,   2,   3,  ..., 604, 605, 606]], device='cuda:0')\n",
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]\n",
      "2025-06-10 00:53:45 - ERROR - Error training fold 1: CUDA out of memory. Tried to allocate 26.98 GiB. GPU 0 has a total capacity of 31.74 GiB of which 23.46 GiB is free. Including non-PyTorch memory, this process has 8.28 GiB memory in use. Of the allocated memory 7.57 GiB is allocated by PyTorch, and 358.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 26.98 GiB. GPU 0 has a total capacity of 31.74 GiB of which 23.46 GiB is free. Including non-PyTorch memory, this process has 8.28 GiB memory in use. Of the allocated memory 7.57 GiB is allocated by PyTorch, and 358.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhybrid\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcnn_bilstm_gcn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EEGCNNBiLSTMGCN\n\u001b[1;32m      3\u001b[0m train_context \u001b[38;5;241m=\u001b[39m training_context\u001b[38;5;241m.\u001b[39mswitch_to(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspatial\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mk_fold_train_shorthand\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEEGCNNBiLSTMGCN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcnn_dropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# slightly higher dropout to avoid overfitting\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlstm_hidden_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlstm_out_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlstm_dropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# slightly higher dropout to avoid overfitting\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder_use_batch_norm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder_use_layer_norm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Parameters for the EEGGCN (graph neural network)\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m192\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mout_channels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpooling_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgcn_use_batch_norm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_conv_layers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgcn_dropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# slightly higher dropout to avoid overfitting\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_channels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m19\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_graph_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHECKPOINT_ROOT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcnn_bilstm_gcn_signal_k_fold_.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# use training loop for GNN models\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# lower batch size (GPU poor)\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 55\u001b[0m, in \u001b[0;36mk_fold_train_shorthand\u001b[0;34m(model_class, model_parameters, save_path, use_gnn, log_wandb, batch_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;66;03m# force import to avoid bug\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m aggregated_train_history, aggregated_val_history, fold_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_k_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# dataset to use\u001b[39;49;00m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_tr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclips\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# train models\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# optimizer\u001b[39;49;00m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBCEWithLogitsLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbetas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# scheduler\u001b[39;49;00m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfactor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hidden attribute\u001b[39;49;00m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m plot_training_loss(aggregated_train_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], aggregated_val_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:1220\u001b[0m, in \u001b[0;36mtrain_k_fold\u001b[0;34m(dataset, labels, model_class, model_kwargs, criterion, optimizer_class, optimizer_kwargs, device, save_dir, k_folds, stratified, scheduler_class, scheduler_kwargs, monitor, patience, num_epochs, grad_clip, batch_size, use_gnn, wandb_config, wandb_project, log_wandb, random_state)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# Train this fold\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1220\u001b[0m     train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_clip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Always overwrite for k-fold\u001b[39;49;00m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_wandb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_run_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_wandb_run_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtry_load_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Don't load checkpoint for k-fold\u001b[39;49;00m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;66;03m# Get best scores for this fold\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m     best_train_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(train_history\u001b[38;5;241m.\u001b[39mget(monitor\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m), [\u001b[38;5;241m0\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m monitor \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(train_history\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)]))\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:428\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, wandb_config, wandb_project, wandb_run_name, log_wandb, try_load_checkpoint, original_dataset)\u001b[0m\n\u001b[1;32m    425\u001b[0m         graph_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;66;03m# Use safe model call that handles graph_features parameter automatically\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_model_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcurr_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcurr_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcurr_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mgraph_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;66;03m# unsqueeze y_targets to match the shape of the logits. used later!\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:149\u001b[0m, in \u001b[0;36m_safe_model_call\u001b[0;34m(model, x, edge_index, batch, graph_features)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph_features\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params \u001b[38;5;129;01mand\u001b[39;00m graph_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m         call_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph_features\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m graph_features\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcall_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# Non-GNN models - just pass x\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     call_args \u001b[38;5;241m=\u001b[39m [x]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/layers/hybrid/cnn_bilstm_gcn.py:150\u001b[0m, in \u001b[0;36mEEGCNNBiLSTMGCN.forward\u001b[0;34m(self, x, edge_index, batch, graph_features)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, edge_index: torch\u001b[38;5;241m.\u001b[39mTensor, batch: torch\u001b[38;5;241m.\u001b[39mTensor, graph_features: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    Forward pass of the CNN-BiLSTM-GCN model.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m        torch.Tensor: Class logits for each graph in the batch. Shape: [num_graphs_in_batch, num_classes].\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     node_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannel_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     gcn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcn(node_features, edge_index, batch) \u001b[38;5;66;03m# [num_graphs_in_batch, out_channels]\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Combine with graph-level features if available\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/layers/encoders/cnnbilstm_encoder.py:64\u001b[0m, in \u001b[0;36mEEGCNNBiLSTMEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# [B, time_steps_out, cnn_out_channels]\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Pass through BiLSTM\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [B, lstm_out_dim] (assuming BiLSTM outputs a single vector per batch item)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Apply classifier if configured\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/layers/encoders/bilstm_encoder.py:33\u001b[0m, in \u001b[0;36mEEGBiLSTMEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# x: [B, T, input_size]\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, T, 2*hidden_dim]\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Take last time step\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1124\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1124\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1138\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1146\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 26.98 GiB. GPU 0 has a total capacity of 31.74 GiB of which 23.46 GiB is free. Including non-PyTorch memory, this process has 8.28 GiB memory in use. Of the allocated memory 7.57 GiB is allocated by PyTorch, and 358.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "%aimport src.layers.hybrid.cnn_bilstm_gcn\n",
    "from src.layers.hybrid.cnn_bilstm_gcn import EEGCNNBiLSTMGCN\n",
    "train_context = training_context.switch_to('spatial')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGCNNBiLSTMGCN,\n",
    "    model_parameters={\n",
    "        # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "        \"cnn_dropout\": 0.25, # slightly higher dropout to avoid overfitting\n",
    "        \"lstm_hidden_dim\": 128,\n",
    "        \"lstm_out_dim\": 128,\n",
    "        \"lstm_dropout\": 0.25, # slightly higher dropout to avoid overfitting\n",
    "        \"encoder_use_batch_norm\": True,\n",
    "        \"encoder_use_layer_norm\": False,\n",
    "        # Parameters for the EEGGCN (graph neural network)\n",
    "        \"hidden_dim\": 192,\n",
    "        \"out_channels\": 128,\n",
    "        \"pooling_type\": \"max\",\n",
    "        \"gcn_use_batch_norm\": True,\n",
    "        \"num_conv_layers\": 4,\n",
    "        \"gcn_dropout\": 0.6, # slightly higher dropout to avoid overfitting\n",
    "        \"num_channels\": 19,\n",
    "        \"use_graph_features\": False\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_signal_k_fold_.pt\",\n",
    "    use_gnn=True, # use training loop for GNN models\n",
    "    batch_size=32 # lower batch size (GPU poor)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177fedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.utils.train\n",
    "def wrap_gnn_train(model, save_path):\n",
    "    global graph_training_context\n",
    "    if 'graph_training_context' not in globals():\n",
    "        raise ValueError(\"Graph training context is not initialized. Please initialize it before calling this function.\")\n",
    "    if not isinstance(graph_training_context, TrainingContext):\n",
    "        raise ValueError(\"graph_training_context must be an instance of TrainingContext.\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "    # optimizer = Lion(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5)\n",
    "    loss = nn.BCEWithLogitsLoss()  # Not weighted as we use a balanced sampler!\n",
    "\n",
    "    # train model\n",
    "    train_history, val_history = train_model(\n",
    "        wandb_config=None,\n",
    "        model=model,\n",
    "        train_loader=graph_training_context.train_loader,\n",
    "        val_loader=graph_training_context.val_loader,\n",
    "        criterion=loss,\n",
    "        scheduler=scheduler,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=config[\"epochs\"],\n",
    "        patience=config[\"patience\"],\n",
    "        save_path=save_path,\n",
    "        use_gnn=True,\n",
    "        # hidden attribute\n",
    "        try_load_checkpoint=True,\n",
    "    )\n",
    "    plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720925b0",
   "metadata": {},
   "source": [
    "### Test 3 - First breakthrough model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036835f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm .checkpoints/cnn_bilstm_gcn_test_3_correlation_test_mean_pooling.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a43eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_test_best_model_{DATASET_TYPE}_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm = True,\n",
    "    encoder_use_layer_norm = False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim = 192,\n",
    "    out_channels = 128,\n",
    "    pooling_type = \"max\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_conv_layers = 4,\n",
    "    gcn_dropout = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    "    use_graph_features=False\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a5bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "from src.layers.hybrid.cnn_bilstm_gat import EEGCNNBiLSTMGAT\n",
    "\n",
    "\n",
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gat_test_3_{DATASET_TYPE}_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGAT(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout=0.25,\n",
    "    lstm_hidden_dim=128,\n",
    "    lstm_out_dim=128,\n",
    "    lstm_dropout=0.25,\n",
    "    encoder_use_batch_norm=True,\n",
    "    encoder_use_layer_norm=False,\n",
    "    # Parameters for the GAT (graph attention network)\n",
    "    hidden_dim=128,\n",
    "    out_channels=96,\n",
    "    pooling_type=\"mean\",\n",
    "    gat_use_batch_norm=True,\n",
    "    num_conv_layers=3,\n",
    "    gat_dropout=0.5,\n",
    "    gat_heads=4,  # Number of attention heads\n",
    "    num_channels=19,\n",
    "    # Graph features configuration\n",
    "    graph_feature_dim=graph_feature_dim,\n",
    "    use_graph_features=False if DATASET_TYPE == 'spatial' else True,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_test_3_{DATASET_TYPE}_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim = 128,\n",
    "    out_channels = 96,\n",
    "    pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_conv_layers = 3,\n",
    "    gcn_dropout = 0.5,\n",
    "    num_channels = 19,\n",
    "    # enable graph features\n",
    "    # NOTE: using graph level features gives worse results with spatial dataset\n",
    "    use_graph_features=False if DATASET_TYPE == 'spatial' else True,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11afb6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_test_3_{DATASET_TYPE}_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout=0.25,\n",
    "    lstm_hidden_dim=128,\n",
    "    lstm_out_dim=128,\n",
    "    lstm_dropout=0.25,\n",
    "    encoder_use_batch_norm=True,\n",
    "    encoder_use_layer_norm=False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim=128,\n",
    "    out_channels=128,\n",
    "    pooling_type=\"mean\",\n",
    "    gcn_use_batch_norm=True,\n",
    "    num_conv_layers=3,\n",
    "    gcn_dropout=0.5,\n",
    "    num_channels=19,\n",
    "    use_graph_features=True\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424f5b8b",
   "metadata": {},
   "source": [
    "### Test 4 - Smaller CGN output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_4.pt\"\n",
    "\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 64,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 3,\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eaaf2d",
   "metadata": {},
   "source": [
    "### Test 5 - Smaller GCN output channels + increased embedding length + Deeper GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1335c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_5.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 64,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da60626",
   "metadata": {},
   "source": [
    "\n",
    "### Test 6: slighly bigger GCN output channels\n",
    ">[HIGHEST F1 SCORE EVER RECORDED]\n",
    "```\n",
    "âœ… Checkpoint loaded. Resuming from epoch 33. Best 'val_f1' score: 0.7346\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_6.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 96,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad429a",
   "metadata": {},
   "source": [
    "### Test 7B: Alternative architecture to improve generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f882bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_8.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.35, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.35, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 96,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b4f3c",
   "metadata": {},
   "source": [
    "### Test 7C: slightly bigger GCN layers\n",
    "\n",
    "BEST MODEL YET!\n",
    "\n",
    "(SPATIAL FEATURES!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_best_model_correlation_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm = True,\n",
    "    encoder_use_layer_norm = False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim = 192,\n",
    "    out_channels = 128,\n",
    "    pooling_type = \"max\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_conv_layers = 4,\n",
    "    gcn_dropout = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    "    graph_feature_dim=graph_feature_dim,\n",
    "    use_graph_features=False if DATASET_TYPE == 'spatial' else True,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6355457",
   "metadata": {},
   "source": [
    "### Test 7D: even bigger GCN layers\n",
    "\n",
    "Comparable performance to best model. We might need to increase the number of GCN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed9d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_even_bigger.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm = True,\n",
    "    encoder_use_layer_norm = False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 224,\n",
    "    gcn_out_channels = 192,\n",
    "    gcn_pooling_type = \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50956294",
   "metadata": {},
   "source": [
    "### Test 7E: increased number of GCN layers\n",
    "\n",
    "Assumption: the previous model was unable to learn enough, maybe the GCN was unable to capture\n",
    "\n",
    "```\n",
    "Epochs:   9%| | 9/100 [17:54<3:23:31, 134.20s/it, train_loss=0.4532, val_loss=0.3489, best_val_f1=0.6695, lr=5.00e-05, b2025-06-07 17:01:05 - INFO - \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_even_more_bigger.pt\"\n",
    "model_generalizable_even_more_bigger = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 224,\n",
    "    gcn_out_channels = 192,\n",
    "    gcn_num_layers = 5,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08442186",
   "metadata": {},
   "source": [
    "### Test 7F: Increased number of BiLSTM layers + Test 7E architecture\n",
    "\n",
    "Assumpion: we saw a drammatical increase in accuracy by increasing the number of GCN layers. This hints that the model was now able to learn the most from the embeddings. To improve the performance even further without having to increase the number of GCN layers even more (overall reduce complexity, improve generalization), we will try to increase the number of BiLSTM layers. \n",
    "\n",
    "Using multiple BiLSTM layers will allow embeddings to be processed in a more complex way, potentially capturing more intricate relationships in the data. The GCN layers will take care of the graph structure, while the BiLSTM layers will enhance the temporal dependencies and relationships in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b963a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_even_more_bigger.pt\"\n",
    "model_generalizable_even_more_bigger = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    lstm_num_layers = 2,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 224,\n",
    "    gcn_out_channels = 192,\n",
    "    gcn_num_layers = 5,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663332c",
   "metadata": {},
   "source": [
    "```\n",
    "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]2025-06-07 18:55:16 - INFO -\n",
    "Epochs:   2%| | 2/100 [04:35<7:29:19, 275.10s/it, train_loss=0.6212, val_loss=0.4619, best_val_f1=0.4055, lr=1.00e-04, b2025-06-07 18:59:51 - INFO -\n",
    "Epochs:   3%| | 3/100 [09:09<7:23:49, 274.53s/it, train_loss=0.5819, val_loss=0.4295, best_val_f1=0.4055, lr=1.00e-04, b2025-06-07 19:04:25 - INFO -\n",
    "Epochs:   4%| | 4/100 [13:42<7:18:31, 274.08s/it, train_loss=0.5628, val_loss=0.4437, best_val_f1=0.4055, lr=1.00e-04, b2025-06-07 19:08:59 - INFO -\n",
    "Epochs:   5%| | 5/100 [18:16<7:13:28, 273.78s/it, train_loss=0.5452, val_loss=0.3942, best_val_f1=0.4858, lr=1.00e-04, b2025-06-07 19:13:32 - INFO -\n",
    "Epochs:   6%| | 6/100 [22:49<7:08:41, 273.63s/it, train_loss=0.5334, val_loss=0.4563, best_val_f1=0.4858, lr=1.00e-04, b2025-06-07 19:18:05 - INFO -\n",
    "Epochs:   7%| | 7/100 [27:22<7:04:01, 273.57s/it, train_loss=0.5319, val_loss=0.3738, best_val_f1=0.5137, lr=1.00e-04, b2025-06-07 19:22:39 - INFO -\n",
    "Epochs:   8%| | 8/100 [31:56<6:59:20, 273.48s/it, train_loss=0.5181, val_loss=0.4369, best_val_f1=0.5695, lr=1.00e-04, b2025-06-07 19:27:12 - INFO -\n",
    "Epochs:   9%| | 9/100 [36:29<6:54:50, 273.52s/it, train_loss=0.5220, val_loss=0.4202, best_val_f1=0.5695, lr=1.00e-04, b2025-06-07 19:31:46 - INFO -\n",
    "Epochs:  10%| | 10/100 [41:03<6:50:17, 273.52s/it, train_loss=0.5286, val_loss=0.4167, best_val_f1=0.5695, lr=1.00e-04, 2025-06-07 19:36:19 - INFO -\n",
    "Epochs:  11%| | 11/100 [45:36<6:45:44, 273.53s/it, train_loss=0.5065, val_loss=0.3864, best_val_f1=0.5695, lr=1.00e-04, 2025-06-07 19:40:53 - INFO -\n",
    "Epochs:  12%| | 12/100 [50:10<6:41:03, 273.45s/it, train_loss=0.5158, val_loss=0.5175, best_val_f1=0.5695, lr=5.00e-05, 2025-06-07 19:45:26 - INFO -\n",
    "Epochs:  13%|â–| 13/100 [54:43<6:36:23, 273.37s/it, train_loss=0.5035, val_loss=0.3785, best_val_f1=0.5940, lr=5.00e-05, 2025-06-07 19:49:59 - INFO -\n",
    "Epochs:  14%|â–| 14/100 [59:16<6:31:50, 273.38s/it, train_loss=0.4842, val_loss=0.3838, best_val_f1=0.5981, lr=5.00e-05, 2025-06-07 19:54:33 - INFO -\n",
    "Epochs:  15%|â–| 15/100 [1:03:50<6:27:17, 273.38s/it, train_loss=0.4644, val_loss=0.3493, best_val_f1=0.6106, lr=5.00e-052025-06-07 19:59:06 - INFO -\n",
    "Epochs:  16%|â–| 16/100 [1:08:23<6:22:46, 273.41s/it, train_loss=0.4887, val_loss=0.3737, best_val_f1=0.6106, lr=5.00e-052025-06-07 20:03:39 - INFO -\n",
    "Epochs:  17%|â–| 17/100 [1:12:57<6:18:12, 273.41s/it, train_loss=0.4775, val_loss=0.3565, best_val_f1=0.6106, lr=5.00e-052025-06-07 20:08:13 - INFO -\n",
    "Epochs:  18%|â–| 18/100 [1:17:30<6:13:42, 273.44s/it, train_loss=0.4635, val_loss=0.3704, best_val_f1=0.6106, lr=2.50e-052025-06-07 20:12:46 - INFO -\n",
    "Epochs:  19%|â–| 19/100 [1:22:04<6:09:15, 273.53s/it, train_loss=0.4501, val_loss=0.3635, best_val_f1=0.6131, lr=2.50e-052025-06-07 20:17:20 - INFO -\n",
    "Epochs:  20%|â–| 20/100 [1:26:37<6:04:39, 273.49s/it, train_loss=0.4379, val_loss=0.3638, best_val_f1=0.6179, lr=2.50e-052025-06-07 20:21:53 - INFO -\n",
    "Epochs:  21%|â–| 21/100 [1:31:10<6:00:01, 273.43s/it, train_loss=0.4494, val_loss=0.3543, best_val_f1=0.6179, lr=2.50e-052025-06-07 20:26:27 - INFO -\n",
    "Epochs:  22%|â–| 22/100 [1:35:44<5:55:26, 273.42s/it, train_loss=0.4616, val_loss=0.3616, best_val_f1=0.6659, lr=2.50e-052025-06-07 20:31:00 - INFO -\n",
    "Epochs:  23%|â–| 23/100 [1:40:17<5:50:54, 273.44s/it, train_loss=0.4381, val_loss=0.3532, best_val_f1=0.6659, lr=2.50e-052025-06-07 20:35:34 - INFO -\n",
    "Epochs:  24%|â–| 24/100 [1:44:51<5:46:22, 273.45s/it, train_loss=0.4423, val_loss=0.3635, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:40:07 - INFO -\n",
    "Epochs:  25%|â–Ž| 25/100 [1:49:24<5:41:52, 273.49s/it, train_loss=0.4291, val_loss=0.3473, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:44:41 - INFO -\n",
    "Epochs:  26%|â–Ž| 26/100 [1:53:58<5:37:12, 273.42s/it, train_loss=0.4403, val_loss=0.3380, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:49:14 - INFO -\n",
    "Epochs:  27%|â–Ž| 27/100 [1:58:31<5:32:38, 273.40s/it, train_loss=0.4312, val_loss=0.3374, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:53:47 - INFO -\n",
    "Epochs:  28%|â–Ž| 28/100 [2:03:05<5:28:07, 273.44s/it, train_loss=0.4393, val_loss=0.3441, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:58:21 - INFO -\n",
    "Epochs:  29%|â–Ž| 29/100 [2:07:38<5:23:35, 273.46s/it, train_loss=0.4226, val_loss=0.3392, best_val_f1=0.6659, lr=1.25e-052025-06-07 21:02:54 - INFO -\n",
    "Epochs:  30%|â–Ž| 30/100 [2:12:11<5:19:02, 273.46s/it, train_loss=0.4240, val_loss=0.3525, best_val_f1=0.6659, lr=6.25e-062025-06-07 21:07:28 - INFO -\n",
    "Epochs:  31%|â–Ž| 31/100 [2:16:45<5:14:28, 273.46s/it, train_loss=0.4249, val_loss=0.3492, best_val_f1=0.6659, lr=6.25e-062025-06-07 21:12:01 - INFO -\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_optimized.pt\"\n",
    "model_generalizable_optimized = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 160,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    lstm_num_layers = 2,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 192,\n",
    "    gcn_out_channels = 128,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.5, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c4b3a",
   "metadata": {},
   "source": [
    "### Test 8: Narrow but Deep GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff01da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_narrow_deep_model.pt\"\n",
    "narrow_deep_model = EEGCNNBiLSTMGCN(\n",
    "    # --- Simplify the Temporal Encoder ---\n",
    "    cnn_dropout_prob = 0.2,\n",
    "    lstm_hidden_dim = 64,  # Reduced\n",
    "    lstm_out_dim = 64,     # Reduced\n",
    "    lstm_dropout_prob = 0.2,\n",
    "    # --- Focus on the GCN ---\n",
    "    gcn_hidden_channels = 128, # Keep GCN capacity high\n",
    "    gcn_out_channels = 64,\n",
    "    gcn_num_layers = 5,      # Try going even deeper\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ef8b3",
   "metadata": {},
   "source": [
    "### Test 9: First best model, with wider + deeper GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_new_old_best_model.pt\"\n",
    "new_old_best_model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 128, # from 64 to 128\n",
    "    gcn_num_layers = 4, # from 3 to 4\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351da028",
   "metadata": {},
   "source": [
    "### Best model + attention BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.hybrid.cnn_bilstm_attention_gcn\n",
    "from src.layers.hybrid.cnn_bilstm_attention_gcn import EEGCNNBiLSTMAttentionGNN\n",
    "\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_attention.pt\"\n",
    "model_first_attention = EEGCNNBiLSTMAttentionGNN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 192,\n",
    "    gcn_out_channels = 128,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a8a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from src.utils.train import train_model\n",
    "\n",
    "model = model_small_gcn_bigger_embedding\n",
    "model = model.to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "# optimizer = Lion(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "loss = nn.BCEWithLogitsLoss() # Not weighted as we use a balanced sampler!\n",
    "\n",
    "# empty cache in order to free up VRAM (if available)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# train model\n",
    "train_history, val_history = train_model(\n",
    "    wandb_config=None,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=loss,\n",
    "    scheduler=scheduler,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=config[\"epochs\"],\n",
    "    patience=config[\"patience\"],\n",
    "    save_path=SAVE_PATH,\n",
    "    use_gnn=True,\n",
    "    # hidden attribute\n",
    "    try_load_checkpoint=True,\n",
    ")\n",
    "\n",
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch cuda clear cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63329a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
