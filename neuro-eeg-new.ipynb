{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d97e92",
   "metadata": {},
   "source": [
    "# NeuroGraphNet\n",
    "\n",
    "*A graph-based deep learning framework for EEG seizure detection, designed to improve accuracy and interpretability by leveraging Graph Neural Networks (GNNs) to capture spatial and temporal brain dynamics.*\n",
    "\n",
    "<hr />\n",
    "\n",
    "This notebook presents different approaches to EEG seizure detection using traditional machine learning and deep learning methods as well as a novel approaches using Graph Neural Networks (GNNs). The dataset used a subset of the TUSZ EEG Seizure dataset, which contains EEG recordings from patients with epilepsy.\n",
    "\n",
    "**Authors**: Luca Di Bello, Guillaume AndrÃ© BÃ©lissent, Abdessalem Ben Ali, Beatriz Izquierdo GonzÃ¡lez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5837c2",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b998b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from src.utils.seeder import seed_everything\n",
    "\n",
    "# set seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# create useful constants\n",
    "RANDOM_SEED = 42\n",
    "IS_SCITAS = True # set to True if running on SCITAS cluster\n",
    "LOCAL_DATA_ROOT = Path(\"./data\")\n",
    "DATA_ROOT = Path(\"/home/ogut/data\") if IS_SCITAS else LOCAL_DATA_ROOT\n",
    "CHECKPOINT_ROOT = Path(\"./.checkpoints\")\n",
    "SUBMISSION_ROOT = Path(\"./.submissions\")\n",
    "\n",
    "# create directories if they do not exist\n",
    "CHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# set dataset root\n",
    "seed_everything(RANDOM_SEED)\n",
    "\n",
    "# setup torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f74a9",
   "metadata": {},
   "source": [
    "## Run feature extraction script\n",
    "\n",
    "In order to run all the models in this notebook, is necessary to run the feature extraction script first. This script extracts features from the EEG signals for both the training and test dataset, saving three files: `X_train.npy`, `y_train.npy`, and `X_test.npy`. The features extracted are the same used in the original paper, which are based on the EEG signals.\n",
    "\n",
    "You can run the script by uncommenting and executing the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d28586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# # execute feature extraction script\n",
    "# process = None\n",
    "# try:\n",
    "#     process = subprocess.Popen([\"python3\", \"scripts/feature_extractor.py\"])\n",
    "#     process.wait()\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"Process interrupted, terminating...\")\n",
    "#     if process:\n",
    "#         process.terminate()\n",
    "#         process.wait()\n",
    "# except Exception as e:\n",
    "#     print(f\"Error occurred: {e}\")\n",
    "#     if process:\n",
    "#         process.terminate()\n",
    "#         process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527547f9",
   "metadata": {},
   "source": [
    "## Define paths\n",
    "\n",
    "The following paths are used to load the required data files and save the results of the models. Make sure to adjust them according to your local setup.\n",
    "\n",
    "The current configuration assumes that the data files are located in a folder named `data` within the current working directory. \n",
    "\n",
    "**NOTE:** to simplify the process on SCITAS cluster, we provide a toggle `IS_SCITAS` to set the paths accordingly (_refer to first cell of the notebook_). If you are running this notebook on your local machine, you can set `IS_SCITAS = False` and adjust the paths as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45999291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacial distance matrix between sensors\n",
    "spatial_distance_file = LOCAL_DATA_ROOT / \"distances_3d.csv\"\n",
    "\n",
    "# absdiff correlation matrix\n",
    "absdiff_correlation_file = LOCAL_DATA_ROOT / \"diff_corr_matrix.csv\"\n",
    "\n",
    "# training data\n",
    "train_dir = DATA_ROOT / \"train\"\n",
    "train_dir_metadata = train_dir / \"segments.parquet\"\n",
    "train_dataset_absdiff_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_absdiff_correlation_train\"\n",
    "train_dataset_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_correlation_train\"\n",
    "train_dataset_spatial_dir = LOCAL_DATA_ROOT / \"graph_dataset_spatial_train\"\n",
    "train_dataset_timeseries_feature_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_features\")\n",
    "train_dataset_timeseries_signal_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_signal\")\n",
    "\n",
    "# test data\n",
    "test_dir = DATA_ROOT / \"test\"\n",
    "test_dir_metadata = test_dir / \"segments.parquet\"\n",
    "test_dataset_absdiff_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_absdiff_correlation_test\"\n",
    "test_dataset_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_correlation_test\"\n",
    "test_dataset_spatial_dir = LOCAL_DATA_ROOT / \"graph_dataset_spatial_test\"\n",
    "test_dataset_timeseries_signal_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_signal\")\n",
    "test_dataset_timeseries_feature_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_features\")\n",
    "\n",
    "# additional features\n",
    "extracted_features_dir = LOCAL_DATA_ROOT / \"extracted_features\"\n",
    "embeddings_dir =  LOCAL_DATA_ROOT / \"embeddings\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f1780",
   "metadata": {},
   "source": [
    "## Loading Train and Test Clips from the Dataset\n",
    "\n",
    "To load patient clips from the dataset, we use the `load_clips` function. This function retrieves EEG signals and labels from the specified paths and returns them as NumPy arrays.\n",
    "\n",
    "Different versions of Pandas may return either a MultiIndex or a single index, even when called with the same parameters. To address this inconsistency, we use the `ensure_eeg_multiindex` function to ensure that the resulting DataFrame has a MultiIndex structure. This is essential for subsequent processing steps.\n",
    "\n",
    "If a MultiIndex is not present, it will be created using the following levels: `patient_id`, `clip_id`, and `channel`. This structure is crucial for organizing the dataset, as EEG signals are grouped by patient, clip, and channel. It also ensures compatibility with existing code that expects this format, such as the `EEGDataset` class from the [seizure-eeg](https://www.piwheels.org/project/seiz-eeg/) package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d93851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.index import ensure_eeg_multiindex \n",
    "\n",
    "# Load clips from datasets\n",
    "clips_tr = pd.read_parquet(train_dir_metadata)\n",
    "clips_tr = ensure_eeg_multiindex(clips_tr)\n",
    "clips_tr['id'] = clips_tr.index.map(lambda x: '_'.join(str(i) for i in x))\n",
    "assert clips_tr.id.nunique() == len(clips_tr), \"There are duplicate IDs\"\n",
    "clips_tr = clips_tr[~clips_tr.label.isna()].reset_index()\n",
    "\n",
    "# Load clips from datasets\n",
    "clips_te = pd.read_parquet(test_dir_metadata)\n",
    "clips_te = ensure_eeg_multiindex(clips_te)\n",
    "clips_te['id'] = clips_te.index.map(lambda x: '_'.join(str(i) for i in x))\n",
    "assert clips_te.id.nunique() == len(clips_te), \"There are duplicate IDs\"\n",
    "clips_te = clips_te.reset_index()\n",
    "\n",
    "# sort in order to maintain the same submission order\n",
    "clips_te = clips_te.sort_values(by=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f7bc0",
   "metadata": {},
   "source": [
    "## Loading Datasets\n",
    "\n",
    "This notebook explores various approaches to EEG seizure detection, requiring multiple dataset variants with distinct preprocessing strategies (e.g., raw EEG signals, extracted features, and diverse graph construction methods). The `GraphEEGDataset` class, a custom implementation of `torch.utils.data.Dataset`, is used to load these datasets based on specified parameters.\n",
    "\n",
    "The `GraphEEGDataset` class is designed to support all preprocessing strategies, including graph-based approaches. It preprocesses EEG data on-the-fly, offering flexibility in data handling and model input preparation. Additionally, it includes a caching mechanism to store preprocessed data on disk. This mechanism ensures that subsequent calls with identical parameters load precomputed data, significantly reducing dataset loading time during repeated runs. This feature has been instrumental in accelerating development and experimentation within this notebook.\n",
    "\n",
    "Specifically, we will load the following datasets:\n",
    "\n",
    "A) **EEG signals**: \n",
    "\n",
    "- **Raw EEG**:\n",
    "\n",
    "    - Feature-based\n",
    "\n",
    "        - Raw EEG signals + signal time-filtering/rereferencing/normalization preprocessing\n",
    "\n",
    "    - Graph-based\n",
    "\n",
    "        - Raw EEG signals + spatial graph construction strategy + signal time-filtering/rereferencing/normalization preprocessing + graph-features\n",
    "\n",
    "        - Raw EEG signals + correlation graph construction strategy (top-k policy with k=10) + signal time-filtering/rereferencing/normalization preprocessing + graph-features\n",
    "\n",
    "        - Raw EEG signals + absolute difference correlation graph construction strategy (top-k policy with k=10) + signal time-filtering/rereferencing/normalization preprocessing + graph-features\n",
    "\n",
    "- **Feature-based EEG**:\n",
    "\n",
    "    - Extracted features + spatial graph construction strategy + signal time-filtering/rereferencing/normalization preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e759ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generael settings\n",
    "\n",
    "# bandpass filter settings (signal time-filtering)\n",
    "low_bandpass_frequency = 0.5\n",
    "high_bandpass_frequency = 50\n",
    "\n",
    "# additional settings\n",
    "# NOTE: the training already fights class imbalance, so this is not used\n",
    "oversampling_power = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef132cd",
   "metadata": {},
   "source": [
    "### Raw-EEG signal datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8110e5",
   "metadata": {},
   "source": [
    "#### A) Spatial graph construction strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a686e9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "2025-06-10 02:38:40 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 02:38:40 - INFO - Dataset parameters:\n",
      "2025-06-10 02:38:40 - INFO -   - Root directory: data/graph_dataset_spatial_train\n",
      "2025-06-10 02:38:40 - INFO -   - Edge strategy: spatial\n",
      "2025-06-10 02:38:40 - INFO -   - Top-k neighbors: None\n",
      "2025-06-10 02:38:40 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 02:38:40 - INFO -   - Force reprocess: False\n",
      "2025-06-10 02:38:40 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 02:38:40 - INFO -   - Segment length: 3000\n",
      "2025-06-10 02:38:40 - INFO -   - Apply filtering: True\n",
      "2025-06-10 02:38:40 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 02:38:40 - INFO -   - Apply normalization: True\n",
      "2025-06-10 02:38:40 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 02:38:40 - INFO -   - Test mode: False\n",
      "2025-06-10 02:38:40 - INFO -   - Extract graph features: False\n",
      "2025-06-10 02:38:40 - INFO -   - Diff Corr Matrix Path: None\n",
      "2025-06-10 02:38:40 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 02:38:40 - INFO - Setting up signal filters...\n",
      "2025-06-10 02:38:40 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 02:38:40 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 02:38:40 - INFO - Loaded 361 spatial distances in 0.01s\n",
      "2025-06-10 02:38:40 - INFO - Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset_spatial_tr: 12993\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_spatial_tr = GraphEEGDataset(\n",
    "    root=train_dataset_spatial_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"spatial\",\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file\n",
    "    ),\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=False,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_spatial_tr: {len(dataset_spatial_tr)}\")\n",
    "print(f' Eliminated IDs: {dataset_spatial_tr.ids_to_eliminate}')\n",
    "clips_spatial_tr = clips_tr[~clips_tr.index.isin(dataset_spatial_tr.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b33fe510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 02:38:40 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 02:38:40 - INFO - Dataset parameters:\n",
      "2025-06-10 02:38:40 - INFO -   - Root directory: data/graph_dataset_spatial_test\n",
      "2025-06-10 02:38:40 - INFO -   - Edge strategy: spatial\n",
      "2025-06-10 02:38:40 - INFO -   - Top-k neighbors: None\n",
      "2025-06-10 02:38:40 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 02:38:40 - INFO -   - Force reprocess: False\n",
      "2025-06-10 02:38:40 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 02:38:40 - INFO -   - Segment length: 3000\n",
      "2025-06-10 02:38:40 - INFO -   - Apply filtering: True\n",
      "2025-06-10 02:38:40 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 02:38:40 - INFO -   - Apply normalization: True\n",
      "2025-06-10 02:38:40 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 02:38:40 - INFO -   - Test mode: True\n",
      "2025-06-10 02:38:40 - INFO -   - Extract graph features: False\n",
      "2025-06-10 02:38:40 - INFO -   - Diff Corr Matrix Path: None\n",
      "2025-06-10 02:38:40 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 02:38:40 - INFO - Setting up signal filters...\n",
      "2025-06-10 02:38:40 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 02:38:40 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 02:38:40 - INFO - Loaded 361 spatial distances in 0.01s\n",
      "2025-06-10 02:38:40 - INFO - Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset_spatial_te: 3614\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "# load training dataset\n",
    "dataset_spatial_te = GraphEEGDataset(\n",
    "    root=test_dataset_spatial_dir,\n",
    "    clips=clips_te,\n",
    "    signal_folder=test_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"spatial\",\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file\n",
    "    ),\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    is_test=True,\n",
    "    # extract graph features\n",
    "    extract_graph_features=False,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_spatial_te: {len(dataset_spatial_te)}\")\n",
    "print(f' Eliminated IDs: {dataset_spatial_te.ids_to_eliminate}')\n",
    "clips_spatial_te = clips_te[~clips_te.index.isin(dataset_spatial_te.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8993f7",
   "metadata": {},
   "source": [
    "#### B) Correlation graph construction strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1472121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "top_k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c3f93ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 02:38:40 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 02:38:40 - INFO - Dataset parameters:\n",
      "2025-06-10 02:38:40 - INFO -   - Root directory: data/graph_dataset_correlation_train\n",
      "2025-06-10 02:38:40 - INFO -   - Edge strategy: correlation\n",
      "2025-06-10 02:38:40 - INFO -   - Top-k neighbors: 5\n",
      "2025-06-10 02:38:40 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 02:38:40 - INFO -   - Force reprocess: False\n",
      "2025-06-10 02:38:40 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 02:38:40 - INFO -   - Segment length: 3000\n",
      "2025-06-10 02:38:40 - INFO -   - Apply filtering: True\n",
      "2025-06-10 02:38:40 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 02:38:40 - INFO -   - Apply normalization: True\n",
      "2025-06-10 02:38:40 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 02:38:40 - INFO -   - Test mode: False\n",
      "2025-06-10 02:38:40 - INFO -   - Extract graph features: True\n",
      "2025-06-10 02:38:40 - INFO -   - Diff Corr Matrix Path: None\n",
      "2025-06-10 02:38:40 - INFO - Initializing graph feature extractor...\n",
      "2025-06-10 02:38:40,293 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:38:40 - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:38:40 - INFO - Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:38:40 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 02:38:40 - INFO - Setting up signal filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "Length of dataset_corr_tr: 12986\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_corr_tr = GraphEEGDataset(\n",
    "    root=train_dataset_correlation_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"correlation\",\n",
    "    spatial_distance_file=None,\n",
    "    top_k=top_k,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None # collect all graph features\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_corr_tr: {len(dataset_corr_tr)}\")\n",
    "print(f' Eliminated IDs: {dataset_corr_tr.ids_to_eliminate}')\n",
    "clips_corr_tr = clips_tr[~clips_tr.index.isin(dataset_corr_tr.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "233887f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 02:38:40 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 02:38:40 - INFO - Dataset parameters:\n",
      "2025-06-10 02:38:40 - INFO -   - Root directory: data/graph_dataset_correlation_test\n",
      "2025-06-10 02:38:40 - INFO -   - Edge strategy: spatial\n",
      "2025-06-10 02:38:40 - INFO -   - Top-k neighbors: None\n",
      "2025-06-10 02:38:40 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 02:38:40 - INFO -   - Force reprocess: False\n",
      "2025-06-10 02:38:40 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 02:38:40 - INFO -   - Segment length: 3000\n",
      "2025-06-10 02:38:40 - INFO -   - Apply filtering: True\n",
      "2025-06-10 02:38:40 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 02:38:40 - INFO -   - Apply normalization: True\n",
      "2025-06-10 02:38:40 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 02:38:40 - INFO -   - Test mode: True\n",
      "2025-06-10 02:38:40 - INFO -   - Extract graph features: True\n",
      "2025-06-10 02:38:40 - INFO -   - Diff Corr Matrix Path: None\n",
      "2025-06-10 02:38:40 - INFO - Initializing graph feature extractor...\n",
      "2025-06-10 02:38:40,363 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:38:40 - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:38:40 - INFO - Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:38:40 - INFO - Number of EEG channels: 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 02:38:40 - INFO - Setting up signal filters...\n",
      "2025-06-10 02:38:40 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 02:38:40 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 02:38:40 - INFO - Loaded 361 spatial distances in 0.01s\n",
      "2025-06-10 02:38:40 - INFO - Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset_corr_te: 3614\n",
      " Eliminated IDs:[]\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_corr_te = GraphEEGDataset(\n",
    "    root=test_dataset_correlation_dir,\n",
    "    clips=clips_te,\n",
    "    signal_folder=test_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"spatial\",\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file\n",
    "    ),\n",
    "    top_k=None,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    is_test=True, # NOTE: needed to let the dataset know that is okay to now have labels!\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None # collect all graph features\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_corr_te: {len(dataset_corr_te)}\")\n",
    "print(f' Eliminated IDs:{dataset_corr_te.ids_to_eliminate}')\n",
    "clips_spatial_te = clips_te[~clips_te.index.isin(dataset_corr_te.ids_to_eliminate)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4461766e",
   "metadata": {},
   "source": [
    "#### C) Absolute difference correlation graph construction strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2328a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "top_k = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bf782ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 02:38:40 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 02:38:40 - INFO - Dataset parameters:\n",
      "2025-06-10 02:38:40 - INFO -   - Root directory: data/graph_dataset_absdiff_correlation_train\n",
      "2025-06-10 02:38:40 - INFO -   - Edge strategy: relevance_diff_correlation\n",
      "2025-06-10 02:38:40 - INFO -   - Top-k neighbors: 8\n",
      "2025-06-10 02:38:40 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 02:38:40 - INFO -   - Force reprocess: False\n",
      "2025-06-10 02:38:40 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 02:38:40 - INFO -   - Segment length: 3000\n",
      "2025-06-10 02:38:40 - INFO -   - Apply filtering: True\n",
      "2025-06-10 02:38:40 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 02:38:40 - INFO -   - Apply normalization: True\n",
      "2025-06-10 02:38:40 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 02:38:40 - INFO -   - Test mode: False\n",
      "2025-06-10 02:38:40 - INFO -   - Extract graph features: True\n",
      "2025-06-10 02:38:40 - INFO -   - Diff Corr Matrix Path: data/diff_corr_matrix.csv\n",
      "2025-06-10 02:38:40 - INFO - Edge strategy: relevance_diff_correlation. Loading average correlation matrices.\n",
      "2025-06-10 02:38:40 - INFO - Loaded absolute difference correlation matrix from data/diff_corr_matrix.csv, shape: (19, 19)\n",
      "2025-06-10 02:38:40 - INFO - Initializing graph feature extractor...\n",
      "2025-06-10 02:38:40,521 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:38:40 - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:38:40 - INFO - Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:38:40 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 02:38:40 - INFO - Setting up signal filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "Length of dataset_absdiff_corr_tr: 4646\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_absdiff_corr_tr = GraphEEGDataset(\n",
    "    root=train_dataset_absdiff_correlation_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"relevance_diff_correlation\",\n",
    "    spatial_distance_file=None,\n",
    "    top_k=top_k,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None, # collect all graph features\n",
    "    # settings for absolute difference correlation\n",
    "    diff_corr_matrix_path=absdiff_correlation_file,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_absdiff_corr_tr: {len(dataset_absdiff_corr_tr)}\")\n",
    "print(f' Eliminated IDs: {dataset_absdiff_corr_tr.ids_to_eliminate}')\n",
    "clips_absdiff_corr_tr = clips_tr[~clips_tr.index.isin(dataset_absdiff_corr_tr.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18240a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 02:38:40 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 02:38:40 - INFO - Dataset parameters:\n",
      "2025-06-10 02:38:40 - INFO -   - Root directory: data/graph_dataset_absdiff_correlation_test\n",
      "2025-06-10 02:38:40 - INFO -   - Edge strategy: relevance_diff_correlation\n",
      "2025-06-10 02:38:40 - INFO -   - Top-k neighbors: 8\n",
      "2025-06-10 02:38:40 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 02:38:40 - INFO -   - Force reprocess: False\n",
      "2025-06-10 02:38:40 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 02:38:40 - INFO -   - Segment length: 3000\n",
      "2025-06-10 02:38:40 - INFO -   - Apply filtering: True\n",
      "2025-06-10 02:38:40 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 02:38:40 - INFO -   - Apply normalization: True\n",
      "2025-06-10 02:38:40 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 02:38:40 - INFO -   - Test mode: False\n",
      "2025-06-10 02:38:40 - INFO -   - Extract graph features: True\n",
      "2025-06-10 02:38:40 - INFO -   - Diff Corr Matrix Path: data/diff_corr_matrix.csv\n",
      "2025-06-10 02:38:40 - INFO - Edge strategy: relevance_diff_correlation. Loading average correlation matrices.\n",
      "2025-06-10 02:38:40 - INFO - Loaded absolute difference correlation matrix from data/diff_corr_matrix.csv, shape: (19, 19)\n",
      "2025-06-10 02:38:40 - INFO - Initializing graph feature extractor...\n",
      "2025-06-10 02:38:40,602 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:38:40 - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:38:40 - INFO - Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 02:38:40 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 02:38:40 - INFO - Setting up signal filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "Length of dataset_absdiff_corr_te: 0\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load test dataset\n",
    "dataset_absdiff_corr_te = GraphEEGDataset(\n",
    "    root=test_dataset_absdiff_correlation_dir,\n",
    "    clips=clips_te,\n",
    "    signal_folder=test_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"relevance_diff_correlation\",\n",
    "    spatial_distance_file=None,\n",
    "    top_k=top_k,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None, # collect all graph features\n",
    "    # settings for absolute difference correlation\n",
    "    diff_corr_matrix_path=absdiff_correlation_file,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_absdiff_corr_te: {len(dataset_absdiff_corr_te)}\")\n",
    "print(f' Eliminated IDs: {dataset_absdiff_corr_te.ids_to_eliminate}')\n",
    "clips_absdiff_corr_te = clips_te[~clips_te.index.isin(dataset_absdiff_corr_te.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa474f0b",
   "metadata": {},
   "source": [
    "### Timeseries datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4a44e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "ðŸš€ Initializing TimeseriesEEGDataset in SIGNAL mode.\n",
      "   - Sampling rate: 250 Hz\n",
      "   - Derived segment length: 3000 timesteps.\n",
      "   - Segment length: 3000 timesteps.\n",
      "   âœ… Using existing cached data from data/timeseries_dataset_train_signal/processed\n",
      "ðŸ TimeseriesEEGDataset initialization complete. Loaded 12993 samples.\n",
      "ðŸš€ Initializing TimeseriesEEGDataset in SIGNAL mode.\n",
      "   - Sampling rate: 250 Hz\n",
      "   - Derived segment length: 3000 timesteps.\n",
      "   - Segment length: 3000 timesteps.\n",
      "   âš ï¸ Info: Column 'label' not found in clips_df. Processing without labels (e.g., test set).\n",
      "   âœ… Using existing cached data from data/timeseries_dataset_test_signal/processed\n",
      "ðŸ TimeseriesEEGDataset initialization complete. Loaded 3614 samples.\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.utils.timeseries_eeg_dataset import TimeseriesEEGDataset\n",
    "\n",
    "dataset_timeseries_signal_tr = TimeseriesEEGDataset(\n",
    "    root=str(train_dataset_timeseries_signal_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_tr,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    force_reprocess=False,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='signal', # Use raw EEG signal data\n",
    ")\n",
    "dataset_timeseries_signal_te = TimeseriesEEGDataset(\n",
    "    root=str(test_dataset_timeseries_signal_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_te,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='signal', # Use raw EEG signal data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fe1f9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "ðŸš€ Initializing TimeseriesEEGDataset in FEATURE mode.\n",
      "   âœ… Using existing cached data from data/timeseries_dataset_train_features/processed\n",
      "ðŸ TimeseriesEEGDataset initialization complete. Loaded 12993 samples.\n",
      "ðŸš€ Initializing TimeseriesEEGDataset in FEATURE mode.\n",
      "   âš ï¸ Info: Column 'label' not found in clips_df. Processing without labels (e.g., test set).\n",
      "   âœ… Using existing cached data from data/timeseries_dataset_test_features/processed\n",
      "ðŸ TimeseriesEEGDataset initialization complete. Loaded 3614 samples.\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.utils.timeseries_eeg_dataset import TimeseriesEEGDataset\n",
    "\n",
    "dataset_timeseries_feature_tr = TimeseriesEEGDataset(\n",
    "    root=str(train_dataset_timeseries_feature_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_tr,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='feature',\n",
    "    feature_file_path=str(extracted_features_dir / \"X_train.npy\"),\n",
    ")\n",
    "dataset_timeseries_feature_te = TimeseriesEEGDataset(\n",
    "    root=str(test_dataset_timeseries_feature_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_te,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='feature',\n",
    "    feature_file_path=str(extracted_features_dir / \"X_test.npy\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f38f791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "âœ… TrainingContext initialized. Use .switch_to('dataset_type') to begin.\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.utils.lazy import LazyDataLoaderManager, TrainingContext\n",
    "\n",
    "datasets = {\n",
    "    # timeseries datasets\n",
    "    \"signal\": {\n",
    "        \"dataset_tr\": dataset_timeseries_signal_tr,\n",
    "        \"dataset_te\": dataset_timeseries_signal_te,\n",
    "        \"clips_tr\": clips_tr\n",
    "    },\n",
    "    \"feature\": {\n",
    "        \"dataset_tr\": dataset_timeseries_feature_tr,\n",
    "        \"dataset_te\": dataset_timeseries_feature_te,\n",
    "        \"clips_tr\": clips_tr\n",
    "    },\n",
    "    # graph datasets\n",
    "    \"spatial\": {\n",
    "        \"dataset_tr\": dataset_spatial_tr,\n",
    "        \"dataset_te\": dataset_spatial_te,\n",
    "        \"clips_tr\": clips_spatial_tr,\n",
    "    },\n",
    "    \"correlation\": {\n",
    "        \"dataset_tr\": dataset_corr_tr,\n",
    "        \"dataset_te\": dataset_corr_te,\n",
    "        \"clips_tr\": clips_corr_tr,\n",
    "    },\n",
    "    \"absolute_difference\": {\n",
    "        \"dataset_tr\": dataset_absdiff_corr_tr,\n",
    "        \"dataset_te\": dataset_absdiff_corr_te,\n",
    "        \"clips_tr\": clips_absdiff_corr_tr,\n",
    "    }\n",
    "}\n",
    "\n",
    "# create loaders for both datasets\n",
    "loader_manager = LazyDataLoaderManager(\n",
    "    datasets,\n",
    "    oversampling_power=oversampling_power,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# create train context\n",
    "training_context = TrainingContext(loader_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef9bd7",
   "metadata": {},
   "source": [
    "## Dataset Selection and Training Configuration\n",
    "\n",
    "This section allows you to select which dataset type to use for training by modifying the `DATASET_TYPE` variable in the next cell. The notebook supports multiple dataset types, each with different preprocessing strategies and model architectures.\n",
    "\n",
    "### Available Dataset Types\n",
    "\n",
    "#### Graph-Based Datasets (for GNN models):\n",
    "- **`'spatial'`** - Uses spatial distance-based graph connections between EEG electrodes\n",
    "- **`'correlation'`** - Uses correlation-based graph connections (top-k=5)\n",
    "- **`'absdiff_correlation'`** - Uses absolute difference correlation graph connections (top-k=8)\n",
    "\n",
    "#### Timeseries Datasets (for traditional deep learning models):\n",
    "- **`'signal'`** - Raw EEG signal data with temporal processing\n",
    "- **`'features'`** - Pre-extracted feature representations\n",
    "\n",
    "### Data Loaders Structure\n",
    "\n",
    "All datasets are automatically split into train/validation/test sets with the following configuration:\n",
    "- **Train/Validation ratio**: 80/20\n",
    "- **Random seed**: 42 (for reproducibility)  \n",
    "- **Class balancing**: WeightedRandomSampler with oversampling power = 1.0\n",
    "- **Batch size**: 64\n",
    "\n",
    "The data loaders are organized in dictionaries for easy access:\n",
    "- `graph_loaders` - Contains loaders for all graph-based datasets\n",
    "- `timeseries_loaders` - Contains loaders for all timeseries datasets\n",
    "\n",
    "### Usage Instructions\n",
    "\n",
    "1. Set the `DATASET_TYPE` variable to your desired dataset type\n",
    "2. The notebook will automatically configure the appropriate data loaders\n",
    "3. Use the selected `train_loader`, `val_loader`, and `te_loader` for model training\n",
    "4. Choose the corresponding model architecture (GNN for graph datasets, traditional models for timeseries datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67042055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from src.layers.hybrid.cnn_bilstm_gcn import EEGCNNBiLSTMGCN\n",
    "from src.utils.train import train_model\n",
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"patience\": 10,\n",
    "    \"epochs\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5446a5",
   "metadata": {},
   "source": [
    "## Training and Evaluation - Timeseries Models\n",
    "\n",
    "In this section, we will train and evaluate traditional deep learning models on the selected timeseries dataset. The models will be trained using the `train_loader` and evaluated on the `val_loader` and `te_loader` (the latter being used for final evaluation after training. Labels are not available for the test set, so we will not compute metrics on it)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d491209e",
   "metadata": {},
   "source": [
    "### Training / k-Fold Cross-Validation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60bc6d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.utils.train\n",
    "from src.utils.train import train_k_fold\n",
    "\n",
    "def wrap_traditional_train(model, save_path):\n",
    "    global train_context\n",
    "    if 'train_context' not in globals():\n",
    "        raise ValueError(\"Timeseries training context is not initialized. Please initialize it before calling this function.\")\n",
    "    if not isinstance(train_context, TrainingContext):\n",
    "        raise ValueError(\"train_context must be an instance of TrainingContext.\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # if multiple GPUs are available, use DataParallel\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for training\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5)\n",
    "    loss = nn.BCEWithLogitsLoss()  # Not weighted as we use a balanced sampler!\n",
    "\n",
    "    # train model\n",
    "    train_history, val_history = train_model(\n",
    "        wandb_config=None,\n",
    "        model=model,\n",
    "        train_loader=train_context.train_loader,\n",
    "        val_loader=train_context.val_loader,\n",
    "        criterion=loss,\n",
    "        scheduler=scheduler,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=config[\"epochs\"],\n",
    "        patience=config[\"patience\"],\n",
    "        save_path=save_path,\n",
    "        use_gnn=False,\n",
    "        # hidden attribute\n",
    "        try_load_checkpoint=True,\n",
    "        # FIXME: remove this before submission\n",
    "        log_wandb=False,\n",
    "    )\n",
    "    plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])\n",
    "\n",
    "def k_fold_train_shorthand(model_class, model_parameters, save_path, use_gnn=False, log_wandb=False, batch_size=64):\n",
    "    global train_context\n",
    "\n",
    "    if 'train_context' not in globals():\n",
    "        raise ValueError(\"Training context is not initialized. Please initialize it before calling this function.\")\n",
    "    if not isinstance(train_context, TrainingContext):\n",
    "        raise ValueError(\"train_context must be an instance of TrainingContext.\")\n",
    "\n",
    "    import torch.nn as nn # force import to avoid bug\n",
    "\n",
    "    # train model\n",
    "    aggregated_train_history, aggregated_val_history, fold_results = train_k_fold(\n",
    "        # dataset to use\n",
    "        dataset=datasets[train_context.dataset_type][\"dataset_tr\"],\n",
    "        labels=train_context.clips[\"label\"].values,\n",
    "        # train models\n",
    "        model_class=model_class,\n",
    "        model_kwargs=model_parameters,\n",
    "        # optimizer\n",
    "        criterion=nn.BCEWithLogitsLoss(),\n",
    "        optimizer_class=torch.optim.AdamW,\n",
    "        optimizer_kwargs={\n",
    "            \"lr\": config[\"learning_rate\"],\n",
    "            \"weight_decay\": config[\"weight_decay\"],\n",
    "            \"betas\": (0.9, 0.999)\n",
    "        },\n",
    "        # scheduler\n",
    "        scheduler_class=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        scheduler_kwargs={\n",
    "            \"mode\": 'min',\n",
    "            \"factor\": 0.5,\n",
    "            \"patience\": 5\n",
    "        },\n",
    "        batch_size=batch_size,\n",
    "        wandb_config=None,\n",
    "        device=device,\n",
    "        num_epochs=config[\"epochs\"],\n",
    "        patience=config[\"patience\"],\n",
    "        use_gnn=use_gnn,\n",
    "        # hidden attribute\n",
    "        log_wandb=log_wandb,\n",
    "        save_dir=save_path,\n",
    "    )\n",
    "    plot_training_loss(aggregated_train_history[\"loss\"], aggregated_val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06a1e4",
   "metadata": {},
   "source": [
    "### LSTM (signal-based model)\n",
    "\n",
    "The LSTM model is a recurrent neural network (RNN) architecture designed to handle sequential data, making it suitable for time-series analysis like EEG signals. It captures temporal dependencies in the data, allowing it to learn patterns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.temporal.lstm\n",
    "from src.layers.temporal.lstm import EEGLSTMClassifier\n",
    "\n",
    "# create loader manager\n",
    "train_context = training_context.switch_to('signal')\n",
    "\n",
    "# build model with current parameters\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"timeseries_signal_lstm_baseline.pt\"\n",
    "\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGLSTMClassifier,\n",
    "    model_parameters={\n",
    "        \"input_dim\": 19,\n",
    "        \"hidden_dim\": 64,\n",
    "        \"num_layers\": 4,\n",
    "        \"dropout\": 0.3,\n",
    "        \"bidirectional\": False,\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_lstm_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89807a",
   "metadata": {},
   "source": [
    "### BiLSTM (signal-based model)\n",
    "\n",
    "The BiLSTM (Bidirectional Long Short-Term Memory) model extends the LSTM by processing the input sequence in both forward and backward directions. This bidirectional approach allows the model to capture context from both past and future time steps, enhancing its ability to understand complex temporal relationships in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ce139",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.temporal.lstm\n",
    "from src.layers.temporal.lstm import EEGLSTMClassifier\n",
    "\n",
    "# create loader manager\n",
    "train_context = training_context.switch_to('signal')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGLSTMClassifier,\n",
    "    model_parameters={\n",
    "        \"input_dim\": 19,\n",
    "        \"hidden_dim\": 64,\n",
    "        \"num_layers\": 4,\n",
    "        \"dropout\": 0.3,\n",
    "        \"bidirectional\": True, # use bidirectional LSTM\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_bilstm_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4a846",
   "metadata": {},
   "source": [
    "### MLP (feature-based model)\n",
    "\n",
    "The MLP (Multi-Layer Perceptron) is a feedforward neural network architecture consisting of multiple layers of neurons. It is designed to learn complex mappings from input features to output labels, making it suitable for tasks like EEG seizure detection when using pre-extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.mlp\n",
    "from src.layers.mlp import EEGMLPClassifier\n",
    "\n",
    "train_context = training_context.switch_to('feature')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGMLPClassifier,\n",
    "    model_parameters={\n",
    "        \"input_dim\": 228, # extracted features dimension\n",
    "        \"hidden_dims\": [1024, 512, 256],\n",
    "        \"output_dim\": 1,\n",
    "        \"dropout_prob\": 0.3,\n",
    "        \"use_batch_norm\": True,\n",
    "        \"use_residual\": False,\n",
    "        \"activation\": \"relu\"\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_feature_mlp_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8f8223",
   "metadata": {},
   "source": [
    "### MLP (signal-based model, flattened EEG signals)\n",
    "\n",
    "The same MLP architecture as above, but trained on raw EEG signals instead of pre-extracted features to evaluate the performance of the model on raw data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.mlp\n",
    "from src.layers.mlp import EEGMLPClassifier\n",
    "\n",
    "# switch to signal dataset\n",
    "train_context = training_context.switch_to('signal')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGMLPClassifier,\n",
    "    model_parameters={\n",
    "        \"input_channels\": 19,\n",
    "        \"input_time_steps\": 3000,\n",
    "        \"hidden_dims\": [4096, 2048, 1024, 512, 256],\n",
    "        \"output_dim\": 1,\n",
    "        \"dropout_prob\": 0.3,\n",
    "        \"use_batch_norm\": True,\n",
    "        \"use_residual\": False,\n",
    "        \"activation\": \"leaky_relu\"\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_mlp_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d45648",
   "metadata": {},
   "source": [
    "### CNN-MLP (signal-based model)\n",
    "\n",
    "We have proven that the MLP model alone is not sufficient to capture the temporal dependencies in the EEG signals. Therefore, we will use a CNN-MLP model that combines convolutional layers to extract spatial features from the EEG signals and MLP layers to learn the mapping from these features to the output labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.cnn.cnn\n",
    "from src.layers.cnn.cnn import EEGCNNClassifier\n",
    "\n",
    "train_context = training_context.switch_to('signal')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGCNNClassifier,\n",
    "    model_parameters={\n",
    "        \"input_channels\": 19,\n",
    "        \"cnn_out_dim\": 128,\n",
    "        \"mlp_hidden_dims\": [256, 128],\n",
    "        \"output_dim\": 1,\n",
    "        \"cnn_dropout_prob\": 0.3,\n",
    "        \"mlp_dropout_prob\": 0.3,\n",
    "        \"activation_mlp\": \"leaky_relu\",\n",
    "        \"activation_cnn\": \"leaky_relu\",\n",
    "        \"cnn_use_batch_norm\": True,\n",
    "        \"use_batch_norm_mlp\": True,\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_cnn_mlp_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febab4cf",
   "metadata": {},
   "source": [
    "### CNN-BiLSTM-MLP (signal-based model)\n",
    "\n",
    "In this section we test the CNN-BiLSTM model, which combines convolutional layers to extract spatial features from the EEG signals and LSTM layers to learn the temporal dependencies in the data. This architecture is particularly effective for EEG seizure detection, as it captures both spatial and temporal patterns in the signals. A final fully connected layer is used to map the extracted features to the output labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322efc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.cuda import clean_cuda_memory_usage\n",
    "clean_cuda_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ffe62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.cnn.cnn_lstm\n",
    "from src.layers.cnn.cnn_lstm import EEGCNNBiLSTMClassifier \n",
    "\n",
    "train_context = training_context.switch_to('signal')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGCNNBiLSTMClassifier,\n",
    "    model_parameters={\n",
    "        \"input_channels\": 19,\n",
    "        \"output_dim\": 1,\n",
    "        \"cnn_dropout_prob\": 0.3,\n",
    "        \"mlp_dropout_prob\": 0.3,\n",
    "        \"activation_mlp\": \"leaky_relu\",\n",
    "        \"cnn_use_batch_norm\": True,\n",
    "        \"use_batch_norm_mlp\": True,\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_cnn_bilstm_k_fold\",\n",
    "    batch_size=128 # slightly higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae02a362",
   "metadata": {},
   "source": [
    "## Training and Evaluation - Graph-Based Models\n",
    "\n",
    "This section focuses on training and evaluating Graph Neural Network (GNN) models on the selected graph-based datasets. These models leverage the spatial and functional relationships between EEG electrodes to improve seizure detection accuracy.\n",
    "\n",
    "### Available Graph-Based Architectures\n",
    "\n",
    "The notebook implements several hybrid architectures that combine temporal and graph processing:\n",
    "\n",
    "- **CNN-BiLSTM-GCN**: Combines Convolutional Neural Networks for feature extraction, Bidirectional LSTM for temporal modeling, and Graph Convolutional Networks for spatial relationships\n",
    "- **CNN-BiLSTM-GAT**: Similar to above but uses Graph Attention Networks instead of GCN for learning adaptive attention weights between electrodes\n",
    "- **CNN-BiLSTM-Attention-GNN**: Enhanced version with attention mechanisms in both temporal and graph components\n",
    "\n",
    "### Graph Construction Strategies\n",
    "\n",
    "The models can be trained on different graph construction approaches:\n",
    "- **Spatial graphs**: Based on physical electrode distances (19 channels)\n",
    "- **Correlation graphs**: Dynamic graphs based on signal correlations (top-k=5)\n",
    "- **Absolute difference correlation**: Advanced correlation-based graphs (top-k=8)\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "All graph models use:\n",
    "- **Optimizer**: AdamW with learning rate 1e-4 and weight decay 0.01\n",
    "- **Loss function**: BCEWithLogitsLoss (unweighted due to balanced sampling)\n",
    "- **Scheduler**: ReduceLROnPlateau with factor 0.5 and patience 5\n",
    "- **Early stopping**: Patience of 10 epochs based on validation F1 score\n",
    "- **Data handling**: GeoDataLoader for efficient graph batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8277022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure to clean up CUDA memory before this big model\n",
    "from src.utils.cuda import clean_cuda_memory_usage\n",
    "clean_cuda_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b336bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 02:43:19 - INFO - Starting 5-fold cross-validation\n",
      "2025-06-10 02:43:19 - INFO - Dataset size: 12993\n",
      "2025-06-10 02:43:19 - INFO - Stratified: True\n",
      "2025-06-10 02:43:19 - INFO - Batch size: 8\n",
      "2025-06-10 02:43:19 - INFO - Using stratified k-fold with label distribution\n",
      "2025-06-10 02:43:20 - INFO - Created 5 folds\n",
      "2025-06-10 02:43:20 - INFO - Folds: [(array([    0,     2,     3, ..., 12990, 12991, 12992]), array([    1,     5,     6, ..., 12979, 12983, 12985])), (array([    0,     1,     2, ..., 12988, 12989, 12990]), array([    3,    19,    27, ..., 12977, 12991, 12992])), (array([    0,     1,     3, ..., 12987, 12991, 12992]), array([    2,     7,    17, ..., 12988, 12989, 12990])), (array([    0,     1,     2, ..., 12990, 12991, 12992]), array([    4,     9,    12, ..., 12975, 12978, 12987])), (array([    1,     2,     3, ..., 12990, 12991, 12992]), array([    0,    10,    16, ..., 12976, 12981, 12986]))]\n",
      "2025-06-10 02:43:20 - INFO - \n",
      "============================================================\n",
      "2025-06-10 02:43:20 - INFO - FOLD 1/5\n",
      "2025-06-10 02:43:20 - INFO - ============================================================\n",
      "2025-06-10 02:43:20 - INFO - Train samples: 10394\n",
      "2025-06-10 02:43:20 - INFO - Val samples: 2599\n",
      "2025-06-10 02:43:20 - INFO - Train positive ratio: 0.194\n",
      "2025-06-10 02:43:20 - INFO - Val positive ratio: 0.194\n",
      "2025-06-10 02:43:20 - INFO - Graph-level features are not included in this loader.\n",
      "2025-06-10 02:43:20 - INFO - Graph-level features are not included in this loader.\n",
      "2025-06-10 02:43:20 - INFO - EEGCNNBiLSTMGCN initialized:\n",
      "2025-06-10 02:43:20 - INFO -   - Node input dim: 3000\n",
      "2025-06-10 02:43:20 - INFO -   - Node feature dim (LSTM output): 128\n",
      "2025-06-10 02:43:20 - INFO -   - GCN hidden dim: 128\n",
      "2025-06-10 02:43:20 - INFO -   - Graph feature dim: 0\n",
      "2025-06-10 02:43:20 - INFO -   - Use graph features: False\n",
      "2025-06-10 02:43:20 - INFO -   - Classifier input dim: 96\n",
      "2025-06-10 02:43:20 - INFO -   - Num classes: 1\n",
      "2025-06-10 02:43:20 - INFO -   - Num channels: 19\n",
      "2025-06-10 02:43:20 - INFO - Initialized model: EEGCNNBiLSTMGCN\n",
      "2025-06-10 02:43:20 - INFO - Initialized optimizer: AdamW\n",
      "2025-06-10 02:43:20 - INFO - Initialized scheduler: ReduceLROnPlateau\n",
      "2025-06-10 02:43:20 - INFO - Starting training for fold 1\n",
      "2025-06-10 02:43:20 - INFO - Starting training setup...\n",
      "2025-06-10 02:43:20 - INFO - Model type: GNN\n",
      "2025-06-10 02:43:20 - INFO - Device: cuda\n",
      "2025-06-10 02:43:20 - INFO - Batch size: 8\n",
      "2025-06-10 02:43:20 - INFO - Number of epochs: 100\n",
      "2025-06-10 02:43:20 - INFO - Patience: 10\n",
      "2025-06-10 02:43:20 - INFO - Monitor metric: val_f1\n",
      "2025-06-10 02:43:20 - INFO - Initializing wandb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒ Switching context to 'SPATIAL' dataset...\n",
      "ðŸ§¹ Cleared CUDA memory. Previous usage: 3575.74 MB. Current usage: 3575.74 MB\n",
      "ðŸš€ Context ready for 'spatial'.\n",
      "   Memory usage: 3575.74 MB\n",
      "   Train batches: 163, Val batches: 41\n",
      "   Type: spatial\n",
      "   Total Train Samples: 12993\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>bad_epochs</td><td>â–</td></tr><tr><td>epoch</td><td>â–</td></tr><tr><td>learning_rate</td><td>â–</td></tr><tr><td>train/accuracy</td><td>â–</td></tr><tr><td>train/auroc</td><td>â–</td></tr><tr><td>train/f1</td><td>â–</td></tr><tr><td>train/loss</td><td>â–</td></tr><tr><td>train/macro_f1</td><td>â–</td></tr><tr><td>train/precision</td><td>â–</td></tr><tr><td>train/recall</td><td>â–</td></tr><tr><td>val/accuracy</td><td>â–</td></tr><tr><td>val/auroc</td><td>â–</td></tr><tr><td>val/f1</td><td>â–</td></tr><tr><td>val/loss</td><td>â–</td></tr><tr><td>val/macro_f1</td><td>â–</td></tr><tr><td>val/median_patient_f1</td><td>â–</td></tr><tr><td>val/median_patient_precision</td><td>â–</td></tr><tr><td>val/median_patient_recall</td><td>â–</td></tr><tr><td>val/precision</td><td>â–</td></tr><tr><td>val/recall</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>bad_epochs</td><td>0</td></tr><tr><td>best_val_f1</td><td>-inf</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>train/accuracy</td><td>0.80133</td></tr><tr><td>train/auroc</td><td>0.63443</td></tr><tr><td>train/f1</td><td>0.00578</td></tr><tr><td>train/loss</td><td>0.49337</td></tr><tr><td>train/macro_f1</td><td>0.00578</td></tr><tr><td>train/precision</td><td>0.09375</td></tr><tr><td>train/recall</td><td>0.00298</td></tr><tr><td>val/accuracy</td><td>0.80608</td></tr><tr><td>val/auroc</td><td>0.70474</td></tr><tr><td>val/f1</td><td>0</td></tr><tr><td>val/loss</td><td>0.4511</td></tr><tr><td>val/macro_f1</td><td>0</td></tr><tr><td>val/median_patient_f1</td><td>0</td></tr><tr><td>val/median_patient_precision</td><td>0</td></tr><tr><td>val/median_patient_recall</td><td>0</td></tr><tr><td>val/precision</td><td>0</td></tr><tr><td>val/recall</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fold_1</strong> at: <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/4vac04it' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/4vac04it</a><br> View project at: <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250610_023845-4vac04it/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ldibello/NeuroGraphNet/wandb/run-20250610_024320-4op4yi1o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/4op4yi1o' target=\"_blank\">fold_1</a></strong> to <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/4op4yi1o' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/4op4yi1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 02:43:22 - INFO - ðŸ”— Wandb run initialized: fold_1\n",
      "2025-06-10 02:43:22 - INFO - Total training batches per epoch: 1300\n",
      "2025-06-10 02:43:22 - INFO - Starting training from epoch 1 to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Wandb initialized: fold_1\n",
      " ðŸ—‘ï¸ Overwrite enabled: Removed existing checkpoint at .checkpoints/cnn_bilstm_gcn_signal_k_fold_.pt/fold_1_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]2025-06-10 02:43:22 - INFO - \n",
      "Epoch 1/100 - Training phase\n",
      "2025-06-10 02:43:22 - INFO - Processing batch 1/1300\n",
      "2025-06-10 02:43:22 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "2025-06-10 02:43:22 - INFO - Batch 1/1300 - Loss: 0.6918 - Avg batch time: 0.16s\n",
      "2025-06-10 02:43:24 - INFO - Processing batch 11/1300\n",
      "2025-06-10 02:43:24 - INFO - Batch 11/1300 - Loss: 0.6826 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:25 - INFO - Processing batch 21/1300\n",
      "2025-06-10 02:43:25 - INFO - Batch 21/1300 - Loss: 0.6890 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:27 - INFO - Processing batch 31/1300\n",
      "2025-06-10 02:43:27 - INFO - Batch 31/1300 - Loss: 0.6763 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:28 - INFO - Processing batch 41/1300\n",
      "2025-06-10 02:43:28 - INFO - Batch 41/1300 - Loss: 0.6192 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:30 - INFO - Processing batch 51/1300\n",
      "2025-06-10 02:43:30 - INFO - Batch 51/1300 - Loss: 0.4917 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:31 - INFO - Processing batch 61/1300\n",
      "2025-06-10 02:43:31 - INFO - Batch 61/1300 - Loss: 0.4077 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:33 - INFO - Processing batch 71/1300\n",
      "2025-06-10 02:43:33 - INFO - Batch 71/1300 - Loss: 0.2214 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:34 - INFO - Processing batch 81/1300\n",
      "2025-06-10 02:43:34 - INFO - Batch 81/1300 - Loss: 0.2260 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:36 - INFO - Processing batch 91/1300\n",
      "2025-06-10 02:43:36 - INFO - Batch 91/1300 - Loss: 0.5660 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:37 - INFO - Processing batch 101/1300\n",
      "2025-06-10 02:43:38 - INFO - Batch 101/1300 - Loss: 0.4472 - Avg batch time: 0.16s\n",
      "2025-06-10 02:43:39 - INFO - Processing batch 111/1300\n",
      "2025-06-10 02:43:39 - INFO - Batch 111/1300 - Loss: 0.1033 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:40 - INFO - Processing batch 121/1300\n",
      "2025-06-10 02:43:41 - INFO - Batch 121/1300 - Loss: 0.4000 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:42 - INFO - Processing batch 131/1300\n",
      "2025-06-10 02:43:42 - INFO - Batch 131/1300 - Loss: 0.7826 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:44 - INFO - Processing batch 141/1300\n",
      "2025-06-10 02:43:44 - INFO - Batch 141/1300 - Loss: 0.8296 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:45 - INFO - Processing batch 151/1300\n",
      "2025-06-10 02:43:45 - INFO - Batch 151/1300 - Loss: 0.3374 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:47 - INFO - Processing batch 161/1300\n",
      "2025-06-10 02:43:47 - INFO - Batch 161/1300 - Loss: 0.3830 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:48 - INFO - Processing batch 171/1300\n",
      "2025-06-10 02:43:48 - INFO - Batch 171/1300 - Loss: 1.1805 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:50 - INFO - Processing batch 181/1300\n",
      "2025-06-10 02:43:50 - INFO - Batch 181/1300 - Loss: 0.4049 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:51 - INFO - Processing batch 191/1300\n",
      "2025-06-10 02:43:51 - INFO - Batch 191/1300 - Loss: 0.2314 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:53 - INFO - Processing batch 201/1300\n",
      "2025-06-10 02:43:53 - INFO - Batch 201/1300 - Loss: 0.3501 - Avg batch time: 0.16s\n",
      "2025-06-10 02:43:54 - INFO - Processing batch 211/1300\n",
      "2025-06-10 02:43:54 - INFO - Batch 211/1300 - Loss: 0.1782 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:56 - INFO - Processing batch 221/1300\n",
      "2025-06-10 02:43:56 - INFO - Batch 221/1300 - Loss: 0.9424 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:57 - INFO - Processing batch 231/1300\n",
      "2025-06-10 02:43:57 - INFO - Batch 231/1300 - Loss: 0.9654 - Avg batch time: 0.15s\n",
      "2025-06-10 02:43:59 - INFO - Processing batch 241/1300\n",
      "2025-06-10 02:43:59 - INFO - Batch 241/1300 - Loss: 0.7338 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:00 - INFO - Processing batch 251/1300\n",
      "2025-06-10 02:44:01 - INFO - Batch 251/1300 - Loss: 0.3272 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:02 - INFO - Processing batch 261/1300\n",
      "2025-06-10 02:44:02 - INFO - Batch 261/1300 - Loss: 0.5591 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:03 - INFO - Processing batch 271/1300\n",
      "2025-06-10 02:44:04 - INFO - Batch 271/1300 - Loss: 0.6082 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:05 - INFO - Processing batch 281/1300\n",
      "2025-06-10 02:44:05 - INFO - Batch 281/1300 - Loss: 0.4622 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:07 - INFO - Processing batch 291/1300\n",
      "2025-06-10 02:44:07 - INFO - Batch 291/1300 - Loss: 0.6577 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:08 - INFO - Processing batch 301/1300\n",
      "2025-06-10 02:44:08 - INFO - Batch 301/1300 - Loss: 0.7857 - Avg batch time: 0.16s\n",
      "2025-06-10 02:44:10 - INFO - Processing batch 311/1300\n",
      "2025-06-10 02:44:10 - INFO - Batch 311/1300 - Loss: 0.4058 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:11 - INFO - Processing batch 321/1300\n",
      "2025-06-10 02:44:11 - INFO - Batch 321/1300 - Loss: 0.3878 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:13 - INFO - Processing batch 331/1300\n",
      "2025-06-10 02:44:13 - INFO - Batch 331/1300 - Loss: 0.3769 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:14 - INFO - Processing batch 341/1300\n",
      "2025-06-10 02:44:14 - INFO - Batch 341/1300 - Loss: 0.2260 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:16 - INFO - Processing batch 351/1300\n",
      "2025-06-10 02:44:16 - INFO - Batch 351/1300 - Loss: 0.5097 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:17 - INFO - Processing batch 361/1300\n",
      "2025-06-10 02:44:17 - INFO - Batch 361/1300 - Loss: 0.5499 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:19 - INFO - Processing batch 371/1300\n",
      "2025-06-10 02:44:19 - INFO - Batch 371/1300 - Loss: 0.3958 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:20 - INFO - Processing batch 381/1300\n",
      "2025-06-10 02:44:21 - INFO - Batch 381/1300 - Loss: 0.2552 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:22 - INFO - Processing batch 391/1300\n",
      "2025-06-10 02:44:22 - INFO - Batch 391/1300 - Loss: 0.8562 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:24 - INFO - Processing batch 401/1300\n",
      "2025-06-10 02:44:24 - INFO - Batch 401/1300 - Loss: 0.9807 - Avg batch time: 0.16s\n",
      "2025-06-10 02:44:25 - INFO - Processing batch 411/1300\n",
      "2025-06-10 02:44:25 - INFO - Batch 411/1300 - Loss: 0.6357 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:27 - INFO - Processing batch 421/1300\n",
      "2025-06-10 02:44:27 - INFO - Batch 421/1300 - Loss: 0.3803 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:28 - INFO - Processing batch 431/1300\n",
      "2025-06-10 02:44:28 - INFO - Batch 431/1300 - Loss: 0.4022 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:30 - INFO - Processing batch 441/1300\n",
      "2025-06-10 02:44:30 - INFO - Batch 441/1300 - Loss: 0.6820 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:31 - INFO - Processing batch 451/1300\n",
      "2025-06-10 02:44:31 - INFO - Batch 451/1300 - Loss: 0.7244 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:33 - INFO - Processing batch 461/1300\n",
      "2025-06-10 02:44:33 - INFO - Batch 461/1300 - Loss: 0.3006 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:34 - INFO - Processing batch 471/1300\n",
      "2025-06-10 02:44:34 - INFO - Batch 471/1300 - Loss: 0.9651 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:36 - INFO - Processing batch 481/1300\n",
      "2025-06-10 02:44:36 - INFO - Batch 481/1300 - Loss: 0.6749 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:37 - INFO - Processing batch 491/1300\n",
      "2025-06-10 02:44:38 - INFO - Batch 491/1300 - Loss: 0.5217 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:39 - INFO - Processing batch 501/1300\n",
      "2025-06-10 02:44:39 - INFO - Batch 501/1300 - Loss: 0.3781 - Avg batch time: 0.16s\n",
      "2025-06-10 02:44:40 - INFO - Processing batch 511/1300\n",
      "2025-06-10 02:44:41 - INFO - Batch 511/1300 - Loss: 0.2677 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:42 - INFO - Processing batch 521/1300\n",
      "2025-06-10 02:44:42 - INFO - Batch 521/1300 - Loss: 0.6166 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:44 - INFO - Processing batch 531/1300\n",
      "2025-06-10 02:44:44 - INFO - Batch 531/1300 - Loss: 0.2501 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:45 - INFO - Processing batch 541/1300\n",
      "2025-06-10 02:44:45 - INFO - Batch 541/1300 - Loss: 0.9701 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:47 - INFO - Processing batch 551/1300\n",
      "2025-06-10 02:44:47 - INFO - Batch 551/1300 - Loss: 0.9176 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:48 - INFO - Processing batch 561/1300\n",
      "2025-06-10 02:44:48 - INFO - Batch 561/1300 - Loss: 0.8374 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:50 - INFO - Processing batch 571/1300\n",
      "2025-06-10 02:44:50 - INFO - Batch 571/1300 - Loss: 0.8614 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:51 - INFO - Processing batch 581/1300\n",
      "2025-06-10 02:44:51 - INFO - Batch 581/1300 - Loss: 0.7413 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:53 - INFO - Processing batch 591/1300\n",
      "2025-06-10 02:44:53 - INFO - Batch 591/1300 - Loss: 0.5593 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:54 - INFO - Processing batch 601/1300\n",
      "2025-06-10 02:44:55 - INFO - Batch 601/1300 - Loss: 0.4870 - Avg batch time: 0.16s\n",
      "2025-06-10 02:44:56 - INFO - Processing batch 611/1300\n",
      "2025-06-10 02:44:56 - INFO - Batch 611/1300 - Loss: 0.4545 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:57 - INFO - Processing batch 621/1300\n",
      "2025-06-10 02:44:58 - INFO - Batch 621/1300 - Loss: 0.2734 - Avg batch time: 0.15s\n",
      "2025-06-10 02:44:59 - INFO - Processing batch 631/1300\n",
      "2025-06-10 02:44:59 - INFO - Batch 631/1300 - Loss: 0.3808 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:00 - INFO - Processing batch 641/1300\n",
      "2025-06-10 02:45:01 - INFO - Batch 641/1300 - Loss: 0.2498 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:02 - INFO - Processing batch 651/1300\n",
      "2025-06-10 02:45:02 - INFO - Batch 651/1300 - Loss: 0.5390 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:04 - INFO - Processing batch 661/1300\n",
      "2025-06-10 02:45:04 - INFO - Batch 661/1300 - Loss: 0.6670 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:05 - INFO - Processing batch 671/1300\n",
      "2025-06-10 02:45:05 - INFO - Batch 671/1300 - Loss: 0.4084 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:07 - INFO - Processing batch 681/1300\n",
      "2025-06-10 02:45:07 - INFO - Batch 681/1300 - Loss: 0.6120 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:08 - INFO - Processing batch 691/1300\n",
      "2025-06-10 02:45:08 - INFO - Batch 691/1300 - Loss: 0.4739 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:10 - INFO - Processing batch 701/1300\n",
      "2025-06-10 02:45:10 - INFO - Batch 701/1300 - Loss: 0.4384 - Avg batch time: 0.16s\n",
      "2025-06-10 02:45:11 - INFO - Processing batch 711/1300\n",
      "2025-06-10 02:45:11 - INFO - Batch 711/1300 - Loss: 0.3705 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:13 - INFO - Processing batch 721/1300\n",
      "2025-06-10 02:45:13 - INFO - Batch 721/1300 - Loss: 0.7182 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:14 - INFO - Processing batch 731/1300\n",
      "2025-06-10 02:45:14 - INFO - Batch 731/1300 - Loss: 0.2411 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:16 - INFO - Processing batch 741/1300\n",
      "2025-06-10 02:45:16 - INFO - Batch 741/1300 - Loss: 0.2316 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:17 - INFO - Processing batch 751/1300\n",
      "2025-06-10 02:45:18 - INFO - Batch 751/1300 - Loss: 0.8419 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:19 - INFO - Processing batch 761/1300\n",
      "2025-06-10 02:45:19 - INFO - Batch 761/1300 - Loss: 0.8361 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:20 - INFO - Processing batch 771/1300\n",
      "2025-06-10 02:45:21 - INFO - Batch 771/1300 - Loss: 0.4410 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:22 - INFO - Processing batch 781/1300\n",
      "2025-06-10 02:45:22 - INFO - Batch 781/1300 - Loss: 0.4240 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:24 - INFO - Processing batch 791/1300\n",
      "2025-06-10 02:45:24 - INFO - Batch 791/1300 - Loss: 0.8780 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:25 - INFO - Processing batch 801/1300\n",
      "2025-06-10 02:45:25 - INFO - Batch 801/1300 - Loss: 0.5988 - Avg batch time: 0.16s\n",
      "2025-06-10 02:45:27 - INFO - Processing batch 811/1300\n",
      "2025-06-10 02:45:27 - INFO - Batch 811/1300 - Loss: 0.3200 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:28 - INFO - Processing batch 821/1300\n",
      "2025-06-10 02:45:28 - INFO - Batch 821/1300 - Loss: 0.1244 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:30 - INFO - Processing batch 831/1300\n",
      "2025-06-10 02:45:30 - INFO - Batch 831/1300 - Loss: 0.2238 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:31 - INFO - Processing batch 841/1300\n",
      "2025-06-10 02:45:31 - INFO - Batch 841/1300 - Loss: 0.3254 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:33 - INFO - Processing batch 851/1300\n",
      "2025-06-10 02:45:33 - INFO - Batch 851/1300 - Loss: 0.1333 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:34 - INFO - Processing batch 861/1300\n",
      "2025-06-10 02:45:35 - INFO - Batch 861/1300 - Loss: 0.7801 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:36 - INFO - Processing batch 871/1300\n",
      "2025-06-10 02:45:36 - INFO - Batch 871/1300 - Loss: 0.6679 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:37 - INFO - Processing batch 881/1300\n",
      "2025-06-10 02:45:38 - INFO - Batch 881/1300 - Loss: 0.4694 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:39 - INFO - Processing batch 891/1300\n",
      "2025-06-10 02:45:39 - INFO - Batch 891/1300 - Loss: 0.4300 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:41 - INFO - Processing batch 901/1300\n",
      "2025-06-10 02:45:41 - INFO - Batch 901/1300 - Loss: 0.4813 - Avg batch time: 0.16s\n",
      "2025-06-10 02:45:42 - INFO - Processing batch 911/1300\n",
      "2025-06-10 02:45:42 - INFO - Batch 911/1300 - Loss: 0.2446 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:44 - INFO - Processing batch 921/1300\n",
      "2025-06-10 02:45:44 - INFO - Batch 921/1300 - Loss: 0.6902 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:45 - INFO - Processing batch 931/1300\n",
      "2025-06-10 02:45:45 - INFO - Batch 931/1300 - Loss: 0.3020 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:47 - INFO - Processing batch 941/1300\n",
      "2025-06-10 02:45:47 - INFO - Batch 941/1300 - Loss: 0.2517 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:48 - INFO - Processing batch 951/1300\n",
      "2025-06-10 02:45:48 - INFO - Batch 951/1300 - Loss: 0.3208 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:50 - INFO - Processing batch 961/1300\n",
      "2025-06-10 02:45:50 - INFO - Batch 961/1300 - Loss: 0.5520 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:51 - INFO - Processing batch 971/1300\n",
      "2025-06-10 02:45:52 - INFO - Batch 971/1300 - Loss: 0.2812 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:53 - INFO - Processing batch 981/1300\n",
      "2025-06-10 02:45:53 - INFO - Batch 981/1300 - Loss: 0.3411 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:54 - INFO - Processing batch 991/1300\n",
      "2025-06-10 02:45:55 - INFO - Batch 991/1300 - Loss: 0.4178 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:56 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 02:45:56 - INFO - Batch 1001/1300 - Loss: 0.6991 - Avg batch time: 0.16s\n",
      "2025-06-10 02:45:58 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 02:45:58 - INFO - Batch 1011/1300 - Loss: 0.5729 - Avg batch time: 0.15s\n",
      "2025-06-10 02:45:59 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 02:45:59 - INFO - Batch 1021/1300 - Loss: 0.0758 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:01 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 02:46:01 - INFO - Batch 1031/1300 - Loss: 0.2677 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:02 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 02:46:02 - INFO - Batch 1041/1300 - Loss: 0.2105 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:04 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 02:46:04 - INFO - Batch 1051/1300 - Loss: 0.2453 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:05 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 02:46:05 - INFO - Batch 1061/1300 - Loss: 0.4118 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:07 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 02:46:07 - INFO - Batch 1071/1300 - Loss: 0.2505 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:08 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 02:46:08 - INFO - Batch 1081/1300 - Loss: 0.4856 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:10 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 02:46:10 - INFO - Batch 1091/1300 - Loss: 0.1572 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:11 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 02:46:12 - INFO - Batch 1101/1300 - Loss: 0.2017 - Avg batch time: 0.16s\n",
      "2025-06-10 02:46:13 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 02:46:13 - INFO - Batch 1111/1300 - Loss: 0.3487 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:15 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 02:46:15 - INFO - Batch 1121/1300 - Loss: 0.5662 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:16 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 02:46:16 - INFO - Batch 1131/1300 - Loss: 0.0586 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:18 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 02:46:18 - INFO - Batch 1141/1300 - Loss: 0.8646 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:19 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 02:46:19 - INFO - Batch 1151/1300 - Loss: 0.4499 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:21 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 02:46:21 - INFO - Batch 1161/1300 - Loss: 0.8447 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:22 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 02:46:22 - INFO - Batch 1171/1300 - Loss: 0.9266 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:24 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 02:46:24 - INFO - Batch 1181/1300 - Loss: 0.2782 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:25 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 02:46:25 - INFO - Batch 1191/1300 - Loss: 0.3983 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:27 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 02:46:27 - INFO - Batch 1201/1300 - Loss: 0.2744 - Avg batch time: 0.16s\n",
      "2025-06-10 02:46:28 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 02:46:29 - INFO - Batch 1211/1300 - Loss: 0.4143 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:30 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 02:46:30 - INFO - Batch 1221/1300 - Loss: 0.7377 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:32 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 02:46:32 - INFO - Batch 1231/1300 - Loss: 1.1366 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:33 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 02:46:33 - INFO - Batch 1241/1300 - Loss: 0.3814 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:35 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 02:46:35 - INFO - Batch 1251/1300 - Loss: 0.3088 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:36 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 02:46:36 - INFO - Batch 1261/1300 - Loss: 0.3099 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:38 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 02:46:38 - INFO - Batch 1271/1300 - Loss: 0.1891 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:39 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 02:46:39 - INFO - Batch 1281/1300 - Loss: 0.3213 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:41 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 02:46:41 - INFO - Batch 1291/1300 - Loss: 0.6135 - Avg batch time: 0.15s\n",
      "2025-06-10 02:46:42 - INFO - \n",
      "Epoch 1 training completed in 200.43s\n",
      "2025-06-10 02:46:42 - INFO - Average training loss: 0.4967\n",
      "2025-06-10 02:47:00 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:   2%| | 2/100 [03:38<5:56:10, 218.07s/it, train_loss=0.4967, val_loss=0.4659, best_val_f1=0.0000, lr=1.00e-04, b2025-06-10 02:47:00 - INFO - \n",
      "Epoch 2/100 - Training phase\n",
      "2025-06-10 02:47:00 - INFO - Processing batch 1/1300\n",
      "2025-06-10 02:47:00 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 02:47:00 - INFO - Batch 1/1300 - Loss: 0.4424 - Avg batch time: 0.16s\n",
      "2025-06-10 02:47:02 - INFO - Processing batch 11/1300\n",
      "2025-06-10 02:47:02 - INFO - Batch 11/1300 - Loss: 0.3511 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:03 - INFO - Processing batch 21/1300\n",
      "2025-06-10 02:47:03 - INFO - Batch 21/1300 - Loss: 0.6390 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:05 - INFO - Processing batch 31/1300\n",
      "2025-06-10 02:47:05 - INFO - Batch 31/1300 - Loss: 1.0195 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:06 - INFO - Processing batch 41/1300\n",
      "2025-06-10 02:47:06 - INFO - Batch 41/1300 - Loss: 0.3392 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:08 - INFO - Processing batch 51/1300\n",
      "2025-06-10 02:47:08 - INFO - Batch 51/1300 - Loss: 0.4041 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:09 - INFO - Processing batch 61/1300\n",
      "2025-06-10 02:47:09 - INFO - Batch 61/1300 - Loss: 0.4798 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:11 - INFO - Processing batch 71/1300\n",
      "2025-06-10 02:47:11 - INFO - Batch 71/1300 - Loss: 0.5692 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:12 - INFO - Processing batch 81/1300\n",
      "2025-06-10 02:47:13 - INFO - Batch 81/1300 - Loss: 0.1921 - Avg batch time: 0.16s\n",
      "2025-06-10 02:47:14 - INFO - Processing batch 91/1300\n",
      "2025-06-10 02:47:14 - INFO - Batch 91/1300 - Loss: 0.6251 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:16 - INFO - Processing batch 101/1300\n",
      "2025-06-10 02:47:16 - INFO - Batch 101/1300 - Loss: 0.8649 - Avg batch time: 0.16s\n",
      "2025-06-10 02:47:17 - INFO - Processing batch 111/1300\n",
      "2025-06-10 02:47:17 - INFO - Batch 111/1300 - Loss: 0.3701 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:19 - INFO - Processing batch 121/1300\n",
      "2025-06-10 02:47:19 - INFO - Batch 121/1300 - Loss: 0.8453 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:20 - INFO - Processing batch 131/1300\n",
      "2025-06-10 02:47:20 - INFO - Batch 131/1300 - Loss: 0.4240 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:22 - INFO - Processing batch 141/1300\n",
      "2025-06-10 02:47:22 - INFO - Batch 141/1300 - Loss: 0.3664 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:23 - INFO - Processing batch 151/1300\n",
      "2025-06-10 02:47:23 - INFO - Batch 151/1300 - Loss: 0.2896 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:25 - INFO - Processing batch 161/1300\n",
      "2025-06-10 02:47:25 - INFO - Batch 161/1300 - Loss: 0.8322 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:26 - INFO - Processing batch 171/1300\n",
      "2025-06-10 02:47:26 - INFO - Batch 171/1300 - Loss: 0.6321 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:28 - INFO - Processing batch 181/1300\n",
      "2025-06-10 02:47:28 - INFO - Batch 181/1300 - Loss: 0.4253 - Avg batch time: 0.16s\n",
      "2025-06-10 02:47:29 - INFO - Processing batch 191/1300\n",
      "2025-06-10 02:47:30 - INFO - Batch 191/1300 - Loss: 0.5354 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:31 - INFO - Processing batch 201/1300\n",
      "2025-06-10 02:47:31 - INFO - Batch 201/1300 - Loss: 0.2066 - Avg batch time: 0.16s\n",
      "2025-06-10 02:47:32 - INFO - Processing batch 211/1300\n",
      "2025-06-10 02:47:33 - INFO - Batch 211/1300 - Loss: 0.4477 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:34 - INFO - Processing batch 221/1300\n",
      "2025-06-10 02:47:34 - INFO - Batch 221/1300 - Loss: 0.5704 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:36 - INFO - Processing batch 231/1300\n",
      "2025-06-10 02:47:36 - INFO - Batch 231/1300 - Loss: 0.4689 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:37 - INFO - Processing batch 241/1300\n",
      "2025-06-10 02:47:37 - INFO - Batch 241/1300 - Loss: 0.4061 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:39 - INFO - Processing batch 251/1300\n",
      "2025-06-10 02:47:39 - INFO - Batch 251/1300 - Loss: 0.2727 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:40 - INFO - Processing batch 261/1300\n",
      "2025-06-10 02:47:40 - INFO - Batch 261/1300 - Loss: 0.7413 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:42 - INFO - Processing batch 271/1300\n",
      "2025-06-10 02:47:42 - INFO - Batch 271/1300 - Loss: 0.3115 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:43 - INFO - Processing batch 281/1300\n",
      "2025-06-10 02:47:43 - INFO - Batch 281/1300 - Loss: 1.1604 - Avg batch time: 0.16s\n",
      "2025-06-10 02:47:45 - INFO - Processing batch 291/1300\n",
      "2025-06-10 02:47:45 - INFO - Batch 291/1300 - Loss: 0.7286 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:46 - INFO - Processing batch 301/1300\n",
      "2025-06-10 02:47:46 - INFO - Batch 301/1300 - Loss: 0.6296 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:48 - INFO - Processing batch 311/1300\n",
      "2025-06-10 02:47:48 - INFO - Batch 311/1300 - Loss: 0.8611 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:49 - INFO - Processing batch 321/1300\n",
      "2025-06-10 02:47:50 - INFO - Batch 321/1300 - Loss: 0.7217 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:51 - INFO - Processing batch 331/1300\n",
      "2025-06-10 02:47:51 - INFO - Batch 331/1300 - Loss: 0.4132 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:52 - INFO - Processing batch 341/1300\n",
      "2025-06-10 02:47:53 - INFO - Batch 341/1300 - Loss: 0.3142 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:54 - INFO - Processing batch 351/1300\n",
      "2025-06-10 02:47:54 - INFO - Batch 351/1300 - Loss: 0.6076 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:55 - INFO - Processing batch 361/1300\n",
      "2025-06-10 02:47:56 - INFO - Batch 361/1300 - Loss: 0.6070 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:57 - INFO - Processing batch 371/1300\n",
      "2025-06-10 02:47:57 - INFO - Batch 371/1300 - Loss: 0.1379 - Avg batch time: 0.15s\n",
      "2025-06-10 02:47:59 - INFO - Processing batch 381/1300\n",
      "2025-06-10 02:47:59 - INFO - Batch 381/1300 - Loss: 0.2082 - Avg batch time: 0.16s\n",
      "2025-06-10 02:48:00 - INFO - Processing batch 391/1300\n",
      "2025-06-10 02:48:00 - INFO - Batch 391/1300 - Loss: 0.5448 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:02 - INFO - Processing batch 401/1300\n",
      "2025-06-10 02:48:02 - INFO - Batch 401/1300 - Loss: 0.4305 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:03 - INFO - Processing batch 411/1300\n",
      "2025-06-10 02:48:03 - INFO - Batch 411/1300 - Loss: 0.5465 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:05 - INFO - Processing batch 421/1300\n",
      "2025-06-10 02:48:05 - INFO - Batch 421/1300 - Loss: 0.2495 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:06 - INFO - Processing batch 431/1300\n",
      "2025-06-10 02:48:06 - INFO - Batch 431/1300 - Loss: 0.5920 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:08 - INFO - Processing batch 441/1300\n",
      "2025-06-10 02:48:08 - INFO - Batch 441/1300 - Loss: 0.6085 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:09 - INFO - Processing batch 451/1300\n",
      "2025-06-10 02:48:09 - INFO - Batch 451/1300 - Loss: 0.6691 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:11 - INFO - Processing batch 461/1300\n",
      "2025-06-10 02:48:11 - INFO - Batch 461/1300 - Loss: 0.3440 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:12 - INFO - Processing batch 471/1300\n",
      "2025-06-10 02:48:13 - INFO - Batch 471/1300 - Loss: 0.8605 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:14 - INFO - Processing batch 481/1300\n",
      "2025-06-10 02:48:14 - INFO - Batch 481/1300 - Loss: 0.4792 - Avg batch time: 0.16s\n",
      "2025-06-10 02:48:16 - INFO - Processing batch 491/1300\n",
      "2025-06-10 02:48:16 - INFO - Batch 491/1300 - Loss: 0.3328 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:17 - INFO - Processing batch 501/1300\n",
      "2025-06-10 02:48:17 - INFO - Batch 501/1300 - Loss: 0.5759 - Avg batch time: 0.16s\n",
      "2025-06-10 02:48:19 - INFO - Processing batch 511/1300\n",
      "2025-06-10 02:48:19 - INFO - Batch 511/1300 - Loss: 0.5272 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:20 - INFO - Processing batch 521/1300\n",
      "2025-06-10 02:48:20 - INFO - Batch 521/1300 - Loss: 0.7154 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:22 - INFO - Processing batch 531/1300\n",
      "2025-06-10 02:48:22 - INFO - Batch 531/1300 - Loss: 0.1828 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:23 - INFO - Processing batch 541/1300\n",
      "2025-06-10 02:48:23 - INFO - Batch 541/1300 - Loss: 0.6346 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:25 - INFO - Processing batch 551/1300\n",
      "2025-06-10 02:48:25 - INFO - Batch 551/1300 - Loss: 0.5056 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:26 - INFO - Processing batch 561/1300\n",
      "2025-06-10 02:48:26 - INFO - Batch 561/1300 - Loss: 0.6106 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:28 - INFO - Processing batch 571/1300\n",
      "2025-06-10 02:48:28 - INFO - Batch 571/1300 - Loss: 0.0807 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:29 - INFO - Processing batch 581/1300\n",
      "2025-06-10 02:48:30 - INFO - Batch 581/1300 - Loss: 0.2624 - Avg batch time: 0.16s\n",
      "2025-06-10 02:48:31 - INFO - Processing batch 591/1300\n",
      "2025-06-10 02:48:31 - INFO - Batch 591/1300 - Loss: 0.6490 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:32 - INFO - Processing batch 601/1300\n",
      "2025-06-10 02:48:33 - INFO - Batch 601/1300 - Loss: 0.4288 - Avg batch time: 0.16s\n",
      "2025-06-10 02:48:34 - INFO - Processing batch 611/1300\n",
      "2025-06-10 02:48:34 - INFO - Batch 611/1300 - Loss: 0.5163 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:36 - INFO - Processing batch 621/1300\n",
      "2025-06-10 02:48:36 - INFO - Batch 621/1300 - Loss: 0.6222 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:37 - INFO - Processing batch 631/1300\n",
      "2025-06-10 02:48:37 - INFO - Batch 631/1300 - Loss: 0.7205 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:39 - INFO - Processing batch 641/1300\n",
      "2025-06-10 02:48:39 - INFO - Batch 641/1300 - Loss: 0.3967 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:40 - INFO - Processing batch 651/1300\n",
      "2025-06-10 02:48:40 - INFO - Batch 651/1300 - Loss: 0.1941 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:42 - INFO - Processing batch 661/1300\n",
      "2025-06-10 02:48:42 - INFO - Batch 661/1300 - Loss: 0.1416 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:43 - INFO - Processing batch 671/1300\n",
      "2025-06-10 02:48:43 - INFO - Batch 671/1300 - Loss: 0.9792 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:45 - INFO - Processing batch 681/1300\n",
      "2025-06-10 02:48:45 - INFO - Batch 681/1300 - Loss: 0.5451 - Avg batch time: 0.16s\n",
      "2025-06-10 02:48:46 - INFO - Processing batch 691/1300\n",
      "2025-06-10 02:48:46 - INFO - Batch 691/1300 - Loss: 1.0325 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:48 - INFO - Processing batch 701/1300\n",
      "2025-06-10 02:48:48 - INFO - Batch 701/1300 - Loss: 0.8545 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:49 - INFO - Processing batch 711/1300\n",
      "2025-06-10 02:48:50 - INFO - Batch 711/1300 - Loss: 0.2953 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:51 - INFO - Processing batch 721/1300\n",
      "2025-06-10 02:48:51 - INFO - Batch 721/1300 - Loss: 0.6527 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:52 - INFO - Processing batch 731/1300\n",
      "2025-06-10 02:48:53 - INFO - Batch 731/1300 - Loss: 0.3569 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:54 - INFO - Processing batch 741/1300\n",
      "2025-06-10 02:48:54 - INFO - Batch 741/1300 - Loss: 0.4476 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:55 - INFO - Processing batch 751/1300\n",
      "2025-06-10 02:48:56 - INFO - Batch 751/1300 - Loss: 0.1110 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:57 - INFO - Processing batch 761/1300\n",
      "2025-06-10 02:48:57 - INFO - Batch 761/1300 - Loss: 0.3079 - Avg batch time: 0.15s\n",
      "2025-06-10 02:48:59 - INFO - Processing batch 771/1300\n",
      "2025-06-10 02:48:59 - INFO - Batch 771/1300 - Loss: 0.2801 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:00 - INFO - Processing batch 781/1300\n",
      "2025-06-10 02:49:00 - INFO - Batch 781/1300 - Loss: 0.4898 - Avg batch time: 0.16s\n",
      "2025-06-10 02:49:02 - INFO - Processing batch 791/1300\n",
      "2025-06-10 02:49:02 - INFO - Batch 791/1300 - Loss: 0.7383 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:03 - INFO - Processing batch 801/1300\n",
      "2025-06-10 02:49:03 - INFO - Batch 801/1300 - Loss: 0.4784 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:05 - INFO - Processing batch 811/1300\n",
      "2025-06-10 02:49:05 - INFO - Batch 811/1300 - Loss: 0.9461 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:06 - INFO - Processing batch 821/1300\n",
      "2025-06-10 02:49:06 - INFO - Batch 821/1300 - Loss: 1.0992 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:08 - INFO - Processing batch 831/1300\n",
      "2025-06-10 02:49:08 - INFO - Batch 831/1300 - Loss: 0.5595 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:09 - INFO - Processing batch 841/1300\n",
      "2025-06-10 02:49:09 - INFO - Batch 841/1300 - Loss: 0.2377 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:11 - INFO - Processing batch 851/1300\n",
      "2025-06-10 02:49:11 - INFO - Batch 851/1300 - Loss: 0.1545 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:12 - INFO - Processing batch 861/1300\n",
      "2025-06-10 02:49:13 - INFO - Batch 861/1300 - Loss: 0.5051 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:14 - INFO - Processing batch 871/1300\n",
      "2025-06-10 02:49:14 - INFO - Batch 871/1300 - Loss: 0.1008 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:15 - INFO - Processing batch 881/1300\n",
      "2025-06-10 02:49:16 - INFO - Batch 881/1300 - Loss: 0.1774 - Avg batch time: 0.16s\n",
      "2025-06-10 02:49:17 - INFO - Processing batch 891/1300\n",
      "2025-06-10 02:49:17 - INFO - Batch 891/1300 - Loss: 0.4971 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:19 - INFO - Processing batch 901/1300\n",
      "2025-06-10 02:49:19 - INFO - Batch 901/1300 - Loss: 0.6083 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:20 - INFO - Processing batch 911/1300\n",
      "2025-06-10 02:49:20 - INFO - Batch 911/1300 - Loss: 0.2048 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:22 - INFO - Processing batch 921/1300\n",
      "2025-06-10 02:49:22 - INFO - Batch 921/1300 - Loss: 0.5030 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:23 - INFO - Processing batch 931/1300\n",
      "2025-06-10 02:49:23 - INFO - Batch 931/1300 - Loss: 0.3606 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:25 - INFO - Processing batch 941/1300\n",
      "2025-06-10 02:49:25 - INFO - Batch 941/1300 - Loss: 0.5074 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:26 - INFO - Processing batch 951/1300\n",
      "2025-06-10 02:49:26 - INFO - Batch 951/1300 - Loss: 0.3766 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:28 - INFO - Processing batch 961/1300\n",
      "2025-06-10 02:49:28 - INFO - Batch 961/1300 - Loss: 1.1128 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:29 - INFO - Processing batch 971/1300\n",
      "2025-06-10 02:49:29 - INFO - Batch 971/1300 - Loss: 0.4481 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:31 - INFO - Processing batch 981/1300\n",
      "2025-06-10 02:49:31 - INFO - Batch 981/1300 - Loss: 0.4055 - Avg batch time: 0.16s\n",
      "2025-06-10 02:49:32 - INFO - Processing batch 991/1300\n",
      "2025-06-10 02:49:33 - INFO - Batch 991/1300 - Loss: 0.4220 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:34 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 02:49:34 - INFO - Batch 1001/1300 - Loss: 0.2762 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:35 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 02:49:36 - INFO - Batch 1011/1300 - Loss: 0.6053 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:37 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 02:49:37 - INFO - Batch 1021/1300 - Loss: 1.5551 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:39 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 02:49:39 - INFO - Batch 1031/1300 - Loss: 0.8912 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:40 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 02:49:40 - INFO - Batch 1041/1300 - Loss: 0.3311 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:42 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 02:49:42 - INFO - Batch 1051/1300 - Loss: 0.2462 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:43 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 02:49:43 - INFO - Batch 1061/1300 - Loss: 0.3800 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:45 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 02:49:45 - INFO - Batch 1071/1300 - Loss: 0.1169 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:46 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 02:49:46 - INFO - Batch 1081/1300 - Loss: 0.4146 - Avg batch time: 0.16s\n",
      "2025-06-10 02:49:48 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 02:49:48 - INFO - Batch 1091/1300 - Loss: 0.2580 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:49 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 02:49:49 - INFO - Batch 1101/1300 - Loss: 0.6124 - Avg batch time: 0.16s\n",
      "2025-06-10 02:49:51 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 02:49:51 - INFO - Batch 1111/1300 - Loss: 0.9409 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:52 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 02:49:53 - INFO - Batch 1121/1300 - Loss: 0.1899 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:54 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 02:49:54 - INFO - Batch 1131/1300 - Loss: 0.4101 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:55 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 02:49:56 - INFO - Batch 1141/1300 - Loss: 0.5598 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:57 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 02:49:57 - INFO - Batch 1151/1300 - Loss: 0.2736 - Avg batch time: 0.15s\n",
      "2025-06-10 02:49:58 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 02:49:59 - INFO - Batch 1161/1300 - Loss: 0.2792 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:00 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 02:50:00 - INFO - Batch 1171/1300 - Loss: 0.4449 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:02 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 02:50:02 - INFO - Batch 1181/1300 - Loss: 0.1968 - Avg batch time: 0.16s\n",
      "2025-06-10 02:50:03 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 02:50:03 - INFO - Batch 1191/1300 - Loss: 0.4416 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:05 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 02:50:05 - INFO - Batch 1201/1300 - Loss: 0.1668 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:06 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 02:50:06 - INFO - Batch 1211/1300 - Loss: 0.0828 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:08 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 02:50:08 - INFO - Batch 1221/1300 - Loss: 1.0053 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:09 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 02:50:09 - INFO - Batch 1231/1300 - Loss: 0.7357 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:11 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 02:50:11 - INFO - Batch 1241/1300 - Loss: 0.1977 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:12 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 02:50:12 - INFO - Batch 1251/1300 - Loss: 0.3224 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:14 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 02:50:14 - INFO - Batch 1261/1300 - Loss: 0.5911 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:15 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 02:50:16 - INFO - Batch 1271/1300 - Loss: 0.3818 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:17 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 02:50:17 - INFO - Batch 1281/1300 - Loss: 0.1278 - Avg batch time: 0.16s\n",
      "2025-06-10 02:50:18 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 02:50:19 - INFO - Batch 1291/1300 - Loss: 0.4910 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:20 - INFO - \n",
      "Epoch 2 training completed in 200.12s\n",
      "2025-06-10 02:50:20 - INFO - Average training loss: 0.4756\n",
      "2025-06-10 02:50:38 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:   3%| | 3/100 [07:15<5:52:15, 217.89s/it, train_loss=0.4756, val_loss=0.4725, best_val_f1=0.0000, lr=1.00e-04, b2025-06-10 02:50:38 - INFO - \n",
      "Epoch 3/100 - Training phase\n",
      "2025-06-10 02:50:38 - INFO - Processing batch 1/1300\n",
      "2025-06-10 02:50:38 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 02:50:38 - INFO - Batch 1/1300 - Loss: 0.2545 - Avg batch time: 0.16s\n",
      "2025-06-10 02:50:39 - INFO - Processing batch 11/1300\n",
      "2025-06-10 02:50:40 - INFO - Batch 11/1300 - Loss: 0.2219 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:41 - INFO - Processing batch 21/1300\n",
      "2025-06-10 02:50:41 - INFO - Batch 21/1300 - Loss: 0.3197 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:43 - INFO - Processing batch 31/1300\n",
      "2025-06-10 02:50:43 - INFO - Batch 31/1300 - Loss: 0.2784 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:44 - INFO - Processing batch 41/1300\n",
      "2025-06-10 02:50:44 - INFO - Batch 41/1300 - Loss: 0.4322 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:46 - INFO - Processing batch 51/1300\n",
      "2025-06-10 02:50:46 - INFO - Batch 51/1300 - Loss: 0.3709 - Avg batch time: 0.16s\n",
      "2025-06-10 02:50:47 - INFO - Processing batch 61/1300\n",
      "2025-06-10 02:50:47 - INFO - Batch 61/1300 - Loss: 0.6739 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:49 - INFO - Processing batch 71/1300\n",
      "2025-06-10 02:50:49 - INFO - Batch 71/1300 - Loss: 0.3014 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:50 - INFO - Processing batch 81/1300\n",
      "2025-06-10 02:50:50 - INFO - Batch 81/1300 - Loss: 0.2600 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:52 - INFO - Processing batch 91/1300\n",
      "2025-06-10 02:50:52 - INFO - Batch 91/1300 - Loss: 0.2681 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:53 - INFO - Processing batch 101/1300\n",
      "2025-06-10 02:50:53 - INFO - Batch 101/1300 - Loss: 0.3975 - Avg batch time: 0.16s\n",
      "2025-06-10 02:50:55 - INFO - Processing batch 111/1300\n",
      "2025-06-10 02:50:55 - INFO - Batch 111/1300 - Loss: 0.5437 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:56 - INFO - Processing batch 121/1300\n",
      "2025-06-10 02:50:57 - INFO - Batch 121/1300 - Loss: 0.7890 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:58 - INFO - Processing batch 131/1300\n",
      "2025-06-10 02:50:58 - INFO - Batch 131/1300 - Loss: 0.3958 - Avg batch time: 0.15s\n",
      "2025-06-10 02:50:59 - INFO - Processing batch 141/1300\n",
      "2025-06-10 02:51:00 - INFO - Batch 141/1300 - Loss: 0.8746 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:01 - INFO - Processing batch 151/1300\n",
      "2025-06-10 02:51:01 - INFO - Batch 151/1300 - Loss: 0.8580 - Avg batch time: 0.16s\n",
      "2025-06-10 02:51:03 - INFO - Processing batch 161/1300\n",
      "2025-06-10 02:51:03 - INFO - Batch 161/1300 - Loss: 0.4979 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:04 - INFO - Processing batch 171/1300\n",
      "2025-06-10 02:51:04 - INFO - Batch 171/1300 - Loss: 0.1264 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:06 - INFO - Processing batch 181/1300\n",
      "2025-06-10 02:51:06 - INFO - Batch 181/1300 - Loss: 0.4442 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:07 - INFO - Processing batch 191/1300\n",
      "2025-06-10 02:51:07 - INFO - Batch 191/1300 - Loss: 0.4898 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:09 - INFO - Processing batch 201/1300\n",
      "2025-06-10 02:51:09 - INFO - Batch 201/1300 - Loss: 0.7327 - Avg batch time: 0.16s\n",
      "2025-06-10 02:51:10 - INFO - Processing batch 211/1300\n",
      "2025-06-10 02:51:10 - INFO - Batch 211/1300 - Loss: 0.3276 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:12 - INFO - Processing batch 221/1300\n",
      "2025-06-10 02:51:12 - INFO - Batch 221/1300 - Loss: 0.7060 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:13 - INFO - Processing batch 231/1300\n",
      "2025-06-10 02:51:14 - INFO - Batch 231/1300 - Loss: 0.2945 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:15 - INFO - Processing batch 241/1300\n",
      "2025-06-10 02:51:15 - INFO - Batch 241/1300 - Loss: 0.4267 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:16 - INFO - Processing batch 251/1300\n",
      "2025-06-10 02:51:17 - INFO - Batch 251/1300 - Loss: 0.2344 - Avg batch time: 0.16s\n",
      "2025-06-10 02:51:18 - INFO - Processing batch 261/1300\n",
      "2025-06-10 02:51:18 - INFO - Batch 261/1300 - Loss: 0.3334 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:20 - INFO - Processing batch 271/1300\n",
      "2025-06-10 02:51:20 - INFO - Batch 271/1300 - Loss: 0.7853 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:21 - INFO - Processing batch 281/1300\n",
      "2025-06-10 02:51:21 - INFO - Batch 281/1300 - Loss: 0.5331 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:23 - INFO - Processing batch 291/1300\n",
      "2025-06-10 02:51:23 - INFO - Batch 291/1300 - Loss: 0.4201 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:24 - INFO - Processing batch 301/1300\n",
      "2025-06-10 02:51:24 - INFO - Batch 301/1300 - Loss: 0.1574 - Avg batch time: 0.16s\n",
      "2025-06-10 02:51:26 - INFO - Processing batch 311/1300\n",
      "2025-06-10 02:51:26 - INFO - Batch 311/1300 - Loss: 0.4411 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:27 - INFO - Processing batch 321/1300\n",
      "2025-06-10 02:51:27 - INFO - Batch 321/1300 - Loss: 0.3999 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:29 - INFO - Processing batch 331/1300\n",
      "2025-06-10 02:51:29 - INFO - Batch 331/1300 - Loss: 0.4347 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:30 - INFO - Processing batch 341/1300\n",
      "2025-06-10 02:51:30 - INFO - Batch 341/1300 - Loss: 0.6901 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:32 - INFO - Processing batch 351/1300\n",
      "2025-06-10 02:51:32 - INFO - Batch 351/1300 - Loss: 0.3158 - Avg batch time: 0.16s\n",
      "2025-06-10 02:51:33 - INFO - Processing batch 361/1300\n",
      "2025-06-10 02:51:34 - INFO - Batch 361/1300 - Loss: 0.7497 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:35 - INFO - Processing batch 371/1300\n",
      "2025-06-10 02:51:35 - INFO - Batch 371/1300 - Loss: 0.6203 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:36 - INFO - Processing batch 381/1300\n",
      "2025-06-10 02:51:37 - INFO - Batch 381/1300 - Loss: 0.6044 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:38 - INFO - Processing batch 391/1300\n",
      "2025-06-10 02:51:38 - INFO - Batch 391/1300 - Loss: 0.7027 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:40 - INFO - Processing batch 401/1300\n",
      "2025-06-10 02:51:40 - INFO - Batch 401/1300 - Loss: 0.6318 - Avg batch time: 0.16s\n",
      "2025-06-10 02:51:41 - INFO - Processing batch 411/1300\n",
      "2025-06-10 02:51:41 - INFO - Batch 411/1300 - Loss: 0.8182 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:43 - INFO - Processing batch 421/1300\n",
      "2025-06-10 02:51:43 - INFO - Batch 421/1300 - Loss: 0.3422 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:44 - INFO - Processing batch 431/1300\n",
      "2025-06-10 02:51:44 - INFO - Batch 431/1300 - Loss: 0.4564 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:46 - INFO - Processing batch 441/1300\n",
      "2025-06-10 02:51:46 - INFO - Batch 441/1300 - Loss: 0.5197 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:47 - INFO - Processing batch 451/1300\n",
      "2025-06-10 02:51:47 - INFO - Batch 451/1300 - Loss: 0.2153 - Avg batch time: 0.16s\n",
      "2025-06-10 02:51:49 - INFO - Processing batch 461/1300\n",
      "2025-06-10 02:51:49 - INFO - Batch 461/1300 - Loss: 0.5314 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:50 - INFO - Processing batch 471/1300\n",
      "2025-06-10 02:51:51 - INFO - Batch 471/1300 - Loss: 0.5179 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:52 - INFO - Processing batch 481/1300\n",
      "2025-06-10 02:51:52 - INFO - Batch 481/1300 - Loss: 0.6727 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:53 - INFO - Processing batch 491/1300\n",
      "2025-06-10 02:51:54 - INFO - Batch 491/1300 - Loss: 0.1181 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:55 - INFO - Processing batch 501/1300\n",
      "2025-06-10 02:51:55 - INFO - Batch 501/1300 - Loss: 0.7708 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:57 - INFO - Processing batch 511/1300\n",
      "2025-06-10 02:51:57 - INFO - Batch 511/1300 - Loss: 0.7858 - Avg batch time: 0.15s\n",
      "2025-06-10 02:51:58 - INFO - Processing batch 521/1300\n",
      "2025-06-10 02:51:58 - INFO - Batch 521/1300 - Loss: 0.3361 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:00 - INFO - Processing batch 531/1300\n",
      "2025-06-10 02:52:00 - INFO - Batch 531/1300 - Loss: 0.5878 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:01 - INFO - Processing batch 541/1300\n",
      "2025-06-10 02:52:01 - INFO - Batch 541/1300 - Loss: 0.4490 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:03 - INFO - Processing batch 551/1300\n",
      "2025-06-10 02:52:03 - INFO - Batch 551/1300 - Loss: 0.4001 - Avg batch time: 0.16s\n",
      "2025-06-10 02:52:04 - INFO - Processing batch 561/1300\n",
      "2025-06-10 02:52:04 - INFO - Batch 561/1300 - Loss: 1.2620 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:06 - INFO - Processing batch 571/1300\n",
      "2025-06-10 02:52:06 - INFO - Batch 571/1300 - Loss: 0.1574 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:07 - INFO - Processing batch 581/1300\n",
      "2025-06-10 02:52:07 - INFO - Batch 581/1300 - Loss: 0.7044 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:09 - INFO - Processing batch 591/1300\n",
      "2025-06-10 02:52:09 - INFO - Batch 591/1300 - Loss: 0.7374 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:10 - INFO - Processing batch 601/1300\n",
      "2025-06-10 02:52:11 - INFO - Batch 601/1300 - Loss: 0.4599 - Avg batch time: 0.16s\n",
      "2025-06-10 02:52:12 - INFO - Processing batch 611/1300\n",
      "2025-06-10 02:52:12 - INFO - Batch 611/1300 - Loss: 0.5284 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:13 - INFO - Processing batch 621/1300\n",
      "2025-06-10 02:52:14 - INFO - Batch 621/1300 - Loss: 0.4494 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:15 - INFO - Processing batch 631/1300\n",
      "2025-06-10 02:52:15 - INFO - Batch 631/1300 - Loss: 0.3876 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:17 - INFO - Processing batch 641/1300\n",
      "2025-06-10 02:52:17 - INFO - Batch 641/1300 - Loss: 0.6275 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:18 - INFO - Processing batch 651/1300\n",
      "2025-06-10 02:52:18 - INFO - Batch 651/1300 - Loss: 0.4904 - Avg batch time: 0.16s\n",
      "2025-06-10 02:52:20 - INFO - Processing batch 661/1300\n",
      "2025-06-10 02:52:20 - INFO - Batch 661/1300 - Loss: 0.7933 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:21 - INFO - Processing batch 671/1300\n",
      "2025-06-10 02:52:21 - INFO - Batch 671/1300 - Loss: 0.7141 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:23 - INFO - Processing batch 681/1300\n",
      "2025-06-10 02:52:23 - INFO - Batch 681/1300 - Loss: 0.9830 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:24 - INFO - Processing batch 691/1300\n",
      "2025-06-10 02:52:24 - INFO - Batch 691/1300 - Loss: 0.2164 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:26 - INFO - Processing batch 701/1300\n",
      "2025-06-10 02:52:26 - INFO - Batch 701/1300 - Loss: 0.5181 - Avg batch time: 0.16s\n",
      "2025-06-10 02:52:27 - INFO - Processing batch 711/1300\n",
      "2025-06-10 02:52:28 - INFO - Batch 711/1300 - Loss: 0.3786 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:29 - INFO - Processing batch 721/1300\n",
      "2025-06-10 02:52:29 - INFO - Batch 721/1300 - Loss: 0.5372 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:31 - INFO - Processing batch 731/1300\n",
      "2025-06-10 02:52:31 - INFO - Batch 731/1300 - Loss: 0.2257 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:32 - INFO - Processing batch 741/1300\n",
      "2025-06-10 02:52:32 - INFO - Batch 741/1300 - Loss: 0.2411 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:34 - INFO - Processing batch 751/1300\n",
      "2025-06-10 02:52:34 - INFO - Batch 751/1300 - Loss: 0.3028 - Avg batch time: 0.16s\n",
      "2025-06-10 02:52:35 - INFO - Processing batch 761/1300\n",
      "2025-06-10 02:52:35 - INFO - Batch 761/1300 - Loss: 0.3669 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:37 - INFO - Processing batch 771/1300\n",
      "2025-06-10 02:52:37 - INFO - Batch 771/1300 - Loss: 0.3563 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:38 - INFO - Processing batch 781/1300\n",
      "2025-06-10 02:52:38 - INFO - Batch 781/1300 - Loss: 0.5030 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:40 - INFO - Processing batch 791/1300\n",
      "2025-06-10 02:52:40 - INFO - Batch 791/1300 - Loss: 0.3761 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:41 - INFO - Processing batch 801/1300\n",
      "2025-06-10 02:52:42 - INFO - Batch 801/1300 - Loss: 0.3938 - Avg batch time: 0.16s\n",
      "2025-06-10 02:52:43 - INFO - Processing batch 811/1300\n",
      "2025-06-10 02:52:43 - INFO - Batch 811/1300 - Loss: 0.1612 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:44 - INFO - Processing batch 821/1300\n",
      "2025-06-10 02:52:45 - INFO - Batch 821/1300 - Loss: 0.2251 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:46 - INFO - Processing batch 831/1300\n",
      "2025-06-10 02:52:46 - INFO - Batch 831/1300 - Loss: 0.2084 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:48 - INFO - Processing batch 841/1300\n",
      "2025-06-10 02:52:48 - INFO - Batch 841/1300 - Loss: 0.4753 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:49 - INFO - Processing batch 851/1300\n",
      "2025-06-10 02:52:49 - INFO - Batch 851/1300 - Loss: 0.4585 - Avg batch time: 0.16s\n",
      "2025-06-10 02:52:51 - INFO - Processing batch 861/1300\n",
      "2025-06-10 02:52:51 - INFO - Batch 861/1300 - Loss: 0.7631 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:52 - INFO - Processing batch 871/1300\n",
      "2025-06-10 02:52:52 - INFO - Batch 871/1300 - Loss: 0.3595 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:54 - INFO - Processing batch 881/1300\n",
      "2025-06-10 02:52:54 - INFO - Batch 881/1300 - Loss: 0.9195 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:55 - INFO - Processing batch 891/1300\n",
      "2025-06-10 02:52:55 - INFO - Batch 891/1300 - Loss: 0.3092 - Avg batch time: 0.15s\n",
      "2025-06-10 02:52:57 - INFO - Processing batch 901/1300\n",
      "2025-06-10 02:52:57 - INFO - Batch 901/1300 - Loss: 0.3853 - Avg batch time: 0.16s\n",
      "2025-06-10 02:52:58 - INFO - Processing batch 911/1300\n",
      "2025-06-10 02:52:59 - INFO - Batch 911/1300 - Loss: 0.7975 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:00 - INFO - Processing batch 921/1300\n",
      "2025-06-10 02:53:00 - INFO - Batch 921/1300 - Loss: 0.4044 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:02 - INFO - Processing batch 931/1300\n",
      "2025-06-10 02:53:02 - INFO - Batch 931/1300 - Loss: 0.4898 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:03 - INFO - Processing batch 941/1300\n",
      "2025-06-10 02:53:03 - INFO - Batch 941/1300 - Loss: 0.8110 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:05 - INFO - Processing batch 951/1300\n",
      "2025-06-10 02:53:05 - INFO - Batch 951/1300 - Loss: 0.1918 - Avg batch time: 0.16s\n",
      "2025-06-10 02:53:06 - INFO - Processing batch 961/1300\n",
      "2025-06-10 02:53:06 - INFO - Batch 961/1300 - Loss: 0.3381 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:08 - INFO - Processing batch 971/1300\n",
      "2025-06-10 02:53:08 - INFO - Batch 971/1300 - Loss: 0.4853 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:09 - INFO - Processing batch 981/1300\n",
      "2025-06-10 02:53:09 - INFO - Batch 981/1300 - Loss: 0.3252 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:11 - INFO - Processing batch 991/1300\n",
      "2025-06-10 02:53:11 - INFO - Batch 991/1300 - Loss: 0.2303 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:12 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 02:53:12 - INFO - Batch 1001/1300 - Loss: 0.9672 - Avg batch time: 0.16s\n",
      "2025-06-10 02:53:14 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 02:53:14 - INFO - Batch 1011/1300 - Loss: 0.2115 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:15 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 02:53:16 - INFO - Batch 1021/1300 - Loss: 0.3691 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:17 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 02:53:17 - INFO - Batch 1031/1300 - Loss: 0.8849 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:18 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 02:53:19 - INFO - Batch 1041/1300 - Loss: 0.8574 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:20 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 02:53:20 - INFO - Batch 1051/1300 - Loss: 0.7463 - Avg batch time: 0.16s\n",
      "2025-06-10 02:53:22 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 02:53:22 - INFO - Batch 1061/1300 - Loss: 0.2806 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:23 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 02:53:23 - INFO - Batch 1071/1300 - Loss: 0.3887 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:25 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 02:53:25 - INFO - Batch 1081/1300 - Loss: 0.2938 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:26 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 02:53:26 - INFO - Batch 1091/1300 - Loss: 0.2338 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:28 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 02:53:28 - INFO - Batch 1101/1300 - Loss: 0.6695 - Avg batch time: 0.16s\n",
      "2025-06-10 02:53:29 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 02:53:29 - INFO - Batch 1111/1300 - Loss: 0.9633 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:31 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 02:53:31 - INFO - Batch 1121/1300 - Loss: 0.4153 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:32 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 02:53:33 - INFO - Batch 1131/1300 - Loss: 0.1188 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:34 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 02:53:34 - INFO - Batch 1141/1300 - Loss: 0.4069 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:36 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 02:53:36 - INFO - Batch 1151/1300 - Loss: 0.9439 - Avg batch time: 0.16s\n",
      "2025-06-10 02:53:37 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 02:53:37 - INFO - Batch 1161/1300 - Loss: 0.2059 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:39 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 02:53:39 - INFO - Batch 1171/1300 - Loss: 0.4921 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:40 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 02:53:40 - INFO - Batch 1181/1300 - Loss: 0.8334 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:42 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 02:53:42 - INFO - Batch 1191/1300 - Loss: 0.9404 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:43 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 02:53:43 - INFO - Batch 1201/1300 - Loss: 0.3251 - Avg batch time: 0.16s\n",
      "2025-06-10 02:53:45 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 02:53:45 - INFO - Batch 1211/1300 - Loss: 0.8231 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:46 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 02:53:46 - INFO - Batch 1221/1300 - Loss: 0.3607 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:48 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 02:53:48 - INFO - Batch 1231/1300 - Loss: 0.4201 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:49 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 02:53:50 - INFO - Batch 1241/1300 - Loss: 0.6882 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:51 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 02:53:51 - INFO - Batch 1251/1300 - Loss: 0.4321 - Avg batch time: 0.16s\n",
      "2025-06-10 02:53:53 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 02:53:53 - INFO - Batch 1261/1300 - Loss: 0.9119 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:54 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 02:53:54 - INFO - Batch 1271/1300 - Loss: 0.3672 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:56 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 02:53:56 - INFO - Batch 1281/1300 - Loss: 0.6733 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:57 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 02:53:57 - INFO - Batch 1291/1300 - Loss: 0.4429 - Avg batch time: 0.15s\n",
      "2025-06-10 02:53:59 - INFO - \n",
      "Epoch 3 training completed in 201.05s\n",
      "2025-06-10 02:53:59 - INFO - Average training loss: 0.4900\n",
      "2025-06-10 02:54:17 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:   4%| | 4/100 [10:54<5:49:21, 218.35s/it, train_loss=0.4900, val_loss=0.4810, best_val_f1=0.0000, lr=1.00e-04, b2025-06-10 02:54:17 - INFO - \n",
      "Epoch 4/100 - Training phase\n",
      "2025-06-10 02:54:17 - INFO - Processing batch 1/1300\n",
      "2025-06-10 02:54:17 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 02:54:17 - INFO - Batch 1/1300 - Loss: 0.3597 - Avg batch time: 0.16s\n",
      "2025-06-10 02:54:18 - INFO - Processing batch 11/1300\n",
      "2025-06-10 02:54:19 - INFO - Batch 11/1300 - Loss: 0.5899 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:20 - INFO - Processing batch 21/1300\n",
      "2025-06-10 02:54:20 - INFO - Batch 21/1300 - Loss: 0.2072 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:21 - INFO - Processing batch 31/1300\n",
      "2025-06-10 02:54:22 - INFO - Batch 31/1300 - Loss: 0.3977 - Avg batch time: 0.16s\n",
      "2025-06-10 02:54:23 - INFO - Processing batch 41/1300\n",
      "2025-06-10 02:54:23 - INFO - Batch 41/1300 - Loss: 0.5670 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:25 - INFO - Processing batch 51/1300\n",
      "2025-06-10 02:54:25 - INFO - Batch 51/1300 - Loss: 0.4771 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:26 - INFO - Processing batch 61/1300\n",
      "2025-06-10 02:54:26 - INFO - Batch 61/1300 - Loss: 0.2773 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:28 - INFO - Processing batch 71/1300\n",
      "2025-06-10 02:54:28 - INFO - Batch 71/1300 - Loss: 0.1907 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:29 - INFO - Processing batch 81/1300\n",
      "2025-06-10 02:54:29 - INFO - Batch 81/1300 - Loss: 0.4760 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:31 - INFO - Processing batch 91/1300\n",
      "2025-06-10 02:54:31 - INFO - Batch 91/1300 - Loss: 0.7779 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:32 - INFO - Processing batch 101/1300\n",
      "2025-06-10 02:54:32 - INFO - Batch 101/1300 - Loss: 0.1960 - Avg batch time: 0.16s\n",
      "2025-06-10 02:54:34 - INFO - Processing batch 111/1300\n",
      "2025-06-10 02:54:34 - INFO - Batch 111/1300 - Loss: 0.5440 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:35 - INFO - Processing batch 121/1300\n",
      "2025-06-10 02:54:35 - INFO - Batch 121/1300 - Loss: 0.0846 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:37 - INFO - Processing batch 131/1300\n",
      "2025-06-10 02:54:37 - INFO - Batch 131/1300 - Loss: 0.3120 - Avg batch time: 0.16s\n",
      "2025-06-10 02:54:38 - INFO - Processing batch 141/1300\n",
      "2025-06-10 02:54:39 - INFO - Batch 141/1300 - Loss: 0.4966 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:40 - INFO - Processing batch 151/1300\n",
      "2025-06-10 02:54:40 - INFO - Batch 151/1300 - Loss: 0.4958 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:41 - INFO - Processing batch 161/1300\n",
      "2025-06-10 02:54:42 - INFO - Batch 161/1300 - Loss: 0.7494 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:43 - INFO - Processing batch 171/1300\n",
      "2025-06-10 02:54:43 - INFO - Batch 171/1300 - Loss: 0.3275 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:45 - INFO - Processing batch 181/1300\n",
      "2025-06-10 02:54:45 - INFO - Batch 181/1300 - Loss: 0.2078 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:46 - INFO - Processing batch 191/1300\n",
      "2025-06-10 02:54:46 - INFO - Batch 191/1300 - Loss: 0.3769 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:48 - INFO - Processing batch 201/1300\n",
      "2025-06-10 02:54:48 - INFO - Batch 201/1300 - Loss: 0.5670 - Avg batch time: 0.16s\n",
      "2025-06-10 02:54:49 - INFO - Processing batch 211/1300\n",
      "2025-06-10 02:54:49 - INFO - Batch 211/1300 - Loss: 0.7036 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:51 - INFO - Processing batch 221/1300\n",
      "2025-06-10 02:54:51 - INFO - Batch 221/1300 - Loss: 0.7726 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:52 - INFO - Processing batch 231/1300\n",
      "2025-06-10 02:54:52 - INFO - Batch 231/1300 - Loss: 0.6499 - Avg batch time: 0.16s\n",
      "2025-06-10 02:54:54 - INFO - Processing batch 241/1300\n",
      "2025-06-10 02:54:54 - INFO - Batch 241/1300 - Loss: 0.6212 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:55 - INFO - Processing batch 251/1300\n",
      "2025-06-10 02:54:56 - INFO - Batch 251/1300 - Loss: 0.3518 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:57 - INFO - Processing batch 261/1300\n",
      "2025-06-10 02:54:57 - INFO - Batch 261/1300 - Loss: 0.3499 - Avg batch time: 0.15s\n",
      "2025-06-10 02:54:58 - INFO - Processing batch 271/1300\n",
      "2025-06-10 02:54:59 - INFO - Batch 271/1300 - Loss: 0.2799 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:00 - INFO - Processing batch 281/1300\n",
      "2025-06-10 02:55:00 - INFO - Batch 281/1300 - Loss: 0.4292 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:02 - INFO - Processing batch 291/1300\n",
      "2025-06-10 02:55:02 - INFO - Batch 291/1300 - Loss: 0.9475 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:03 - INFO - Processing batch 301/1300\n",
      "2025-06-10 02:55:03 - INFO - Batch 301/1300 - Loss: 0.4909 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:05 - INFO - Processing batch 311/1300\n",
      "2025-06-10 02:55:05 - INFO - Batch 311/1300 - Loss: 0.3969 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:06 - INFO - Processing batch 321/1300\n",
      "2025-06-10 02:55:06 - INFO - Batch 321/1300 - Loss: 0.5852 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:08 - INFO - Processing batch 331/1300\n",
      "2025-06-10 02:55:08 - INFO - Batch 331/1300 - Loss: 0.5668 - Avg batch time: 0.16s\n",
      "2025-06-10 02:55:09 - INFO - Processing batch 341/1300\n",
      "2025-06-10 02:55:09 - INFO - Batch 341/1300 - Loss: 0.3642 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:11 - INFO - Processing batch 351/1300\n",
      "2025-06-10 02:55:11 - INFO - Batch 351/1300 - Loss: 0.1241 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:12 - INFO - Processing batch 361/1300\n",
      "2025-06-10 02:55:12 - INFO - Batch 361/1300 - Loss: 0.8051 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:14 - INFO - Processing batch 371/1300\n",
      "2025-06-10 02:55:14 - INFO - Batch 371/1300 - Loss: 0.4558 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:15 - INFO - Processing batch 381/1300\n",
      "2025-06-10 02:55:15 - INFO - Batch 381/1300 - Loss: 0.4141 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:17 - INFO - Processing batch 391/1300\n",
      "2025-06-10 02:55:17 - INFO - Batch 391/1300 - Loss: 0.5778 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:18 - INFO - Processing batch 401/1300\n",
      "2025-06-10 02:55:19 - INFO - Batch 401/1300 - Loss: 0.9155 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:20 - INFO - Processing batch 411/1300\n",
      "2025-06-10 02:55:20 - INFO - Batch 411/1300 - Loss: 0.5572 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:21 - INFO - Processing batch 421/1300\n",
      "2025-06-10 02:55:22 - INFO - Batch 421/1300 - Loss: 0.2939 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:23 - INFO - Processing batch 431/1300\n",
      "2025-06-10 02:55:23 - INFO - Batch 431/1300 - Loss: 0.4043 - Avg batch time: 0.16s\n",
      "2025-06-10 02:55:25 - INFO - Processing batch 441/1300\n",
      "2025-06-10 02:55:25 - INFO - Batch 441/1300 - Loss: 0.4153 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:26 - INFO - Processing batch 451/1300\n",
      "2025-06-10 02:55:26 - INFO - Batch 451/1300 - Loss: 0.3873 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:28 - INFO - Processing batch 461/1300\n",
      "2025-06-10 02:55:28 - INFO - Batch 461/1300 - Loss: 0.4920 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:29 - INFO - Processing batch 471/1300\n",
      "2025-06-10 02:55:29 - INFO - Batch 471/1300 - Loss: 0.6196 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:31 - INFO - Processing batch 481/1300\n",
      "2025-06-10 02:55:31 - INFO - Batch 481/1300 - Loss: 0.7748 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:32 - INFO - Processing batch 491/1300\n",
      "2025-06-10 02:55:32 - INFO - Batch 491/1300 - Loss: 0.5013 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:34 - INFO - Processing batch 501/1300\n",
      "2025-06-10 02:55:34 - INFO - Batch 501/1300 - Loss: 0.6664 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:35 - INFO - Processing batch 511/1300\n",
      "2025-06-10 02:55:35 - INFO - Batch 511/1300 - Loss: 0.6035 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:37 - INFO - Processing batch 521/1300\n",
      "2025-06-10 02:55:37 - INFO - Batch 521/1300 - Loss: 0.5601 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:38 - INFO - Processing batch 531/1300\n",
      "2025-06-10 02:55:39 - INFO - Batch 531/1300 - Loss: 0.2741 - Avg batch time: 0.16s\n",
      "2025-06-10 02:55:40 - INFO - Processing batch 541/1300\n",
      "2025-06-10 02:55:40 - INFO - Batch 541/1300 - Loss: 0.6522 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:41 - INFO - Processing batch 551/1300\n",
      "2025-06-10 02:55:42 - INFO - Batch 551/1300 - Loss: 0.0768 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:43 - INFO - Processing batch 561/1300\n",
      "2025-06-10 02:55:43 - INFO - Batch 561/1300 - Loss: 0.2656 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:45 - INFO - Processing batch 571/1300\n",
      "2025-06-10 02:55:45 - INFO - Batch 571/1300 - Loss: 1.0380 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:46 - INFO - Processing batch 581/1300\n",
      "2025-06-10 02:55:46 - INFO - Batch 581/1300 - Loss: 0.7790 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:48 - INFO - Processing batch 591/1300\n",
      "2025-06-10 02:55:48 - INFO - Batch 591/1300 - Loss: 0.5227 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:49 - INFO - Processing batch 601/1300\n",
      "2025-06-10 02:55:49 - INFO - Batch 601/1300 - Loss: 0.5339 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:51 - INFO - Processing batch 611/1300\n",
      "2025-06-10 02:55:51 - INFO - Batch 611/1300 - Loss: 0.5709 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:52 - INFO - Processing batch 621/1300\n",
      "2025-06-10 02:55:52 - INFO - Batch 621/1300 - Loss: 0.8449 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:54 - INFO - Processing batch 631/1300\n",
      "2025-06-10 02:55:54 - INFO - Batch 631/1300 - Loss: 0.2922 - Avg batch time: 0.16s\n",
      "2025-06-10 02:55:55 - INFO - Processing batch 641/1300\n",
      "2025-06-10 02:55:55 - INFO - Batch 641/1300 - Loss: 0.2328 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:57 - INFO - Processing batch 651/1300\n",
      "2025-06-10 02:55:57 - INFO - Batch 651/1300 - Loss: 0.2273 - Avg batch time: 0.15s\n",
      "2025-06-10 02:55:58 - INFO - Processing batch 661/1300\n",
      "2025-06-10 02:55:59 - INFO - Batch 661/1300 - Loss: 0.1524 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:00 - INFO - Processing batch 671/1300\n",
      "2025-06-10 02:56:00 - INFO - Batch 671/1300 - Loss: 0.1582 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:01 - INFO - Processing batch 681/1300\n",
      "2025-06-10 02:56:02 - INFO - Batch 681/1300 - Loss: 0.3240 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:03 - INFO - Processing batch 691/1300\n",
      "2025-06-10 02:56:03 - INFO - Batch 691/1300 - Loss: 0.6084 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:05 - INFO - Processing batch 701/1300\n",
      "2025-06-10 02:56:05 - INFO - Batch 701/1300 - Loss: 0.8026 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:06 - INFO - Processing batch 711/1300\n",
      "2025-06-10 02:56:06 - INFO - Batch 711/1300 - Loss: 0.4820 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:08 - INFO - Processing batch 721/1300\n",
      "2025-06-10 02:56:08 - INFO - Batch 721/1300 - Loss: 0.3443 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:09 - INFO - Processing batch 731/1300\n",
      "2025-06-10 02:56:09 - INFO - Batch 731/1300 - Loss: 0.9944 - Avg batch time: 0.16s\n",
      "2025-06-10 02:56:11 - INFO - Processing batch 741/1300\n",
      "2025-06-10 02:56:11 - INFO - Batch 741/1300 - Loss: 0.4291 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:12 - INFO - Processing batch 751/1300\n",
      "2025-06-10 02:56:12 - INFO - Batch 751/1300 - Loss: 0.2436 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:14 - INFO - Processing batch 761/1300\n",
      "2025-06-10 02:56:14 - INFO - Batch 761/1300 - Loss: 0.1637 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:15 - INFO - Processing batch 771/1300\n",
      "2025-06-10 02:56:15 - INFO - Batch 771/1300 - Loss: 0.3895 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:17 - INFO - Processing batch 781/1300\n",
      "2025-06-10 02:56:17 - INFO - Batch 781/1300 - Loss: 0.5278 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:18 - INFO - Processing batch 791/1300\n",
      "2025-06-10 02:56:18 - INFO - Batch 791/1300 - Loss: 0.7083 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:20 - INFO - Processing batch 801/1300\n",
      "2025-06-10 02:56:20 - INFO - Batch 801/1300 - Loss: 0.3452 - Avg batch time: 0.16s\n",
      "2025-06-10 02:56:21 - INFO - Processing batch 811/1300\n",
      "2025-06-10 02:56:22 - INFO - Batch 811/1300 - Loss: 0.2794 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:23 - INFO - Processing batch 821/1300\n",
      "2025-06-10 02:56:23 - INFO - Batch 821/1300 - Loss: 0.4931 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:25 - INFO - Processing batch 831/1300\n",
      "2025-06-10 02:56:25 - INFO - Batch 831/1300 - Loss: 0.2226 - Avg batch time: 0.16s\n",
      "2025-06-10 02:56:26 - INFO - Processing batch 841/1300\n",
      "2025-06-10 02:56:26 - INFO - Batch 841/1300 - Loss: 0.6114 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:28 - INFO - Processing batch 851/1300\n",
      "2025-06-10 02:56:28 - INFO - Batch 851/1300 - Loss: 0.6847 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:29 - INFO - Processing batch 861/1300\n",
      "2025-06-10 02:56:29 - INFO - Batch 861/1300 - Loss: 0.2983 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:31 - INFO - Processing batch 871/1300\n",
      "2025-06-10 02:56:31 - INFO - Batch 871/1300 - Loss: 0.5815 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:32 - INFO - Processing batch 881/1300\n",
      "2025-06-10 02:56:32 - INFO - Batch 881/1300 - Loss: 0.2248 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:34 - INFO - Processing batch 891/1300\n",
      "2025-06-10 02:56:34 - INFO - Batch 891/1300 - Loss: 0.3911 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:35 - INFO - Processing batch 901/1300\n",
      "2025-06-10 02:56:35 - INFO - Batch 901/1300 - Loss: 0.3922 - Avg batch time: 0.16s\n",
      "2025-06-10 02:56:37 - INFO - Processing batch 911/1300\n",
      "2025-06-10 02:56:37 - INFO - Batch 911/1300 - Loss: 0.4346 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:38 - INFO - Processing batch 921/1300\n",
      "2025-06-10 02:56:38 - INFO - Batch 921/1300 - Loss: 0.9916 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:40 - INFO - Processing batch 931/1300\n",
      "2025-06-10 02:56:40 - INFO - Batch 931/1300 - Loss: 0.7312 - Avg batch time: 0.16s\n",
      "2025-06-10 02:56:41 - INFO - Processing batch 941/1300\n",
      "2025-06-10 02:56:42 - INFO - Batch 941/1300 - Loss: 0.2937 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:43 - INFO - Processing batch 951/1300\n",
      "2025-06-10 02:56:43 - INFO - Batch 951/1300 - Loss: 0.7229 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:44 - INFO - Processing batch 961/1300\n",
      "2025-06-10 02:56:45 - INFO - Batch 961/1300 - Loss: 0.0767 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:46 - INFO - Processing batch 971/1300\n",
      "2025-06-10 02:56:46 - INFO - Batch 971/1300 - Loss: 0.4189 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:48 - INFO - Processing batch 981/1300\n",
      "2025-06-10 02:56:48 - INFO - Batch 981/1300 - Loss: 0.3153 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:49 - INFO - Processing batch 991/1300\n",
      "2025-06-10 02:56:49 - INFO - Batch 991/1300 - Loss: 0.3317 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:51 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 02:56:51 - INFO - Batch 1001/1300 - Loss: 0.2689 - Avg batch time: 0.16s\n",
      "2025-06-10 02:56:52 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 02:56:52 - INFO - Batch 1011/1300 - Loss: 0.8132 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:54 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 02:56:54 - INFO - Batch 1021/1300 - Loss: 0.4332 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:55 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 02:56:55 - INFO - Batch 1031/1300 - Loss: 0.1285 - Avg batch time: 0.16s\n",
      "2025-06-10 02:56:57 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 02:56:57 - INFO - Batch 1041/1300 - Loss: 0.6613 - Avg batch time: 0.15s\n",
      "2025-06-10 02:56:58 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 02:56:58 - INFO - Batch 1051/1300 - Loss: 0.4886 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:00 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 02:57:00 - INFO - Batch 1061/1300 - Loss: 0.2605 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:01 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 02:57:02 - INFO - Batch 1071/1300 - Loss: 0.4706 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:03 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 02:57:03 - INFO - Batch 1081/1300 - Loss: 0.3332 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:04 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 02:57:05 - INFO - Batch 1091/1300 - Loss: 0.1481 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:06 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 02:57:06 - INFO - Batch 1101/1300 - Loss: 0.3546 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:07 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 02:57:08 - INFO - Batch 1111/1300 - Loss: 0.4878 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:09 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 02:57:09 - INFO - Batch 1121/1300 - Loss: 0.3283 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:11 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 02:57:11 - INFO - Batch 1131/1300 - Loss: 0.5115 - Avg batch time: 0.16s\n",
      "2025-06-10 02:57:12 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 02:57:12 - INFO - Batch 1141/1300 - Loss: 0.5237 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:14 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 02:57:14 - INFO - Batch 1151/1300 - Loss: 0.4140 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:15 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 02:57:15 - INFO - Batch 1161/1300 - Loss: 0.7576 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:17 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 02:57:17 - INFO - Batch 1171/1300 - Loss: 0.9410 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:18 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 02:57:18 - INFO - Batch 1181/1300 - Loss: 0.5171 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:20 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 02:57:20 - INFO - Batch 1191/1300 - Loss: 0.6769 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:21 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 02:57:21 - INFO - Batch 1201/1300 - Loss: 0.3866 - Avg batch time: 0.16s\n",
      "2025-06-10 02:57:23 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 02:57:23 - INFO - Batch 1211/1300 - Loss: 0.9186 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:24 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 02:57:25 - INFO - Batch 1221/1300 - Loss: 0.8554 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:26 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 02:57:26 - INFO - Batch 1231/1300 - Loss: 0.5846 - Avg batch time: 0.16s\n",
      "2025-06-10 02:57:28 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 02:57:28 - INFO - Batch 1241/1300 - Loss: 0.1262 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:29 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 02:57:29 - INFO - Batch 1251/1300 - Loss: 0.3732 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:31 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 02:57:31 - INFO - Batch 1261/1300 - Loss: 0.3846 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:32 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 02:57:32 - INFO - Batch 1271/1300 - Loss: 0.8325 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:34 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 02:57:34 - INFO - Batch 1281/1300 - Loss: 1.1447 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:35 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 02:57:35 - INFO - Batch 1291/1300 - Loss: 0.5791 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:37 - INFO - \n",
      "Epoch 4 training completed in 200.14s\n",
      "2025-06-10 02:57:37 - INFO - Average training loss: 0.4808\n",
      "2025-06-10 02:57:54 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:   5%| | 5/100 [14:32<5:45:26, 218.17s/it, train_loss=0.4808, val_loss=0.4624, best_val_f1=0.0000, lr=1.00e-04, b2025-06-10 02:57:54 - INFO - \n",
      "Epoch 5/100 - Training phase\n",
      "2025-06-10 02:57:55 - INFO - Processing batch 1/1300\n",
      "2025-06-10 02:57:55 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 02:57:55 - INFO - Batch 1/1300 - Loss: 0.5834 - Avg batch time: 0.16s\n",
      "2025-06-10 02:57:56 - INFO - Processing batch 11/1300\n",
      "2025-06-10 02:57:56 - INFO - Batch 11/1300 - Loss: 0.6675 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:58 - INFO - Processing batch 21/1300\n",
      "2025-06-10 02:57:58 - INFO - Batch 21/1300 - Loss: 0.3268 - Avg batch time: 0.15s\n",
      "2025-06-10 02:57:59 - INFO - Processing batch 31/1300\n",
      "2025-06-10 02:58:00 - INFO - Batch 31/1300 - Loss: 0.5596 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:01 - INFO - Processing batch 41/1300\n",
      "2025-06-10 02:58:01 - INFO - Batch 41/1300 - Loss: 0.5421 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:02 - INFO - Processing batch 51/1300\n",
      "2025-06-10 02:58:03 - INFO - Batch 51/1300 - Loss: 0.5296 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:04 - INFO - Processing batch 61/1300\n",
      "2025-06-10 02:58:04 - INFO - Batch 61/1300 - Loss: 0.2929 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:06 - INFO - Processing batch 71/1300\n",
      "2025-06-10 02:58:06 - INFO - Batch 71/1300 - Loss: 0.3782 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:07 - INFO - Processing batch 81/1300\n",
      "2025-06-10 02:58:07 - INFO - Batch 81/1300 - Loss: 0.1173 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:09 - INFO - Processing batch 91/1300\n",
      "2025-06-10 02:58:09 - INFO - Batch 91/1300 - Loss: 0.3679 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:10 - INFO - Processing batch 101/1300\n",
      "2025-06-10 02:58:10 - INFO - Batch 101/1300 - Loss: 0.4858 - Avg batch time: 0.16s\n",
      "2025-06-10 02:58:12 - INFO - Processing batch 111/1300\n",
      "2025-06-10 02:58:12 - INFO - Batch 111/1300 - Loss: 0.7323 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:13 - INFO - Processing batch 121/1300\n",
      "2025-06-10 02:58:13 - INFO - Batch 121/1300 - Loss: 0.5181 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:15 - INFO - Processing batch 131/1300\n",
      "2025-06-10 02:58:15 - INFO - Batch 131/1300 - Loss: 0.5374 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:16 - INFO - Processing batch 141/1300\n",
      "2025-06-10 02:58:16 - INFO - Batch 141/1300 - Loss: 0.4253 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:18 - INFO - Processing batch 151/1300\n",
      "2025-06-10 02:58:18 - INFO - Batch 151/1300 - Loss: 0.2405 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:19 - INFO - Processing batch 161/1300\n",
      "2025-06-10 02:58:20 - INFO - Batch 161/1300 - Loss: 0.4745 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:21 - INFO - Processing batch 171/1300\n",
      "2025-06-10 02:58:21 - INFO - Batch 171/1300 - Loss: 0.8866 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:22 - INFO - Processing batch 181/1300\n",
      "2025-06-10 02:58:23 - INFO - Batch 181/1300 - Loss: 0.8516 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:24 - INFO - Processing batch 191/1300\n",
      "2025-06-10 02:58:24 - INFO - Batch 191/1300 - Loss: 0.3221 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:26 - INFO - Processing batch 201/1300\n",
      "2025-06-10 02:58:26 - INFO - Batch 201/1300 - Loss: 0.4660 - Avg batch time: 0.16s\n",
      "2025-06-10 02:58:27 - INFO - Processing batch 211/1300\n",
      "2025-06-10 02:58:27 - INFO - Batch 211/1300 - Loss: 0.1687 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:29 - INFO - Processing batch 221/1300\n",
      "2025-06-10 02:58:29 - INFO - Batch 221/1300 - Loss: 0.0819 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:30 - INFO - Processing batch 231/1300\n",
      "2025-06-10 02:58:30 - INFO - Batch 231/1300 - Loss: 0.5193 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:32 - INFO - Processing batch 241/1300\n",
      "2025-06-10 02:58:32 - INFO - Batch 241/1300 - Loss: 0.8470 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:33 - INFO - Processing batch 251/1300\n",
      "2025-06-10 02:58:33 - INFO - Batch 251/1300 - Loss: 0.2190 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:35 - INFO - Processing batch 261/1300\n",
      "2025-06-10 02:58:35 - INFO - Batch 261/1300 - Loss: 0.4978 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:36 - INFO - Processing batch 271/1300\n",
      "2025-06-10 02:58:36 - INFO - Batch 271/1300 - Loss: 0.3612 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:38 - INFO - Processing batch 281/1300\n",
      "2025-06-10 02:58:38 - INFO - Batch 281/1300 - Loss: 0.1314 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:39 - INFO - Processing batch 291/1300\n",
      "2025-06-10 02:58:40 - INFO - Batch 291/1300 - Loss: 0.4297 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:41 - INFO - Processing batch 301/1300\n",
      "2025-06-10 02:58:41 - INFO - Batch 301/1300 - Loss: 0.1501 - Avg batch time: 0.16s\n",
      "2025-06-10 02:58:43 - INFO - Processing batch 311/1300\n",
      "2025-06-10 02:58:43 - INFO - Batch 311/1300 - Loss: 1.5419 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:44 - INFO - Processing batch 321/1300\n",
      "2025-06-10 02:58:44 - INFO - Batch 321/1300 - Loss: 0.5837 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:46 - INFO - Processing batch 331/1300\n",
      "2025-06-10 02:58:46 - INFO - Batch 331/1300 - Loss: 0.7515 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:47 - INFO - Processing batch 341/1300\n",
      "2025-06-10 02:58:47 - INFO - Batch 341/1300 - Loss: 0.3789 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:49 - INFO - Processing batch 351/1300\n",
      "2025-06-10 02:58:49 - INFO - Batch 351/1300 - Loss: 0.4835 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:50 - INFO - Processing batch 361/1300\n",
      "2025-06-10 02:58:50 - INFO - Batch 361/1300 - Loss: 0.3520 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:52 - INFO - Processing batch 371/1300\n",
      "2025-06-10 02:58:52 - INFO - Batch 371/1300 - Loss: 0.3425 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:53 - INFO - Processing batch 381/1300\n",
      "2025-06-10 02:58:53 - INFO - Batch 381/1300 - Loss: 0.4028 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:55 - INFO - Processing batch 391/1300\n",
      "2025-06-10 02:58:55 - INFO - Batch 391/1300 - Loss: 0.6667 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:56 - INFO - Processing batch 401/1300\n",
      "2025-06-10 02:58:57 - INFO - Batch 401/1300 - Loss: 0.8700 - Avg batch time: 0.16s\n",
      "2025-06-10 02:58:58 - INFO - Processing batch 411/1300\n",
      "2025-06-10 02:58:58 - INFO - Batch 411/1300 - Loss: 0.4750 - Avg batch time: 0.15s\n",
      "2025-06-10 02:58:59 - INFO - Processing batch 421/1300\n",
      "2025-06-10 02:59:00 - INFO - Batch 421/1300 - Loss: 0.6779 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:01 - INFO - Processing batch 431/1300\n",
      "2025-06-10 02:59:01 - INFO - Batch 431/1300 - Loss: 0.4987 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:03 - INFO - Processing batch 441/1300\n",
      "2025-06-10 02:59:03 - INFO - Batch 441/1300 - Loss: 0.6366 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:04 - INFO - Processing batch 451/1300\n",
      "2025-06-10 02:59:04 - INFO - Batch 451/1300 - Loss: 0.4904 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:06 - INFO - Processing batch 461/1300\n",
      "2025-06-10 02:59:06 - INFO - Batch 461/1300 - Loss: 0.6537 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:07 - INFO - Processing batch 471/1300\n",
      "2025-06-10 02:59:07 - INFO - Batch 471/1300 - Loss: 0.7415 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:09 - INFO - Processing batch 481/1300\n",
      "2025-06-10 02:59:09 - INFO - Batch 481/1300 - Loss: 0.6908 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:10 - INFO - Processing batch 491/1300\n",
      "2025-06-10 02:59:10 - INFO - Batch 491/1300 - Loss: 0.7267 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:12 - INFO - Processing batch 501/1300\n",
      "2025-06-10 02:59:12 - INFO - Batch 501/1300 - Loss: 0.7551 - Avg batch time: 0.16s\n",
      "2025-06-10 02:59:13 - INFO - Processing batch 511/1300\n",
      "2025-06-10 02:59:13 - INFO - Batch 511/1300 - Loss: 0.5493 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:15 - INFO - Processing batch 521/1300\n",
      "2025-06-10 02:59:15 - INFO - Batch 521/1300 - Loss: 0.4851 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:16 - INFO - Processing batch 531/1300\n",
      "2025-06-10 02:59:17 - INFO - Batch 531/1300 - Loss: 0.9252 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:18 - INFO - Processing batch 541/1300\n",
      "2025-06-10 02:59:18 - INFO - Batch 541/1300 - Loss: 0.4596 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:19 - INFO - Processing batch 551/1300\n",
      "2025-06-10 02:59:20 - INFO - Batch 551/1300 - Loss: 0.5075 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:21 - INFO - Processing batch 561/1300\n",
      "2025-06-10 02:59:21 - INFO - Batch 561/1300 - Loss: 0.7741 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:23 - INFO - Processing batch 571/1300\n",
      "2025-06-10 02:59:23 - INFO - Batch 571/1300 - Loss: 1.0976 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:24 - INFO - Processing batch 581/1300\n",
      "2025-06-10 02:59:24 - INFO - Batch 581/1300 - Loss: 0.1434 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:26 - INFO - Processing batch 591/1300\n",
      "2025-06-10 02:59:26 - INFO - Batch 591/1300 - Loss: 1.0319 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:27 - INFO - Processing batch 601/1300\n",
      "2025-06-10 02:59:27 - INFO - Batch 601/1300 - Loss: 0.5163 - Avg batch time: 0.16s\n",
      "2025-06-10 02:59:29 - INFO - Processing batch 611/1300\n",
      "2025-06-10 02:59:29 - INFO - Batch 611/1300 - Loss: 0.2174 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:30 - INFO - Processing batch 621/1300\n",
      "2025-06-10 02:59:30 - INFO - Batch 621/1300 - Loss: 0.1348 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:32 - INFO - Processing batch 631/1300\n",
      "2025-06-10 02:59:32 - INFO - Batch 631/1300 - Loss: 0.4186 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:33 - INFO - Processing batch 641/1300\n",
      "2025-06-10 02:59:33 - INFO - Batch 641/1300 - Loss: 0.6160 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:35 - INFO - Processing batch 651/1300\n",
      "2025-06-10 02:59:35 - INFO - Batch 651/1300 - Loss: 0.3876 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:36 - INFO - Processing batch 661/1300\n",
      "2025-06-10 02:59:37 - INFO - Batch 661/1300 - Loss: 0.4960 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:38 - INFO - Processing batch 671/1300\n",
      "2025-06-10 02:59:38 - INFO - Batch 671/1300 - Loss: 0.7061 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:39 - INFO - Processing batch 681/1300\n",
      "2025-06-10 02:59:40 - INFO - Batch 681/1300 - Loss: 0.3451 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:41 - INFO - Processing batch 691/1300\n",
      "2025-06-10 02:59:41 - INFO - Batch 691/1300 - Loss: 0.4228 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:43 - INFO - Processing batch 701/1300\n",
      "2025-06-10 02:59:43 - INFO - Batch 701/1300 - Loss: 0.7985 - Avg batch time: 0.16s\n",
      "2025-06-10 02:59:44 - INFO - Processing batch 711/1300\n",
      "2025-06-10 02:59:44 - INFO - Batch 711/1300 - Loss: 0.5526 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:46 - INFO - Processing batch 721/1300\n",
      "2025-06-10 02:59:46 - INFO - Batch 721/1300 - Loss: 0.3721 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:47 - INFO - Processing batch 731/1300\n",
      "2025-06-10 02:59:47 - INFO - Batch 731/1300 - Loss: 0.2483 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:49 - INFO - Processing batch 741/1300\n",
      "2025-06-10 02:59:49 - INFO - Batch 741/1300 - Loss: 0.7193 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:50 - INFO - Processing batch 751/1300\n",
      "2025-06-10 02:59:50 - INFO - Batch 751/1300 - Loss: 0.2887 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:52 - INFO - Processing batch 761/1300\n",
      "2025-06-10 02:59:52 - INFO - Batch 761/1300 - Loss: 0.7469 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:53 - INFO - Processing batch 771/1300\n",
      "2025-06-10 02:59:53 - INFO - Batch 771/1300 - Loss: 0.3281 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:55 - INFO - Processing batch 781/1300\n",
      "2025-06-10 02:59:55 - INFO - Batch 781/1300 - Loss: 0.2343 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:56 - INFO - Processing batch 791/1300\n",
      "2025-06-10 02:59:57 - INFO - Batch 791/1300 - Loss: 0.5972 - Avg batch time: 0.15s\n",
      "2025-06-10 02:59:58 - INFO - Processing batch 801/1300\n",
      "2025-06-10 02:59:58 - INFO - Batch 801/1300 - Loss: 0.4454 - Avg batch time: 0.16s\n",
      "2025-06-10 02:59:59 - INFO - Processing batch 811/1300\n",
      "2025-06-10 03:00:00 - INFO - Batch 811/1300 - Loss: 0.5184 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:01 - INFO - Processing batch 821/1300\n",
      "2025-06-10 03:00:01 - INFO - Batch 821/1300 - Loss: 0.3833 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:03 - INFO - Processing batch 831/1300\n",
      "2025-06-10 03:00:03 - INFO - Batch 831/1300 - Loss: 0.5107 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:04 - INFO - Processing batch 841/1300\n",
      "2025-06-10 03:00:04 - INFO - Batch 841/1300 - Loss: 0.3251 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:06 - INFO - Processing batch 851/1300\n",
      "2025-06-10 03:00:06 - INFO - Batch 851/1300 - Loss: 0.6044 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:07 - INFO - Processing batch 861/1300\n",
      "2025-06-10 03:00:07 - INFO - Batch 861/1300 - Loss: 0.4318 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:09 - INFO - Processing batch 871/1300\n",
      "2025-06-10 03:00:09 - INFO - Batch 871/1300 - Loss: 0.1990 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:10 - INFO - Processing batch 881/1300\n",
      "2025-06-10 03:00:10 - INFO - Batch 881/1300 - Loss: 0.2022 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:12 - INFO - Processing batch 891/1300\n",
      "2025-06-10 03:00:12 - INFO - Batch 891/1300 - Loss: 0.6595 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:13 - INFO - Processing batch 901/1300\n",
      "2025-06-10 03:00:14 - INFO - Batch 901/1300 - Loss: 0.1578 - Avg batch time: 0.16s\n",
      "2025-06-10 03:00:15 - INFO - Processing batch 911/1300\n",
      "2025-06-10 03:00:15 - INFO - Batch 911/1300 - Loss: 0.2433 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:16 - INFO - Processing batch 921/1300\n",
      "2025-06-10 03:00:17 - INFO - Batch 921/1300 - Loss: 0.6362 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:18 - INFO - Processing batch 931/1300\n",
      "2025-06-10 03:00:18 - INFO - Batch 931/1300 - Loss: 0.1608 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:20 - INFO - Processing batch 941/1300\n",
      "2025-06-10 03:00:20 - INFO - Batch 941/1300 - Loss: 0.1451 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:21 - INFO - Processing batch 951/1300\n",
      "2025-06-10 03:00:21 - INFO - Batch 951/1300 - Loss: 0.5893 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:23 - INFO - Processing batch 961/1300\n",
      "2025-06-10 03:00:23 - INFO - Batch 961/1300 - Loss: 1.0529 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:24 - INFO - Processing batch 971/1300\n",
      "2025-06-10 03:00:24 - INFO - Batch 971/1300 - Loss: 0.3125 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:26 - INFO - Processing batch 981/1300\n",
      "2025-06-10 03:00:26 - INFO - Batch 981/1300 - Loss: 0.1511 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:27 - INFO - Processing batch 991/1300\n",
      "2025-06-10 03:00:27 - INFO - Batch 991/1300 - Loss: 0.1452 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:29 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 03:00:29 - INFO - Batch 1001/1300 - Loss: 0.4656 - Avg batch time: 0.16s\n",
      "2025-06-10 03:00:30 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 03:00:30 - INFO - Batch 1011/1300 - Loss: 0.4999 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:32 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 03:00:32 - INFO - Batch 1021/1300 - Loss: 0.2396 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:33 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 03:00:34 - INFO - Batch 1031/1300 - Loss: 0.9901 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:35 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 03:00:35 - INFO - Batch 1041/1300 - Loss: 0.3895 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:36 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 03:00:37 - INFO - Batch 1051/1300 - Loss: 0.5205 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:38 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 03:00:38 - INFO - Batch 1061/1300 - Loss: 0.4435 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:39 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 03:00:40 - INFO - Batch 1071/1300 - Loss: 0.4910 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:41 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 03:00:41 - INFO - Batch 1081/1300 - Loss: 0.4475 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:43 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 03:00:43 - INFO - Batch 1091/1300 - Loss: 0.1509 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:44 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 03:00:44 - INFO - Batch 1101/1300 - Loss: 0.0855 - Avg batch time: 0.16s\n",
      "2025-06-10 03:00:46 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 03:00:46 - INFO - Batch 1111/1300 - Loss: 0.2836 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:47 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 03:00:47 - INFO - Batch 1121/1300 - Loss: 0.4811 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:49 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 03:00:49 - INFO - Batch 1131/1300 - Loss: 0.1685 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:50 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 03:00:50 - INFO - Batch 1141/1300 - Loss: 0.4540 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:52 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 03:00:52 - INFO - Batch 1151/1300 - Loss: 0.3032 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:53 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 03:00:54 - INFO - Batch 1161/1300 - Loss: 0.3332 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:55 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 03:00:55 - INFO - Batch 1171/1300 - Loss: 0.6494 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:56 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 03:00:57 - INFO - Batch 1181/1300 - Loss: 0.1825 - Avg batch time: 0.15s\n",
      "2025-06-10 03:00:58 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 03:00:58 - INFO - Batch 1191/1300 - Loss: 0.6694 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:00 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 03:01:00 - INFO - Batch 1201/1300 - Loss: 0.1872 - Avg batch time: 0.16s\n",
      "2025-06-10 03:01:01 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 03:01:01 - INFO - Batch 1211/1300 - Loss: 0.8329 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:03 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 03:01:03 - INFO - Batch 1221/1300 - Loss: 0.8570 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:04 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 03:01:04 - INFO - Batch 1231/1300 - Loss: 0.6794 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:06 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 03:01:06 - INFO - Batch 1241/1300 - Loss: 0.2749 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:07 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 03:01:07 - INFO - Batch 1251/1300 - Loss: 0.4404 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:09 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 03:01:09 - INFO - Batch 1261/1300 - Loss: 0.0836 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:10 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 03:01:10 - INFO - Batch 1271/1300 - Loss: 0.6089 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:12 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 03:01:12 - INFO - Batch 1281/1300 - Loss: 0.4141 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:13 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 03:01:14 - INFO - Batch 1291/1300 - Loss: 0.3643 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:15 - INFO - \n",
      "Epoch 5 training completed in 200.45s\n",
      "2025-06-10 03:01:15 - INFO - Average training loss: 0.4687\n",
      "2025-06-10 03:01:33 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:   6%| | 6/100 [18:10<5:41:48, 218.18s/it, train_loss=0.4687, val_loss=0.4582, best_val_f1=0.3125, lr=1.00e-04, b2025-06-10 03:01:33 - INFO - \n",
      "Epoch 6/100 - Training phase\n",
      "2025-06-10 03:01:33 - INFO - Processing batch 1/1300\n",
      "2025-06-10 03:01:33 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 03:01:33 - INFO - Batch 1/1300 - Loss: 0.2623 - Avg batch time: 0.16s\n",
      "2025-06-10 03:01:34 - INFO - Processing batch 11/1300\n",
      "2025-06-10 03:01:35 - INFO - Batch 11/1300 - Loss: 0.5035 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:36 - INFO - Processing batch 21/1300\n",
      "2025-06-10 03:01:36 - INFO - Batch 21/1300 - Loss: 0.3916 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:38 - INFO - Processing batch 31/1300\n",
      "2025-06-10 03:01:38 - INFO - Batch 31/1300 - Loss: 0.3646 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:39 - INFO - Processing batch 41/1300\n",
      "2025-06-10 03:01:39 - INFO - Batch 41/1300 - Loss: 0.7207 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:41 - INFO - Processing batch 51/1300\n",
      "2025-06-10 03:01:41 - INFO - Batch 51/1300 - Loss: 0.4979 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:42 - INFO - Processing batch 61/1300\n",
      "2025-06-10 03:01:42 - INFO - Batch 61/1300 - Loss: 0.4352 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:44 - INFO - Processing batch 71/1300\n",
      "2025-06-10 03:01:44 - INFO - Batch 71/1300 - Loss: 0.5682 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:45 - INFO - Processing batch 81/1300\n",
      "2025-06-10 03:01:45 - INFO - Batch 81/1300 - Loss: 0.5923 - Avg batch time: 0.16s\n",
      "2025-06-10 03:01:47 - INFO - Processing batch 91/1300\n",
      "2025-06-10 03:01:47 - INFO - Batch 91/1300 - Loss: 0.2739 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:48 - INFO - Processing batch 101/1300\n",
      "2025-06-10 03:01:49 - INFO - Batch 101/1300 - Loss: 0.3026 - Avg batch time: 0.16s\n",
      "2025-06-10 03:01:50 - INFO - Processing batch 111/1300\n",
      "2025-06-10 03:01:50 - INFO - Batch 111/1300 - Loss: 0.5486 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:51 - INFO - Processing batch 121/1300\n",
      "2025-06-10 03:01:52 - INFO - Batch 121/1300 - Loss: 0.1886 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:53 - INFO - Processing batch 131/1300\n",
      "2025-06-10 03:01:53 - INFO - Batch 131/1300 - Loss: 0.3521 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:55 - INFO - Processing batch 141/1300\n",
      "2025-06-10 03:01:55 - INFO - Batch 141/1300 - Loss: 0.3359 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:56 - INFO - Processing batch 151/1300\n",
      "2025-06-10 03:01:56 - INFO - Batch 151/1300 - Loss: 0.2565 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:58 - INFO - Processing batch 161/1300\n",
      "2025-06-10 03:01:58 - INFO - Batch 161/1300 - Loss: 0.1671 - Avg batch time: 0.15s\n",
      "2025-06-10 03:01:59 - INFO - Processing batch 171/1300\n",
      "2025-06-10 03:01:59 - INFO - Batch 171/1300 - Loss: 0.9677 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:01 - INFO - Processing batch 181/1300\n",
      "2025-06-10 03:02:01 - INFO - Batch 181/1300 - Loss: 0.4988 - Avg batch time: 0.16s\n",
      "2025-06-10 03:02:02 - INFO - Processing batch 191/1300\n",
      "2025-06-10 03:02:02 - INFO - Batch 191/1300 - Loss: 0.5963 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:04 - INFO - Processing batch 201/1300\n",
      "2025-06-10 03:02:04 - INFO - Batch 201/1300 - Loss: 0.4013 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:05 - INFO - Processing batch 211/1300\n",
      "2025-06-10 03:02:05 - INFO - Batch 211/1300 - Loss: 0.9783 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:07 - INFO - Processing batch 221/1300\n",
      "2025-06-10 03:02:07 - INFO - Batch 221/1300 - Loss: 0.8894 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:08 - INFO - Processing batch 231/1300\n",
      "2025-06-10 03:02:09 - INFO - Batch 231/1300 - Loss: 0.3418 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:10 - INFO - Processing batch 241/1300\n",
      "2025-06-10 03:02:10 - INFO - Batch 241/1300 - Loss: 0.1542 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:11 - INFO - Processing batch 251/1300\n",
      "2025-06-10 03:02:12 - INFO - Batch 251/1300 - Loss: 0.1053 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:13 - INFO - Processing batch 261/1300\n",
      "2025-06-10 03:02:13 - INFO - Batch 261/1300 - Loss: 0.2115 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:14 - INFO - Processing batch 271/1300\n",
      "2025-06-10 03:02:15 - INFO - Batch 271/1300 - Loss: 0.3989 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:16 - INFO - Processing batch 281/1300\n",
      "2025-06-10 03:02:16 - INFO - Batch 281/1300 - Loss: 0.8175 - Avg batch time: 0.16s\n",
      "2025-06-10 03:02:18 - INFO - Processing batch 291/1300\n",
      "2025-06-10 03:02:18 - INFO - Batch 291/1300 - Loss: 0.5215 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:19 - INFO - Processing batch 301/1300\n",
      "2025-06-10 03:02:19 - INFO - Batch 301/1300 - Loss: 0.4360 - Avg batch time: 0.16s\n",
      "2025-06-10 03:02:21 - INFO - Processing batch 311/1300\n",
      "2025-06-10 03:02:21 - INFO - Batch 311/1300 - Loss: 0.1928 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:22 - INFO - Processing batch 321/1300\n",
      "2025-06-10 03:02:22 - INFO - Batch 321/1300 - Loss: 0.3499 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:24 - INFO - Processing batch 331/1300\n",
      "2025-06-10 03:02:24 - INFO - Batch 331/1300 - Loss: 0.5240 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:25 - INFO - Processing batch 341/1300\n",
      "2025-06-10 03:02:25 - INFO - Batch 341/1300 - Loss: 0.2382 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:27 - INFO - Processing batch 351/1300\n",
      "2025-06-10 03:02:27 - INFO - Batch 351/1300 - Loss: 0.4978 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:28 - INFO - Processing batch 361/1300\n",
      "2025-06-10 03:02:29 - INFO - Batch 361/1300 - Loss: 0.4222 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:30 - INFO - Processing batch 371/1300\n",
      "2025-06-10 03:02:30 - INFO - Batch 371/1300 - Loss: 0.5215 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:31 - INFO - Processing batch 381/1300\n",
      "2025-06-10 03:02:32 - INFO - Batch 381/1300 - Loss: 0.3101 - Avg batch time: 0.16s\n",
      "2025-06-10 03:02:33 - INFO - Processing batch 391/1300\n",
      "2025-06-10 03:02:33 - INFO - Batch 391/1300 - Loss: 0.4344 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:35 - INFO - Processing batch 401/1300\n",
      "2025-06-10 03:02:35 - INFO - Batch 401/1300 - Loss: 0.2307 - Avg batch time: 0.16s\n",
      "2025-06-10 03:02:36 - INFO - Processing batch 411/1300\n",
      "2025-06-10 03:02:36 - INFO - Batch 411/1300 - Loss: 0.9416 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:38 - INFO - Processing batch 421/1300\n",
      "2025-06-10 03:02:38 - INFO - Batch 421/1300 - Loss: 0.3022 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:39 - INFO - Processing batch 431/1300\n",
      "2025-06-10 03:02:39 - INFO - Batch 431/1300 - Loss: 0.3413 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:41 - INFO - Processing batch 441/1300\n",
      "2025-06-10 03:02:41 - INFO - Batch 441/1300 - Loss: 0.5216 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:42 - INFO - Processing batch 451/1300\n",
      "2025-06-10 03:02:42 - INFO - Batch 451/1300 - Loss: 0.4950 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:44 - INFO - Processing batch 461/1300\n",
      "2025-06-10 03:02:44 - INFO - Batch 461/1300 - Loss: 0.7923 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:45 - INFO - Processing batch 471/1300\n",
      "2025-06-10 03:02:45 - INFO - Batch 471/1300 - Loss: 0.1478 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:47 - INFO - Processing batch 481/1300\n",
      "2025-06-10 03:02:47 - INFO - Batch 481/1300 - Loss: 0.4616 - Avg batch time: 0.16s\n",
      "2025-06-10 03:02:48 - INFO - Processing batch 491/1300\n",
      "2025-06-10 03:02:49 - INFO - Batch 491/1300 - Loss: 0.4595 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:50 - INFO - Processing batch 501/1300\n",
      "2025-06-10 03:02:50 - INFO - Batch 501/1300 - Loss: 0.4904 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:51 - INFO - Processing batch 511/1300\n",
      "2025-06-10 03:02:52 - INFO - Batch 511/1300 - Loss: 0.2041 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:53 - INFO - Processing batch 521/1300\n",
      "2025-06-10 03:02:53 - INFO - Batch 521/1300 - Loss: 0.2801 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:55 - INFO - Processing batch 531/1300\n",
      "2025-06-10 03:02:55 - INFO - Batch 531/1300 - Loss: 0.4560 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:56 - INFO - Processing batch 541/1300\n",
      "2025-06-10 03:02:56 - INFO - Batch 541/1300 - Loss: 0.3684 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:58 - INFO - Processing batch 551/1300\n",
      "2025-06-10 03:02:58 - INFO - Batch 551/1300 - Loss: 0.7026 - Avg batch time: 0.15s\n",
      "2025-06-10 03:02:59 - INFO - Processing batch 561/1300\n",
      "2025-06-10 03:02:59 - INFO - Batch 561/1300 - Loss: 1.4413 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:01 - INFO - Processing batch 571/1300\n",
      "2025-06-10 03:03:01 - INFO - Batch 571/1300 - Loss: 0.9880 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:02 - INFO - Processing batch 581/1300\n",
      "2025-06-10 03:03:02 - INFO - Batch 581/1300 - Loss: 0.1960 - Avg batch time: 0.16s\n",
      "2025-06-10 03:03:04 - INFO - Processing batch 591/1300\n",
      "2025-06-10 03:03:04 - INFO - Batch 591/1300 - Loss: 0.2902 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:05 - INFO - Processing batch 601/1300\n",
      "2025-06-10 03:03:05 - INFO - Batch 601/1300 - Loss: 0.6135 - Avg batch time: 0.16s\n",
      "2025-06-10 03:03:07 - INFO - Processing batch 611/1300\n",
      "2025-06-10 03:03:07 - INFO - Batch 611/1300 - Loss: 0.3450 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:08 - INFO - Processing batch 621/1300\n",
      "2025-06-10 03:03:09 - INFO - Batch 621/1300 - Loss: 0.3584 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:10 - INFO - Processing batch 631/1300\n",
      "2025-06-10 03:03:10 - INFO - Batch 631/1300 - Loss: 0.4130 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:11 - INFO - Processing batch 641/1300\n",
      "2025-06-10 03:03:12 - INFO - Batch 641/1300 - Loss: 0.5254 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:13 - INFO - Processing batch 651/1300\n",
      "2025-06-10 03:03:13 - INFO - Batch 651/1300 - Loss: 0.6198 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:15 - INFO - Processing batch 661/1300\n",
      "2025-06-10 03:03:15 - INFO - Batch 661/1300 - Loss: 0.2316 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:16 - INFO - Processing batch 671/1300\n",
      "2025-06-10 03:03:16 - INFO - Batch 671/1300 - Loss: 0.3303 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:18 - INFO - Processing batch 681/1300\n",
      "2025-06-10 03:03:18 - INFO - Batch 681/1300 - Loss: 0.7612 - Avg batch time: 0.16s\n",
      "2025-06-10 03:03:19 - INFO - Processing batch 691/1300\n",
      "2025-06-10 03:03:19 - INFO - Batch 691/1300 - Loss: 0.6624 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:21 - INFO - Processing batch 701/1300\n",
      "2025-06-10 03:03:21 - INFO - Batch 701/1300 - Loss: 0.2268 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:22 - INFO - Processing batch 711/1300\n",
      "2025-06-10 03:03:22 - INFO - Batch 711/1300 - Loss: 0.3481 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:24 - INFO - Processing batch 721/1300\n",
      "2025-06-10 03:03:24 - INFO - Batch 721/1300 - Loss: 0.5117 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:25 - INFO - Processing batch 731/1300\n",
      "2025-06-10 03:03:25 - INFO - Batch 731/1300 - Loss: 0.4113 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:27 - INFO - Processing batch 741/1300\n",
      "2025-06-10 03:03:27 - INFO - Batch 741/1300 - Loss: 0.8888 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:28 - INFO - Processing batch 751/1300\n",
      "2025-06-10 03:03:28 - INFO - Batch 751/1300 - Loss: 0.2626 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:30 - INFO - Processing batch 761/1300\n",
      "2025-06-10 03:03:30 - INFO - Batch 761/1300 - Loss: 0.1579 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:31 - INFO - Processing batch 771/1300\n",
      "2025-06-10 03:03:32 - INFO - Batch 771/1300 - Loss: 0.4934 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:33 - INFO - Processing batch 781/1300\n",
      "2025-06-10 03:03:33 - INFO - Batch 781/1300 - Loss: 0.6755 - Avg batch time: 0.16s\n",
      "2025-06-10 03:03:34 - INFO - Processing batch 791/1300\n",
      "2025-06-10 03:03:35 - INFO - Batch 791/1300 - Loss: 0.0882 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:36 - INFO - Processing batch 801/1300\n",
      "2025-06-10 03:03:36 - INFO - Batch 801/1300 - Loss: 0.2525 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:38 - INFO - Processing batch 811/1300\n",
      "2025-06-10 03:03:38 - INFO - Batch 811/1300 - Loss: 0.7599 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:39 - INFO - Processing batch 821/1300\n",
      "2025-06-10 03:03:39 - INFO - Batch 821/1300 - Loss: 1.0066 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:41 - INFO - Processing batch 831/1300\n",
      "2025-06-10 03:03:41 - INFO - Batch 831/1300 - Loss: 0.4074 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:42 - INFO - Processing batch 841/1300\n",
      "2025-06-10 03:03:42 - INFO - Batch 841/1300 - Loss: 0.2219 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:44 - INFO - Processing batch 851/1300\n",
      "2025-06-10 03:03:44 - INFO - Batch 851/1300 - Loss: 0.2136 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:45 - INFO - Processing batch 861/1300\n",
      "2025-06-10 03:03:45 - INFO - Batch 861/1300 - Loss: 0.4214 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:47 - INFO - Processing batch 871/1300\n",
      "2025-06-10 03:03:47 - INFO - Batch 871/1300 - Loss: 1.3331 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:48 - INFO - Processing batch 881/1300\n",
      "2025-06-10 03:03:48 - INFO - Batch 881/1300 - Loss: 0.1408 - Avg batch time: 0.16s\n",
      "2025-06-10 03:03:50 - INFO - Processing batch 891/1300\n",
      "2025-06-10 03:03:50 - INFO - Batch 891/1300 - Loss: 0.4881 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:51 - INFO - Processing batch 901/1300\n",
      "2025-06-10 03:03:52 - INFO - Batch 901/1300 - Loss: 0.3960 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:53 - INFO - Processing batch 911/1300\n",
      "2025-06-10 03:03:53 - INFO - Batch 911/1300 - Loss: 0.6317 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:54 - INFO - Processing batch 921/1300\n",
      "2025-06-10 03:03:55 - INFO - Batch 921/1300 - Loss: 0.3867 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:56 - INFO - Processing batch 931/1300\n",
      "2025-06-10 03:03:56 - INFO - Batch 931/1300 - Loss: 0.3690 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:58 - INFO - Processing batch 941/1300\n",
      "2025-06-10 03:03:58 - INFO - Batch 941/1300 - Loss: 0.1393 - Avg batch time: 0.15s\n",
      "2025-06-10 03:03:59 - INFO - Processing batch 951/1300\n",
      "2025-06-10 03:03:59 - INFO - Batch 951/1300 - Loss: 0.1017 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:01 - INFO - Processing batch 961/1300\n",
      "2025-06-10 03:04:01 - INFO - Batch 961/1300 - Loss: 0.8486 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:02 - INFO - Processing batch 971/1300\n",
      "2025-06-10 03:04:02 - INFO - Batch 971/1300 - Loss: 0.3726 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:04 - INFO - Processing batch 981/1300\n",
      "2025-06-10 03:04:04 - INFO - Batch 981/1300 - Loss: 0.2823 - Avg batch time: 0.16s\n",
      "2025-06-10 03:04:05 - INFO - Processing batch 991/1300\n",
      "2025-06-10 03:04:05 - INFO - Batch 991/1300 - Loss: 0.7268 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:07 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 03:04:07 - INFO - Batch 1001/1300 - Loss: 0.6859 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:08 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 03:04:08 - INFO - Batch 1011/1300 - Loss: 0.7318 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:10 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 03:04:10 - INFO - Batch 1021/1300 - Loss: 0.6907 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:11 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 03:04:12 - INFO - Batch 1031/1300 - Loss: 0.6006 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:13 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 03:04:13 - INFO - Batch 1041/1300 - Loss: 0.2937 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:14 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 03:04:15 - INFO - Batch 1051/1300 - Loss: 0.4190 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:16 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 03:04:16 - INFO - Batch 1061/1300 - Loss: 0.6129 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:18 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 03:04:18 - INFO - Batch 1071/1300 - Loss: 0.2386 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:19 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 03:04:19 - INFO - Batch 1081/1300 - Loss: 0.3458 - Avg batch time: 0.16s\n",
      "2025-06-10 03:04:21 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 03:04:21 - INFO - Batch 1091/1300 - Loss: 0.8991 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:22 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 03:04:22 - INFO - Batch 1101/1300 - Loss: 0.4503 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:24 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 03:04:24 - INFO - Batch 1111/1300 - Loss: 0.4253 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:25 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 03:04:25 - INFO - Batch 1121/1300 - Loss: 0.9084 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:27 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 03:04:27 - INFO - Batch 1131/1300 - Loss: 0.3140 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:28 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 03:04:28 - INFO - Batch 1141/1300 - Loss: 0.2429 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:30 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 03:04:30 - INFO - Batch 1151/1300 - Loss: 0.3515 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:31 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 03:04:32 - INFO - Batch 1161/1300 - Loss: 0.2476 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:33 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 03:04:33 - INFO - Batch 1171/1300 - Loss: 0.2340 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:34 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 03:04:35 - INFO - Batch 1181/1300 - Loss: 0.9783 - Avg batch time: 0.16s\n",
      "2025-06-10 03:04:36 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 03:04:36 - INFO - Batch 1191/1300 - Loss: 0.5194 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:38 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 03:04:38 - INFO - Batch 1201/1300 - Loss: 0.5829 - Avg batch time: 0.16s\n",
      "2025-06-10 03:04:39 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 03:04:39 - INFO - Batch 1211/1300 - Loss: 0.9027 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:41 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 03:04:41 - INFO - Batch 1221/1300 - Loss: 0.4456 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:42 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 03:04:42 - INFO - Batch 1231/1300 - Loss: 0.5269 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:44 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 03:04:44 - INFO - Batch 1241/1300 - Loss: 0.5205 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:45 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 03:04:45 - INFO - Batch 1251/1300 - Loss: 0.1373 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:47 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 03:04:47 - INFO - Batch 1261/1300 - Loss: 0.1813 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:48 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 03:04:48 - INFO - Batch 1271/1300 - Loss: 0.2063 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:50 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 03:04:50 - INFO - Batch 1281/1300 - Loss: 1.1323 - Avg batch time: 0.16s\n",
      "2025-06-10 03:04:51 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 03:04:51 - INFO - Batch 1291/1300 - Loss: 0.5845 - Avg batch time: 0.15s\n",
      "2025-06-10 03:04:53 - INFO - \n",
      "Epoch 6 training completed in 200.24s\n",
      "2025-06-10 03:04:53 - INFO - Average training loss: 0.4681\n",
      "2025-06-10 03:05:11 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:   7%| | 7/100 [21:48<5:38:03, 218.11s/it, train_loss=0.4681, val_loss=0.4568, best_val_f1=0.3125, lr=1.00e-04, b2025-06-10 03:05:11 - INFO - \n",
      "Epoch 7/100 - Training phase\n",
      "2025-06-10 03:05:11 - INFO - Processing batch 1/1300\n",
      "2025-06-10 03:05:11 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 03:05:11 - INFO - Batch 1/1300 - Loss: 0.6495 - Avg batch time: 0.16s\n",
      "2025-06-10 03:05:12 - INFO - Processing batch 11/1300\n",
      "2025-06-10 03:05:13 - INFO - Batch 11/1300 - Loss: 0.6358 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:14 - INFO - Processing batch 21/1300\n",
      "2025-06-10 03:05:14 - INFO - Batch 21/1300 - Loss: 0.4650 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:15 - INFO - Processing batch 31/1300\n",
      "2025-06-10 03:05:16 - INFO - Batch 31/1300 - Loss: 0.5851 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:17 - INFO - Processing batch 41/1300\n",
      "2025-06-10 03:05:17 - INFO - Batch 41/1300 - Loss: 0.6108 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:19 - INFO - Processing batch 51/1300\n",
      "2025-06-10 03:05:19 - INFO - Batch 51/1300 - Loss: 0.3992 - Avg batch time: 0.16s\n",
      "2025-06-10 03:05:20 - INFO - Processing batch 61/1300\n",
      "2025-06-10 03:05:20 - INFO - Batch 61/1300 - Loss: 0.4183 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:22 - INFO - Processing batch 71/1300\n",
      "2025-06-10 03:05:22 - INFO - Batch 71/1300 - Loss: 0.3902 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:23 - INFO - Processing batch 81/1300\n",
      "2025-06-10 03:05:23 - INFO - Batch 81/1300 - Loss: 0.4023 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:25 - INFO - Processing batch 91/1300\n",
      "2025-06-10 03:05:25 - INFO - Batch 91/1300 - Loss: 0.2627 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:26 - INFO - Processing batch 101/1300\n",
      "2025-06-10 03:05:26 - INFO - Batch 101/1300 - Loss: 0.3930 - Avg batch time: 0.16s\n",
      "2025-06-10 03:05:28 - INFO - Processing batch 111/1300\n",
      "2025-06-10 03:05:28 - INFO - Batch 111/1300 - Loss: 0.2242 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:29 - INFO - Processing batch 121/1300\n",
      "2025-06-10 03:05:30 - INFO - Batch 121/1300 - Loss: 0.8474 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:31 - INFO - Processing batch 131/1300\n",
      "2025-06-10 03:05:31 - INFO - Batch 131/1300 - Loss: 0.5299 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:32 - INFO - Processing batch 141/1300\n",
      "2025-06-10 03:05:33 - INFO - Batch 141/1300 - Loss: 0.4898 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:34 - INFO - Processing batch 151/1300\n",
      "2025-06-10 03:05:34 - INFO - Batch 151/1300 - Loss: 0.2660 - Avg batch time: 0.16s\n",
      "2025-06-10 03:05:36 - INFO - Processing batch 161/1300\n",
      "2025-06-10 03:05:36 - INFO - Batch 161/1300 - Loss: 0.6576 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:37 - INFO - Processing batch 171/1300\n",
      "2025-06-10 03:05:37 - INFO - Batch 171/1300 - Loss: 0.1734 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:39 - INFO - Processing batch 181/1300\n",
      "2025-06-10 03:05:39 - INFO - Batch 181/1300 - Loss: 0.8172 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:40 - INFO - Processing batch 191/1300\n",
      "2025-06-10 03:05:40 - INFO - Batch 191/1300 - Loss: 0.2337 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:42 - INFO - Processing batch 201/1300\n",
      "2025-06-10 03:05:42 - INFO - Batch 201/1300 - Loss: 0.1536 - Avg batch time: 0.16s\n",
      "2025-06-10 03:05:43 - INFO - Processing batch 211/1300\n",
      "2025-06-10 03:05:43 - INFO - Batch 211/1300 - Loss: 0.9481 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:45 - INFO - Processing batch 221/1300\n",
      "2025-06-10 03:05:45 - INFO - Batch 221/1300 - Loss: 0.7940 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:46 - INFO - Processing batch 231/1300\n",
      "2025-06-10 03:05:46 - INFO - Batch 231/1300 - Loss: 0.3270 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:48 - INFO - Processing batch 241/1300\n",
      "2025-06-10 03:05:48 - INFO - Batch 241/1300 - Loss: 0.9465 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:49 - INFO - Processing batch 251/1300\n",
      "2025-06-10 03:05:50 - INFO - Batch 251/1300 - Loss: 0.2799 - Avg batch time: 0.16s\n",
      "2025-06-10 03:05:51 - INFO - Processing batch 261/1300\n",
      "2025-06-10 03:05:51 - INFO - Batch 261/1300 - Loss: 0.4241 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:52 - INFO - Processing batch 271/1300\n",
      "2025-06-10 03:05:53 - INFO - Batch 271/1300 - Loss: 0.1667 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:54 - INFO - Processing batch 281/1300\n",
      "2025-06-10 03:05:54 - INFO - Batch 281/1300 - Loss: 0.1767 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:56 - INFO - Processing batch 291/1300\n",
      "2025-06-10 03:05:56 - INFO - Batch 291/1300 - Loss: 0.3257 - Avg batch time: 0.15s\n",
      "2025-06-10 03:05:57 - INFO - Processing batch 301/1300\n",
      "2025-06-10 03:05:57 - INFO - Batch 301/1300 - Loss: 0.2228 - Avg batch time: 0.16s\n",
      "2025-06-10 03:05:59 - INFO - Processing batch 311/1300\n",
      "2025-06-10 03:05:59 - INFO - Batch 311/1300 - Loss: 0.4630 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:00 - INFO - Processing batch 321/1300\n",
      "2025-06-10 03:06:00 - INFO - Batch 321/1300 - Loss: 0.6032 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:02 - INFO - Processing batch 331/1300\n",
      "2025-06-10 03:06:02 - INFO - Batch 331/1300 - Loss: 0.7368 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:03 - INFO - Processing batch 341/1300\n",
      "2025-06-10 03:06:03 - INFO - Batch 341/1300 - Loss: 0.4433 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:05 - INFO - Processing batch 351/1300\n",
      "2025-06-10 03:06:05 - INFO - Batch 351/1300 - Loss: 0.4043 - Avg batch time: 0.16s\n",
      "2025-06-10 03:06:06 - INFO - Processing batch 361/1300\n",
      "2025-06-10 03:06:07 - INFO - Batch 361/1300 - Loss: 0.4952 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:08 - INFO - Processing batch 371/1300\n",
      "2025-06-10 03:06:08 - INFO - Batch 371/1300 - Loss: 0.5955 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:09 - INFO - Processing batch 381/1300\n",
      "2025-06-10 03:06:10 - INFO - Batch 381/1300 - Loss: 0.6842 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:11 - INFO - Processing batch 391/1300\n",
      "2025-06-10 03:06:11 - INFO - Batch 391/1300 - Loss: 0.3564 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:13 - INFO - Processing batch 401/1300\n",
      "2025-06-10 03:06:13 - INFO - Batch 401/1300 - Loss: 0.2442 - Avg batch time: 0.16s\n",
      "2025-06-10 03:06:14 - INFO - Processing batch 411/1300\n",
      "2025-06-10 03:06:14 - INFO - Batch 411/1300 - Loss: 0.1334 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:16 - INFO - Processing batch 421/1300\n",
      "2025-06-10 03:06:16 - INFO - Batch 421/1300 - Loss: 0.5944 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:17 - INFO - Processing batch 431/1300\n",
      "2025-06-10 03:06:17 - INFO - Batch 431/1300 - Loss: 0.4606 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:19 - INFO - Processing batch 441/1300\n",
      "2025-06-10 03:06:19 - INFO - Batch 441/1300 - Loss: 0.3678 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:20 - INFO - Processing batch 451/1300\n",
      "2025-06-10 03:06:20 - INFO - Batch 451/1300 - Loss: 0.6427 - Avg batch time: 0.16s\n",
      "2025-06-10 03:06:22 - INFO - Processing batch 461/1300\n",
      "2025-06-10 03:06:22 - INFO - Batch 461/1300 - Loss: 0.7217 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:23 - INFO - Processing batch 471/1300\n",
      "2025-06-10 03:06:23 - INFO - Batch 471/1300 - Loss: 0.1182 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:25 - INFO - Processing batch 481/1300\n",
      "2025-06-10 03:06:25 - INFO - Batch 481/1300 - Loss: 0.8500 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:26 - INFO - Processing batch 491/1300\n",
      "2025-06-10 03:06:27 - INFO - Batch 491/1300 - Loss: 0.6809 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:28 - INFO - Processing batch 501/1300\n",
      "2025-06-10 03:06:28 - INFO - Batch 501/1300 - Loss: 0.5296 - Avg batch time: 0.16s\n",
      "2025-06-10 03:06:29 - INFO - Processing batch 511/1300\n",
      "2025-06-10 03:06:30 - INFO - Batch 511/1300 - Loss: 0.6674 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:31 - INFO - Processing batch 521/1300\n",
      "2025-06-10 03:06:31 - INFO - Batch 521/1300 - Loss: 0.4592 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:33 - INFO - Processing batch 531/1300\n",
      "2025-06-10 03:06:33 - INFO - Batch 531/1300 - Loss: 0.3513 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:34 - INFO - Processing batch 541/1300\n",
      "2025-06-10 03:06:34 - INFO - Batch 541/1300 - Loss: 0.4459 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:36 - INFO - Processing batch 551/1300\n",
      "2025-06-10 03:06:36 - INFO - Batch 551/1300 - Loss: 0.8410 - Avg batch time: 0.16s\n",
      "2025-06-10 03:06:37 - INFO - Processing batch 561/1300\n",
      "2025-06-10 03:06:37 - INFO - Batch 561/1300 - Loss: 0.5228 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:39 - INFO - Processing batch 571/1300\n",
      "2025-06-10 03:06:39 - INFO - Batch 571/1300 - Loss: 0.3995 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:40 - INFO - Processing batch 581/1300\n",
      "2025-06-10 03:06:40 - INFO - Batch 581/1300 - Loss: 0.9485 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:42 - INFO - Processing batch 591/1300\n",
      "2025-06-10 03:06:42 - INFO - Batch 591/1300 - Loss: 0.3556 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:43 - INFO - Processing batch 601/1300\n",
      "2025-06-10 03:06:43 - INFO - Batch 601/1300 - Loss: 0.6019 - Avg batch time: 0.16s\n",
      "2025-06-10 03:06:45 - INFO - Processing batch 611/1300\n",
      "2025-06-10 03:06:45 - INFO - Batch 611/1300 - Loss: 0.4143 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:46 - INFO - Processing batch 621/1300\n",
      "2025-06-10 03:06:47 - INFO - Batch 621/1300 - Loss: 0.3514 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:48 - INFO - Processing batch 631/1300\n",
      "2025-06-10 03:06:48 - INFO - Batch 631/1300 - Loss: 0.3641 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:49 - INFO - Processing batch 641/1300\n",
      "2025-06-10 03:06:50 - INFO - Batch 641/1300 - Loss: 0.4130 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:51 - INFO - Processing batch 651/1300\n",
      "2025-06-10 03:06:51 - INFO - Batch 651/1300 - Loss: 0.5205 - Avg batch time: 0.16s\n",
      "2025-06-10 03:06:53 - INFO - Processing batch 661/1300\n",
      "2025-06-10 03:06:53 - INFO - Batch 661/1300 - Loss: 0.2876 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:54 - INFO - Processing batch 671/1300\n",
      "2025-06-10 03:06:54 - INFO - Batch 671/1300 - Loss: 0.1591 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:56 - INFO - Processing batch 681/1300\n",
      "2025-06-10 03:06:56 - INFO - Batch 681/1300 - Loss: 0.2679 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:57 - INFO - Processing batch 691/1300\n",
      "2025-06-10 03:06:57 - INFO - Batch 691/1300 - Loss: 0.5556 - Avg batch time: 0.15s\n",
      "2025-06-10 03:06:59 - INFO - Processing batch 701/1300\n",
      "2025-06-10 03:06:59 - INFO - Batch 701/1300 - Loss: 0.1292 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:00 - INFO - Processing batch 711/1300\n",
      "2025-06-10 03:07:00 - INFO - Batch 711/1300 - Loss: 0.6395 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:02 - INFO - Processing batch 721/1300\n",
      "2025-06-10 03:07:02 - INFO - Batch 721/1300 - Loss: 0.3138 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:03 - INFO - Processing batch 731/1300\n",
      "2025-06-10 03:07:03 - INFO - Batch 731/1300 - Loss: 0.2351 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:05 - INFO - Processing batch 741/1300\n",
      "2025-06-10 03:07:05 - INFO - Batch 741/1300 - Loss: 0.3412 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:06 - INFO - Processing batch 751/1300\n",
      "2025-06-10 03:07:07 - INFO - Batch 751/1300 - Loss: 0.4576 - Avg batch time: 0.16s\n",
      "2025-06-10 03:07:08 - INFO - Processing batch 761/1300\n",
      "2025-06-10 03:07:08 - INFO - Batch 761/1300 - Loss: 0.5329 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:10 - INFO - Processing batch 771/1300\n",
      "2025-06-10 03:07:10 - INFO - Batch 771/1300 - Loss: 0.1616 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:11 - INFO - Processing batch 781/1300\n",
      "2025-06-10 03:07:11 - INFO - Batch 781/1300 - Loss: 0.2630 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:13 - INFO - Processing batch 791/1300\n",
      "2025-06-10 03:07:13 - INFO - Batch 791/1300 - Loss: 0.3938 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:14 - INFO - Processing batch 801/1300\n",
      "2025-06-10 03:07:14 - INFO - Batch 801/1300 - Loss: 0.5413 - Avg batch time: 0.16s\n",
      "2025-06-10 03:07:16 - INFO - Processing batch 811/1300\n",
      "2025-06-10 03:07:16 - INFO - Batch 811/1300 - Loss: 0.8263 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:17 - INFO - Processing batch 821/1300\n",
      "2025-06-10 03:07:17 - INFO - Batch 821/1300 - Loss: 0.3799 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:19 - INFO - Processing batch 831/1300\n",
      "2025-06-10 03:07:19 - INFO - Batch 831/1300 - Loss: 0.3957 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:20 - INFO - Processing batch 841/1300\n",
      "2025-06-10 03:07:20 - INFO - Batch 841/1300 - Loss: 0.3853 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:22 - INFO - Processing batch 851/1300\n",
      "2025-06-10 03:07:22 - INFO - Batch 851/1300 - Loss: 0.2949 - Avg batch time: 0.16s\n",
      "2025-06-10 03:07:23 - INFO - Processing batch 861/1300\n",
      "2025-06-10 03:07:24 - INFO - Batch 861/1300 - Loss: 0.1346 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:25 - INFO - Processing batch 871/1300\n",
      "2025-06-10 03:07:25 - INFO - Batch 871/1300 - Loss: 0.3036 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:26 - INFO - Processing batch 881/1300\n",
      "2025-06-10 03:07:27 - INFO - Batch 881/1300 - Loss: 0.5269 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:28 - INFO - Processing batch 891/1300\n",
      "2025-06-10 03:07:28 - INFO - Batch 891/1300 - Loss: 0.4040 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:30 - INFO - Processing batch 901/1300\n",
      "2025-06-10 03:07:30 - INFO - Batch 901/1300 - Loss: 1.5389 - Avg batch time: 0.16s\n",
      "2025-06-10 03:07:31 - INFO - Processing batch 911/1300\n",
      "2025-06-10 03:07:31 - INFO - Batch 911/1300 - Loss: 0.8699 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:33 - INFO - Processing batch 921/1300\n",
      "2025-06-10 03:07:33 - INFO - Batch 921/1300 - Loss: 0.1594 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:34 - INFO - Processing batch 931/1300\n",
      "2025-06-10 03:07:34 - INFO - Batch 931/1300 - Loss: 0.2390 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:36 - INFO - Processing batch 941/1300\n",
      "2025-06-10 03:07:36 - INFO - Batch 941/1300 - Loss: 0.1876 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:37 - INFO - Processing batch 951/1300\n",
      "2025-06-10 03:07:37 - INFO - Batch 951/1300 - Loss: 0.4234 - Avg batch time: 0.16s\n",
      "2025-06-10 03:07:39 - INFO - Processing batch 961/1300\n",
      "2025-06-10 03:07:39 - INFO - Batch 961/1300 - Loss: 0.4784 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:40 - INFO - Processing batch 971/1300\n",
      "2025-06-10 03:07:40 - INFO - Batch 971/1300 - Loss: 0.2916 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:42 - INFO - Processing batch 981/1300\n",
      "2025-06-10 03:07:42 - INFO - Batch 981/1300 - Loss: 0.5037 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:43 - INFO - Processing batch 991/1300\n",
      "2025-06-10 03:07:44 - INFO - Batch 991/1300 - Loss: 0.6069 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:45 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 03:07:45 - INFO - Batch 1001/1300 - Loss: 0.3554 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:46 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 03:07:47 - INFO - Batch 1011/1300 - Loss: 0.6173 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:48 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 03:07:48 - INFO - Batch 1021/1300 - Loss: 0.1527 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:49 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 03:07:50 - INFO - Batch 1031/1300 - Loss: 0.2976 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:51 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 03:07:51 - INFO - Batch 1041/1300 - Loss: 0.2993 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:53 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 03:07:53 - INFO - Batch 1051/1300 - Loss: 0.0947 - Avg batch time: 0.16s\n",
      "2025-06-10 03:07:54 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 03:07:54 - INFO - Batch 1061/1300 - Loss: 0.9073 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:56 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 03:07:56 - INFO - Batch 1071/1300 - Loss: 0.1015 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:57 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 03:07:57 - INFO - Batch 1081/1300 - Loss: 0.4361 - Avg batch time: 0.15s\n",
      "2025-06-10 03:07:59 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 03:07:59 - INFO - Batch 1091/1300 - Loss: 0.4801 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:00 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 03:08:00 - INFO - Batch 1101/1300 - Loss: 0.1952 - Avg batch time: 0.16s\n",
      "2025-06-10 03:08:02 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 03:08:02 - INFO - Batch 1111/1300 - Loss: 0.7304 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:03 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 03:08:03 - INFO - Batch 1121/1300 - Loss: 0.4486 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:05 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 03:08:05 - INFO - Batch 1131/1300 - Loss: 0.4423 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:06 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 03:08:07 - INFO - Batch 1141/1300 - Loss: 0.3951 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:08 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 03:08:08 - INFO - Batch 1151/1300 - Loss: 0.4150 - Avg batch time: 0.16s\n",
      "2025-06-10 03:08:10 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 03:08:10 - INFO - Batch 1161/1300 - Loss: 0.4790 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:11 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 03:08:11 - INFO - Batch 1171/1300 - Loss: 0.6980 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:13 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 03:08:13 - INFO - Batch 1181/1300 - Loss: 1.0124 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:14 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 03:08:14 - INFO - Batch 1191/1300 - Loss: 0.6219 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:16 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 03:08:16 - INFO - Batch 1201/1300 - Loss: 0.3502 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:17 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 03:08:17 - INFO - Batch 1211/1300 - Loss: 0.1384 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:19 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 03:08:19 - INFO - Batch 1221/1300 - Loss: 0.6409 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:20 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 03:08:20 - INFO - Batch 1231/1300 - Loss: 0.5350 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:22 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 03:08:22 - INFO - Batch 1241/1300 - Loss: 0.2962 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:23 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 03:08:24 - INFO - Batch 1251/1300 - Loss: 0.3099 - Avg batch time: 0.16s\n",
      "2025-06-10 03:08:25 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 03:08:25 - INFO - Batch 1261/1300 - Loss: 0.7879 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:26 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 03:08:27 - INFO - Batch 1271/1300 - Loss: 0.9477 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:28 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 03:08:28 - INFO - Batch 1281/1300 - Loss: 0.3863 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:30 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 03:08:30 - INFO - Batch 1291/1300 - Loss: 0.7422 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:31 - INFO - \n",
      "Epoch 7 training completed in 200.42s\n",
      "2025-06-10 03:08:31 - INFO - Average training loss: 0.4713\n",
      "2025-06-10 03:08:49 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:   8%| | 8/100 [25:27<5:34:30, 218.16s/it, train_loss=0.4713, val_loss=0.4734, best_val_f1=0.3125, lr=5.00e-05, b2025-06-10 03:08:49 - INFO - \n",
      "Epoch 8/100 - Training phase\n",
      "2025-06-10 03:08:49 - INFO - Processing batch 1/1300\n",
      "2025-06-10 03:08:49 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 03:08:49 - INFO - Batch 1/1300 - Loss: 0.2535 - Avg batch time: 0.16s\n",
      "2025-06-10 03:08:51 - INFO - Processing batch 11/1300\n",
      "2025-06-10 03:08:51 - INFO - Batch 11/1300 - Loss: 0.4867 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:52 - INFO - Processing batch 21/1300\n",
      "2025-06-10 03:08:52 - INFO - Batch 21/1300 - Loss: 0.3988 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:54 - INFO - Processing batch 31/1300\n",
      "2025-06-10 03:08:54 - INFO - Batch 31/1300 - Loss: 1.3689 - Avg batch time: 0.16s\n",
      "2025-06-10 03:08:55 - INFO - Processing batch 41/1300\n",
      "2025-06-10 03:08:56 - INFO - Batch 41/1300 - Loss: 0.4316 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:57 - INFO - Processing batch 51/1300\n",
      "2025-06-10 03:08:57 - INFO - Batch 51/1300 - Loss: 0.1681 - Avg batch time: 0.15s\n",
      "2025-06-10 03:08:58 - INFO - Processing batch 61/1300\n",
      "2025-06-10 03:08:59 - INFO - Batch 61/1300 - Loss: 0.2796 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:00 - INFO - Processing batch 71/1300\n",
      "2025-06-10 03:09:00 - INFO - Batch 71/1300 - Loss: 0.3949 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:02 - INFO - Processing batch 81/1300\n",
      "2025-06-10 03:09:02 - INFO - Batch 81/1300 - Loss: 0.3579 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:03 - INFO - Processing batch 91/1300\n",
      "2025-06-10 03:09:03 - INFO - Batch 91/1300 - Loss: 0.9339 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:05 - INFO - Processing batch 101/1300\n",
      "2025-06-10 03:09:05 - INFO - Batch 101/1300 - Loss: 0.6722 - Avg batch time: 0.16s\n",
      "2025-06-10 03:09:06 - INFO - Processing batch 111/1300\n",
      "2025-06-10 03:09:06 - INFO - Batch 111/1300 - Loss: 0.9052 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:08 - INFO - Processing batch 121/1300\n",
      "2025-06-10 03:09:08 - INFO - Batch 121/1300 - Loss: 0.2505 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:09 - INFO - Processing batch 131/1300\n",
      "2025-06-10 03:09:09 - INFO - Batch 131/1300 - Loss: 0.3293 - Avg batch time: 0.16s\n",
      "2025-06-10 03:09:11 - INFO - Processing batch 141/1300\n",
      "2025-06-10 03:09:11 - INFO - Batch 141/1300 - Loss: 0.2439 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:12 - INFO - Processing batch 151/1300\n",
      "2025-06-10 03:09:13 - INFO - Batch 151/1300 - Loss: 0.7868 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:14 - INFO - Processing batch 161/1300\n",
      "2025-06-10 03:09:14 - INFO - Batch 161/1300 - Loss: 0.4359 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:15 - INFO - Processing batch 171/1300\n",
      "2025-06-10 03:09:16 - INFO - Batch 171/1300 - Loss: 0.7558 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:17 - INFO - Processing batch 181/1300\n",
      "2025-06-10 03:09:17 - INFO - Batch 181/1300 - Loss: 0.2768 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:19 - INFO - Processing batch 191/1300\n",
      "2025-06-10 03:09:19 - INFO - Batch 191/1300 - Loss: 0.5465 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:20 - INFO - Processing batch 201/1300\n",
      "2025-06-10 03:09:20 - INFO - Batch 201/1300 - Loss: 0.4795 - Avg batch time: 0.16s\n",
      "2025-06-10 03:09:22 - INFO - Processing batch 211/1300\n",
      "2025-06-10 03:09:22 - INFO - Batch 211/1300 - Loss: 0.5469 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:23 - INFO - Processing batch 221/1300\n",
      "2025-06-10 03:09:23 - INFO - Batch 221/1300 - Loss: 0.5958 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:25 - INFO - Processing batch 231/1300\n",
      "2025-06-10 03:09:25 - INFO - Batch 231/1300 - Loss: 0.3829 - Avg batch time: 0.16s\n",
      "2025-06-10 03:09:26 - INFO - Processing batch 241/1300\n",
      "2025-06-10 03:09:26 - INFO - Batch 241/1300 - Loss: 0.2212 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:28 - INFO - Processing batch 251/1300\n",
      "2025-06-10 03:09:28 - INFO - Batch 251/1300 - Loss: 0.5722 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:29 - INFO - Processing batch 261/1300\n",
      "2025-06-10 03:09:30 - INFO - Batch 261/1300 - Loss: 0.3657 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:31 - INFO - Processing batch 271/1300\n",
      "2025-06-10 03:09:31 - INFO - Batch 271/1300 - Loss: 0.2951 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:32 - INFO - Processing batch 281/1300\n",
      "2025-06-10 03:09:33 - INFO - Batch 281/1300 - Loss: 0.3195 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:34 - INFO - Processing batch 291/1300\n",
      "2025-06-10 03:09:34 - INFO - Batch 291/1300 - Loss: 0.2909 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:36 - INFO - Processing batch 301/1300\n",
      "2025-06-10 03:09:36 - INFO - Batch 301/1300 - Loss: 0.1680 - Avg batch time: 0.16s\n",
      "2025-06-10 03:09:37 - INFO - Processing batch 311/1300\n",
      "2025-06-10 03:09:37 - INFO - Batch 311/1300 - Loss: 0.2610 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:39 - INFO - Processing batch 321/1300\n",
      "2025-06-10 03:09:39 - INFO - Batch 321/1300 - Loss: 0.3828 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:40 - INFO - Processing batch 331/1300\n",
      "2025-06-10 03:09:40 - INFO - Batch 331/1300 - Loss: 0.4780 - Avg batch time: 0.16s\n",
      "2025-06-10 03:09:42 - INFO - Processing batch 341/1300\n",
      "2025-06-10 03:09:42 - INFO - Batch 341/1300 - Loss: 0.4225 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:43 - INFO - Processing batch 351/1300\n",
      "2025-06-10 03:09:43 - INFO - Batch 351/1300 - Loss: 0.4128 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:45 - INFO - Processing batch 361/1300\n",
      "2025-06-10 03:09:45 - INFO - Batch 361/1300 - Loss: 0.5894 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:46 - INFO - Processing batch 371/1300\n",
      "2025-06-10 03:09:47 - INFO - Batch 371/1300 - Loss: 0.3169 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:48 - INFO - Processing batch 381/1300\n",
      "2025-06-10 03:09:48 - INFO - Batch 381/1300 - Loss: 0.2818 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:49 - INFO - Processing batch 391/1300\n",
      "2025-06-10 03:09:50 - INFO - Batch 391/1300 - Loss: 0.8894 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:51 - INFO - Processing batch 401/1300\n",
      "2025-06-10 03:09:51 - INFO - Batch 401/1300 - Loss: 0.5440 - Avg batch time: 0.16s\n",
      "2025-06-10 03:09:53 - INFO - Processing batch 411/1300\n",
      "2025-06-10 03:09:53 - INFO - Batch 411/1300 - Loss: 0.2860 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:54 - INFO - Processing batch 421/1300\n",
      "2025-06-10 03:09:54 - INFO - Batch 421/1300 - Loss: 0.4322 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:56 - INFO - Processing batch 431/1300\n",
      "2025-06-10 03:09:56 - INFO - Batch 431/1300 - Loss: 0.4418 - Avg batch time: 0.16s\n",
      "2025-06-10 03:09:57 - INFO - Processing batch 441/1300\n",
      "2025-06-10 03:09:57 - INFO - Batch 441/1300 - Loss: 0.1456 - Avg batch time: 0.15s\n",
      "2025-06-10 03:09:59 - INFO - Processing batch 451/1300\n",
      "2025-06-10 03:09:59 - INFO - Batch 451/1300 - Loss: 0.5543 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:00 - INFO - Processing batch 461/1300\n",
      "2025-06-10 03:10:00 - INFO - Batch 461/1300 - Loss: 0.6239 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:02 - INFO - Processing batch 471/1300\n",
      "2025-06-10 03:10:02 - INFO - Batch 471/1300 - Loss: 0.2350 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:03 - INFO - Processing batch 481/1300\n",
      "2025-06-10 03:10:04 - INFO - Batch 481/1300 - Loss: 0.4359 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:05 - INFO - Processing batch 491/1300\n",
      "2025-06-10 03:10:05 - INFO - Batch 491/1300 - Loss: 0.2944 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:06 - INFO - Processing batch 501/1300\n",
      "2025-06-10 03:10:07 - INFO - Batch 501/1300 - Loss: 0.3439 - Avg batch time: 0.16s\n",
      "2025-06-10 03:10:08 - INFO - Processing batch 511/1300\n",
      "2025-06-10 03:10:08 - INFO - Batch 511/1300 - Loss: 0.1196 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:10 - INFO - Processing batch 521/1300\n",
      "2025-06-10 03:10:10 - INFO - Batch 521/1300 - Loss: 0.3842 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:11 - INFO - Processing batch 531/1300\n",
      "2025-06-10 03:10:11 - INFO - Batch 531/1300 - Loss: 0.4595 - Avg batch time: 0.16s\n",
      "2025-06-10 03:10:13 - INFO - Processing batch 541/1300\n",
      "2025-06-10 03:10:13 - INFO - Batch 541/1300 - Loss: 1.0888 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:14 - INFO - Processing batch 551/1300\n",
      "2025-06-10 03:10:14 - INFO - Batch 551/1300 - Loss: 1.2371 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:16 - INFO - Processing batch 561/1300\n",
      "2025-06-10 03:10:16 - INFO - Batch 561/1300 - Loss: 0.5370 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:17 - INFO - Processing batch 571/1300\n",
      "2025-06-10 03:10:17 - INFO - Batch 571/1300 - Loss: 0.3788 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:19 - INFO - Processing batch 581/1300\n",
      "2025-06-10 03:10:19 - INFO - Batch 581/1300 - Loss: 0.5095 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:20 - INFO - Processing batch 591/1300\n",
      "2025-06-10 03:10:20 - INFO - Batch 591/1300 - Loss: 0.6211 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:22 - INFO - Processing batch 601/1300\n",
      "2025-06-10 03:10:22 - INFO - Batch 601/1300 - Loss: 0.8023 - Avg batch time: 0.16s\n",
      "2025-06-10 03:10:23 - INFO - Processing batch 611/1300\n",
      "2025-06-10 03:10:24 - INFO - Batch 611/1300 - Loss: 0.1652 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:25 - INFO - Processing batch 621/1300\n",
      "2025-06-10 03:10:25 - INFO - Batch 621/1300 - Loss: 0.4960 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:27 - INFO - Processing batch 631/1300\n",
      "2025-06-10 03:10:27 - INFO - Batch 631/1300 - Loss: 0.2680 - Avg batch time: 0.18s\n",
      "2025-06-10 03:10:28 - INFO - Processing batch 641/1300\n",
      "2025-06-10 03:10:28 - INFO - Batch 641/1300 - Loss: 0.4983 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:30 - INFO - Processing batch 651/1300\n",
      "2025-06-10 03:10:30 - INFO - Batch 651/1300 - Loss: 0.6126 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:31 - INFO - Processing batch 661/1300\n",
      "2025-06-10 03:10:32 - INFO - Batch 661/1300 - Loss: 0.2355 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:33 - INFO - Processing batch 671/1300\n",
      "2025-06-10 03:10:33 - INFO - Batch 671/1300 - Loss: 0.3704 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:34 - INFO - Processing batch 681/1300\n",
      "2025-06-10 03:10:35 - INFO - Batch 681/1300 - Loss: 0.3126 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:36 - INFO - Processing batch 691/1300\n",
      "2025-06-10 03:10:36 - INFO - Batch 691/1300 - Loss: 0.2008 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:38 - INFO - Processing batch 701/1300\n",
      "2025-06-10 03:10:38 - INFO - Batch 701/1300 - Loss: 0.6191 - Avg batch time: 0.16s\n",
      "2025-06-10 03:10:39 - INFO - Processing batch 711/1300\n",
      "2025-06-10 03:10:39 - INFO - Batch 711/1300 - Loss: 1.0662 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:41 - INFO - Processing batch 721/1300\n",
      "2025-06-10 03:10:41 - INFO - Batch 721/1300 - Loss: 1.4734 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:42 - INFO - Processing batch 731/1300\n",
      "2025-06-10 03:10:42 - INFO - Batch 731/1300 - Loss: 0.1553 - Avg batch time: 0.16s\n",
      "2025-06-10 03:10:44 - INFO - Processing batch 741/1300\n",
      "2025-06-10 03:10:44 - INFO - Batch 741/1300 - Loss: 0.7914 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:45 - INFO - Processing batch 751/1300\n",
      "2025-06-10 03:10:45 - INFO - Batch 751/1300 - Loss: 0.5689 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:47 - INFO - Processing batch 761/1300\n",
      "2025-06-10 03:10:47 - INFO - Batch 761/1300 - Loss: 0.3268 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:48 - INFO - Processing batch 771/1300\n",
      "2025-06-10 03:10:48 - INFO - Batch 771/1300 - Loss: 0.6071 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:50 - INFO - Processing batch 781/1300\n",
      "2025-06-10 03:10:50 - INFO - Batch 781/1300 - Loss: 0.5981 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:51 - INFO - Processing batch 791/1300\n",
      "2025-06-10 03:10:52 - INFO - Batch 791/1300 - Loss: 0.2145 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:53 - INFO - Processing batch 801/1300\n",
      "2025-06-10 03:10:53 - INFO - Batch 801/1300 - Loss: 0.7680 - Avg batch time: 0.16s\n",
      "2025-06-10 03:10:54 - INFO - Processing batch 811/1300\n",
      "2025-06-10 03:10:55 - INFO - Batch 811/1300 - Loss: 0.2765 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:56 - INFO - Processing batch 821/1300\n",
      "2025-06-10 03:10:56 - INFO - Batch 821/1300 - Loss: 0.8677 - Avg batch time: 0.15s\n",
      "2025-06-10 03:10:58 - INFO - Processing batch 831/1300\n",
      "2025-06-10 03:10:58 - INFO - Batch 831/1300 - Loss: 0.2657 - Avg batch time: 0.16s\n",
      "2025-06-10 03:10:59 - INFO - Processing batch 841/1300\n",
      "2025-06-10 03:10:59 - INFO - Batch 841/1300 - Loss: 0.3329 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:01 - INFO - Processing batch 851/1300\n",
      "2025-06-10 03:11:01 - INFO - Batch 851/1300 - Loss: 0.4013 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:02 - INFO - Processing batch 861/1300\n",
      "2025-06-10 03:11:02 - INFO - Batch 861/1300 - Loss: 0.2281 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:04 - INFO - Processing batch 871/1300\n",
      "2025-06-10 03:11:04 - INFO - Batch 871/1300 - Loss: 0.0952 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:05 - INFO - Processing batch 881/1300\n",
      "2025-06-10 03:11:05 - INFO - Batch 881/1300 - Loss: 0.2651 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:07 - INFO - Processing batch 891/1300\n",
      "2025-06-10 03:11:07 - INFO - Batch 891/1300 - Loss: 0.2212 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:08 - INFO - Processing batch 901/1300\n",
      "2025-06-10 03:11:08 - INFO - Batch 901/1300 - Loss: 0.1376 - Avg batch time: 0.16s\n",
      "2025-06-10 03:11:10 - INFO - Processing batch 911/1300\n",
      "2025-06-10 03:11:10 - INFO - Batch 911/1300 - Loss: 0.5638 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:11 - INFO - Processing batch 921/1300\n",
      "2025-06-10 03:11:12 - INFO - Batch 921/1300 - Loss: 0.5648 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:13 - INFO - Processing batch 931/1300\n",
      "2025-06-10 03:11:13 - INFO - Batch 931/1300 - Loss: 0.7794 - Avg batch time: 0.16s\n",
      "2025-06-10 03:11:15 - INFO - Processing batch 941/1300\n",
      "2025-06-10 03:11:15 - INFO - Batch 941/1300 - Loss: 1.3229 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:16 - INFO - Processing batch 951/1300\n",
      "2025-06-10 03:11:16 - INFO - Batch 951/1300 - Loss: 0.2630 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:18 - INFO - Processing batch 961/1300\n",
      "2025-06-10 03:11:18 - INFO - Batch 961/1300 - Loss: 0.4719 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:19 - INFO - Processing batch 971/1300\n",
      "2025-06-10 03:11:19 - INFO - Batch 971/1300 - Loss: 0.5918 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:21 - INFO - Processing batch 981/1300\n",
      "2025-06-10 03:11:21 - INFO - Batch 981/1300 - Loss: 0.5754 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:22 - INFO - Processing batch 991/1300\n",
      "2025-06-10 03:11:22 - INFO - Batch 991/1300 - Loss: 0.5587 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:24 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 03:11:24 - INFO - Batch 1001/1300 - Loss: 0.4497 - Avg batch time: 0.16s\n",
      "2025-06-10 03:11:25 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 03:11:25 - INFO - Batch 1011/1300 - Loss: 0.4686 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:27 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 03:11:27 - INFO - Batch 1021/1300 - Loss: 0.5441 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:28 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 03:11:29 - INFO - Batch 1031/1300 - Loss: 0.9219 - Avg batch time: 0.16s\n",
      "2025-06-10 03:11:30 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 03:11:30 - INFO - Batch 1041/1300 - Loss: 0.3052 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:31 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 03:11:32 - INFO - Batch 1051/1300 - Loss: 0.4849 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:33 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 03:11:33 - INFO - Batch 1061/1300 - Loss: 0.8709 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:35 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 03:11:35 - INFO - Batch 1071/1300 - Loss: 0.1647 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:36 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 03:11:36 - INFO - Batch 1081/1300 - Loss: 0.5536 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:38 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 03:11:38 - INFO - Batch 1091/1300 - Loss: 0.6489 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:39 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 03:11:39 - INFO - Batch 1101/1300 - Loss: 0.2625 - Avg batch time: 0.16s\n",
      "2025-06-10 03:11:41 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 03:11:41 - INFO - Batch 1111/1300 - Loss: 0.2289 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:42 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 03:11:42 - INFO - Batch 1121/1300 - Loss: 0.1922 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:44 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 03:11:44 - INFO - Batch 1131/1300 - Loss: 0.7157 - Avg batch time: 0.16s\n",
      "2025-06-10 03:11:45 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 03:11:46 - INFO - Batch 1141/1300 - Loss: 0.7850 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:47 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 03:11:47 - INFO - Batch 1151/1300 - Loss: 0.4011 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:48 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 03:11:49 - INFO - Batch 1161/1300 - Loss: 0.4835 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:50 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 03:11:50 - INFO - Batch 1171/1300 - Loss: 0.4882 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:51 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 03:11:52 - INFO - Batch 1181/1300 - Loss: 0.5723 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:53 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 03:11:53 - INFO - Batch 1191/1300 - Loss: 0.2764 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:55 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 03:11:55 - INFO - Batch 1201/1300 - Loss: 0.3107 - Avg batch time: 0.16s\n",
      "2025-06-10 03:11:56 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 03:11:56 - INFO - Batch 1211/1300 - Loss: 0.6396 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:58 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 03:11:58 - INFO - Batch 1221/1300 - Loss: 0.4712 - Avg batch time: 0.15s\n",
      "2025-06-10 03:11:59 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 03:11:59 - INFO - Batch 1231/1300 - Loss: 0.5194 - Avg batch time: 0.16s\n",
      "2025-06-10 03:12:01 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 03:12:01 - INFO - Batch 1241/1300 - Loss: 0.1639 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:02 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 03:12:02 - INFO - Batch 1251/1300 - Loss: 0.4311 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:04 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 03:12:04 - INFO - Batch 1261/1300 - Loss: 0.7206 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:05 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 03:12:06 - INFO - Batch 1271/1300 - Loss: 0.1381 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:07 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 03:12:07 - INFO - Batch 1281/1300 - Loss: 0.6026 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:08 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 03:12:09 - INFO - Batch 1291/1300 - Loss: 0.6003 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:10 - INFO - \n",
      "Epoch 8 training completed in 201.11s\n",
      "2025-06-10 03:12:10 - INFO - Average training loss: 0.4731\n",
      "2025-06-10 03:12:28 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:   9%| | 9/100 [29:05<5:31:14, 218.40s/it, train_loss=0.4731, val_loss=0.4523, best_val_f1=0.3125, lr=5.00e-05, b2025-06-10 03:12:28 - INFO - \n",
      "Epoch 9/100 - Training phase\n",
      "2025-06-10 03:12:28 - INFO - Processing batch 1/1300\n",
      "2025-06-10 03:12:28 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 03:12:28 - INFO - Batch 1/1300 - Loss: 0.4664 - Avg batch time: 0.16s\n",
      "2025-06-10 03:12:30 - INFO - Processing batch 11/1300\n",
      "2025-06-10 03:12:30 - INFO - Batch 11/1300 - Loss: 0.3847 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:31 - INFO - Processing batch 21/1300\n",
      "2025-06-10 03:12:31 - INFO - Batch 21/1300 - Loss: 0.5677 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:33 - INFO - Processing batch 31/1300\n",
      "2025-06-10 03:12:33 - INFO - Batch 31/1300 - Loss: 0.2503 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:34 - INFO - Processing batch 41/1300\n",
      "2025-06-10 03:12:34 - INFO - Batch 41/1300 - Loss: 0.3701 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:36 - INFO - Processing batch 51/1300\n",
      "2025-06-10 03:12:36 - INFO - Batch 51/1300 - Loss: 0.1224 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:37 - INFO - Processing batch 61/1300\n",
      "2025-06-10 03:12:37 - INFO - Batch 61/1300 - Loss: 0.3968 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:39 - INFO - Processing batch 71/1300\n",
      "2025-06-10 03:12:39 - INFO - Batch 71/1300 - Loss: 0.6789 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:40 - INFO - Processing batch 81/1300\n",
      "2025-06-10 03:12:40 - INFO - Batch 81/1300 - Loss: 0.2654 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:42 - INFO - Processing batch 91/1300\n",
      "2025-06-10 03:12:42 - INFO - Batch 91/1300 - Loss: 0.6514 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:43 - INFO - Processing batch 101/1300\n",
      "2025-06-10 03:12:44 - INFO - Batch 101/1300 - Loss: 0.2246 - Avg batch time: 0.16s\n",
      "2025-06-10 03:12:45 - INFO - Processing batch 111/1300\n",
      "2025-06-10 03:12:45 - INFO - Batch 111/1300 - Loss: 0.2005 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:46 - INFO - Processing batch 121/1300\n",
      "2025-06-10 03:12:47 - INFO - Batch 121/1300 - Loss: 0.4929 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:48 - INFO - Processing batch 131/1300\n",
      "2025-06-10 03:12:48 - INFO - Batch 131/1300 - Loss: 0.1050 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:50 - INFO - Processing batch 141/1300\n",
      "2025-06-10 03:12:50 - INFO - Batch 141/1300 - Loss: 0.4768 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:51 - INFO - Processing batch 151/1300\n",
      "2025-06-10 03:12:51 - INFO - Batch 151/1300 - Loss: 0.6119 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:53 - INFO - Processing batch 161/1300\n",
      "2025-06-10 03:12:53 - INFO - Batch 161/1300 - Loss: 0.6416 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:54 - INFO - Processing batch 171/1300\n",
      "2025-06-10 03:12:54 - INFO - Batch 171/1300 - Loss: 0.4265 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:56 - INFO - Processing batch 181/1300\n",
      "2025-06-10 03:12:56 - INFO - Batch 181/1300 - Loss: 0.1323 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:57 - INFO - Processing batch 191/1300\n",
      "2025-06-10 03:12:57 - INFO - Batch 191/1300 - Loss: 0.6988 - Avg batch time: 0.15s\n",
      "2025-06-10 03:12:59 - INFO - Processing batch 201/1300\n",
      "2025-06-10 03:12:59 - INFO - Batch 201/1300 - Loss: 0.5201 - Avg batch time: 0.16s\n",
      "2025-06-10 03:13:00 - INFO - Processing batch 211/1300\n",
      "2025-06-10 03:13:00 - INFO - Batch 211/1300 - Loss: 0.7712 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:02 - INFO - Processing batch 221/1300\n",
      "2025-06-10 03:13:02 - INFO - Batch 221/1300 - Loss: 0.8417 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:03 - INFO - Processing batch 231/1300\n",
      "2025-06-10 03:13:04 - INFO - Batch 231/1300 - Loss: 0.2421 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:05 - INFO - Processing batch 241/1300\n",
      "2025-06-10 03:13:05 - INFO - Batch 241/1300 - Loss: 0.2800 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:06 - INFO - Processing batch 251/1300\n",
      "2025-06-10 03:13:07 - INFO - Batch 251/1300 - Loss: 0.5057 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:08 - INFO - Processing batch 261/1300\n",
      "2025-06-10 03:13:08 - INFO - Batch 261/1300 - Loss: 0.2628 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:10 - INFO - Processing batch 271/1300\n",
      "2025-06-10 03:13:10 - INFO - Batch 271/1300 - Loss: 0.4864 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:11 - INFO - Processing batch 281/1300\n",
      "2025-06-10 03:13:11 - INFO - Batch 281/1300 - Loss: 0.4787 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:13 - INFO - Processing batch 291/1300\n",
      "2025-06-10 03:13:13 - INFO - Batch 291/1300 - Loss: 0.4267 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:14 - INFO - Processing batch 301/1300\n",
      "2025-06-10 03:13:14 - INFO - Batch 301/1300 - Loss: 0.8025 - Avg batch time: 0.16s\n",
      "2025-06-10 03:13:16 - INFO - Processing batch 311/1300\n",
      "2025-06-10 03:13:16 - INFO - Batch 311/1300 - Loss: 0.4080 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:17 - INFO - Processing batch 321/1300\n",
      "2025-06-10 03:13:17 - INFO - Batch 321/1300 - Loss: 0.3742 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:19 - INFO - Processing batch 331/1300\n",
      "2025-06-10 03:13:19 - INFO - Batch 331/1300 - Loss: 0.3152 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:20 - INFO - Processing batch 341/1300\n",
      "2025-06-10 03:13:20 - INFO - Batch 341/1300 - Loss: 0.5621 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:22 - INFO - Processing batch 351/1300\n",
      "2025-06-10 03:13:22 - INFO - Batch 351/1300 - Loss: 0.3857 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:23 - INFO - Processing batch 361/1300\n",
      "2025-06-10 03:13:23 - INFO - Batch 361/1300 - Loss: 0.7905 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:25 - INFO - Processing batch 371/1300\n",
      "2025-06-10 03:13:25 - INFO - Batch 371/1300 - Loss: 0.7739 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:26 - INFO - Processing batch 381/1300\n",
      "2025-06-10 03:13:27 - INFO - Batch 381/1300 - Loss: 0.7379 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:28 - INFO - Processing batch 391/1300\n",
      "2025-06-10 03:13:28 - INFO - Batch 391/1300 - Loss: 0.3289 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:30 - INFO - Processing batch 401/1300\n",
      "2025-06-10 03:13:30 - INFO - Batch 401/1300 - Loss: 0.4251 - Avg batch time: 0.16s\n",
      "2025-06-10 03:13:31 - INFO - Processing batch 411/1300\n",
      "2025-06-10 03:13:31 - INFO - Batch 411/1300 - Loss: 0.5771 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:33 - INFO - Processing batch 421/1300\n",
      "2025-06-10 03:13:33 - INFO - Batch 421/1300 - Loss: 0.6447 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:34 - INFO - Processing batch 431/1300\n",
      "2025-06-10 03:13:34 - INFO - Batch 431/1300 - Loss: 0.7202 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:36 - INFO - Processing batch 441/1300\n",
      "2025-06-10 03:13:36 - INFO - Batch 441/1300 - Loss: 0.2091 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:37 - INFO - Processing batch 451/1300\n",
      "2025-06-10 03:13:37 - INFO - Batch 451/1300 - Loss: 0.9442 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:39 - INFO - Processing batch 461/1300\n",
      "2025-06-10 03:13:39 - INFO - Batch 461/1300 - Loss: 0.5029 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:40 - INFO - Processing batch 471/1300\n",
      "2025-06-10 03:13:40 - INFO - Batch 471/1300 - Loss: 0.8192 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:42 - INFO - Processing batch 481/1300\n",
      "2025-06-10 03:13:42 - INFO - Batch 481/1300 - Loss: 0.3434 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:43 - INFO - Processing batch 491/1300\n",
      "2025-06-10 03:13:43 - INFO - Batch 491/1300 - Loss: 0.1918 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:45 - INFO - Processing batch 501/1300\n",
      "2025-06-10 03:13:45 - INFO - Batch 501/1300 - Loss: 0.6964 - Avg batch time: 0.16s\n",
      "2025-06-10 03:13:46 - INFO - Processing batch 511/1300\n",
      "2025-06-10 03:13:47 - INFO - Batch 511/1300 - Loss: 0.0787 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:48 - INFO - Processing batch 521/1300\n",
      "2025-06-10 03:13:48 - INFO - Batch 521/1300 - Loss: 0.7744 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:49 - INFO - Processing batch 531/1300\n",
      "2025-06-10 03:13:50 - INFO - Batch 531/1300 - Loss: 0.2248 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:51 - INFO - Processing batch 541/1300\n",
      "2025-06-10 03:13:51 - INFO - Batch 541/1300 - Loss: 0.3104 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:53 - INFO - Processing batch 551/1300\n",
      "2025-06-10 03:13:53 - INFO - Batch 551/1300 - Loss: 0.7108 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:54 - INFO - Processing batch 561/1300\n",
      "2025-06-10 03:13:54 - INFO - Batch 561/1300 - Loss: 0.2403 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:56 - INFO - Processing batch 571/1300\n",
      "2025-06-10 03:13:56 - INFO - Batch 571/1300 - Loss: 0.4770 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:57 - INFO - Processing batch 581/1300\n",
      "2025-06-10 03:13:57 - INFO - Batch 581/1300 - Loss: 0.4183 - Avg batch time: 0.15s\n",
      "2025-06-10 03:13:59 - INFO - Processing batch 591/1300\n",
      "2025-06-10 03:13:59 - INFO - Batch 591/1300 - Loss: 0.3574 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:00 - INFO - Processing batch 601/1300\n",
      "2025-06-10 03:14:00 - INFO - Batch 601/1300 - Loss: 0.3912 - Avg batch time: 0.16s\n",
      "2025-06-10 03:14:02 - INFO - Processing batch 611/1300\n",
      "2025-06-10 03:14:02 - INFO - Batch 611/1300 - Loss: 0.9732 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:03 - INFO - Processing batch 621/1300\n",
      "2025-06-10 03:14:03 - INFO - Batch 621/1300 - Loss: 0.2640 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:05 - INFO - Processing batch 631/1300\n",
      "2025-06-10 03:14:05 - INFO - Batch 631/1300 - Loss: 0.0770 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:06 - INFO - Processing batch 641/1300\n",
      "2025-06-10 03:14:06 - INFO - Batch 641/1300 - Loss: 0.1950 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:08 - INFO - Processing batch 651/1300\n",
      "2025-06-10 03:14:08 - INFO - Batch 651/1300 - Loss: 0.8353 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:09 - INFO - Processing batch 661/1300\n",
      "2025-06-10 03:14:10 - INFO - Batch 661/1300 - Loss: 0.0974 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:11 - INFO - Processing batch 671/1300\n",
      "2025-06-10 03:14:11 - INFO - Batch 671/1300 - Loss: 0.7755 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:12 - INFO - Processing batch 681/1300\n",
      "2025-06-10 03:14:13 - INFO - Batch 681/1300 - Loss: 0.2236 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:14 - INFO - Processing batch 691/1300\n",
      "2025-06-10 03:14:14 - INFO - Batch 691/1300 - Loss: 0.1643 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:16 - INFO - Processing batch 701/1300\n",
      "2025-06-10 03:14:16 - INFO - Batch 701/1300 - Loss: 0.3287 - Avg batch time: 0.16s\n",
      "2025-06-10 03:14:17 - INFO - Processing batch 711/1300\n",
      "2025-06-10 03:14:17 - INFO - Batch 711/1300 - Loss: 0.3612 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:19 - INFO - Processing batch 721/1300\n",
      "2025-06-10 03:14:19 - INFO - Batch 721/1300 - Loss: 0.6693 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:20 - INFO - Processing batch 731/1300\n",
      "2025-06-10 03:14:20 - INFO - Batch 731/1300 - Loss: 0.5767 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:22 - INFO - Processing batch 741/1300\n",
      "2025-06-10 03:14:22 - INFO - Batch 741/1300 - Loss: 0.1121 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:23 - INFO - Processing batch 751/1300\n",
      "2025-06-10 03:14:23 - INFO - Batch 751/1300 - Loss: 0.2704 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:25 - INFO - Processing batch 761/1300\n",
      "2025-06-10 03:14:25 - INFO - Batch 761/1300 - Loss: 0.1718 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:26 - INFO - Processing batch 771/1300\n",
      "2025-06-10 03:14:26 - INFO - Batch 771/1300 - Loss: 0.8929 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:28 - INFO - Processing batch 781/1300\n",
      "2025-06-10 03:14:28 - INFO - Batch 781/1300 - Loss: 0.3012 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:29 - INFO - Processing batch 791/1300\n",
      "2025-06-10 03:14:29 - INFO - Batch 791/1300 - Loss: 0.8164 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:31 - INFO - Processing batch 801/1300\n",
      "2025-06-10 03:14:31 - INFO - Batch 801/1300 - Loss: 0.5332 - Avg batch time: 0.16s\n",
      "2025-06-10 03:14:32 - INFO - Processing batch 811/1300\n",
      "2025-06-10 03:14:33 - INFO - Batch 811/1300 - Loss: 0.3329 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:34 - INFO - Processing batch 821/1300\n",
      "2025-06-10 03:14:34 - INFO - Batch 821/1300 - Loss: 0.3302 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:35 - INFO - Processing batch 831/1300\n",
      "2025-06-10 03:14:36 - INFO - Batch 831/1300 - Loss: 0.2616 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:37 - INFO - Processing batch 841/1300\n",
      "2025-06-10 03:14:37 - INFO - Batch 841/1300 - Loss: 0.2979 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:39 - INFO - Processing batch 851/1300\n",
      "2025-06-10 03:14:39 - INFO - Batch 851/1300 - Loss: 0.1317 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:40 - INFO - Processing batch 861/1300\n",
      "2025-06-10 03:14:40 - INFO - Batch 861/1300 - Loss: 0.4887 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:42 - INFO - Processing batch 871/1300\n",
      "2025-06-10 03:14:42 - INFO - Batch 871/1300 - Loss: 0.4120 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:43 - INFO - Processing batch 881/1300\n",
      "2025-06-10 03:14:43 - INFO - Batch 881/1300 - Loss: 0.4006 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:45 - INFO - Processing batch 891/1300\n",
      "2025-06-10 03:14:45 - INFO - Batch 891/1300 - Loss: 0.3356 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:46 - INFO - Processing batch 901/1300\n",
      "2025-06-10 03:14:46 - INFO - Batch 901/1300 - Loss: 0.5002 - Avg batch time: 0.16s\n",
      "2025-06-10 03:14:48 - INFO - Processing batch 911/1300\n",
      "2025-06-10 03:14:48 - INFO - Batch 911/1300 - Loss: 0.6134 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:49 - INFO - Processing batch 921/1300\n",
      "2025-06-10 03:14:49 - INFO - Batch 921/1300 - Loss: 0.3539 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:51 - INFO - Processing batch 931/1300\n",
      "2025-06-10 03:14:51 - INFO - Batch 931/1300 - Loss: 0.9444 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:52 - INFO - Processing batch 941/1300\n",
      "2025-06-10 03:14:53 - INFO - Batch 941/1300 - Loss: 0.7853 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:54 - INFO - Processing batch 951/1300\n",
      "2025-06-10 03:14:54 - INFO - Batch 951/1300 - Loss: 0.1700 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:55 - INFO - Processing batch 961/1300\n",
      "2025-06-10 03:14:56 - INFO - Batch 961/1300 - Loss: 0.4081 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:57 - INFO - Processing batch 971/1300\n",
      "2025-06-10 03:14:57 - INFO - Batch 971/1300 - Loss: 0.3977 - Avg batch time: 0.15s\n",
      "2025-06-10 03:14:58 - INFO - Processing batch 981/1300\n",
      "2025-06-10 03:14:59 - INFO - Batch 981/1300 - Loss: 0.2142 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:00 - INFO - Processing batch 991/1300\n",
      "2025-06-10 03:15:00 - INFO - Batch 991/1300 - Loss: 0.2329 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:02 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 03:15:02 - INFO - Batch 1001/1300 - Loss: 1.0396 - Avg batch time: 0.16s\n",
      "2025-06-10 03:15:03 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 03:15:03 - INFO - Batch 1011/1300 - Loss: 0.8003 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:05 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 03:15:05 - INFO - Batch 1021/1300 - Loss: 0.8069 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:06 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 03:15:06 - INFO - Batch 1031/1300 - Loss: 0.2960 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:08 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 03:15:08 - INFO - Batch 1041/1300 - Loss: 0.3978 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:09 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 03:15:09 - INFO - Batch 1051/1300 - Loss: 0.0727 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:11 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 03:15:11 - INFO - Batch 1061/1300 - Loss: 0.4598 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:12 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 03:15:12 - INFO - Batch 1071/1300 - Loss: 0.6518 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:14 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 03:15:14 - INFO - Batch 1081/1300 - Loss: 1.0484 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:15 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 03:15:16 - INFO - Batch 1091/1300 - Loss: 0.4813 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:17 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 03:15:17 - INFO - Batch 1101/1300 - Loss: 0.3371 - Avg batch time: 0.16s\n",
      "2025-06-10 03:15:18 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 03:15:19 - INFO - Batch 1111/1300 - Loss: 0.2292 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:20 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 03:15:20 - INFO - Batch 1121/1300 - Loss: 0.3946 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:22 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 03:15:22 - INFO - Batch 1131/1300 - Loss: 0.1654 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:23 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 03:15:23 - INFO - Batch 1141/1300 - Loss: 0.3362 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:25 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 03:15:25 - INFO - Batch 1151/1300 - Loss: 0.6526 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:26 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 03:15:26 - INFO - Batch 1161/1300 - Loss: 0.8952 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:28 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 03:15:28 - INFO - Batch 1171/1300 - Loss: 0.1136 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:29 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 03:15:29 - INFO - Batch 1181/1300 - Loss: 0.7469 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:31 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 03:15:31 - INFO - Batch 1191/1300 - Loss: 0.5461 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:32 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 03:15:32 - INFO - Batch 1201/1300 - Loss: 0.7587 - Avg batch time: 0.16s\n",
      "2025-06-10 03:15:34 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 03:15:34 - INFO - Batch 1211/1300 - Loss: 0.1913 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:35 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 03:15:36 - INFO - Batch 1221/1300 - Loss: 0.4574 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:37 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 03:15:37 - INFO - Batch 1231/1300 - Loss: 0.6290 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:38 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 03:15:39 - INFO - Batch 1241/1300 - Loss: 0.3477 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:40 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 03:15:40 - INFO - Batch 1251/1300 - Loss: 0.6571 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:41 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 03:15:42 - INFO - Batch 1261/1300 - Loss: 0.2011 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:43 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 03:15:43 - INFO - Batch 1271/1300 - Loss: 0.3211 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:45 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 03:15:45 - INFO - Batch 1281/1300 - Loss: 0.3568 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:46 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 03:15:46 - INFO - Batch 1291/1300 - Loss: 0.3751 - Avg batch time: 0.15s\n",
      "2025-06-10 03:15:48 - INFO - \n",
      "Epoch 9 training completed in 199.81s\n",
      "2025-06-10 03:15:48 - INFO - Average training loss: 0.4574\n",
      "2025-06-10 03:16:05 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:  10%| | 10/100 [32:43<5:27:11, 218.13s/it, train_loss=0.4574, val_loss=0.4700, best_val_f1=0.3125, lr=5.00e-05, 2025-06-10 03:16:05 - INFO - \n",
      "Epoch 10/100 - Training phase\n",
      "2025-06-10 03:16:06 - INFO - Processing batch 1/1300\n",
      "2025-06-10 03:16:06 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 03:16:06 - INFO - Batch 1/1300 - Loss: 0.0860 - Avg batch time: 0.16s\n",
      "2025-06-10 03:16:07 - INFO - Processing batch 11/1300\n",
      "2025-06-10 03:16:07 - INFO - Batch 11/1300 - Loss: 0.6206 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:09 - INFO - Processing batch 21/1300\n",
      "2025-06-10 03:16:09 - INFO - Batch 21/1300 - Loss: 0.2871 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:10 - INFO - Processing batch 31/1300\n",
      "2025-06-10 03:16:10 - INFO - Batch 31/1300 - Loss: 0.2849 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:12 - INFO - Processing batch 41/1300\n",
      "2025-06-10 03:16:12 - INFO - Batch 41/1300 - Loss: 0.2317 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:13 - INFO - Processing batch 51/1300\n",
      "2025-06-10 03:16:13 - INFO - Batch 51/1300 - Loss: 0.6609 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:15 - INFO - Processing batch 61/1300\n",
      "2025-06-10 03:16:15 - INFO - Batch 61/1300 - Loss: 0.0842 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:16 - INFO - Processing batch 71/1300\n",
      "2025-06-10 03:16:17 - INFO - Batch 71/1300 - Loss: 0.9013 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:18 - INFO - Processing batch 81/1300\n",
      "2025-06-10 03:16:18 - INFO - Batch 81/1300 - Loss: 0.5069 - Avg batch time: 0.16s\n",
      "2025-06-10 03:16:20 - INFO - Processing batch 91/1300\n",
      "2025-06-10 03:16:20 - INFO - Batch 91/1300 - Loss: 0.6601 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:21 - INFO - Processing batch 101/1300\n",
      "2025-06-10 03:16:21 - INFO - Batch 101/1300 - Loss: 0.3057 - Avg batch time: 0.16s\n",
      "2025-06-10 03:16:23 - INFO - Processing batch 111/1300\n",
      "2025-06-10 03:16:23 - INFO - Batch 111/1300 - Loss: 0.1275 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:24 - INFO - Processing batch 121/1300\n",
      "2025-06-10 03:16:24 - INFO - Batch 121/1300 - Loss: 1.8101 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:26 - INFO - Processing batch 131/1300\n",
      "2025-06-10 03:16:26 - INFO - Batch 131/1300 - Loss: 0.7293 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:27 - INFO - Processing batch 141/1300\n",
      "2025-06-10 03:16:27 - INFO - Batch 141/1300 - Loss: 0.3558 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:29 - INFO - Processing batch 151/1300\n",
      "2025-06-10 03:16:29 - INFO - Batch 151/1300 - Loss: 0.9434 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:30 - INFO - Processing batch 161/1300\n",
      "2025-06-10 03:16:30 - INFO - Batch 161/1300 - Loss: 0.9527 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:32 - INFO - Processing batch 171/1300\n",
      "2025-06-10 03:16:32 - INFO - Batch 171/1300 - Loss: 0.3129 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:33 - INFO - Processing batch 181/1300\n",
      "2025-06-10 03:16:34 - INFO - Batch 181/1300 - Loss: 0.2275 - Avg batch time: 0.16s\n",
      "2025-06-10 03:16:35 - INFO - Processing batch 191/1300\n",
      "2025-06-10 03:16:35 - INFO - Batch 191/1300 - Loss: 0.7551 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:36 - INFO - Processing batch 201/1300\n",
      "2025-06-10 03:16:37 - INFO - Batch 201/1300 - Loss: 0.6280 - Avg batch time: 0.16s\n",
      "2025-06-10 03:16:38 - INFO - Processing batch 211/1300\n",
      "2025-06-10 03:16:38 - INFO - Batch 211/1300 - Loss: 0.2609 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:40 - INFO - Processing batch 221/1300\n",
      "2025-06-10 03:16:40 - INFO - Batch 221/1300 - Loss: 0.1824 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:41 - INFO - Processing batch 231/1300\n",
      "2025-06-10 03:16:41 - INFO - Batch 231/1300 - Loss: 0.2305 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:43 - INFO - Processing batch 241/1300\n",
      "2025-06-10 03:16:43 - INFO - Batch 241/1300 - Loss: 0.1416 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:44 - INFO - Processing batch 251/1300\n",
      "2025-06-10 03:16:44 - INFO - Batch 251/1300 - Loss: 0.7223 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:46 - INFO - Processing batch 261/1300\n",
      "2025-06-10 03:16:46 - INFO - Batch 261/1300 - Loss: 0.3036 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:47 - INFO - Processing batch 271/1300\n",
      "2025-06-10 03:16:47 - INFO - Batch 271/1300 - Loss: 1.8663 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:49 - INFO - Processing batch 281/1300\n",
      "2025-06-10 03:16:49 - INFO - Batch 281/1300 - Loss: 0.6579 - Avg batch time: 0.16s\n",
      "2025-06-10 03:16:50 - INFO - Processing batch 291/1300\n",
      "2025-06-10 03:16:51 - INFO - Batch 291/1300 - Loss: 0.1584 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:52 - INFO - Processing batch 301/1300\n",
      "2025-06-10 03:16:52 - INFO - Batch 301/1300 - Loss: 0.1211 - Avg batch time: 0.16s\n",
      "2025-06-10 03:16:53 - INFO - Processing batch 311/1300\n",
      "2025-06-10 03:16:54 - INFO - Batch 311/1300 - Loss: 0.2656 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:55 - INFO - Processing batch 321/1300\n",
      "2025-06-10 03:16:55 - INFO - Batch 321/1300 - Loss: 0.1375 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:57 - INFO - Processing batch 331/1300\n",
      "2025-06-10 03:16:57 - INFO - Batch 331/1300 - Loss: 0.3796 - Avg batch time: 0.15s\n",
      "2025-06-10 03:16:58 - INFO - Processing batch 341/1300\n",
      "2025-06-10 03:16:58 - INFO - Batch 341/1300 - Loss: 0.4841 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:00 - INFO - Processing batch 351/1300\n",
      "2025-06-10 03:17:00 - INFO - Batch 351/1300 - Loss: 0.1150 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:01 - INFO - Processing batch 361/1300\n",
      "2025-06-10 03:17:01 - INFO - Batch 361/1300 - Loss: 0.5874 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:03 - INFO - Processing batch 371/1300\n",
      "2025-06-10 03:17:03 - INFO - Batch 371/1300 - Loss: 0.1523 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:04 - INFO - Processing batch 381/1300\n",
      "2025-06-10 03:17:04 - INFO - Batch 381/1300 - Loss: 0.2416 - Avg batch time: 0.16s\n",
      "2025-06-10 03:17:06 - INFO - Processing batch 391/1300\n",
      "2025-06-10 03:17:06 - INFO - Batch 391/1300 - Loss: 0.7509 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:07 - INFO - Processing batch 401/1300\n",
      "2025-06-10 03:17:07 - INFO - Batch 401/1300 - Loss: 0.5077 - Avg batch time: 0.16s\n",
      "2025-06-10 03:17:09 - INFO - Processing batch 411/1300\n",
      "2025-06-10 03:17:09 - INFO - Batch 411/1300 - Loss: 0.4623 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:10 - INFO - Processing batch 421/1300\n",
      "2025-06-10 03:17:11 - INFO - Batch 421/1300 - Loss: 0.5653 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:12 - INFO - Processing batch 431/1300\n",
      "2025-06-10 03:17:12 - INFO - Batch 431/1300 - Loss: 0.2000 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:13 - INFO - Processing batch 441/1300\n",
      "2025-06-10 03:17:14 - INFO - Batch 441/1300 - Loss: 0.4374 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:15 - INFO - Processing batch 451/1300\n",
      "2025-06-10 03:17:15 - INFO - Batch 451/1300 - Loss: 0.5228 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:17 - INFO - Processing batch 461/1300\n",
      "2025-06-10 03:17:17 - INFO - Batch 461/1300 - Loss: 0.1737 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:18 - INFO - Processing batch 471/1300\n",
      "2025-06-10 03:17:18 - INFO - Batch 471/1300 - Loss: 0.6527 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:20 - INFO - Processing batch 481/1300\n",
      "2025-06-10 03:17:20 - INFO - Batch 481/1300 - Loss: 0.4152 - Avg batch time: 0.16s\n",
      "2025-06-10 03:17:21 - INFO - Processing batch 491/1300\n",
      "2025-06-10 03:17:21 - INFO - Batch 491/1300 - Loss: 0.4575 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:23 - INFO - Processing batch 501/1300\n",
      "2025-06-10 03:17:23 - INFO - Batch 501/1300 - Loss: 0.6524 - Avg batch time: 0.16s\n",
      "2025-06-10 03:17:24 - INFO - Processing batch 511/1300\n",
      "2025-06-10 03:17:24 - INFO - Batch 511/1300 - Loss: 0.6727 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:26 - INFO - Processing batch 521/1300\n",
      "2025-06-10 03:17:26 - INFO - Batch 521/1300 - Loss: 0.2459 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:27 - INFO - Processing batch 531/1300\n",
      "2025-06-10 03:17:27 - INFO - Batch 531/1300 - Loss: 0.4037 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:29 - INFO - Processing batch 541/1300\n",
      "2025-06-10 03:17:29 - INFO - Batch 541/1300 - Loss: 0.3638 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:30 - INFO - Processing batch 551/1300\n",
      "2025-06-10 03:17:31 - INFO - Batch 551/1300 - Loss: 0.2692 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:32 - INFO - Processing batch 561/1300\n",
      "2025-06-10 03:17:32 - INFO - Batch 561/1300 - Loss: 0.2571 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:33 - INFO - Processing batch 571/1300\n",
      "2025-06-10 03:17:34 - INFO - Batch 571/1300 - Loss: 0.8057 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:35 - INFO - Processing batch 581/1300\n",
      "2025-06-10 03:17:35 - INFO - Batch 581/1300 - Loss: 0.4203 - Avg batch time: 0.16s\n",
      "2025-06-10 03:17:37 - INFO - Processing batch 591/1300\n",
      "2025-06-10 03:17:37 - INFO - Batch 591/1300 - Loss: 0.3325 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:38 - INFO - Processing batch 601/1300\n",
      "2025-06-10 03:17:38 - INFO - Batch 601/1300 - Loss: 0.8436 - Avg batch time: 0.16s\n",
      "2025-06-10 03:17:40 - INFO - Processing batch 611/1300\n",
      "2025-06-10 03:17:40 - INFO - Batch 611/1300 - Loss: 0.1616 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:41 - INFO - Processing batch 621/1300\n",
      "2025-06-10 03:17:41 - INFO - Batch 621/1300 - Loss: 0.3660 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:43 - INFO - Processing batch 631/1300\n",
      "2025-06-10 03:17:43 - INFO - Batch 631/1300 - Loss: 0.4808 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:44 - INFO - Processing batch 641/1300\n",
      "2025-06-10 03:17:44 - INFO - Batch 641/1300 - Loss: 0.3145 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:46 - INFO - Processing batch 651/1300\n",
      "2025-06-10 03:17:46 - INFO - Batch 651/1300 - Loss: 0.3386 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:47 - INFO - Processing batch 661/1300\n",
      "2025-06-10 03:17:48 - INFO - Batch 661/1300 - Loss: 0.4095 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:49 - INFO - Processing batch 671/1300\n",
      "2025-06-10 03:17:49 - INFO - Batch 671/1300 - Loss: 0.9513 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:50 - INFO - Processing batch 681/1300\n",
      "2025-06-10 03:17:51 - INFO - Batch 681/1300 - Loss: 0.2035 - Avg batch time: 0.16s\n",
      "2025-06-10 03:17:52 - INFO - Processing batch 691/1300\n",
      "2025-06-10 03:17:52 - INFO - Batch 691/1300 - Loss: 0.6272 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:54 - INFO - Processing batch 701/1300\n",
      "2025-06-10 03:17:54 - INFO - Batch 701/1300 - Loss: 0.1293 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:55 - INFO - Processing batch 711/1300\n",
      "2025-06-10 03:17:55 - INFO - Batch 711/1300 - Loss: 0.3537 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:57 - INFO - Processing batch 721/1300\n",
      "2025-06-10 03:17:57 - INFO - Batch 721/1300 - Loss: 0.2210 - Avg batch time: 0.15s\n",
      "2025-06-10 03:17:58 - INFO - Processing batch 731/1300\n",
      "2025-06-10 03:17:58 - INFO - Batch 731/1300 - Loss: 0.2018 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:00 - INFO - Processing batch 741/1300\n",
      "2025-06-10 03:18:00 - INFO - Batch 741/1300 - Loss: 0.0959 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:01 - INFO - Processing batch 751/1300\n",
      "2025-06-10 03:18:01 - INFO - Batch 751/1300 - Loss: 0.4828 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:03 - INFO - Processing batch 761/1300\n",
      "2025-06-10 03:18:03 - INFO - Batch 761/1300 - Loss: 0.8708 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:04 - INFO - Processing batch 771/1300\n",
      "2025-06-10 03:18:04 - INFO - Batch 771/1300 - Loss: 0.1759 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:06 - INFO - Processing batch 781/1300\n",
      "2025-06-10 03:18:06 - INFO - Batch 781/1300 - Loss: 0.1297 - Avg batch time: 0.16s\n",
      "2025-06-10 03:18:07 - INFO - Processing batch 791/1300\n",
      "2025-06-10 03:18:08 - INFO - Batch 791/1300 - Loss: 0.1934 - Avg batch time: 0.18s\n",
      "2025-06-10 03:18:09 - INFO - Processing batch 801/1300\n",
      "2025-06-10 03:18:09 - INFO - Batch 801/1300 - Loss: 0.2502 - Avg batch time: 0.16s\n",
      "2025-06-10 03:18:11 - INFO - Processing batch 811/1300\n",
      "2025-06-10 03:18:11 - INFO - Batch 811/1300 - Loss: 0.5185 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:12 - INFO - Processing batch 821/1300\n",
      "2025-06-10 03:18:12 - INFO - Batch 821/1300 - Loss: 0.1342 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:14 - INFO - Processing batch 831/1300\n",
      "2025-06-10 03:18:14 - INFO - Batch 831/1300 - Loss: 0.5190 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:15 - INFO - Processing batch 841/1300\n",
      "2025-06-10 03:18:15 - INFO - Batch 841/1300 - Loss: 0.3948 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:17 - INFO - Processing batch 851/1300\n",
      "2025-06-10 03:18:17 - INFO - Batch 851/1300 - Loss: 1.5093 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:18 - INFO - Processing batch 861/1300\n",
      "2025-06-10 03:18:19 - INFO - Batch 861/1300 - Loss: 0.9124 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:20 - INFO - Processing batch 871/1300\n",
      "2025-06-10 03:18:20 - INFO - Batch 871/1300 - Loss: 0.1946 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:21 - INFO - Processing batch 881/1300\n",
      "2025-06-10 03:18:22 - INFO - Batch 881/1300 - Loss: 0.6226 - Avg batch time: 0.16s\n",
      "2025-06-10 03:18:23 - INFO - Processing batch 891/1300\n",
      "2025-06-10 03:18:23 - INFO - Batch 891/1300 - Loss: 0.0993 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:25 - INFO - Processing batch 901/1300\n",
      "2025-06-10 03:18:25 - INFO - Batch 901/1300 - Loss: 1.0663 - Avg batch time: 0.16s\n",
      "2025-06-10 03:18:26 - INFO - Processing batch 911/1300\n",
      "2025-06-10 03:18:26 - INFO - Batch 911/1300 - Loss: 0.4622 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:28 - INFO - Processing batch 921/1300\n",
      "2025-06-10 03:18:28 - INFO - Batch 921/1300 - Loss: 0.2502 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:29 - INFO - Processing batch 931/1300\n",
      "2025-06-10 03:18:29 - INFO - Batch 931/1300 - Loss: 0.1415 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:31 - INFO - Processing batch 941/1300\n",
      "2025-06-10 03:18:31 - INFO - Batch 941/1300 - Loss: 0.4495 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:32 - INFO - Processing batch 951/1300\n",
      "2025-06-10 03:18:32 - INFO - Batch 951/1300 - Loss: 0.9930 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:34 - INFO - Processing batch 961/1300\n",
      "2025-06-10 03:18:34 - INFO - Batch 961/1300 - Loss: 0.2498 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:35 - INFO - Processing batch 971/1300\n",
      "2025-06-10 03:18:35 - INFO - Batch 971/1300 - Loss: 0.6257 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:37 - INFO - Processing batch 981/1300\n",
      "2025-06-10 03:18:37 - INFO - Batch 981/1300 - Loss: 0.9538 - Avg batch time: 0.16s\n",
      "2025-06-10 03:18:38 - INFO - Processing batch 991/1300\n",
      "2025-06-10 03:18:39 - INFO - Batch 991/1300 - Loss: 0.5393 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:40 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 03:18:40 - INFO - Batch 1001/1300 - Loss: 0.7421 - Avg batch time: 0.16s\n",
      "2025-06-10 03:18:42 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 03:18:42 - INFO - Batch 1011/1300 - Loss: 0.3121 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:43 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 03:18:43 - INFO - Batch 1021/1300 - Loss: 0.5527 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:45 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 03:18:45 - INFO - Batch 1031/1300 - Loss: 0.7251 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:46 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 03:18:46 - INFO - Batch 1041/1300 - Loss: 0.2158 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:48 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 03:18:48 - INFO - Batch 1051/1300 - Loss: 0.3774 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:49 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 03:18:49 - INFO - Batch 1061/1300 - Loss: 0.2563 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:51 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 03:18:51 - INFO - Batch 1071/1300 - Loss: 0.4295 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:52 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 03:18:52 - INFO - Batch 1081/1300 - Loss: 0.1542 - Avg batch time: 0.16s\n",
      "2025-06-10 03:18:54 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 03:18:54 - INFO - Batch 1091/1300 - Loss: 0.4896 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:55 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 03:18:56 - INFO - Batch 1101/1300 - Loss: 0.8275 - Avg batch time: 0.16s\n",
      "2025-06-10 03:18:57 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 03:18:57 - INFO - Batch 1111/1300 - Loss: 0.5178 - Avg batch time: 0.15s\n",
      "2025-06-10 03:18:58 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 03:18:59 - INFO - Batch 1121/1300 - Loss: 0.2771 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:00 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 03:19:00 - INFO - Batch 1131/1300 - Loss: 0.9760 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:02 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 03:19:02 - INFO - Batch 1141/1300 - Loss: 0.6944 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:03 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 03:19:03 - INFO - Batch 1151/1300 - Loss: 1.0002 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:05 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 03:19:05 - INFO - Batch 1161/1300 - Loss: 0.1543 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:06 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 03:19:06 - INFO - Batch 1171/1300 - Loss: 0.1949 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:08 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 03:19:08 - INFO - Batch 1181/1300 - Loss: 0.7110 - Avg batch time: 0.16s\n",
      "2025-06-10 03:19:09 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 03:19:09 - INFO - Batch 1191/1300 - Loss: 0.2123 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:11 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 03:19:11 - INFO - Batch 1201/1300 - Loss: 0.8787 - Avg batch time: 0.16s\n",
      "2025-06-10 03:19:12 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 03:19:13 - INFO - Batch 1211/1300 - Loss: 0.9882 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:14 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 03:19:14 - INFO - Batch 1221/1300 - Loss: 0.4003 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:15 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 03:19:16 - INFO - Batch 1231/1300 - Loss: 0.3886 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:17 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 03:19:17 - INFO - Batch 1241/1300 - Loss: 0.2028 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:19 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 03:19:19 - INFO - Batch 1251/1300 - Loss: 0.5564 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:20 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 03:19:20 - INFO - Batch 1261/1300 - Loss: 0.3972 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:22 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 03:19:22 - INFO - Batch 1271/1300 - Loss: 1.3839 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:23 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 03:19:23 - INFO - Batch 1281/1300 - Loss: 0.2997 - Avg batch time: 0.16s\n",
      "2025-06-10 03:19:25 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 03:19:25 - INFO - Batch 1291/1300 - Loss: 0.4867 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:26 - INFO - \n",
      "Epoch 10 training completed in 200.93s\n",
      "2025-06-10 03:19:26 - INFO - Average training loss: 0.4573\n",
      "2025-06-10 03:19:44 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:  11%| | 11/100 [36:22<5:23:48, 218.29s/it, train_loss=0.4573, val_loss=0.4786, best_val_f1=0.3125, lr=5.00e-05, 2025-06-10 03:19:44 - INFO - \n",
      "Epoch 11/100 - Training phase\n",
      "2025-06-10 03:19:44 - INFO - Processing batch 1/1300\n",
      "2025-06-10 03:19:44 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 03:19:44 - INFO - Batch 1/1300 - Loss: 0.6674 - Avg batch time: 0.16s\n",
      "2025-06-10 03:19:46 - INFO - Processing batch 11/1300\n",
      "2025-06-10 03:19:46 - INFO - Batch 11/1300 - Loss: 0.6038 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:47 - INFO - Processing batch 21/1300\n",
      "2025-06-10 03:19:47 - INFO - Batch 21/1300 - Loss: 0.7771 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:49 - INFO - Processing batch 31/1300\n",
      "2025-06-10 03:19:49 - INFO - Batch 31/1300 - Loss: 0.5145 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:50 - INFO - Processing batch 41/1300\n",
      "2025-06-10 03:19:51 - INFO - Batch 41/1300 - Loss: 0.0953 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:52 - INFO - Processing batch 51/1300\n",
      "2025-06-10 03:19:52 - INFO - Batch 51/1300 - Loss: 0.9508 - Avg batch time: 0.16s\n",
      "2025-06-10 03:19:54 - INFO - Processing batch 61/1300\n",
      "2025-06-10 03:19:54 - INFO - Batch 61/1300 - Loss: 0.3107 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:55 - INFO - Processing batch 71/1300\n",
      "2025-06-10 03:19:55 - INFO - Batch 71/1300 - Loss: 0.3854 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:57 - INFO - Processing batch 81/1300\n",
      "2025-06-10 03:19:57 - INFO - Batch 81/1300 - Loss: 0.2589 - Avg batch time: 0.15s\n",
      "2025-06-10 03:19:58 - INFO - Processing batch 91/1300\n",
      "2025-06-10 03:19:58 - INFO - Batch 91/1300 - Loss: 0.4447 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:00 - INFO - Processing batch 101/1300\n",
      "2025-06-10 03:20:00 - INFO - Batch 101/1300 - Loss: 0.2283 - Avg batch time: 0.16s\n",
      "2025-06-10 03:20:01 - INFO - Processing batch 111/1300\n",
      "2025-06-10 03:20:01 - INFO - Batch 111/1300 - Loss: 0.1901 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:03 - INFO - Processing batch 121/1300\n",
      "2025-06-10 03:20:03 - INFO - Batch 121/1300 - Loss: 0.2488 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:04 - INFO - Processing batch 131/1300\n",
      "2025-06-10 03:20:04 - INFO - Batch 131/1300 - Loss: 0.3451 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:06 - INFO - Processing batch 141/1300\n",
      "2025-06-10 03:20:06 - INFO - Batch 141/1300 - Loss: 0.3701 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:07 - INFO - Processing batch 151/1300\n",
      "2025-06-10 03:20:08 - INFO - Batch 151/1300 - Loss: 0.1128 - Avg batch time: 0.16s\n",
      "2025-06-10 03:20:09 - INFO - Processing batch 161/1300\n",
      "2025-06-10 03:20:09 - INFO - Batch 161/1300 - Loss: 0.5847 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:10 - INFO - Processing batch 171/1300\n",
      "2025-06-10 03:20:11 - INFO - Batch 171/1300 - Loss: 0.4136 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:12 - INFO - Processing batch 181/1300\n",
      "2025-06-10 03:20:12 - INFO - Batch 181/1300 - Loss: 0.4890 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:14 - INFO - Processing batch 191/1300\n",
      "2025-06-10 03:20:14 - INFO - Batch 191/1300 - Loss: 0.4338 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:15 - INFO - Processing batch 201/1300\n",
      "2025-06-10 03:20:15 - INFO - Batch 201/1300 - Loss: 0.4753 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:17 - INFO - Processing batch 211/1300\n",
      "2025-06-10 03:20:17 - INFO - Batch 211/1300 - Loss: 0.1859 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:18 - INFO - Processing batch 221/1300\n",
      "2025-06-10 03:20:18 - INFO - Batch 221/1300 - Loss: 0.5890 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:20 - INFO - Processing batch 231/1300\n",
      "2025-06-10 03:20:20 - INFO - Batch 231/1300 - Loss: 0.9316 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:21 - INFO - Processing batch 241/1300\n",
      "2025-06-10 03:20:21 - INFO - Batch 241/1300 - Loss: 0.1165 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:23 - INFO - Processing batch 251/1300\n",
      "2025-06-10 03:20:23 - INFO - Batch 251/1300 - Loss: 0.1770 - Avg batch time: 0.16s\n",
      "2025-06-10 03:20:24 - INFO - Processing batch 261/1300\n",
      "2025-06-10 03:20:24 - INFO - Batch 261/1300 - Loss: 0.6163 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:26 - INFO - Processing batch 271/1300\n",
      "2025-06-10 03:20:26 - INFO - Batch 271/1300 - Loss: 0.3637 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:27 - INFO - Processing batch 281/1300\n",
      "2025-06-10 03:20:28 - INFO - Batch 281/1300 - Loss: 0.1913 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:29 - INFO - Processing batch 291/1300\n",
      "2025-06-10 03:20:29 - INFO - Batch 291/1300 - Loss: 0.3576 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:30 - INFO - Processing batch 301/1300\n",
      "2025-06-10 03:20:31 - INFO - Batch 301/1300 - Loss: 0.6886 - Avg batch time: 0.16s\n",
      "2025-06-10 03:20:32 - INFO - Processing batch 311/1300\n",
      "2025-06-10 03:20:32 - INFO - Batch 311/1300 - Loss: 0.3902 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:34 - INFO - Processing batch 321/1300\n",
      "2025-06-10 03:20:34 - INFO - Batch 321/1300 - Loss: 0.9423 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:35 - INFO - Processing batch 331/1300\n",
      "2025-06-10 03:20:35 - INFO - Batch 331/1300 - Loss: 0.2471 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:37 - INFO - Processing batch 341/1300\n",
      "2025-06-10 03:20:37 - INFO - Batch 341/1300 - Loss: 1.9295 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:38 - INFO - Processing batch 351/1300\n",
      "2025-06-10 03:20:38 - INFO - Batch 351/1300 - Loss: 0.4941 - Avg batch time: 0.16s\n",
      "2025-06-10 03:20:40 - INFO - Processing batch 361/1300\n",
      "2025-06-10 03:20:40 - INFO - Batch 361/1300 - Loss: 0.4928 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:41 - INFO - Processing batch 371/1300\n",
      "2025-06-10 03:20:41 - INFO - Batch 371/1300 - Loss: 0.2702 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:43 - INFO - Processing batch 381/1300\n",
      "2025-06-10 03:20:43 - INFO - Batch 381/1300 - Loss: 0.6600 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:44 - INFO - Processing batch 391/1300\n",
      "2025-06-10 03:20:44 - INFO - Batch 391/1300 - Loss: 0.0796 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:46 - INFO - Processing batch 401/1300\n",
      "2025-06-10 03:20:46 - INFO - Batch 401/1300 - Loss: 0.2463 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:47 - INFO - Processing batch 411/1300\n",
      "2025-06-10 03:20:48 - INFO - Batch 411/1300 - Loss: 0.4580 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:49 - INFO - Processing batch 421/1300\n",
      "2025-06-10 03:20:49 - INFO - Batch 421/1300 - Loss: 0.3794 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:50 - INFO - Processing batch 431/1300\n",
      "2025-06-10 03:20:51 - INFO - Batch 431/1300 - Loss: 0.5789 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:52 - INFO - Processing batch 441/1300\n",
      "2025-06-10 03:20:52 - INFO - Batch 441/1300 - Loss: 0.1750 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:54 - INFO - Processing batch 451/1300\n",
      "2025-06-10 03:20:54 - INFO - Batch 451/1300 - Loss: 0.4433 - Avg batch time: 0.16s\n",
      "2025-06-10 03:20:55 - INFO - Processing batch 461/1300\n",
      "2025-06-10 03:20:55 - INFO - Batch 461/1300 - Loss: 0.4538 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:57 - INFO - Processing batch 471/1300\n",
      "2025-06-10 03:20:57 - INFO - Batch 471/1300 - Loss: 0.4718 - Avg batch time: 0.15s\n",
      "2025-06-10 03:20:58 - INFO - Processing batch 481/1300\n",
      "2025-06-10 03:20:58 - INFO - Batch 481/1300 - Loss: 0.2700 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:00 - INFO - Processing batch 491/1300\n",
      "2025-06-10 03:21:00 - INFO - Batch 491/1300 - Loss: 0.3791 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:01 - INFO - Processing batch 501/1300\n",
      "2025-06-10 03:21:01 - INFO - Batch 501/1300 - Loss: 0.1404 - Avg batch time: 0.16s\n",
      "2025-06-10 03:21:03 - INFO - Processing batch 511/1300\n",
      "2025-06-10 03:21:03 - INFO - Batch 511/1300 - Loss: 0.9635 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:04 - INFO - Processing batch 521/1300\n",
      "2025-06-10 03:21:04 - INFO - Batch 521/1300 - Loss: 0.5900 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:06 - INFO - Processing batch 531/1300\n",
      "2025-06-10 03:21:06 - INFO - Batch 531/1300 - Loss: 0.5258 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:07 - INFO - Processing batch 541/1300\n",
      "2025-06-10 03:21:08 - INFO - Batch 541/1300 - Loss: 0.1630 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:09 - INFO - Processing batch 551/1300\n",
      "2025-06-10 03:21:09 - INFO - Batch 551/1300 - Loss: 0.5581 - Avg batch time: 0.16s\n",
      "2025-06-10 03:21:10 - INFO - Processing batch 561/1300\n",
      "2025-06-10 03:21:11 - INFO - Batch 561/1300 - Loss: 0.2490 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:12 - INFO - Processing batch 571/1300\n",
      "2025-06-10 03:21:12 - INFO - Batch 571/1300 - Loss: 1.3802 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:14 - INFO - Processing batch 581/1300\n",
      "2025-06-10 03:21:14 - INFO - Batch 581/1300 - Loss: 0.3833 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:15 - INFO - Processing batch 591/1300\n",
      "2025-06-10 03:21:15 - INFO - Batch 591/1300 - Loss: 0.3779 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:17 - INFO - Processing batch 601/1300\n",
      "2025-06-10 03:21:17 - INFO - Batch 601/1300 - Loss: 0.2840 - Avg batch time: 0.16s\n",
      "2025-06-10 03:21:18 - INFO - Processing batch 611/1300\n",
      "2025-06-10 03:21:18 - INFO - Batch 611/1300 - Loss: 0.1946 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:20 - INFO - Processing batch 621/1300\n",
      "2025-06-10 03:21:20 - INFO - Batch 621/1300 - Loss: 0.2783 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:21 - INFO - Processing batch 631/1300\n",
      "2025-06-10 03:21:21 - INFO - Batch 631/1300 - Loss: 0.6348 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:23 - INFO - Processing batch 641/1300\n",
      "2025-06-10 03:21:23 - INFO - Batch 641/1300 - Loss: 0.6247 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:24 - INFO - Processing batch 651/1300\n",
      "2025-06-10 03:21:24 - INFO - Batch 651/1300 - Loss: 0.4549 - Avg batch time: 0.16s\n",
      "2025-06-10 03:21:26 - INFO - Processing batch 661/1300\n",
      "2025-06-10 03:21:26 - INFO - Batch 661/1300 - Loss: 0.7061 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:27 - INFO - Processing batch 671/1300\n",
      "2025-06-10 03:21:28 - INFO - Batch 671/1300 - Loss: 0.2581 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:29 - INFO - Processing batch 681/1300\n",
      "2025-06-10 03:21:29 - INFO - Batch 681/1300 - Loss: 0.7271 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:30 - INFO - Processing batch 691/1300\n",
      "2025-06-10 03:21:31 - INFO - Batch 691/1300 - Loss: 0.9226 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:32 - INFO - Processing batch 701/1300\n",
      "2025-06-10 03:21:32 - INFO - Batch 701/1300 - Loss: 0.9498 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:34 - INFO - Processing batch 711/1300\n",
      "2025-06-10 03:21:34 - INFO - Batch 711/1300 - Loss: 0.5014 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:35 - INFO - Processing batch 721/1300\n",
      "2025-06-10 03:21:35 - INFO - Batch 721/1300 - Loss: 0.6396 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:37 - INFO - Processing batch 731/1300\n",
      "2025-06-10 03:21:37 - INFO - Batch 731/1300 - Loss: 0.1816 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:38 - INFO - Processing batch 741/1300\n",
      "2025-06-10 03:21:38 - INFO - Batch 741/1300 - Loss: 0.5387 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:40 - INFO - Processing batch 751/1300\n",
      "2025-06-10 03:21:40 - INFO - Batch 751/1300 - Loss: 0.4360 - Avg batch time: 0.16s\n",
      "2025-06-10 03:21:41 - INFO - Processing batch 761/1300\n",
      "2025-06-10 03:21:41 - INFO - Batch 761/1300 - Loss: 0.4328 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:43 - INFO - Processing batch 771/1300\n",
      "2025-06-10 03:21:43 - INFO - Batch 771/1300 - Loss: 0.9689 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:44 - INFO - Processing batch 781/1300\n",
      "2025-06-10 03:21:44 - INFO - Batch 781/1300 - Loss: 0.1661 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:46 - INFO - Processing batch 791/1300\n",
      "2025-06-10 03:21:46 - INFO - Batch 791/1300 - Loss: 0.2745 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:47 - INFO - Processing batch 801/1300\n",
      "2025-06-10 03:21:48 - INFO - Batch 801/1300 - Loss: 0.6652 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:49 - INFO - Processing batch 811/1300\n",
      "2025-06-10 03:21:49 - INFO - Batch 811/1300 - Loss: 0.2001 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:50 - INFO - Processing batch 821/1300\n",
      "2025-06-10 03:21:51 - INFO - Batch 821/1300 - Loss: 0.2642 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:52 - INFO - Processing batch 831/1300\n",
      "2025-06-10 03:21:52 - INFO - Batch 831/1300 - Loss: 0.7221 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:54 - INFO - Processing batch 841/1300\n",
      "2025-06-10 03:21:54 - INFO - Batch 841/1300 - Loss: 0.8044 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:55 - INFO - Processing batch 851/1300\n",
      "2025-06-10 03:21:55 - INFO - Batch 851/1300 - Loss: 0.3455 - Avg batch time: 0.16s\n",
      "2025-06-10 03:21:57 - INFO - Processing batch 861/1300\n",
      "2025-06-10 03:21:57 - INFO - Batch 861/1300 - Loss: 0.0733 - Avg batch time: 0.15s\n",
      "2025-06-10 03:21:58 - INFO - Processing batch 871/1300\n",
      "2025-06-10 03:21:58 - INFO - Batch 871/1300 - Loss: 0.3611 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:00 - INFO - Processing batch 881/1300\n",
      "2025-06-10 03:22:00 - INFO - Batch 881/1300 - Loss: 0.4577 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:01 - INFO - Processing batch 891/1300\n",
      "2025-06-10 03:22:01 - INFO - Batch 891/1300 - Loss: 0.4132 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:03 - INFO - Processing batch 901/1300\n",
      "2025-06-10 03:22:03 - INFO - Batch 901/1300 - Loss: 0.2100 - Avg batch time: 0.16s\n",
      "2025-06-10 03:22:04 - INFO - Processing batch 911/1300\n",
      "2025-06-10 03:22:04 - INFO - Batch 911/1300 - Loss: 0.2087 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:06 - INFO - Processing batch 921/1300\n",
      "2025-06-10 03:22:06 - INFO - Batch 921/1300 - Loss: 0.4888 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:07 - INFO - Processing batch 931/1300\n",
      "2025-06-10 03:22:08 - INFO - Batch 931/1300 - Loss: 0.8423 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:09 - INFO - Processing batch 941/1300\n",
      "2025-06-10 03:22:09 - INFO - Batch 941/1300 - Loss: 0.2094 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:10 - INFO - Processing batch 951/1300\n",
      "2025-06-10 03:22:11 - INFO - Batch 951/1300 - Loss: 0.4777 - Avg batch time: 0.16s\n",
      "2025-06-10 03:22:12 - INFO - Processing batch 961/1300\n",
      "2025-06-10 03:22:12 - INFO - Batch 961/1300 - Loss: 0.5737 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:14 - INFO - Processing batch 971/1300\n",
      "2025-06-10 03:22:14 - INFO - Batch 971/1300 - Loss: 0.6421 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:15 - INFO - Processing batch 981/1300\n",
      "2025-06-10 03:22:15 - INFO - Batch 981/1300 - Loss: 0.3378 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:17 - INFO - Processing batch 991/1300\n",
      "2025-06-10 03:22:17 - INFO - Batch 991/1300 - Loss: 0.3057 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:18 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 03:22:18 - INFO - Batch 1001/1300 - Loss: 0.5550 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:20 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 03:22:20 - INFO - Batch 1011/1300 - Loss: 0.5431 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:21 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 03:22:21 - INFO - Batch 1021/1300 - Loss: 0.1484 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:23 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 03:22:23 - INFO - Batch 1031/1300 - Loss: 0.1924 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:24 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 03:22:24 - INFO - Batch 1041/1300 - Loss: 0.5553 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:26 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 03:22:26 - INFO - Batch 1051/1300 - Loss: 0.3872 - Avg batch time: 0.16s\n",
      "2025-06-10 03:22:27 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 03:22:28 - INFO - Batch 1061/1300 - Loss: 0.3886 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:29 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 03:22:29 - INFO - Batch 1071/1300 - Loss: 0.9263 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:30 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 03:22:31 - INFO - Batch 1081/1300 - Loss: 0.2291 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:32 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 03:22:32 - INFO - Batch 1091/1300 - Loss: 0.4509 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:34 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 03:22:34 - INFO - Batch 1101/1300 - Loss: 0.3158 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:35 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 03:22:35 - INFO - Batch 1111/1300 - Loss: 0.7376 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:37 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 03:22:37 - INFO - Batch 1121/1300 - Loss: 0.3834 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:38 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 03:22:38 - INFO - Batch 1131/1300 - Loss: 0.2350 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:40 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 03:22:40 - INFO - Batch 1141/1300 - Loss: 0.7787 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:41 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 03:22:41 - INFO - Batch 1151/1300 - Loss: 0.7884 - Avg batch time: 0.16s\n",
      "2025-06-10 03:22:43 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 03:22:43 - INFO - Batch 1161/1300 - Loss: 0.3824 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:44 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 03:22:44 - INFO - Batch 1171/1300 - Loss: 0.6124 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:46 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 03:22:46 - INFO - Batch 1181/1300 - Loss: 0.3306 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:47 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 03:22:48 - INFO - Batch 1191/1300 - Loss: 0.4678 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:49 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 03:22:49 - INFO - Batch 1201/1300 - Loss: 0.5430 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:50 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 03:22:51 - INFO - Batch 1211/1300 - Loss: 0.5823 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:52 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 03:22:52 - INFO - Batch 1221/1300 - Loss: 0.5653 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:53 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 03:22:54 - INFO - Batch 1231/1300 - Loss: 0.5507 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:55 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 03:22:55 - INFO - Batch 1241/1300 - Loss: 0.1989 - Avg batch time: 0.15s\n",
      "2025-06-10 03:22:57 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 03:22:57 - INFO - Batch 1251/1300 - Loss: 0.3959 - Avg batch time: 0.16s\n",
      "2025-06-10 03:22:58 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 03:22:58 - INFO - Batch 1261/1300 - Loss: 0.4711 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:00 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 03:23:00 - INFO - Batch 1271/1300 - Loss: 0.2431 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:01 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 03:23:01 - INFO - Batch 1281/1300 - Loss: 0.5032 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:03 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 03:23:03 - INFO - Batch 1291/1300 - Loss: 0.5147 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:04 - INFO - \n",
      "Epoch 11 training completed in 200.25s\n",
      "2025-06-10 03:23:04 - INFO - Average training loss: 0.4624\n",
      "2025-06-10 03:23:22 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:  12%| | 12/100 [40:00<5:20:01, 218.20s/it, train_loss=0.4624, val_loss=0.4640, best_val_f1=0.3125, lr=5.00e-05, 2025-06-10 03:23:22 - INFO - \n",
      "Epoch 12/100 - Training phase\n",
      "2025-06-10 03:23:22 - INFO - Processing batch 1/1300\n",
      "2025-06-10 03:23:22 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 03:23:22 - INFO - Batch 1/1300 - Loss: 0.5089 - Avg batch time: 0.16s\n",
      "2025-06-10 03:23:24 - INFO - Processing batch 11/1300\n",
      "2025-06-10 03:23:24 - INFO - Batch 11/1300 - Loss: 0.2126 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:25 - INFO - Processing batch 21/1300\n",
      "2025-06-10 03:23:25 - INFO - Batch 21/1300 - Loss: 0.9549 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:27 - INFO - Processing batch 31/1300\n",
      "2025-06-10 03:23:27 - INFO - Batch 31/1300 - Loss: 0.6241 - Avg batch time: 0.16s\n",
      "2025-06-10 03:23:28 - INFO - Processing batch 41/1300\n",
      "2025-06-10 03:23:29 - INFO - Batch 41/1300 - Loss: 0.5078 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:30 - INFO - Processing batch 51/1300\n",
      "2025-06-10 03:23:30 - INFO - Batch 51/1300 - Loss: 0.1196 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:32 - INFO - Processing batch 61/1300\n",
      "2025-06-10 03:23:32 - INFO - Batch 61/1300 - Loss: 0.3773 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:33 - INFO - Processing batch 71/1300\n",
      "2025-06-10 03:23:33 - INFO - Batch 71/1300 - Loss: 0.3736 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:35 - INFO - Processing batch 81/1300\n",
      "2025-06-10 03:23:35 - INFO - Batch 81/1300 - Loss: 0.8280 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:36 - INFO - Processing batch 91/1300\n",
      "2025-06-10 03:23:36 - INFO - Batch 91/1300 - Loss: 0.2337 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:38 - INFO - Processing batch 101/1300\n",
      "2025-06-10 03:23:38 - INFO - Batch 101/1300 - Loss: 0.4250 - Avg batch time: 0.16s\n",
      "2025-06-10 03:23:39 - INFO - Processing batch 111/1300\n",
      "2025-06-10 03:23:39 - INFO - Batch 111/1300 - Loss: 0.1926 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:41 - INFO - Processing batch 121/1300\n",
      "2025-06-10 03:23:41 - INFO - Batch 121/1300 - Loss: 0.9936 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:42 - INFO - Processing batch 131/1300\n",
      "2025-06-10 03:23:43 - INFO - Batch 131/1300 - Loss: 0.3075 - Avg batch time: 0.16s\n",
      "2025-06-10 03:23:44 - INFO - Processing batch 141/1300\n",
      "2025-06-10 03:23:44 - INFO - Batch 141/1300 - Loss: 0.6324 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:45 - INFO - Processing batch 151/1300\n",
      "2025-06-10 03:23:46 - INFO - Batch 151/1300 - Loss: 0.8318 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:47 - INFO - Processing batch 161/1300\n",
      "2025-06-10 03:23:47 - INFO - Batch 161/1300 - Loss: 0.3828 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:49 - INFO - Processing batch 171/1300\n",
      "2025-06-10 03:23:49 - INFO - Batch 171/1300 - Loss: 0.1669 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:50 - INFO - Processing batch 181/1300\n",
      "2025-06-10 03:23:50 - INFO - Batch 181/1300 - Loss: 0.3299 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:52 - INFO - Processing batch 191/1300\n",
      "2025-06-10 03:23:52 - INFO - Batch 191/1300 - Loss: 0.5622 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:53 - INFO - Processing batch 201/1300\n",
      "2025-06-10 03:23:53 - INFO - Batch 201/1300 - Loss: 0.4259 - Avg batch time: 0.16s\n",
      "2025-06-10 03:23:55 - INFO - Processing batch 211/1300\n",
      "2025-06-10 03:23:55 - INFO - Batch 211/1300 - Loss: 0.6167 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:56 - INFO - Processing batch 221/1300\n",
      "2025-06-10 03:23:56 - INFO - Batch 221/1300 - Loss: 0.2937 - Avg batch time: 0.15s\n",
      "2025-06-10 03:23:58 - INFO - Processing batch 231/1300\n",
      "2025-06-10 03:23:58 - INFO - Batch 231/1300 - Loss: 0.7146 - Avg batch time: 0.16s\n",
      "2025-06-10 03:23:59 - INFO - Processing batch 241/1300\n",
      "2025-06-10 03:23:59 - INFO - Batch 241/1300 - Loss: 0.3180 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:01 - INFO - Processing batch 251/1300\n",
      "2025-06-10 03:24:01 - INFO - Batch 251/1300 - Loss: 0.5381 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:02 - INFO - Processing batch 261/1300\n",
      "2025-06-10 03:24:03 - INFO - Batch 261/1300 - Loss: 0.2459 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:04 - INFO - Processing batch 271/1300\n",
      "2025-06-10 03:24:04 - INFO - Batch 271/1300 - Loss: 0.4874 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:05 - INFO - Processing batch 281/1300\n",
      "2025-06-10 03:24:06 - INFO - Batch 281/1300 - Loss: 0.1266 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:07 - INFO - Processing batch 291/1300\n",
      "2025-06-10 03:24:07 - INFO - Batch 291/1300 - Loss: 0.6735 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:09 - INFO - Processing batch 301/1300\n",
      "2025-06-10 03:24:09 - INFO - Batch 301/1300 - Loss: 0.1427 - Avg batch time: 0.16s\n",
      "2025-06-10 03:24:10 - INFO - Processing batch 311/1300\n",
      "2025-06-10 03:24:10 - INFO - Batch 311/1300 - Loss: 0.6407 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:12 - INFO - Processing batch 321/1300\n",
      "2025-06-10 03:24:12 - INFO - Batch 321/1300 - Loss: 0.4313 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:13 - INFO - Processing batch 331/1300\n",
      "2025-06-10 03:24:13 - INFO - Batch 331/1300 - Loss: 0.2197 - Avg batch time: 0.16s\n",
      "2025-06-10 03:24:15 - INFO - Processing batch 341/1300\n",
      "2025-06-10 03:24:15 - INFO - Batch 341/1300 - Loss: 0.3977 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:16 - INFO - Processing batch 351/1300\n",
      "2025-06-10 03:24:16 - INFO - Batch 351/1300 - Loss: 0.4219 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:18 - INFO - Processing batch 361/1300\n",
      "2025-06-10 03:24:18 - INFO - Batch 361/1300 - Loss: 0.5888 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:19 - INFO - Processing batch 371/1300\n",
      "2025-06-10 03:24:19 - INFO - Batch 371/1300 - Loss: 0.4071 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:21 - INFO - Processing batch 381/1300\n",
      "2025-06-10 03:24:21 - INFO - Batch 381/1300 - Loss: 0.4136 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:22 - INFO - Processing batch 391/1300\n",
      "2025-06-10 03:24:23 - INFO - Batch 391/1300 - Loss: 0.7044 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:24 - INFO - Processing batch 401/1300\n",
      "2025-06-10 03:24:24 - INFO - Batch 401/1300 - Loss: 0.8768 - Avg batch time: 0.16s\n",
      "2025-06-10 03:24:25 - INFO - Processing batch 411/1300\n",
      "2025-06-10 03:24:26 - INFO - Batch 411/1300 - Loss: 0.4648 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:27 - INFO - Processing batch 421/1300\n",
      "2025-06-10 03:24:27 - INFO - Batch 421/1300 - Loss: 0.3721 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:29 - INFO - Processing batch 431/1300\n",
      "2025-06-10 03:24:29 - INFO - Batch 431/1300 - Loss: 0.2441 - Avg batch time: 0.16s\n",
      "2025-06-10 03:24:30 - INFO - Processing batch 441/1300\n",
      "2025-06-10 03:24:30 - INFO - Batch 441/1300 - Loss: 0.3193 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:32 - INFO - Processing batch 451/1300\n",
      "2025-06-10 03:24:32 - INFO - Batch 451/1300 - Loss: 0.5111 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:33 - INFO - Processing batch 461/1300\n",
      "2025-06-10 03:24:33 - INFO - Batch 461/1300 - Loss: 0.8165 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:35 - INFO - Processing batch 471/1300\n",
      "2025-06-10 03:24:35 - INFO - Batch 471/1300 - Loss: 0.4313 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:36 - INFO - Processing batch 481/1300\n",
      "2025-06-10 03:24:36 - INFO - Batch 481/1300 - Loss: 0.1145 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:38 - INFO - Processing batch 491/1300\n",
      "2025-06-10 03:24:38 - INFO - Batch 491/1300 - Loss: 0.3634 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:39 - INFO - Processing batch 501/1300\n",
      "2025-06-10 03:24:40 - INFO - Batch 501/1300 - Loss: 0.2642 - Avg batch time: 0.16s\n",
      "2025-06-10 03:24:41 - INFO - Processing batch 511/1300\n",
      "2025-06-10 03:24:41 - INFO - Batch 511/1300 - Loss: 0.4740 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:42 - INFO - Processing batch 521/1300\n",
      "2025-06-10 03:24:43 - INFO - Batch 521/1300 - Loss: 0.5903 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:44 - INFO - Processing batch 531/1300\n",
      "2025-06-10 03:24:44 - INFO - Batch 531/1300 - Loss: 0.3871 - Avg batch time: 0.16s\n",
      "2025-06-10 03:24:46 - INFO - Processing batch 541/1300\n",
      "2025-06-10 03:24:46 - INFO - Batch 541/1300 - Loss: 0.3261 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:47 - INFO - Processing batch 551/1300\n",
      "2025-06-10 03:24:47 - INFO - Batch 551/1300 - Loss: 0.4510 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:49 - INFO - Processing batch 561/1300\n",
      "2025-06-10 03:24:49 - INFO - Batch 561/1300 - Loss: 0.0870 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:50 - INFO - Processing batch 571/1300\n",
      "2025-06-10 03:24:50 - INFO - Batch 571/1300 - Loss: 0.3581 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:52 - INFO - Processing batch 581/1300\n",
      "2025-06-10 03:24:52 - INFO - Batch 581/1300 - Loss: 1.0910 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:53 - INFO - Processing batch 591/1300\n",
      "2025-06-10 03:24:53 - INFO - Batch 591/1300 - Loss: 0.2595 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:55 - INFO - Processing batch 601/1300\n",
      "2025-06-10 03:24:55 - INFO - Batch 601/1300 - Loss: 0.6505 - Avg batch time: 0.16s\n",
      "2025-06-10 03:24:56 - INFO - Processing batch 611/1300\n",
      "2025-06-10 03:24:56 - INFO - Batch 611/1300 - Loss: 0.6373 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:58 - INFO - Processing batch 621/1300\n",
      "2025-06-10 03:24:58 - INFO - Batch 621/1300 - Loss: 0.1302 - Avg batch time: 0.15s\n",
      "2025-06-10 03:24:59 - INFO - Processing batch 631/1300\n",
      "2025-06-10 03:25:00 - INFO - Batch 631/1300 - Loss: 0.6433 - Avg batch time: 0.16s\n",
      "2025-06-10 03:25:01 - INFO - Processing batch 641/1300\n",
      "2025-06-10 03:25:01 - INFO - Batch 641/1300 - Loss: 0.8273 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:02 - INFO - Processing batch 651/1300\n",
      "2025-06-10 03:25:03 - INFO - Batch 651/1300 - Loss: 1.4347 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:04 - INFO - Processing batch 661/1300\n",
      "2025-06-10 03:25:04 - INFO - Batch 661/1300 - Loss: 0.3677 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:06 - INFO - Processing batch 671/1300\n",
      "2025-06-10 03:25:06 - INFO - Batch 671/1300 - Loss: 0.1066 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:07 - INFO - Processing batch 681/1300\n",
      "2025-06-10 03:25:07 - INFO - Batch 681/1300 - Loss: 0.5064 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:09 - INFO - Processing batch 691/1300\n",
      "2025-06-10 03:25:09 - INFO - Batch 691/1300 - Loss: 0.3685 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:10 - INFO - Processing batch 701/1300\n",
      "2025-06-10 03:25:10 - INFO - Batch 701/1300 - Loss: 0.0877 - Avg batch time: 0.16s\n",
      "2025-06-10 03:25:12 - INFO - Processing batch 711/1300\n",
      "2025-06-10 03:25:12 - INFO - Batch 711/1300 - Loss: 0.5122 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:13 - INFO - Processing batch 721/1300\n",
      "2025-06-10 03:25:13 - INFO - Batch 721/1300 - Loss: 0.7782 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:15 - INFO - Processing batch 731/1300\n",
      "2025-06-10 03:25:15 - INFO - Batch 731/1300 - Loss: 0.6555 - Avg batch time: 0.16s\n",
      "2025-06-10 03:25:16 - INFO - Processing batch 741/1300\n",
      "2025-06-10 03:25:16 - INFO - Batch 741/1300 - Loss: 0.4489 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:18 - INFO - Processing batch 751/1300\n",
      "2025-06-10 03:25:18 - INFO - Batch 751/1300 - Loss: 1.2474 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:19 - INFO - Processing batch 761/1300\n",
      "2025-06-10 03:25:20 - INFO - Batch 761/1300 - Loss: 0.2393 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:21 - INFO - Processing batch 771/1300\n",
      "2025-06-10 03:25:21 - INFO - Batch 771/1300 - Loss: 0.5533 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:22 - INFO - Processing batch 781/1300\n",
      "2025-06-10 03:25:23 - INFO - Batch 781/1300 - Loss: 0.1582 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:24 - INFO - Processing batch 791/1300\n",
      "2025-06-10 03:25:24 - INFO - Batch 791/1300 - Loss: 1.0612 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:26 - INFO - Processing batch 801/1300\n",
      "2025-06-10 03:25:26 - INFO - Batch 801/1300 - Loss: 0.6185 - Avg batch time: 0.16s\n",
      "2025-06-10 03:25:27 - INFO - Processing batch 811/1300\n",
      "2025-06-10 03:25:27 - INFO - Batch 811/1300 - Loss: 0.3284 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:29 - INFO - Processing batch 821/1300\n",
      "2025-06-10 03:25:29 - INFO - Batch 821/1300 - Loss: 0.5157 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:30 - INFO - Processing batch 831/1300\n",
      "2025-06-10 03:25:30 - INFO - Batch 831/1300 - Loss: 0.5706 - Avg batch time: 0.16s\n",
      "2025-06-10 03:25:32 - INFO - Processing batch 841/1300\n",
      "2025-06-10 03:25:32 - INFO - Batch 841/1300 - Loss: 0.4949 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:33 - INFO - Processing batch 851/1300\n",
      "2025-06-10 03:25:33 - INFO - Batch 851/1300 - Loss: 0.1625 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:35 - INFO - Processing batch 861/1300\n",
      "2025-06-10 03:25:35 - INFO - Batch 861/1300 - Loss: 0.6254 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:36 - INFO - Processing batch 871/1300\n",
      "2025-06-10 03:25:36 - INFO - Batch 871/1300 - Loss: 0.1246 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:38 - INFO - Processing batch 881/1300\n",
      "2025-06-10 03:25:38 - INFO - Batch 881/1300 - Loss: 0.5929 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:39 - INFO - Processing batch 891/1300\n",
      "2025-06-10 03:25:40 - INFO - Batch 891/1300 - Loss: 0.3437 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:41 - INFO - Processing batch 901/1300\n",
      "2025-06-10 03:25:41 - INFO - Batch 901/1300 - Loss: 0.1576 - Avg batch time: 0.16s\n",
      "2025-06-10 03:25:42 - INFO - Processing batch 911/1300\n",
      "2025-06-10 03:25:43 - INFO - Batch 911/1300 - Loss: 0.1712 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:44 - INFO - Processing batch 921/1300\n",
      "2025-06-10 03:25:44 - INFO - Batch 921/1300 - Loss: 0.2487 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:46 - INFO - Processing batch 931/1300\n",
      "2025-06-10 03:25:46 - INFO - Batch 931/1300 - Loss: 0.6703 - Avg batch time: 0.16s\n",
      "2025-06-10 03:25:47 - INFO - Processing batch 941/1300\n",
      "2025-06-10 03:25:47 - INFO - Batch 941/1300 - Loss: 1.0332 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:49 - INFO - Processing batch 951/1300\n",
      "2025-06-10 03:25:49 - INFO - Batch 951/1300 - Loss: 0.3024 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:50 - INFO - Processing batch 961/1300\n",
      "2025-06-10 03:25:50 - INFO - Batch 961/1300 - Loss: 0.4869 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:52 - INFO - Processing batch 971/1300\n",
      "2025-06-10 03:25:52 - INFO - Batch 971/1300 - Loss: 0.1436 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:53 - INFO - Processing batch 981/1300\n",
      "2025-06-10 03:25:53 - INFO - Batch 981/1300 - Loss: 0.3445 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:55 - INFO - Processing batch 991/1300\n",
      "2025-06-10 03:25:55 - INFO - Batch 991/1300 - Loss: 0.8076 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:56 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 03:25:56 - INFO - Batch 1001/1300 - Loss: 0.6388 - Avg batch time: 0.16s\n",
      "2025-06-10 03:25:58 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 03:25:58 - INFO - Batch 1011/1300 - Loss: 0.3711 - Avg batch time: 0.15s\n",
      "2025-06-10 03:25:59 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 03:26:00 - INFO - Batch 1021/1300 - Loss: 0.6100 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:01 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 03:26:01 - INFO - Batch 1031/1300 - Loss: 0.5483 - Avg batch time: 0.16s\n",
      "2025-06-10 03:26:03 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 03:26:03 - INFO - Batch 1041/1300 - Loss: 0.2650 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:04 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 03:26:04 - INFO - Batch 1051/1300 - Loss: 0.5349 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:06 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 03:26:06 - INFO - Batch 1061/1300 - Loss: 0.1191 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:07 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 03:26:07 - INFO - Batch 1071/1300 - Loss: 0.4923 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:09 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 03:26:09 - INFO - Batch 1081/1300 - Loss: 0.2273 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:10 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 03:26:10 - INFO - Batch 1091/1300 - Loss: 0.4632 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:12 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 03:26:12 - INFO - Batch 1101/1300 - Loss: 0.6604 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:13 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 03:26:13 - INFO - Batch 1111/1300 - Loss: 0.4428 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:15 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 03:26:15 - INFO - Batch 1121/1300 - Loss: 0.4581 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:16 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 03:26:17 - INFO - Batch 1131/1300 - Loss: 0.3893 - Avg batch time: 0.16s\n",
      "2025-06-10 03:26:18 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 03:26:18 - INFO - Batch 1141/1300 - Loss: 0.4683 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:19 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 03:26:20 - INFO - Batch 1151/1300 - Loss: 0.5622 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:21 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 03:26:21 - INFO - Batch 1161/1300 - Loss: 0.4822 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:23 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 03:26:23 - INFO - Batch 1171/1300 - Loss: 0.4062 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:24 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 03:26:24 - INFO - Batch 1181/1300 - Loss: 0.7949 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:26 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 03:26:26 - INFO - Batch 1191/1300 - Loss: 0.6259 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:27 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 03:26:27 - INFO - Batch 1201/1300 - Loss: 0.1420 - Avg batch time: 0.16s\n",
      "2025-06-10 03:26:29 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 03:26:29 - INFO - Batch 1211/1300 - Loss: 0.9687 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:30 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 03:26:30 - INFO - Batch 1221/1300 - Loss: 0.6256 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:32 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 03:26:32 - INFO - Batch 1231/1300 - Loss: 0.3541 - Avg batch time: 0.16s\n",
      "2025-06-10 03:26:33 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 03:26:33 - INFO - Batch 1241/1300 - Loss: 0.4882 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:35 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 03:26:35 - INFO - Batch 1251/1300 - Loss: 0.2957 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:36 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 03:26:37 - INFO - Batch 1261/1300 - Loss: 0.0935 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:38 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 03:26:38 - INFO - Batch 1271/1300 - Loss: 0.4212 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:39 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 03:26:40 - INFO - Batch 1281/1300 - Loss: 0.2987 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:41 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 03:26:41 - INFO - Batch 1291/1300 - Loss: 0.5597 - Avg batch time: 0.15s\n",
      "2025-06-10 03:26:42 - INFO - \n",
      "Epoch 12 training completed in 200.54s\n",
      "2025-06-10 03:26:42 - INFO - Average training loss: 0.4568\n",
      "2025-06-10 03:27:00 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:  13%|â–| 13/100 [43:38<5:16:26, 218.24s/it, train_loss=0.4568, val_loss=0.4529, best_val_f1=0.3125, lr=5.00e-05, 2025-06-10 03:27:00 - INFO - \n",
      "Epoch 13/100 - Training phase\n",
      "2025-06-10 03:27:01 - INFO - Processing batch 1/1300\n",
      "2025-06-10 03:27:01 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 03:27:01 - INFO - Batch 1/1300 - Loss: 0.7245 - Avg batch time: 0.16s\n",
      "2025-06-10 03:27:02 - INFO - Processing batch 11/1300\n",
      "2025-06-10 03:27:02 - INFO - Batch 11/1300 - Loss: 0.5939 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:04 - INFO - Processing batch 21/1300\n",
      "2025-06-10 03:27:04 - INFO - Batch 21/1300 - Loss: 0.6361 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:05 - INFO - Processing batch 31/1300\n",
      "2025-06-10 03:27:05 - INFO - Batch 31/1300 - Loss: 0.3755 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:07 - INFO - Processing batch 41/1300\n",
      "2025-06-10 03:27:07 - INFO - Batch 41/1300 - Loss: 0.6763 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:08 - INFO - Processing batch 51/1300\n",
      "2025-06-10 03:27:08 - INFO - Batch 51/1300 - Loss: 0.2661 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:10 - INFO - Processing batch 61/1300\n",
      "2025-06-10 03:27:10 - INFO - Batch 61/1300 - Loss: 0.6866 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:11 - INFO - Processing batch 71/1300\n",
      "2025-06-10 03:27:11 - INFO - Batch 71/1300 - Loss: 1.0480 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:13 - INFO - Processing batch 81/1300\n",
      "2025-06-10 03:27:13 - INFO - Batch 81/1300 - Loss: 0.2923 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:14 - INFO - Processing batch 91/1300\n",
      "2025-06-10 03:27:15 - INFO - Batch 91/1300 - Loss: 0.6380 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:16 - INFO - Processing batch 101/1300\n",
      "2025-06-10 03:27:16 - INFO - Batch 101/1300 - Loss: 0.2631 - Avg batch time: 0.16s\n",
      "2025-06-10 03:27:18 - INFO - Processing batch 111/1300\n",
      "2025-06-10 03:27:18 - INFO - Batch 111/1300 - Loss: 0.4213 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:19 - INFO - Processing batch 121/1300\n",
      "2025-06-10 03:27:19 - INFO - Batch 121/1300 - Loss: 0.7336 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:21 - INFO - Processing batch 131/1300\n",
      "2025-06-10 03:27:21 - INFO - Batch 131/1300 - Loss: 0.2319 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:22 - INFO - Processing batch 141/1300\n",
      "2025-06-10 03:27:22 - INFO - Batch 141/1300 - Loss: 0.3416 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:24 - INFO - Processing batch 151/1300\n",
      "2025-06-10 03:27:24 - INFO - Batch 151/1300 - Loss: 0.1681 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:25 - INFO - Processing batch 161/1300\n",
      "2025-06-10 03:27:25 - INFO - Batch 161/1300 - Loss: 0.7337 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:27 - INFO - Processing batch 171/1300\n",
      "2025-06-10 03:27:27 - INFO - Batch 171/1300 - Loss: 0.3608 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:28 - INFO - Processing batch 181/1300\n",
      "2025-06-10 03:27:28 - INFO - Batch 181/1300 - Loss: 0.6894 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:30 - INFO - Processing batch 191/1300\n",
      "2025-06-10 03:27:30 - INFO - Batch 191/1300 - Loss: 0.3667 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:31 - INFO - Processing batch 201/1300\n",
      "2025-06-10 03:27:32 - INFO - Batch 201/1300 - Loss: 0.3961 - Avg batch time: 0.16s\n",
      "2025-06-10 03:27:33 - INFO - Processing batch 211/1300\n",
      "2025-06-10 03:27:33 - INFO - Batch 211/1300 - Loss: 0.1877 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:35 - INFO - Processing batch 221/1300\n",
      "2025-06-10 03:27:35 - INFO - Batch 221/1300 - Loss: 0.0951 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:36 - INFO - Processing batch 231/1300\n",
      "2025-06-10 03:27:36 - INFO - Batch 231/1300 - Loss: 0.5435 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:38 - INFO - Processing batch 241/1300\n",
      "2025-06-10 03:27:38 - INFO - Batch 241/1300 - Loss: 0.3607 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:39 - INFO - Processing batch 251/1300\n",
      "2025-06-10 03:27:39 - INFO - Batch 251/1300 - Loss: 0.3527 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:41 - INFO - Processing batch 261/1300\n",
      "2025-06-10 03:27:41 - INFO - Batch 261/1300 - Loss: 0.2947 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:42 - INFO - Processing batch 271/1300\n",
      "2025-06-10 03:27:42 - INFO - Batch 271/1300 - Loss: 0.4480 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:44 - INFO - Processing batch 281/1300\n",
      "2025-06-10 03:27:44 - INFO - Batch 281/1300 - Loss: 0.4321 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:45 - INFO - Processing batch 291/1300\n",
      "2025-06-10 03:27:45 - INFO - Batch 291/1300 - Loss: 0.1729 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:47 - INFO - Processing batch 301/1300\n",
      "2025-06-10 03:27:47 - INFO - Batch 301/1300 - Loss: 0.1152 - Avg batch time: 0.16s\n",
      "2025-06-10 03:27:48 - INFO - Processing batch 311/1300\n",
      "2025-06-10 03:27:49 - INFO - Batch 311/1300 - Loss: 0.1778 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:50 - INFO - Processing batch 321/1300\n",
      "2025-06-10 03:27:50 - INFO - Batch 321/1300 - Loss: 0.4128 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:51 - INFO - Processing batch 331/1300\n",
      "2025-06-10 03:27:52 - INFO - Batch 331/1300 - Loss: 0.6313 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:53 - INFO - Processing batch 341/1300\n",
      "2025-06-10 03:27:53 - INFO - Batch 341/1300 - Loss: 0.3783 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:55 - INFO - Processing batch 351/1300\n",
      "2025-06-10 03:27:55 - INFO - Batch 351/1300 - Loss: 0.1857 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:56 - INFO - Processing batch 361/1300\n",
      "2025-06-10 03:27:56 - INFO - Batch 361/1300 - Loss: 0.1062 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:58 - INFO - Processing batch 371/1300\n",
      "2025-06-10 03:27:58 - INFO - Batch 371/1300 - Loss: 0.9165 - Avg batch time: 0.15s\n",
      "2025-06-10 03:27:59 - INFO - Processing batch 381/1300\n",
      "2025-06-10 03:27:59 - INFO - Batch 381/1300 - Loss: 0.6045 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:01 - INFO - Processing batch 391/1300\n",
      "2025-06-10 03:28:01 - INFO - Batch 391/1300 - Loss: 0.1286 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:02 - INFO - Processing batch 401/1300\n",
      "2025-06-10 03:28:02 - INFO - Batch 401/1300 - Loss: 0.7385 - Avg batch time: 0.16s\n",
      "2025-06-10 03:28:04 - INFO - Processing batch 411/1300\n",
      "2025-06-10 03:28:04 - INFO - Batch 411/1300 - Loss: 0.6585 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:05 - INFO - Processing batch 421/1300\n",
      "2025-06-10 03:28:06 - INFO - Batch 421/1300 - Loss: 0.1017 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:07 - INFO - Processing batch 431/1300\n",
      "2025-06-10 03:28:07 - INFO - Batch 431/1300 - Loss: 0.4039 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:08 - INFO - Processing batch 441/1300\n",
      "2025-06-10 03:28:09 - INFO - Batch 441/1300 - Loss: 0.1079 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:10 - INFO - Processing batch 451/1300\n",
      "2025-06-10 03:28:10 - INFO - Batch 451/1300 - Loss: 0.8154 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:11 - INFO - Processing batch 461/1300\n",
      "2025-06-10 03:28:12 - INFO - Batch 461/1300 - Loss: 0.4823 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:13 - INFO - Processing batch 471/1300\n",
      "2025-06-10 03:28:13 - INFO - Batch 471/1300 - Loss: 0.4901 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:15 - INFO - Processing batch 481/1300\n",
      "2025-06-10 03:28:15 - INFO - Batch 481/1300 - Loss: 0.1769 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:16 - INFO - Processing batch 491/1300\n",
      "2025-06-10 03:28:16 - INFO - Batch 491/1300 - Loss: 0.3015 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:18 - INFO - Processing batch 501/1300\n",
      "2025-06-10 03:28:18 - INFO - Batch 501/1300 - Loss: 0.3577 - Avg batch time: 0.16s\n",
      "2025-06-10 03:28:19 - INFO - Processing batch 511/1300\n",
      "2025-06-10 03:28:19 - INFO - Batch 511/1300 - Loss: 0.7130 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:21 - INFO - Processing batch 521/1300\n",
      "2025-06-10 03:28:21 - INFO - Batch 521/1300 - Loss: 0.2487 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:22 - INFO - Processing batch 531/1300\n",
      "2025-06-10 03:28:22 - INFO - Batch 531/1300 - Loss: 0.4998 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:24 - INFO - Processing batch 541/1300\n",
      "2025-06-10 03:28:24 - INFO - Batch 541/1300 - Loss: 0.2642 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:25 - INFO - Processing batch 551/1300\n",
      "2025-06-10 03:28:26 - INFO - Batch 551/1300 - Loss: 0.3642 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:27 - INFO - Processing batch 561/1300\n",
      "2025-06-10 03:28:27 - INFO - Batch 561/1300 - Loss: 0.1036 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:28 - INFO - Processing batch 571/1300\n",
      "2025-06-10 03:28:29 - INFO - Batch 571/1300 - Loss: 0.7686 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:30 - INFO - Processing batch 581/1300\n",
      "2025-06-10 03:28:30 - INFO - Batch 581/1300 - Loss: 0.3934 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:31 - INFO - Processing batch 591/1300\n",
      "2025-06-10 03:28:32 - INFO - Batch 591/1300 - Loss: 0.7847 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:33 - INFO - Processing batch 601/1300\n",
      "2025-06-10 03:28:33 - INFO - Batch 601/1300 - Loss: 0.4941 - Avg batch time: 0.16s\n",
      "2025-06-10 03:28:35 - INFO - Processing batch 611/1300\n",
      "2025-06-10 03:28:35 - INFO - Batch 611/1300 - Loss: 0.1951 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:36 - INFO - Processing batch 621/1300\n",
      "2025-06-10 03:28:36 - INFO - Batch 621/1300 - Loss: 0.4299 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:38 - INFO - Processing batch 631/1300\n",
      "2025-06-10 03:28:38 - INFO - Batch 631/1300 - Loss: 0.5052 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:39 - INFO - Processing batch 641/1300\n",
      "2025-06-10 03:28:39 - INFO - Batch 641/1300 - Loss: 0.6201 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:41 - INFO - Processing batch 651/1300\n",
      "2025-06-10 03:28:41 - INFO - Batch 651/1300 - Loss: 0.4263 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:42 - INFO - Processing batch 661/1300\n",
      "2025-06-10 03:28:42 - INFO - Batch 661/1300 - Loss: 0.1815 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:44 - INFO - Processing batch 671/1300\n",
      "2025-06-10 03:28:44 - INFO - Batch 671/1300 - Loss: 0.4041 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:45 - INFO - Processing batch 681/1300\n",
      "2025-06-10 03:28:46 - INFO - Batch 681/1300 - Loss: 0.1220 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:47 - INFO - Processing batch 691/1300\n",
      "2025-06-10 03:28:47 - INFO - Batch 691/1300 - Loss: 0.4285 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:49 - INFO - Processing batch 701/1300\n",
      "2025-06-10 03:28:49 - INFO - Batch 701/1300 - Loss: 0.6574 - Avg batch time: 0.16s\n",
      "2025-06-10 03:28:50 - INFO - Processing batch 711/1300\n",
      "2025-06-10 03:28:50 - INFO - Batch 711/1300 - Loss: 1.0094 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:52 - INFO - Processing batch 721/1300\n",
      "2025-06-10 03:28:52 - INFO - Batch 721/1300 - Loss: 0.2397 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:53 - INFO - Processing batch 731/1300\n",
      "2025-06-10 03:28:53 - INFO - Batch 731/1300 - Loss: 0.4820 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:55 - INFO - Processing batch 741/1300\n",
      "2025-06-10 03:28:55 - INFO - Batch 741/1300 - Loss: 0.5711 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:56 - INFO - Processing batch 751/1300\n",
      "2025-06-10 03:28:56 - INFO - Batch 751/1300 - Loss: 0.2410 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:58 - INFO - Processing batch 761/1300\n",
      "2025-06-10 03:28:58 - INFO - Batch 761/1300 - Loss: 0.5616 - Avg batch time: 0.15s\n",
      "2025-06-10 03:28:59 - INFO - Processing batch 771/1300\n",
      "2025-06-10 03:28:59 - INFO - Batch 771/1300 - Loss: 0.2883 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:01 - INFO - Processing batch 781/1300\n",
      "2025-06-10 03:29:01 - INFO - Batch 781/1300 - Loss: 0.2528 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:02 - INFO - Processing batch 791/1300\n",
      "2025-06-10 03:29:02 - INFO - Batch 791/1300 - Loss: 0.8054 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:04 - INFO - Processing batch 801/1300\n",
      "2025-06-10 03:29:04 - INFO - Batch 801/1300 - Loss: 0.8883 - Avg batch time: 0.16s\n",
      "2025-06-10 03:29:05 - INFO - Processing batch 811/1300\n",
      "2025-06-10 03:29:06 - INFO - Batch 811/1300 - Loss: 0.1175 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:07 - INFO - Processing batch 821/1300\n",
      "2025-06-10 03:29:07 - INFO - Batch 821/1300 - Loss: 0.6914 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:09 - INFO - Processing batch 831/1300\n",
      "2025-06-10 03:29:09 - INFO - Batch 831/1300 - Loss: 0.4096 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:10 - INFO - Processing batch 841/1300\n",
      "2025-06-10 03:29:10 - INFO - Batch 841/1300 - Loss: 0.1681 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:12 - INFO - Processing batch 851/1300\n",
      "2025-06-10 03:29:12 - INFO - Batch 851/1300 - Loss: 0.7078 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:13 - INFO - Processing batch 861/1300\n",
      "2025-06-10 03:29:13 - INFO - Batch 861/1300 - Loss: 0.9599 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:15 - INFO - Processing batch 871/1300\n",
      "2025-06-10 03:29:15 - INFO - Batch 871/1300 - Loss: 0.6129 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:16 - INFO - Processing batch 881/1300\n",
      "2025-06-10 03:29:16 - INFO - Batch 881/1300 - Loss: 0.8239 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:18 - INFO - Processing batch 891/1300\n",
      "2025-06-10 03:29:18 - INFO - Batch 891/1300 - Loss: 0.2391 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:19 - INFO - Processing batch 901/1300\n",
      "2025-06-10 03:29:19 - INFO - Batch 901/1300 - Loss: 0.3870 - Avg batch time: 0.16s\n",
      "2025-06-10 03:29:21 - INFO - Processing batch 911/1300\n",
      "2025-06-10 03:29:21 - INFO - Batch 911/1300 - Loss: 0.4537 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:22 - INFO - Processing batch 921/1300\n",
      "2025-06-10 03:29:23 - INFO - Batch 921/1300 - Loss: 0.4027 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:24 - INFO - Processing batch 931/1300\n",
      "2025-06-10 03:29:24 - INFO - Batch 931/1300 - Loss: 0.5607 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:25 - INFO - Processing batch 941/1300\n",
      "2025-06-10 03:29:26 - INFO - Batch 941/1300 - Loss: 0.8869 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:27 - INFO - Processing batch 951/1300\n",
      "2025-06-10 03:29:27 - INFO - Batch 951/1300 - Loss: 0.3253 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:29 - INFO - Processing batch 961/1300\n",
      "2025-06-10 03:29:29 - INFO - Batch 961/1300 - Loss: 0.4783 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:30 - INFO - Processing batch 971/1300\n",
      "2025-06-10 03:29:30 - INFO - Batch 971/1300 - Loss: 0.6268 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:32 - INFO - Processing batch 981/1300\n",
      "2025-06-10 03:29:32 - INFO - Batch 981/1300 - Loss: 0.4791 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:33 - INFO - Processing batch 991/1300\n",
      "2025-06-10 03:29:33 - INFO - Batch 991/1300 - Loss: 0.4612 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:35 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 03:29:35 - INFO - Batch 1001/1300 - Loss: 0.6979 - Avg batch time: 0.16s\n",
      "2025-06-10 03:29:36 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 03:29:36 - INFO - Batch 1011/1300 - Loss: 0.5373 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:38 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 03:29:38 - INFO - Batch 1021/1300 - Loss: 0.6395 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:39 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 03:29:39 - INFO - Batch 1031/1300 - Loss: 0.5660 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:41 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 03:29:41 - INFO - Batch 1041/1300 - Loss: 0.2028 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:42 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 03:29:43 - INFO - Batch 1051/1300 - Loss: 0.3678 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:44 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 03:29:44 - INFO - Batch 1061/1300 - Loss: 0.2470 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:45 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 03:29:46 - INFO - Batch 1071/1300 - Loss: 0.3746 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:47 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 03:29:47 - INFO - Batch 1081/1300 - Loss: 0.3356 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:49 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 03:29:49 - INFO - Batch 1091/1300 - Loss: 0.1420 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:50 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 03:29:50 - INFO - Batch 1101/1300 - Loss: 0.3638 - Avg batch time: 0.16s\n",
      "2025-06-10 03:29:52 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 03:29:52 - INFO - Batch 1111/1300 - Loss: 1.0960 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:53 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 03:29:53 - INFO - Batch 1121/1300 - Loss: 0.1846 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:55 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 03:29:55 - INFO - Batch 1131/1300 - Loss: 0.6242 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:56 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 03:29:56 - INFO - Batch 1141/1300 - Loss: 0.2141 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:58 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 03:29:58 - INFO - Batch 1151/1300 - Loss: 0.5394 - Avg batch time: 0.15s\n",
      "2025-06-10 03:29:59 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 03:29:59 - INFO - Batch 1161/1300 - Loss: 0.7589 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:01 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 03:30:01 - INFO - Batch 1171/1300 - Loss: 0.1872 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:02 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 03:30:03 - INFO - Batch 1181/1300 - Loss: 0.6147 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:04 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 03:30:04 - INFO - Batch 1191/1300 - Loss: 0.1033 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:06 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 03:30:06 - INFO - Batch 1201/1300 - Loss: 0.6468 - Avg batch time: 0.16s\n",
      "2025-06-10 03:30:07 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 03:30:07 - INFO - Batch 1211/1300 - Loss: 0.2573 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:09 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 03:30:09 - INFO - Batch 1221/1300 - Loss: 0.2416 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:10 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 03:30:10 - INFO - Batch 1231/1300 - Loss: 0.3161 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:12 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 03:30:12 - INFO - Batch 1241/1300 - Loss: 0.5992 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:13 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 03:30:13 - INFO - Batch 1251/1300 - Loss: 0.7373 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:15 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 03:30:15 - INFO - Batch 1261/1300 - Loss: 1.1498 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:16 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 03:30:16 - INFO - Batch 1271/1300 - Loss: 0.2930 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:18 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 03:30:18 - INFO - Batch 1281/1300 - Loss: 0.2881 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:19 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 03:30:20 - INFO - Batch 1291/1300 - Loss: 0.2810 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:21 - INFO - \n",
      "Epoch 13 training completed in 200.61s\n",
      "2025-06-10 03:30:21 - INFO - Average training loss: 0.4549\n",
      "2025-06-10 03:30:39 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:  14%|â–| 14/100 [47:16<5:12:52, 218.29s/it, train_loss=0.4549, val_loss=0.4533, best_val_f1=0.3125, lr=2.50e-05, 2025-06-10 03:30:39 - INFO - \n",
      "Epoch 14/100 - Training phase\n",
      "2025-06-10 03:30:39 - INFO - Processing batch 1/1300\n",
      "2025-06-10 03:30:39 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 03:30:39 - INFO - Batch 1/1300 - Loss: 0.4002 - Avg batch time: 0.16s\n",
      "2025-06-10 03:30:40 - INFO - Processing batch 11/1300\n",
      "2025-06-10 03:30:41 - INFO - Batch 11/1300 - Loss: 0.7392 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:42 - INFO - Processing batch 21/1300\n",
      "2025-06-10 03:30:42 - INFO - Batch 21/1300 - Loss: 0.6368 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:44 - INFO - Processing batch 31/1300\n",
      "2025-06-10 03:30:44 - INFO - Batch 31/1300 - Loss: 0.2327 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:45 - INFO - Processing batch 41/1300\n",
      "2025-06-10 03:30:45 - INFO - Batch 41/1300 - Loss: 0.4437 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:47 - INFO - Processing batch 51/1300\n",
      "2025-06-10 03:30:47 - INFO - Batch 51/1300 - Loss: 0.4956 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:48 - INFO - Processing batch 61/1300\n",
      "2025-06-10 03:30:48 - INFO - Batch 61/1300 - Loss: 0.2624 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:50 - INFO - Processing batch 71/1300\n",
      "2025-06-10 03:30:50 - INFO - Batch 71/1300 - Loss: 0.2313 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:51 - INFO - Processing batch 81/1300\n",
      "2025-06-10 03:30:51 - INFO - Batch 81/1300 - Loss: 0.1695 - Avg batch time: 0.16s\n",
      "2025-06-10 03:30:53 - INFO - Processing batch 91/1300\n",
      "2025-06-10 03:30:53 - INFO - Batch 91/1300 - Loss: 0.5138 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:54 - INFO - Processing batch 101/1300\n",
      "2025-06-10 03:30:55 - INFO - Batch 101/1300 - Loss: 0.3776 - Avg batch time: 0.16s\n",
      "2025-06-10 03:30:56 - INFO - Processing batch 111/1300\n",
      "2025-06-10 03:30:56 - INFO - Batch 111/1300 - Loss: 0.3640 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:57 - INFO - Processing batch 121/1300\n",
      "2025-06-10 03:30:58 - INFO - Batch 121/1300 - Loss: 0.1417 - Avg batch time: 0.15s\n",
      "2025-06-10 03:30:59 - INFO - Processing batch 131/1300\n",
      "2025-06-10 03:30:59 - INFO - Batch 131/1300 - Loss: 0.5541 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:00 - INFO - Processing batch 141/1300\n",
      "2025-06-10 03:31:01 - INFO - Batch 141/1300 - Loss: 0.5424 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:02 - INFO - Processing batch 151/1300\n",
      "2025-06-10 03:31:02 - INFO - Batch 151/1300 - Loss: 0.7978 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:04 - INFO - Processing batch 161/1300\n",
      "2025-06-10 03:31:04 - INFO - Batch 161/1300 - Loss: 0.4763 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:05 - INFO - Processing batch 171/1300\n",
      "2025-06-10 03:31:05 - INFO - Batch 171/1300 - Loss: 0.5795 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:07 - INFO - Processing batch 181/1300\n",
      "2025-06-10 03:31:07 - INFO - Batch 181/1300 - Loss: 0.4647 - Avg batch time: 0.16s\n",
      "2025-06-10 03:31:08 - INFO - Processing batch 191/1300\n",
      "2025-06-10 03:31:08 - INFO - Batch 191/1300 - Loss: 0.5291 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:10 - INFO - Processing batch 201/1300\n",
      "2025-06-10 03:31:10 - INFO - Batch 201/1300 - Loss: 0.5308 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:11 - INFO - Processing batch 211/1300\n",
      "2025-06-10 03:31:11 - INFO - Batch 211/1300 - Loss: 0.1543 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:13 - INFO - Processing batch 221/1300\n",
      "2025-06-10 03:31:13 - INFO - Batch 221/1300 - Loss: 0.2492 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:14 - INFO - Processing batch 231/1300\n",
      "2025-06-10 03:31:14 - INFO - Batch 231/1300 - Loss: 0.2962 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:16 - INFO - Processing batch 241/1300\n",
      "2025-06-10 03:31:16 - INFO - Batch 241/1300 - Loss: 0.3524 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:17 - INFO - Processing batch 251/1300\n",
      "2025-06-10 03:31:18 - INFO - Batch 251/1300 - Loss: 0.3870 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:19 - INFO - Processing batch 261/1300\n",
      "2025-06-10 03:31:19 - INFO - Batch 261/1300 - Loss: 0.3507 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:20 - INFO - Processing batch 271/1300\n",
      "2025-06-10 03:31:21 - INFO - Batch 271/1300 - Loss: 0.8070 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:22 - INFO - Processing batch 281/1300\n",
      "2025-06-10 03:31:22 - INFO - Batch 281/1300 - Loss: 0.1414 - Avg batch time: 0.16s\n",
      "2025-06-10 03:31:24 - INFO - Processing batch 291/1300\n",
      "2025-06-10 03:31:24 - INFO - Batch 291/1300 - Loss: 0.3003 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:25 - INFO - Processing batch 301/1300\n",
      "2025-06-10 03:31:25 - INFO - Batch 301/1300 - Loss: 0.5610 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:27 - INFO - Processing batch 311/1300\n",
      "2025-06-10 03:31:27 - INFO - Batch 311/1300 - Loss: 0.6703 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:28 - INFO - Processing batch 321/1300\n",
      "2025-06-10 03:31:28 - INFO - Batch 321/1300 - Loss: 0.1202 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:30 - INFO - Processing batch 331/1300\n",
      "2025-06-10 03:31:30 - INFO - Batch 331/1300 - Loss: 0.6938 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:31 - INFO - Processing batch 341/1300\n",
      "2025-06-10 03:31:31 - INFO - Batch 341/1300 - Loss: 0.3860 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:33 - INFO - Processing batch 351/1300\n",
      "2025-06-10 03:31:33 - INFO - Batch 351/1300 - Loss: 0.7864 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:34 - INFO - Processing batch 361/1300\n",
      "2025-06-10 03:31:34 - INFO - Batch 361/1300 - Loss: 0.6449 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:36 - INFO - Processing batch 371/1300\n",
      "2025-06-10 03:31:36 - INFO - Batch 371/1300 - Loss: 0.6486 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:37 - INFO - Processing batch 381/1300\n",
      "2025-06-10 03:31:38 - INFO - Batch 381/1300 - Loss: 0.2112 - Avg batch time: 0.16s\n",
      "2025-06-10 03:31:39 - INFO - Processing batch 391/1300\n",
      "2025-06-10 03:31:39 - INFO - Batch 391/1300 - Loss: 0.8051 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:40 - INFO - Processing batch 401/1300\n",
      "2025-06-10 03:31:41 - INFO - Batch 401/1300 - Loss: 0.1106 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:42 - INFO - Processing batch 411/1300\n",
      "2025-06-10 03:31:42 - INFO - Batch 411/1300 - Loss: 0.8057 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:44 - INFO - Processing batch 421/1300\n",
      "2025-06-10 03:31:44 - INFO - Batch 421/1300 - Loss: 0.3934 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:45 - INFO - Processing batch 431/1300\n",
      "2025-06-10 03:31:45 - INFO - Batch 431/1300 - Loss: 0.5076 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:47 - INFO - Processing batch 441/1300\n",
      "2025-06-10 03:31:47 - INFO - Batch 441/1300 - Loss: 1.3307 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:48 - INFO - Processing batch 451/1300\n",
      "2025-06-10 03:31:48 - INFO - Batch 451/1300 - Loss: 0.5256 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:50 - INFO - Processing batch 461/1300\n",
      "2025-06-10 03:31:50 - INFO - Batch 461/1300 - Loss: 0.2609 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:51 - INFO - Processing batch 471/1300\n",
      "2025-06-10 03:31:51 - INFO - Batch 471/1300 - Loss: 0.4111 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:53 - INFO - Processing batch 481/1300\n",
      "2025-06-10 03:31:53 - INFO - Batch 481/1300 - Loss: 0.4876 - Avg batch time: 0.16s\n",
      "2025-06-10 03:31:54 - INFO - Processing batch 491/1300\n",
      "2025-06-10 03:31:54 - INFO - Batch 491/1300 - Loss: 0.8673 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:56 - INFO - Processing batch 501/1300\n",
      "2025-06-10 03:31:56 - INFO - Batch 501/1300 - Loss: 0.6297 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:57 - INFO - Processing batch 511/1300\n",
      "2025-06-10 03:31:58 - INFO - Batch 511/1300 - Loss: 0.1047 - Avg batch time: 0.15s\n",
      "2025-06-10 03:31:59 - INFO - Processing batch 521/1300\n",
      "2025-06-10 03:31:59 - INFO - Batch 521/1300 - Loss: 0.1223 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:00 - INFO - Processing batch 531/1300\n",
      "2025-06-10 03:32:01 - INFO - Batch 531/1300 - Loss: 0.1232 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:02 - INFO - Processing batch 541/1300\n",
      "2025-06-10 03:32:02 - INFO - Batch 541/1300 - Loss: 0.2539 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:03 - INFO - Processing batch 551/1300\n",
      "2025-06-10 03:32:04 - INFO - Batch 551/1300 - Loss: 0.1332 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:05 - INFO - Processing batch 561/1300\n",
      "2025-06-10 03:32:05 - INFO - Batch 561/1300 - Loss: 0.2377 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:07 - INFO - Processing batch 571/1300\n",
      "2025-06-10 03:32:07 - INFO - Batch 571/1300 - Loss: 0.6552 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:08 - INFO - Processing batch 581/1300\n",
      "2025-06-10 03:32:08 - INFO - Batch 581/1300 - Loss: 0.3005 - Avg batch time: 0.16s\n",
      "2025-06-10 03:32:10 - INFO - Processing batch 591/1300\n",
      "2025-06-10 03:32:10 - INFO - Batch 591/1300 - Loss: 1.2674 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:11 - INFO - Processing batch 601/1300\n",
      "2025-06-10 03:32:11 - INFO - Batch 601/1300 - Loss: 0.7257 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:13 - INFO - Processing batch 611/1300\n",
      "2025-06-10 03:32:13 - INFO - Batch 611/1300 - Loss: 0.5503 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:14 - INFO - Processing batch 621/1300\n",
      "2025-06-10 03:32:14 - INFO - Batch 621/1300 - Loss: 0.2680 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:16 - INFO - Processing batch 631/1300\n",
      "2025-06-10 03:32:16 - INFO - Batch 631/1300 - Loss: 0.8493 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:17 - INFO - Processing batch 641/1300\n",
      "2025-06-10 03:32:17 - INFO - Batch 641/1300 - Loss: 0.2094 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:19 - INFO - Processing batch 651/1300\n",
      "2025-06-10 03:32:19 - INFO - Batch 651/1300 - Loss: 0.4444 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:20 - INFO - Processing batch 661/1300\n",
      "2025-06-10 03:32:21 - INFO - Batch 661/1300 - Loss: 0.6986 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:22 - INFO - Processing batch 671/1300\n",
      "2025-06-10 03:32:22 - INFO - Batch 671/1300 - Loss: 0.8978 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:23 - INFO - Processing batch 681/1300\n",
      "2025-06-10 03:32:24 - INFO - Batch 681/1300 - Loss: 0.2118 - Avg batch time: 0.16s\n",
      "2025-06-10 03:32:25 - INFO - Processing batch 691/1300\n",
      "2025-06-10 03:32:25 - INFO - Batch 691/1300 - Loss: 0.3096 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:27 - INFO - Processing batch 701/1300\n",
      "2025-06-10 03:32:27 - INFO - Batch 701/1300 - Loss: 0.6949 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:28 - INFO - Processing batch 711/1300\n",
      "2025-06-10 03:32:28 - INFO - Batch 711/1300 - Loss: 0.4659 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:30 - INFO - Processing batch 721/1300\n",
      "2025-06-10 03:32:30 - INFO - Batch 721/1300 - Loss: 0.6512 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:31 - INFO - Processing batch 731/1300\n",
      "2025-06-10 03:32:31 - INFO - Batch 731/1300 - Loss: 0.1763 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:33 - INFO - Processing batch 741/1300\n",
      "2025-06-10 03:32:33 - INFO - Batch 741/1300 - Loss: 0.4918 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:34 - INFO - Processing batch 751/1300\n",
      "2025-06-10 03:32:34 - INFO - Batch 751/1300 - Loss: 0.6049 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:36 - INFO - Processing batch 761/1300\n",
      "2025-06-10 03:32:36 - INFO - Batch 761/1300 - Loss: 0.1105 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:37 - INFO - Processing batch 771/1300\n",
      "2025-06-10 03:32:37 - INFO - Batch 771/1300 - Loss: 0.6446 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:39 - INFO - Processing batch 781/1300\n",
      "2025-06-10 03:32:39 - INFO - Batch 781/1300 - Loss: 0.6649 - Avg batch time: 0.16s\n",
      "2025-06-10 03:32:40 - INFO - Processing batch 791/1300\n",
      "2025-06-10 03:32:40 - INFO - Batch 791/1300 - Loss: 0.1465 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:42 - INFO - Processing batch 801/1300\n",
      "2025-06-10 03:32:42 - INFO - Batch 801/1300 - Loss: 0.6956 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:43 - INFO - Processing batch 811/1300\n",
      "2025-06-10 03:32:44 - INFO - Batch 811/1300 - Loss: 0.7823 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:45 - INFO - Processing batch 821/1300\n",
      "2025-06-10 03:32:45 - INFO - Batch 821/1300 - Loss: 1.8424 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:46 - INFO - Processing batch 831/1300\n",
      "2025-06-10 03:32:47 - INFO - Batch 831/1300 - Loss: 0.1205 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:48 - INFO - Processing batch 841/1300\n",
      "2025-06-10 03:32:48 - INFO - Batch 841/1300 - Loss: 0.2463 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:50 - INFO - Processing batch 851/1300\n",
      "2025-06-10 03:32:50 - INFO - Batch 851/1300 - Loss: 0.1474 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:51 - INFO - Processing batch 861/1300\n",
      "2025-06-10 03:32:51 - INFO - Batch 861/1300 - Loss: 0.3829 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:53 - INFO - Processing batch 871/1300\n",
      "2025-06-10 03:32:53 - INFO - Batch 871/1300 - Loss: 0.3542 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:54 - INFO - Processing batch 881/1300\n",
      "2025-06-10 03:32:54 - INFO - Batch 881/1300 - Loss: 0.1555 - Avg batch time: 0.16s\n",
      "2025-06-10 03:32:56 - INFO - Processing batch 891/1300\n",
      "2025-06-10 03:32:56 - INFO - Batch 891/1300 - Loss: 1.1391 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:57 - INFO - Processing batch 901/1300\n",
      "2025-06-10 03:32:57 - INFO - Batch 901/1300 - Loss: 0.8754 - Avg batch time: 0.15s\n",
      "2025-06-10 03:32:59 - INFO - Processing batch 911/1300\n",
      "2025-06-10 03:32:59 - INFO - Batch 911/1300 - Loss: 0.1515 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:00 - INFO - Processing batch 921/1300\n",
      "2025-06-10 03:33:00 - INFO - Batch 921/1300 - Loss: 0.9968 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:02 - INFO - Processing batch 931/1300\n",
      "2025-06-10 03:33:02 - INFO - Batch 931/1300 - Loss: 0.4292 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:03 - INFO - Processing batch 941/1300\n",
      "2025-06-10 03:33:04 - INFO - Batch 941/1300 - Loss: 0.5897 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:05 - INFO - Processing batch 951/1300\n",
      "2025-06-10 03:33:05 - INFO - Batch 951/1300 - Loss: 0.4537 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:06 - INFO - Processing batch 961/1300\n",
      "2025-06-10 03:33:07 - INFO - Batch 961/1300 - Loss: 0.5811 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:08 - INFO - Processing batch 971/1300\n",
      "2025-06-10 03:33:08 - INFO - Batch 971/1300 - Loss: 0.7332 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:10 - INFO - Processing batch 981/1300\n",
      "2025-06-10 03:33:10 - INFO - Batch 981/1300 - Loss: 0.7110 - Avg batch time: 0.16s\n",
      "2025-06-10 03:33:11 - INFO - Processing batch 991/1300\n",
      "2025-06-10 03:33:11 - INFO - Batch 991/1300 - Loss: 0.2272 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:13 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 03:33:13 - INFO - Batch 1001/1300 - Loss: 0.8236 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:14 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 03:33:14 - INFO - Batch 1011/1300 - Loss: 0.4339 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:16 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 03:33:16 - INFO - Batch 1021/1300 - Loss: 0.1720 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:17 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 03:33:17 - INFO - Batch 1031/1300 - Loss: 0.9826 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:19 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 03:33:19 - INFO - Batch 1041/1300 - Loss: 0.9456 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:20 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 03:33:20 - INFO - Batch 1051/1300 - Loss: 0.5610 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:22 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 03:33:22 - INFO - Batch 1061/1300 - Loss: 0.4992 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:23 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 03:33:23 - INFO - Batch 1071/1300 - Loss: 0.2424 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:25 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 03:33:25 - INFO - Batch 1081/1300 - Loss: 0.4792 - Avg batch time: 0.16s\n",
      "2025-06-10 03:33:26 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 03:33:27 - INFO - Batch 1091/1300 - Loss: 0.6311 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:28 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 03:33:28 - INFO - Batch 1101/1300 - Loss: 0.4616 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:29 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 03:33:30 - INFO - Batch 1111/1300 - Loss: 0.3968 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:31 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 03:33:31 - INFO - Batch 1121/1300 - Loss: 0.3338 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:33 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 03:33:33 - INFO - Batch 1131/1300 - Loss: 0.3850 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:34 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 03:33:34 - INFO - Batch 1141/1300 - Loss: 0.2481 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:36 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 03:33:36 - INFO - Batch 1151/1300 - Loss: 0.5845 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:37 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 03:33:37 - INFO - Batch 1161/1300 - Loss: 0.4003 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:39 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 03:33:39 - INFO - Batch 1171/1300 - Loss: 0.5349 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:40 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 03:33:40 - INFO - Batch 1181/1300 - Loss: 0.4838 - Avg batch time: 0.16s\n",
      "2025-06-10 03:33:42 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 03:33:42 - INFO - Batch 1191/1300 - Loss: 0.2101 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:43 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 03:33:43 - INFO - Batch 1201/1300 - Loss: 0.1782 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:45 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 03:33:45 - INFO - Batch 1211/1300 - Loss: 0.2318 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:46 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 03:33:47 - INFO - Batch 1221/1300 - Loss: 0.4195 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:48 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 03:33:48 - INFO - Batch 1231/1300 - Loss: 0.6986 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:49 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 03:33:50 - INFO - Batch 1241/1300 - Loss: 0.5970 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:51 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 03:33:51 - INFO - Batch 1251/1300 - Loss: 0.1328 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:52 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 03:33:53 - INFO - Batch 1261/1300 - Loss: 0.0823 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:54 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 03:33:54 - INFO - Batch 1271/1300 - Loss: 0.5194 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:56 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 03:33:56 - INFO - Batch 1281/1300 - Loss: 0.1968 - Avg batch time: 0.16s\n",
      "2025-06-10 03:33:57 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 03:33:57 - INFO - Batch 1291/1300 - Loss: 0.3837 - Avg batch time: 0.15s\n",
      "2025-06-10 03:33:59 - INFO - \n",
      "Epoch 14 training completed in 199.95s\n",
      "2025-06-10 03:33:59 - INFO - Average training loss: 0.4495\n",
      "2025-06-10 03:34:16 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:  15%|â–| 15/100 [50:54<5:09:01, 218.14s/it, train_loss=0.4495, val_loss=0.4727, best_val_f1=0.3125, lr=2.50e-05, 2025-06-10 03:34:16 - INFO - \n",
      "Epoch 15/100 - Training phase\n",
      "2025-06-10 03:34:17 - INFO - Processing batch 1/1300\n",
      "2025-06-10 03:34:17 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 03:34:17 - INFO - Batch 1/1300 - Loss: 0.8150 - Avg batch time: 0.16s\n",
      "2025-06-10 03:34:18 - INFO - Processing batch 11/1300\n",
      "2025-06-10 03:34:18 - INFO - Batch 11/1300 - Loss: 0.3082 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:20 - INFO - Processing batch 21/1300\n",
      "2025-06-10 03:34:20 - INFO - Batch 21/1300 - Loss: 0.7267 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:21 - INFO - Processing batch 31/1300\n",
      "2025-06-10 03:34:22 - INFO - Batch 31/1300 - Loss: 0.1522 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:23 - INFO - Processing batch 41/1300\n",
      "2025-06-10 03:34:23 - INFO - Batch 41/1300 - Loss: 0.8094 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:25 - INFO - Processing batch 51/1300\n",
      "2025-06-10 03:34:25 - INFO - Batch 51/1300 - Loss: 1.1693 - Avg batch time: 0.16s\n",
      "2025-06-10 03:34:26 - INFO - Processing batch 61/1300\n",
      "2025-06-10 03:34:26 - INFO - Batch 61/1300 - Loss: 0.3168 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:28 - INFO - Processing batch 71/1300\n",
      "2025-06-10 03:34:28 - INFO - Batch 71/1300 - Loss: 0.2200 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:29 - INFO - Processing batch 81/1300\n",
      "2025-06-10 03:34:29 - INFO - Batch 81/1300 - Loss: 0.5764 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:31 - INFO - Processing batch 91/1300\n",
      "2025-06-10 03:34:31 - INFO - Batch 91/1300 - Loss: 0.3977 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:32 - INFO - Processing batch 101/1300\n",
      "2025-06-10 03:34:32 - INFO - Batch 101/1300 - Loss: 0.2770 - Avg batch time: 0.16s\n",
      "2025-06-10 03:34:34 - INFO - Processing batch 111/1300\n",
      "2025-06-10 03:34:34 - INFO - Batch 111/1300 - Loss: 0.5929 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:35 - INFO - Processing batch 121/1300\n",
      "2025-06-10 03:34:35 - INFO - Batch 121/1300 - Loss: 0.5537 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:37 - INFO - Processing batch 131/1300\n",
      "2025-06-10 03:34:37 - INFO - Batch 131/1300 - Loss: 0.4209 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:38 - INFO - Processing batch 141/1300\n",
      "2025-06-10 03:34:39 - INFO - Batch 141/1300 - Loss: 0.7121 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:40 - INFO - Processing batch 151/1300\n",
      "2025-06-10 03:34:40 - INFO - Batch 151/1300 - Loss: 0.2638 - Avg batch time: 0.16s\n",
      "2025-06-10 03:34:41 - INFO - Processing batch 161/1300\n",
      "2025-06-10 03:34:42 - INFO - Batch 161/1300 - Loss: 0.8477 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:43 - INFO - Processing batch 171/1300\n",
      "2025-06-10 03:34:43 - INFO - Batch 171/1300 - Loss: 0.1550 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:45 - INFO - Processing batch 181/1300\n",
      "2025-06-10 03:34:45 - INFO - Batch 181/1300 - Loss: 0.9229 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:46 - INFO - Processing batch 191/1300\n",
      "2025-06-10 03:34:46 - INFO - Batch 191/1300 - Loss: 0.2958 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:48 - INFO - Processing batch 201/1300\n",
      "2025-06-10 03:34:48 - INFO - Batch 201/1300 - Loss: 0.3915 - Avg batch time: 0.16s\n",
      "2025-06-10 03:34:49 - INFO - Processing batch 211/1300\n",
      "2025-06-10 03:34:49 - INFO - Batch 211/1300 - Loss: 0.2141 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:51 - INFO - Processing batch 221/1300\n",
      "2025-06-10 03:34:51 - INFO - Batch 221/1300 - Loss: 0.7549 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:52 - INFO - Processing batch 231/1300\n",
      "2025-06-10 03:34:52 - INFO - Batch 231/1300 - Loss: 0.3933 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:54 - INFO - Processing batch 241/1300\n",
      "2025-06-10 03:34:54 - INFO - Batch 241/1300 - Loss: 0.1902 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:55 - INFO - Processing batch 251/1300\n",
      "2025-06-10 03:34:56 - INFO - Batch 251/1300 - Loss: 0.4641 - Avg batch time: 0.16s\n",
      "2025-06-10 03:34:57 - INFO - Processing batch 261/1300\n",
      "2025-06-10 03:34:57 - INFO - Batch 261/1300 - Loss: 0.3422 - Avg batch time: 0.15s\n",
      "2025-06-10 03:34:58 - INFO - Processing batch 271/1300\n",
      "2025-06-10 03:34:59 - INFO - Batch 271/1300 - Loss: 0.5653 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:00 - INFO - Processing batch 281/1300\n",
      "2025-06-10 03:35:00 - INFO - Batch 281/1300 - Loss: 0.5139 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:02 - INFO - Processing batch 291/1300\n",
      "2025-06-10 03:35:02 - INFO - Batch 291/1300 - Loss: 0.1583 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:03 - INFO - Processing batch 301/1300\n",
      "2025-06-10 03:35:03 - INFO - Batch 301/1300 - Loss: 0.4505 - Avg batch time: 0.16s\n",
      "2025-06-10 03:35:05 - INFO - Processing batch 311/1300\n",
      "2025-06-10 03:35:05 - INFO - Batch 311/1300 - Loss: 0.1490 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:06 - INFO - Processing batch 321/1300\n",
      "2025-06-10 03:35:06 - INFO - Batch 321/1300 - Loss: 0.2399 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:08 - INFO - Processing batch 331/1300\n",
      "2025-06-10 03:35:08 - INFO - Batch 331/1300 - Loss: 0.4894 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:09 - INFO - Processing batch 341/1300\n",
      "2025-06-10 03:35:09 - INFO - Batch 341/1300 - Loss: 0.1137 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:11 - INFO - Processing batch 351/1300\n",
      "2025-06-10 03:35:11 - INFO - Batch 351/1300 - Loss: 0.2527 - Avg batch time: 0.16s\n",
      "2025-06-10 03:35:12 - INFO - Processing batch 361/1300\n",
      "2025-06-10 03:35:12 - INFO - Batch 361/1300 - Loss: 0.3574 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:14 - INFO - Processing batch 371/1300\n",
      "2025-06-10 03:35:14 - INFO - Batch 371/1300 - Loss: 0.9385 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:15 - INFO - Processing batch 381/1300\n",
      "2025-06-10 03:35:16 - INFO - Batch 381/1300 - Loss: 0.8468 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:17 - INFO - Processing batch 391/1300\n",
      "2025-06-10 03:35:17 - INFO - Batch 391/1300 - Loss: 0.1116 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:18 - INFO - Processing batch 401/1300\n",
      "2025-06-10 03:35:19 - INFO - Batch 401/1300 - Loss: 0.1281 - Avg batch time: 0.16s\n",
      "2025-06-10 03:35:20 - INFO - Processing batch 411/1300\n",
      "2025-06-10 03:35:20 - INFO - Batch 411/1300 - Loss: 0.3179 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:22 - INFO - Processing batch 421/1300\n",
      "2025-06-10 03:35:22 - INFO - Batch 421/1300 - Loss: 0.8647 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:23 - INFO - Processing batch 431/1300\n",
      "2025-06-10 03:35:23 - INFO - Batch 431/1300 - Loss: 0.3943 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:25 - INFO - Processing batch 441/1300\n",
      "2025-06-10 03:35:25 - INFO - Batch 441/1300 - Loss: 0.2189 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:26 - INFO - Processing batch 451/1300\n",
      "2025-06-10 03:35:26 - INFO - Batch 451/1300 - Loss: 0.7524 - Avg batch time: 0.16s\n",
      "2025-06-10 03:35:28 - INFO - Processing batch 461/1300\n",
      "2025-06-10 03:35:28 - INFO - Batch 461/1300 - Loss: 0.2990 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:29 - INFO - Processing batch 471/1300\n",
      "2025-06-10 03:35:29 - INFO - Batch 471/1300 - Loss: 0.4214 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:31 - INFO - Processing batch 481/1300\n",
      "2025-06-10 03:35:31 - INFO - Batch 481/1300 - Loss: 0.4337 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:32 - INFO - Processing batch 491/1300\n",
      "2025-06-10 03:35:32 - INFO - Batch 491/1300 - Loss: 0.3411 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:34 - INFO - Processing batch 501/1300\n",
      "2025-06-10 03:35:34 - INFO - Batch 501/1300 - Loss: 0.6470 - Avg batch time: 0.16s\n",
      "2025-06-10 03:35:35 - INFO - Processing batch 511/1300\n",
      "2025-06-10 03:35:36 - INFO - Batch 511/1300 - Loss: 0.5855 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:37 - INFO - Processing batch 521/1300\n",
      "2025-06-10 03:35:37 - INFO - Batch 521/1300 - Loss: 0.1378 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:38 - INFO - Processing batch 531/1300\n",
      "2025-06-10 03:35:39 - INFO - Batch 531/1300 - Loss: 0.0934 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:40 - INFO - Processing batch 541/1300\n",
      "2025-06-10 03:35:40 - INFO - Batch 541/1300 - Loss: 0.1841 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:42 - INFO - Processing batch 551/1300\n",
      "2025-06-10 03:35:42 - INFO - Batch 551/1300 - Loss: 0.3791 - Avg batch time: 0.16s\n",
      "2025-06-10 03:35:43 - INFO - Processing batch 561/1300\n",
      "2025-06-10 03:35:43 - INFO - Batch 561/1300 - Loss: 0.4847 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:45 - INFO - Processing batch 571/1300\n",
      "2025-06-10 03:35:45 - INFO - Batch 571/1300 - Loss: 0.6020 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:46 - INFO - Processing batch 581/1300\n",
      "2025-06-10 03:35:46 - INFO - Batch 581/1300 - Loss: 0.4532 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:48 - INFO - Processing batch 591/1300\n",
      "2025-06-10 03:35:48 - INFO - Batch 591/1300 - Loss: 0.5964 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:49 - INFO - Processing batch 601/1300\n",
      "2025-06-10 03:35:49 - INFO - Batch 601/1300 - Loss: 0.3870 - Avg batch time: 0.16s\n",
      "2025-06-10 03:35:51 - INFO - Processing batch 611/1300\n",
      "2025-06-10 03:35:51 - INFO - Batch 611/1300 - Loss: 0.3706 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:52 - INFO - Processing batch 621/1300\n",
      "2025-06-10 03:35:53 - INFO - Batch 621/1300 - Loss: 0.1916 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:54 - INFO - Processing batch 631/1300\n",
      "2025-06-10 03:35:54 - INFO - Batch 631/1300 - Loss: 0.4052 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:55 - INFO - Processing batch 641/1300\n",
      "2025-06-10 03:35:56 - INFO - Batch 641/1300 - Loss: 0.7075 - Avg batch time: 0.15s\n",
      "2025-06-10 03:35:57 - INFO - Processing batch 651/1300\n",
      "2025-06-10 03:35:57 - INFO - Batch 651/1300 - Loss: 0.1758 - Avg batch time: 0.16s\n",
      "2025-06-10 03:35:59 - INFO - Processing batch 661/1300\n",
      "2025-06-10 03:35:59 - INFO - Batch 661/1300 - Loss: 0.1219 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:00 - INFO - Processing batch 671/1300\n",
      "2025-06-10 03:36:00 - INFO - Batch 671/1300 - Loss: 0.3876 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:02 - INFO - Processing batch 681/1300\n",
      "2025-06-10 03:36:02 - INFO - Batch 681/1300 - Loss: 0.2843 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:03 - INFO - Processing batch 691/1300\n",
      "2025-06-10 03:36:03 - INFO - Batch 691/1300 - Loss: 0.3444 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:05 - INFO - Processing batch 701/1300\n",
      "2025-06-10 03:36:05 - INFO - Batch 701/1300 - Loss: 0.2792 - Avg batch time: 0.16s\n",
      "2025-06-10 03:36:06 - INFO - Processing batch 711/1300\n",
      "2025-06-10 03:36:06 - INFO - Batch 711/1300 - Loss: 0.9712 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:08 - INFO - Processing batch 721/1300\n",
      "2025-06-10 03:36:08 - INFO - Batch 721/1300 - Loss: 0.8480 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:09 - INFO - Processing batch 731/1300\n",
      "2025-06-10 03:36:09 - INFO - Batch 731/1300 - Loss: 0.4472 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:11 - INFO - Processing batch 741/1300\n",
      "2025-06-10 03:36:11 - INFO - Batch 741/1300 - Loss: 0.1905 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:12 - INFO - Processing batch 751/1300\n",
      "2025-06-10 03:36:13 - INFO - Batch 751/1300 - Loss: 0.3316 - Avg batch time: 0.16s\n",
      "2025-06-10 03:36:14 - INFO - Processing batch 761/1300\n",
      "2025-06-10 03:36:14 - INFO - Batch 761/1300 - Loss: 0.1525 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:15 - INFO - Processing batch 771/1300\n",
      "2025-06-10 03:36:16 - INFO - Batch 771/1300 - Loss: 0.2987 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:17 - INFO - Processing batch 781/1300\n",
      "2025-06-10 03:36:17 - INFO - Batch 781/1300 - Loss: 0.6905 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:19 - INFO - Processing batch 791/1300\n",
      "2025-06-10 03:36:19 - INFO - Batch 791/1300 - Loss: 0.0818 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:20 - INFO - Processing batch 801/1300\n",
      "2025-06-10 03:36:20 - INFO - Batch 801/1300 - Loss: 0.8958 - Avg batch time: 0.16s\n",
      "2025-06-10 03:36:22 - INFO - Processing batch 811/1300\n",
      "2025-06-10 03:36:22 - INFO - Batch 811/1300 - Loss: 0.2998 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:23 - INFO - Processing batch 821/1300\n",
      "2025-06-10 03:36:23 - INFO - Batch 821/1300 - Loss: 0.2365 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:25 - INFO - Processing batch 831/1300\n",
      "2025-06-10 03:36:25 - INFO - Batch 831/1300 - Loss: 0.5644 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:26 - INFO - Processing batch 841/1300\n",
      "2025-06-10 03:36:26 - INFO - Batch 841/1300 - Loss: 0.1290 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:28 - INFO - Processing batch 851/1300\n",
      "2025-06-10 03:36:28 - INFO - Batch 851/1300 - Loss: 0.0859 - Avg batch time: 0.16s\n",
      "2025-06-10 03:36:29 - INFO - Processing batch 861/1300\n",
      "2025-06-10 03:36:30 - INFO - Batch 861/1300 - Loss: 0.2173 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:31 - INFO - Processing batch 871/1300\n",
      "2025-06-10 03:36:31 - INFO - Batch 871/1300 - Loss: 0.5261 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:32 - INFO - Processing batch 881/1300\n",
      "2025-06-10 03:36:33 - INFO - Batch 881/1300 - Loss: 0.4679 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:34 - INFO - Processing batch 891/1300\n",
      "2025-06-10 03:36:34 - INFO - Batch 891/1300 - Loss: 0.7582 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:36 - INFO - Processing batch 901/1300\n",
      "2025-06-10 03:36:36 - INFO - Batch 901/1300 - Loss: 0.8069 - Avg batch time: 0.16s\n",
      "2025-06-10 03:36:37 - INFO - Processing batch 911/1300\n",
      "2025-06-10 03:36:37 - INFO - Batch 911/1300 - Loss: 0.5723 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:39 - INFO - Processing batch 921/1300\n",
      "2025-06-10 03:36:39 - INFO - Batch 921/1300 - Loss: 0.4810 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:40 - INFO - Processing batch 931/1300\n",
      "2025-06-10 03:36:40 - INFO - Batch 931/1300 - Loss: 0.2970 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:42 - INFO - Processing batch 941/1300\n",
      "2025-06-10 03:36:42 - INFO - Batch 941/1300 - Loss: 0.4281 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:43 - INFO - Processing batch 951/1300\n",
      "2025-06-10 03:36:43 - INFO - Batch 951/1300 - Loss: 0.6078 - Avg batch time: 0.16s\n",
      "2025-06-10 03:36:45 - INFO - Processing batch 961/1300\n",
      "2025-06-10 03:36:45 - INFO - Batch 961/1300 - Loss: 0.4652 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:46 - INFO - Processing batch 971/1300\n",
      "2025-06-10 03:36:46 - INFO - Batch 971/1300 - Loss: 0.1113 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:48 - INFO - Processing batch 981/1300\n",
      "2025-06-10 03:36:48 - INFO - Batch 981/1300 - Loss: 0.7428 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:49 - INFO - Processing batch 991/1300\n",
      "2025-06-10 03:36:50 - INFO - Batch 991/1300 - Loss: 0.9498 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:51 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 03:36:51 - INFO - Batch 1001/1300 - Loss: 0.7376 - Avg batch time: 0.16s\n",
      "2025-06-10 03:36:52 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 03:36:53 - INFO - Batch 1011/1300 - Loss: 0.3773 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:54 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 03:36:54 - INFO - Batch 1021/1300 - Loss: 0.8769 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:56 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 03:36:56 - INFO - Batch 1031/1300 - Loss: 0.3844 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:57 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 03:36:57 - INFO - Batch 1041/1300 - Loss: 0.2972 - Avg batch time: 0.15s\n",
      "2025-06-10 03:36:59 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 03:36:59 - INFO - Batch 1051/1300 - Loss: 0.2303 - Avg batch time: 0.16s\n",
      "2025-06-10 03:37:00 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 03:37:00 - INFO - Batch 1061/1300 - Loss: 0.5425 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:02 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 03:37:02 - INFO - Batch 1071/1300 - Loss: 1.0190 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:03 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 03:37:03 - INFO - Batch 1081/1300 - Loss: 0.5260 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:05 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 03:37:05 - INFO - Batch 1091/1300 - Loss: 0.2308 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:06 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 03:37:07 - INFO - Batch 1101/1300 - Loss: 0.8576 - Avg batch time: 0.16s\n",
      "2025-06-10 03:37:08 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 03:37:08 - INFO - Batch 1111/1300 - Loss: 0.3364 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:09 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 03:37:10 - INFO - Batch 1121/1300 - Loss: 0.4176 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:11 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 03:37:11 - INFO - Batch 1131/1300 - Loss: 0.7741 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:13 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 03:37:13 - INFO - Batch 1141/1300 - Loss: 0.2377 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:14 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 03:37:14 - INFO - Batch 1151/1300 - Loss: 1.0769 - Avg batch time: 0.16s\n",
      "2025-06-10 03:37:16 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 03:37:16 - INFO - Batch 1161/1300 - Loss: 0.3031 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:17 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 03:37:17 - INFO - Batch 1171/1300 - Loss: 0.1534 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:19 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 03:37:19 - INFO - Batch 1181/1300 - Loss: 0.2204 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:20 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 03:37:20 - INFO - Batch 1191/1300 - Loss: 0.9071 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:22 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 03:37:22 - INFO - Batch 1201/1300 - Loss: 0.2411 - Avg batch time: 0.16s\n",
      "2025-06-10 03:37:23 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 03:37:23 - INFO - Batch 1211/1300 - Loss: 0.2136 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:25 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 03:37:25 - INFO - Batch 1221/1300 - Loss: 0.4138 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:26 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 03:37:27 - INFO - Batch 1231/1300 - Loss: 0.5008 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:28 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 03:37:28 - INFO - Batch 1241/1300 - Loss: 1.1864 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:30 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 03:37:30 - INFO - Batch 1251/1300 - Loss: 0.4973 - Avg batch time: 0.16s\n",
      "2025-06-10 03:37:31 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 03:37:31 - INFO - Batch 1261/1300 - Loss: 0.2337 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:33 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 03:37:33 - INFO - Batch 1271/1300 - Loss: 0.5329 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:34 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 03:37:34 - INFO - Batch 1281/1300 - Loss: 0.3432 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:36 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 03:37:36 - INFO - Batch 1291/1300 - Loss: 0.4733 - Avg batch time: 0.15s\n",
      "2025-06-10 03:37:37 - INFO - \n",
      "Epoch 15 training completed in 200.74s\n",
      "2025-06-10 03:37:37 - INFO - Average training loss: 0.4515\n",
      "2025-06-10 03:37:55 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:  15%|â–| 15/100 [54:33<5:09:01, 218.14s/it, train_loss=0.4515, val_loss=0.4520, best_val_f1=0.3125, lr=2.50e-05, 2025-06-10 03:37:55 - INFO - Early stopping triggered: no 'val_f1' improvement in 10 epochs\n",
      "Epochs:  15%|â–| 15/100 [54:33<5:31:12, 233.80s/it, train_loss=0.4515, val_loss=0.4520, best_val_f1=0.3125, lr=2.50e-05, \n",
      "2025-06-10 03:37:55 - INFO - Training completed successfully!\n",
      "2025-06-10 03:37:55 - INFO - Final results: {'final_best_score': 0.3125, 'final_epoch': 15, 'total_bad_epochs': 10}\n",
      "2025-06-10 03:37:55 - INFO - Fold 1 completed successfully\n",
      "2025-06-10 03:37:55 - INFO - Best val_f1: 0.3125\n",
      "2025-06-10 03:37:55 - INFO - \n",
      "============================================================\n",
      "2025-06-10 03:37:55 - INFO - FOLD 2/5\n",
      "2025-06-10 03:37:55 - INFO - ============================================================\n",
      "2025-06-10 03:37:55 - INFO - Train samples: 10394\n",
      "2025-06-10 03:37:55 - INFO - Val samples: 2599\n",
      "2025-06-10 03:37:55 - INFO - Train positive ratio: 0.194\n",
      "2025-06-10 03:37:55 - INFO - Val positive ratio: 0.194\n",
      "2025-06-10 03:37:55 - INFO - Graph-level features are not included in this loader.\n",
      "2025-06-10 03:37:55 - INFO - Graph-level features are not included in this loader.\n",
      "2025-06-10 03:37:55 - INFO - EEGCNNBiLSTMGCN initialized:\n",
      "2025-06-10 03:37:55 - INFO -   - Node input dim: 3000\n",
      "2025-06-10 03:37:55 - INFO -   - Node feature dim (LSTM output): 128\n",
      "2025-06-10 03:37:55 - INFO -   - GCN hidden dim: 128\n",
      "2025-06-10 03:37:55 - INFO -   - Graph feature dim: 0\n",
      "2025-06-10 03:37:55 - INFO -   - Use graph features: False\n",
      "2025-06-10 03:37:55 - INFO -   - Classifier input dim: 96\n",
      "2025-06-10 03:37:55 - INFO -   - Num classes: 1\n",
      "2025-06-10 03:37:55 - INFO -   - Num channels: 19\n",
      "2025-06-10 03:37:55 - INFO - Initialized model: EEGCNNBiLSTMGCN\n",
      "2025-06-10 03:37:55 - INFO - Initialized optimizer: AdamW\n",
      "2025-06-10 03:37:55 - INFO - Initialized scheduler: ReduceLROnPlateau\n",
      "2025-06-10 03:37:55 - INFO - Starting training for fold 2\n",
      "2025-06-10 03:37:55 - INFO - Starting training setup...\n",
      "2025-06-10 03:37:55 - INFO - Model type: GNN\n",
      "2025-06-10 03:37:55 - INFO - Device: cuda\n",
      "2025-06-10 03:37:55 - INFO - Batch size: 8\n",
      "2025-06-10 03:37:55 - INFO - Number of epochs: 100\n",
      "2025-06-10 03:37:55 - INFO - Patience: 10\n",
      "2025-06-10 03:37:55 - INFO - Monitor metric: val_f1\n",
      "2025-06-10 03:37:55 - INFO - Initializing wandb...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>bad_epochs</td><td>â–â–â–‚â–ƒâ–ƒâ–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ</td></tr><tr><td>best_val_f1</td><td> â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch</td><td>â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ</td></tr><tr><td>learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–</td></tr><tr><td>train/accuracy</td><td>â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–†â–…â–„â–…â–‡â–‡â–ˆ</td></tr><tr><td>train/auroc</td><td>â–â–…â–‚â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆ</td></tr><tr><td>train/f1</td><td>â–â–â–â–â–ƒâ–„â–„â–â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>train/loss</td><td>â–ˆâ–…â–‡â–†â–„â–„â–„â–„â–‚â–‚â–ƒâ–‚â–‚â–â–</td></tr><tr><td>train/macro_f1</td><td>â–â–â–â–â–ƒâ–„â–„â–â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>train/precision</td><td>â–„â–‡â–†â–â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/recall</td><td>â–â–â–â–â–‚â–„â–„â–â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>val/accuracy</td><td>â–â–â–â–â–‡â–…â–â–…â–ƒâ–‚â–†â–ˆâ–†â–ˆâ–ˆ</td></tr><tr><td>val/auroc</td><td>â–ƒâ–„â–â–‡â–†â–…â–„â–…â–„â–ƒâ–†â–‡â–†â–‡â–ˆ</td></tr><tr><td>val/f1</td><td>â–â–â–â–â–ˆâ–…â–â–„â–„â–„â–…â–†â–†â–†â–‡</td></tr><tr><td>val/loss</td><td>â–„â–†â–ˆâ–„â–ƒâ–‚â–†â–â–…â–‡â–„â–â–â–†â–</td></tr><tr><td>val/macro_f1</td><td>â–â–â–â–â–ˆâ–…â–â–„â–„â–„â–…â–†â–†â–†â–‡</td></tr><tr><td>val/median_patient_f1</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val/median_patient_precision</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val/median_patient_recall</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val/precision</td><td>â–â–â–â–â–‡â–ˆâ–â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val/recall</td><td>â–â–â–â–â–ˆâ–„â–â–ƒâ–„â–„â–„â–…â–…â–†â–†</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>bad_epochs</td><td>9</td></tr><tr><td>best_val_f1</td><td>0.3125</td></tr><tr><td>epoch</td><td>15</td></tr><tr><td>learning_rate</td><td>3e-05</td></tr><tr><td>train/accuracy</td><td>0.82413</td></tr><tr><td>train/auroc</td><td>0.69705</td></tr><tr><td>train/f1</td><td>0.3372</td></tr><tr><td>train/loss</td><td>0.45148</td></tr><tr><td>train/macro_f1</td><td>0.3372</td></tr><tr><td>train/precision</td><td>0.62416</td></tr><tr><td>train/recall</td><td>0.231</td></tr><tr><td>val/accuracy</td><td>0.81493</td></tr><tr><td>val/auroc</td><td>0.71431</td></tr><tr><td>val/f1</td><td>0.25886</td></tr><tr><td>val/loss</td><td>0.45197</td></tr><tr><td>val/macro_f1</td><td>0.25886</td></tr><tr><td>val/median_patient_f1</td><td>0</td></tr><tr><td>val/median_patient_precision</td><td>0</td></tr><tr><td>val/median_patient_recall</td><td>0</td></tr><tr><td>val/precision</td><td>0.57931</td></tr><tr><td>val/recall</td><td>0.16667</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fold_1</strong> at: <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/4op4yi1o' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/4op4yi1o</a><br> View project at: <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250610_024320-4op4yi1o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ldibello/NeuroGraphNet/wandb/run-20250610_033755-zj0z0gon</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/zj0z0gon' target=\"_blank\">fold_2</a></strong> to <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/zj0z0gon' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/zj0z0gon</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 03:37:58 - INFO - ðŸ”— Wandb run initialized: fold_2\n",
      "2025-06-10 03:37:58 - INFO - Total training batches per epoch: 1300\n",
      "2025-06-10 03:37:58 - INFO - Starting training from epoch 1 to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Wandb initialized: fold_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]2025-06-10 03:37:58 - INFO - \n",
      "Epoch 1/100 - Training phase\n",
      "2025-06-10 03:37:58 - INFO - Processing batch 1/1300\n",
      "2025-06-10 03:37:58 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "2025-06-10 03:37:58 - INFO - Batch 1/1300 - Loss: 0.6842 - Avg batch time: 0.16s\n",
      "2025-06-10 03:37:59 - INFO - Processing batch 11/1300\n",
      "2025-06-10 03:38:00 - INFO - Batch 11/1300 - Loss: 0.6321 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:01 - INFO - Processing batch 21/1300\n",
      "2025-06-10 03:38:01 - INFO - Batch 21/1300 - Loss: 0.6732 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:03 - INFO - Processing batch 31/1300\n",
      "2025-06-10 03:38:03 - INFO - Batch 31/1300 - Loss: 0.6146 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:04 - INFO - Processing batch 41/1300\n",
      "2025-06-10 03:38:04 - INFO - Batch 41/1300 - Loss: 0.5850 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:06 - INFO - Processing batch 51/1300\n",
      "2025-06-10 03:38:06 - INFO - Batch 51/1300 - Loss: 0.4679 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:07 - INFO - Processing batch 61/1300\n",
      "2025-06-10 03:38:07 - INFO - Batch 61/1300 - Loss: 0.5557 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:09 - INFO - Processing batch 71/1300\n",
      "2025-06-10 03:38:09 - INFO - Batch 71/1300 - Loss: 0.7736 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:10 - INFO - Processing batch 81/1300\n",
      "2025-06-10 03:38:10 - INFO - Batch 81/1300 - Loss: 0.6089 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:12 - INFO - Processing batch 91/1300\n",
      "2025-06-10 03:38:12 - INFO - Batch 91/1300 - Loss: 0.9453 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:13 - INFO - Processing batch 101/1300\n",
      "2025-06-10 03:38:13 - INFO - Batch 101/1300 - Loss: 0.5930 - Avg batch time: 0.16s\n",
      "2025-06-10 03:38:15 - INFO - Processing batch 111/1300\n",
      "2025-06-10 03:38:15 - INFO - Batch 111/1300 - Loss: 0.4726 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:16 - INFO - Processing batch 121/1300\n",
      "2025-06-10 03:38:16 - INFO - Batch 121/1300 - Loss: 0.6659 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:18 - INFO - Processing batch 131/1300\n",
      "2025-06-10 03:38:18 - INFO - Batch 131/1300 - Loss: 0.8416 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:19 - INFO - Processing batch 141/1300\n",
      "2025-06-10 03:38:20 - INFO - Batch 141/1300 - Loss: 0.5416 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:21 - INFO - Processing batch 151/1300\n",
      "2025-06-10 03:38:21 - INFO - Batch 151/1300 - Loss: 0.5225 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:22 - INFO - Processing batch 161/1300\n",
      "2025-06-10 03:38:23 - INFO - Batch 161/1300 - Loss: 0.5752 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:24 - INFO - Processing batch 171/1300\n",
      "2025-06-10 03:38:24 - INFO - Batch 171/1300 - Loss: 0.1942 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:25 - INFO - Processing batch 181/1300\n",
      "2025-06-10 03:38:26 - INFO - Batch 181/1300 - Loss: 0.3279 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:27 - INFO - Processing batch 191/1300\n",
      "2025-06-10 03:38:27 - INFO - Batch 191/1300 - Loss: 0.3188 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:29 - INFO - Processing batch 201/1300\n",
      "2025-06-10 03:38:29 - INFO - Batch 201/1300 - Loss: 0.2274 - Avg batch time: 0.16s\n",
      "2025-06-10 03:38:30 - INFO - Processing batch 211/1300\n",
      "2025-06-10 03:38:30 - INFO - Batch 211/1300 - Loss: 0.7682 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:32 - INFO - Processing batch 221/1300\n",
      "2025-06-10 03:38:32 - INFO - Batch 221/1300 - Loss: 0.2151 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:33 - INFO - Processing batch 231/1300\n",
      "2025-06-10 03:38:33 - INFO - Batch 231/1300 - Loss: 0.2099 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:35 - INFO - Processing batch 241/1300\n",
      "2025-06-10 03:38:35 - INFO - Batch 241/1300 - Loss: 0.3183 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:36 - INFO - Processing batch 251/1300\n",
      "2025-06-10 03:38:36 - INFO - Batch 251/1300 - Loss: 0.4076 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:38 - INFO - Processing batch 261/1300\n",
      "2025-06-10 03:38:38 - INFO - Batch 261/1300 - Loss: 0.3186 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:39 - INFO - Processing batch 271/1300\n",
      "2025-06-10 03:38:39 - INFO - Batch 271/1300 - Loss: 0.3913 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:41 - INFO - Processing batch 281/1300\n",
      "2025-06-10 03:38:41 - INFO - Batch 281/1300 - Loss: 1.0877 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:42 - INFO - Processing batch 291/1300\n",
      "2025-06-10 03:38:43 - INFO - Batch 291/1300 - Loss: 1.0127 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:44 - INFO - Processing batch 301/1300\n",
      "2025-06-10 03:38:44 - INFO - Batch 301/1300 - Loss: 0.3358 - Avg batch time: 0.16s\n",
      "2025-06-10 03:38:45 - INFO - Processing batch 311/1300\n",
      "2025-06-10 03:38:46 - INFO - Batch 311/1300 - Loss: 0.3204 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:47 - INFO - Processing batch 321/1300\n",
      "2025-06-10 03:38:47 - INFO - Batch 321/1300 - Loss: 0.3871 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:49 - INFO - Processing batch 331/1300\n",
      "2025-06-10 03:38:49 - INFO - Batch 331/1300 - Loss: 0.6857 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:50 - INFO - Processing batch 341/1300\n",
      "2025-06-10 03:38:50 - INFO - Batch 341/1300 - Loss: 0.6921 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:52 - INFO - Processing batch 351/1300\n",
      "2025-06-10 03:38:52 - INFO - Batch 351/1300 - Loss: 0.6034 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:53 - INFO - Processing batch 361/1300\n",
      "2025-06-10 03:38:53 - INFO - Batch 361/1300 - Loss: 0.3960 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:55 - INFO - Processing batch 371/1300\n",
      "2025-06-10 03:38:55 - INFO - Batch 371/1300 - Loss: 0.7588 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:56 - INFO - Processing batch 381/1300\n",
      "2025-06-10 03:38:56 - INFO - Batch 381/1300 - Loss: 0.3851 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:58 - INFO - Processing batch 391/1300\n",
      "2025-06-10 03:38:58 - INFO - Batch 391/1300 - Loss: 0.1619 - Avg batch time: 0.15s\n",
      "2025-06-10 03:38:59 - INFO - Processing batch 401/1300\n",
      "2025-06-10 03:38:59 - INFO - Batch 401/1300 - Loss: 0.1946 - Avg batch time: 0.16s\n",
      "2025-06-10 03:39:01 - INFO - Processing batch 411/1300\n",
      "2025-06-10 03:39:01 - INFO - Batch 411/1300 - Loss: 0.7611 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:02 - INFO - Processing batch 421/1300\n",
      "2025-06-10 03:39:02 - INFO - Batch 421/1300 - Loss: 0.4632 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:04 - INFO - Processing batch 431/1300\n",
      "2025-06-10 03:39:04 - INFO - Batch 431/1300 - Loss: 0.0974 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:05 - INFO - Processing batch 441/1300\n",
      "2025-06-10 03:39:06 - INFO - Batch 441/1300 - Loss: 0.9339 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:07 - INFO - Processing batch 451/1300\n",
      "2025-06-10 03:39:07 - INFO - Batch 451/1300 - Loss: 0.3060 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:08 - INFO - Processing batch 461/1300\n",
      "2025-06-10 03:39:09 - INFO - Batch 461/1300 - Loss: 0.4100 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:10 - INFO - Processing batch 471/1300\n",
      "2025-06-10 03:39:10 - INFO - Batch 471/1300 - Loss: 0.5790 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:11 - INFO - Processing batch 481/1300\n",
      "2025-06-10 03:39:12 - INFO - Batch 481/1300 - Loss: 0.2758 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:13 - INFO - Processing batch 491/1300\n",
      "2025-06-10 03:39:13 - INFO - Batch 491/1300 - Loss: 0.3997 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:15 - INFO - Processing batch 501/1300\n",
      "2025-06-10 03:39:15 - INFO - Batch 501/1300 - Loss: 0.4215 - Avg batch time: 0.16s\n",
      "2025-06-10 03:39:16 - INFO - Processing batch 511/1300\n",
      "2025-06-10 03:39:16 - INFO - Batch 511/1300 - Loss: 0.7006 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:18 - INFO - Processing batch 521/1300\n",
      "2025-06-10 03:39:18 - INFO - Batch 521/1300 - Loss: 0.4160 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:19 - INFO - Processing batch 531/1300\n",
      "2025-06-10 03:39:19 - INFO - Batch 531/1300 - Loss: 0.3980 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:21 - INFO - Processing batch 541/1300\n",
      "2025-06-10 03:39:21 - INFO - Batch 541/1300 - Loss: 1.0257 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:22 - INFO - Processing batch 551/1300\n",
      "2025-06-10 03:39:22 - INFO - Batch 551/1300 - Loss: 0.1138 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:24 - INFO - Processing batch 561/1300\n",
      "2025-06-10 03:39:24 - INFO - Batch 561/1300 - Loss: 0.2959 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:25 - INFO - Processing batch 571/1300\n",
      "2025-06-10 03:39:25 - INFO - Batch 571/1300 - Loss: 0.7445 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:27 - INFO - Processing batch 581/1300\n",
      "2025-06-10 03:39:27 - INFO - Batch 581/1300 - Loss: 0.5767 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:28 - INFO - Processing batch 591/1300\n",
      "2025-06-10 03:39:29 - INFO - Batch 591/1300 - Loss: 0.6342 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:30 - INFO - Processing batch 601/1300\n",
      "2025-06-10 03:39:30 - INFO - Batch 601/1300 - Loss: 0.3297 - Avg batch time: 0.16s\n",
      "2025-06-10 03:39:32 - INFO - Processing batch 611/1300\n",
      "2025-06-10 03:39:32 - INFO - Batch 611/1300 - Loss: 0.2856 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:33 - INFO - Processing batch 621/1300\n",
      "2025-06-10 03:39:33 - INFO - Batch 621/1300 - Loss: 0.5210 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:35 - INFO - Processing batch 631/1300\n",
      "2025-06-10 03:39:35 - INFO - Batch 631/1300 - Loss: 0.3922 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:36 - INFO - Processing batch 641/1300\n",
      "2025-06-10 03:39:36 - INFO - Batch 641/1300 - Loss: 0.5668 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:38 - INFO - Processing batch 651/1300\n",
      "2025-06-10 03:39:38 - INFO - Batch 651/1300 - Loss: 0.4147 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:39 - INFO - Processing batch 661/1300\n",
      "2025-06-10 03:39:39 - INFO - Batch 661/1300 - Loss: 0.6298 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:41 - INFO - Processing batch 671/1300\n",
      "2025-06-10 03:39:41 - INFO - Batch 671/1300 - Loss: 0.2277 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:42 - INFO - Processing batch 681/1300\n",
      "2025-06-10 03:39:42 - INFO - Batch 681/1300 - Loss: 0.4196 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:44 - INFO - Processing batch 691/1300\n",
      "2025-06-10 03:39:44 - INFO - Batch 691/1300 - Loss: 0.3653 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:45 - INFO - Processing batch 701/1300\n",
      "2025-06-10 03:39:45 - INFO - Batch 701/1300 - Loss: 0.1955 - Avg batch time: 0.16s\n",
      "2025-06-10 03:39:47 - INFO - Processing batch 711/1300\n",
      "2025-06-10 03:39:47 - INFO - Batch 711/1300 - Loss: 0.3015 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:48 - INFO - Processing batch 721/1300\n",
      "2025-06-10 03:39:49 - INFO - Batch 721/1300 - Loss: 1.1353 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:50 - INFO - Processing batch 731/1300\n",
      "2025-06-10 03:39:50 - INFO - Batch 731/1300 - Loss: 0.6337 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:51 - INFO - Processing batch 741/1300\n",
      "2025-06-10 03:39:52 - INFO - Batch 741/1300 - Loss: 0.4797 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:53 - INFO - Processing batch 751/1300\n",
      "2025-06-10 03:39:53 - INFO - Batch 751/1300 - Loss: 0.5191 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:54 - INFO - Processing batch 761/1300\n",
      "2025-06-10 03:39:55 - INFO - Batch 761/1300 - Loss: 0.1867 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:56 - INFO - Processing batch 771/1300\n",
      "2025-06-10 03:39:56 - INFO - Batch 771/1300 - Loss: 0.6798 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:58 - INFO - Processing batch 781/1300\n",
      "2025-06-10 03:39:58 - INFO - Batch 781/1300 - Loss: 1.3524 - Avg batch time: 0.15s\n",
      "2025-06-10 03:39:59 - INFO - Processing batch 791/1300\n",
      "2025-06-10 03:39:59 - INFO - Batch 791/1300 - Loss: 0.4987 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:01 - INFO - Processing batch 801/1300\n",
      "2025-06-10 03:40:01 - INFO - Batch 801/1300 - Loss: 0.5428 - Avg batch time: 0.16s\n",
      "2025-06-10 03:40:02 - INFO - Processing batch 811/1300\n",
      "2025-06-10 03:40:02 - INFO - Batch 811/1300 - Loss: 0.1941 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:04 - INFO - Processing batch 821/1300\n",
      "2025-06-10 03:40:04 - INFO - Batch 821/1300 - Loss: 1.0206 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:05 - INFO - Processing batch 831/1300\n",
      "2025-06-10 03:40:05 - INFO - Batch 831/1300 - Loss: 0.5794 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:07 - INFO - Processing batch 841/1300\n",
      "2025-06-10 03:40:07 - INFO - Batch 841/1300 - Loss: 0.2599 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:08 - INFO - Processing batch 851/1300\n",
      "2025-06-10 03:40:08 - INFO - Batch 851/1300 - Loss: 0.4911 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:10 - INFO - Processing batch 861/1300\n",
      "2025-06-10 03:40:10 - INFO - Batch 861/1300 - Loss: 0.5069 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:11 - INFO - Processing batch 871/1300\n",
      "2025-06-10 03:40:12 - INFO - Batch 871/1300 - Loss: 0.4889 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:13 - INFO - Processing batch 881/1300\n",
      "2025-06-10 03:40:13 - INFO - Batch 881/1300 - Loss: 0.5143 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:14 - INFO - Processing batch 891/1300\n",
      "2025-06-10 03:40:15 - INFO - Batch 891/1300 - Loss: 0.3036 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:16 - INFO - Processing batch 901/1300\n",
      "2025-06-10 03:40:16 - INFO - Batch 901/1300 - Loss: 0.5954 - Avg batch time: 0.16s\n",
      "2025-06-10 03:40:18 - INFO - Processing batch 911/1300\n",
      "2025-06-10 03:40:18 - INFO - Batch 911/1300 - Loss: 0.3277 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:19 - INFO - Processing batch 921/1300\n",
      "2025-06-10 03:40:19 - INFO - Batch 921/1300 - Loss: 1.0427 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:21 - INFO - Processing batch 931/1300\n",
      "2025-06-10 03:40:21 - INFO - Batch 931/1300 - Loss: 0.9025 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:22 - INFO - Processing batch 941/1300\n",
      "2025-06-10 03:40:22 - INFO - Batch 941/1300 - Loss: 0.1490 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:24 - INFO - Processing batch 951/1300\n",
      "2025-06-10 03:40:24 - INFO - Batch 951/1300 - Loss: 0.4342 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:25 - INFO - Processing batch 961/1300\n",
      "2025-06-10 03:40:25 - INFO - Batch 961/1300 - Loss: 0.9322 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:27 - INFO - Processing batch 971/1300\n",
      "2025-06-10 03:40:27 - INFO - Batch 971/1300 - Loss: 0.2337 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:28 - INFO - Processing batch 981/1300\n",
      "2025-06-10 03:40:28 - INFO - Batch 981/1300 - Loss: 0.4028 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:30 - INFO - Processing batch 991/1300\n",
      "2025-06-10 03:40:30 - INFO - Batch 991/1300 - Loss: 0.5517 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:31 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 03:40:32 - INFO - Batch 1001/1300 - Loss: 0.6305 - Avg batch time: 0.16s\n",
      "2025-06-10 03:40:33 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 03:40:33 - INFO - Batch 1011/1300 - Loss: 0.4446 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:34 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 03:40:35 - INFO - Batch 1021/1300 - Loss: 1.0390 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:36 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 03:40:36 - INFO - Batch 1031/1300 - Loss: 0.0950 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:37 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 03:40:38 - INFO - Batch 1041/1300 - Loss: 0.2758 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:39 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 03:40:39 - INFO - Batch 1051/1300 - Loss: 0.6403 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:41 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 03:40:41 - INFO - Batch 1061/1300 - Loss: 0.5654 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:42 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 03:40:42 - INFO - Batch 1071/1300 - Loss: 0.2534 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:44 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 03:40:44 - INFO - Batch 1081/1300 - Loss: 0.5960 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:45 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 03:40:45 - INFO - Batch 1091/1300 - Loss: 0.6392 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:47 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 03:40:47 - INFO - Batch 1101/1300 - Loss: 0.2995 - Avg batch time: 0.16s\n",
      "2025-06-10 03:40:48 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 03:40:48 - INFO - Batch 1111/1300 - Loss: 0.9605 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:50 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 03:40:50 - INFO - Batch 1121/1300 - Loss: 0.6837 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:51 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 03:40:51 - INFO - Batch 1131/1300 - Loss: 0.3251 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:53 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 03:40:53 - INFO - Batch 1141/1300 - Loss: 0.4844 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:54 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 03:40:54 - INFO - Batch 1151/1300 - Loss: 0.3625 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:56 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 03:40:56 - INFO - Batch 1161/1300 - Loss: 0.2682 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:57 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 03:40:58 - INFO - Batch 1171/1300 - Loss: 0.5919 - Avg batch time: 0.15s\n",
      "2025-06-10 03:40:59 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 03:40:59 - INFO - Batch 1181/1300 - Loss: 0.0769 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:00 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 03:41:01 - INFO - Batch 1191/1300 - Loss: 0.2912 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:02 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 03:41:02 - INFO - Batch 1201/1300 - Loss: 0.7402 - Avg batch time: 0.16s\n",
      "2025-06-10 03:41:04 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 03:41:04 - INFO - Batch 1211/1300 - Loss: 0.4297 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:05 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 03:41:05 - INFO - Batch 1221/1300 - Loss: 0.6911 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:07 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 03:41:07 - INFO - Batch 1231/1300 - Loss: 1.0809 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:08 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 03:41:08 - INFO - Batch 1241/1300 - Loss: 0.4257 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:10 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 03:41:10 - INFO - Batch 1251/1300 - Loss: 0.3337 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:11 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 03:41:11 - INFO - Batch 1261/1300 - Loss: 0.4539 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:13 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 03:41:13 - INFO - Batch 1271/1300 - Loss: 0.7427 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:14 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 03:41:14 - INFO - Batch 1281/1300 - Loss: 0.1150 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:16 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 03:41:16 - INFO - Batch 1291/1300 - Loss: 0.5497 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:17 - INFO - \n",
      "Epoch 1 training completed in 199.64s\n",
      "2025-06-10 03:41:17 - INFO - Average training loss: 0.4900\n",
      "2025-06-10 03:41:35 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:   2%| | 2/100 [03:37<5:54:51, 217.26s/it, train_loss=0.4900, val_loss=0.4567, best_val_f1=0.0000, lr=1.00e-04, b2025-06-10 03:41:35 - INFO - \n",
      "Epoch 2/100 - Training phase\n",
      "2025-06-10 03:41:35 - INFO - Processing batch 1/1300\n",
      "2025-06-10 03:41:35 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 03:41:35 - INFO - Batch 1/1300 - Loss: 0.3458 - Avg batch time: 0.16s\n",
      "2025-06-10 03:41:37 - INFO - Processing batch 11/1300\n",
      "2025-06-10 03:41:37 - INFO - Batch 11/1300 - Loss: 0.1882 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:38 - INFO - Processing batch 21/1300\n",
      "2025-06-10 03:41:38 - INFO - Batch 21/1300 - Loss: 0.4760 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:40 - INFO - Processing batch 31/1300\n",
      "2025-06-10 03:41:40 - INFO - Batch 31/1300 - Loss: 0.6306 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:41 - INFO - Processing batch 41/1300\n",
      "2025-06-10 03:41:42 - INFO - Batch 41/1300 - Loss: 0.0730 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:43 - INFO - Processing batch 51/1300\n",
      "2025-06-10 03:41:43 - INFO - Batch 51/1300 - Loss: 0.6020 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:44 - INFO - Processing batch 61/1300\n",
      "2025-06-10 03:41:45 - INFO - Batch 61/1300 - Loss: 0.6597 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:46 - INFO - Processing batch 71/1300\n",
      "2025-06-10 03:41:46 - INFO - Batch 71/1300 - Loss: 0.2616 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:48 - INFO - Processing batch 81/1300\n",
      "2025-06-10 03:41:48 - INFO - Batch 81/1300 - Loss: 0.4349 - Avg batch time: 0.16s\n",
      "2025-06-10 03:41:49 - INFO - Processing batch 91/1300\n",
      "2025-06-10 03:41:49 - INFO - Batch 91/1300 - Loss: 0.7469 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:51 - INFO - Processing batch 101/1300\n",
      "2025-06-10 03:41:51 - INFO - Batch 101/1300 - Loss: 0.4527 - Avg batch time: 0.16s\n",
      "2025-06-10 03:41:52 - INFO - Processing batch 111/1300\n",
      "2025-06-10 03:41:52 - INFO - Batch 111/1300 - Loss: 0.4092 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:54 - INFO - Processing batch 121/1300\n",
      "2025-06-10 03:41:54 - INFO - Batch 121/1300 - Loss: 0.6873 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:55 - INFO - Processing batch 131/1300\n",
      "2025-06-10 03:41:55 - INFO - Batch 131/1300 - Loss: 0.4967 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:57 - INFO - Processing batch 141/1300\n",
      "2025-06-10 03:41:57 - INFO - Batch 141/1300 - Loss: 0.6518 - Avg batch time: 0.15s\n",
      "2025-06-10 03:41:58 - INFO - Processing batch 151/1300\n",
      "2025-06-10 03:41:58 - INFO - Batch 151/1300 - Loss: 0.4414 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:00 - INFO - Processing batch 161/1300\n",
      "2025-06-10 03:42:00 - INFO - Batch 161/1300 - Loss: 0.4626 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:01 - INFO - Processing batch 171/1300\n",
      "2025-06-10 03:42:02 - INFO - Batch 171/1300 - Loss: 0.5386 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:03 - INFO - Processing batch 181/1300\n",
      "2025-06-10 03:42:03 - INFO - Batch 181/1300 - Loss: 0.4546 - Avg batch time: 0.16s\n",
      "2025-06-10 03:42:05 - INFO - Processing batch 191/1300\n",
      "2025-06-10 03:42:05 - INFO - Batch 191/1300 - Loss: 0.5928 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:06 - INFO - Processing batch 201/1300\n",
      "2025-06-10 03:42:06 - INFO - Batch 201/1300 - Loss: 0.4486 - Avg batch time: 0.16s\n",
      "2025-06-10 03:42:08 - INFO - Processing batch 211/1300\n",
      "2025-06-10 03:42:08 - INFO - Batch 211/1300 - Loss: 0.4191 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:09 - INFO - Processing batch 221/1300\n",
      "2025-06-10 03:42:09 - INFO - Batch 221/1300 - Loss: 0.3189 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:11 - INFO - Processing batch 231/1300\n",
      "2025-06-10 03:42:11 - INFO - Batch 231/1300 - Loss: 1.0947 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:12 - INFO - Processing batch 241/1300\n",
      "2025-06-10 03:42:12 - INFO - Batch 241/1300 - Loss: 0.9618 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:14 - INFO - Processing batch 251/1300\n",
      "2025-06-10 03:42:14 - INFO - Batch 251/1300 - Loss: 0.5922 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:15 - INFO - Processing batch 261/1300\n",
      "2025-06-10 03:42:15 - INFO - Batch 261/1300 - Loss: 0.7806 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:17 - INFO - Processing batch 271/1300\n",
      "2025-06-10 03:42:17 - INFO - Batch 271/1300 - Loss: 0.6946 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:18 - INFO - Processing batch 281/1300\n",
      "2025-06-10 03:42:19 - INFO - Batch 281/1300 - Loss: 0.7266 - Avg batch time: 0.16s\n",
      "2025-06-10 03:42:20 - INFO - Processing batch 291/1300\n",
      "2025-06-10 03:42:20 - INFO - Batch 291/1300 - Loss: 0.3732 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:22 - INFO - Processing batch 301/1300\n",
      "2025-06-10 03:42:22 - INFO - Batch 301/1300 - Loss: 0.5479 - Avg batch time: 0.16s\n",
      "2025-06-10 03:42:23 - INFO - Processing batch 311/1300\n",
      "2025-06-10 03:42:23 - INFO - Batch 311/1300 - Loss: 0.2018 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:25 - INFO - Processing batch 321/1300\n",
      "2025-06-10 03:42:25 - INFO - Batch 321/1300 - Loss: 0.8003 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:26 - INFO - Processing batch 331/1300\n",
      "2025-06-10 03:42:26 - INFO - Batch 331/1300 - Loss: 0.1216 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:28 - INFO - Processing batch 341/1300\n",
      "2025-06-10 03:42:28 - INFO - Batch 341/1300 - Loss: 0.4933 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:29 - INFO - Processing batch 351/1300\n",
      "2025-06-10 03:42:29 - INFO - Batch 351/1300 - Loss: 0.8471 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:31 - INFO - Processing batch 361/1300\n",
      "2025-06-10 03:42:31 - INFO - Batch 361/1300 - Loss: 0.3411 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:32 - INFO - Processing batch 371/1300\n",
      "2025-06-10 03:42:32 - INFO - Batch 371/1300 - Loss: 0.4995 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:34 - INFO - Processing batch 381/1300\n",
      "2025-06-10 03:42:34 - INFO - Batch 381/1300 - Loss: 0.5253 - Avg batch time: 0.16s\n",
      "2025-06-10 03:42:35 - INFO - Processing batch 391/1300\n",
      "2025-06-10 03:42:36 - INFO - Batch 391/1300 - Loss: 0.3288 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:37 - INFO - Processing batch 401/1300\n",
      "2025-06-10 03:42:37 - INFO - Batch 401/1300 - Loss: 0.9745 - Avg batch time: 0.16s\n",
      "2025-06-10 03:42:38 - INFO - Processing batch 411/1300\n",
      "2025-06-10 03:42:39 - INFO - Batch 411/1300 - Loss: 0.4714 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:40 - INFO - Processing batch 421/1300\n",
      "2025-06-10 03:42:40 - INFO - Batch 421/1300 - Loss: 0.7179 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:42 - INFO - Processing batch 431/1300\n",
      "2025-06-10 03:42:42 - INFO - Batch 431/1300 - Loss: 0.2337 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:43 - INFO - Processing batch 441/1300\n",
      "2025-06-10 03:42:43 - INFO - Batch 441/1300 - Loss: 0.2015 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:45 - INFO - Processing batch 451/1300\n",
      "2025-06-10 03:42:45 - INFO - Batch 451/1300 - Loss: 0.6836 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:46 - INFO - Processing batch 461/1300\n",
      "2025-06-10 03:42:46 - INFO - Batch 461/1300 - Loss: 0.2383 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:48 - INFO - Processing batch 471/1300\n",
      "2025-06-10 03:42:48 - INFO - Batch 471/1300 - Loss: 0.6119 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:49 - INFO - Processing batch 481/1300\n",
      "2025-06-10 03:42:49 - INFO - Batch 481/1300 - Loss: 0.5842 - Avg batch time: 0.16s\n",
      "2025-06-10 03:42:51 - INFO - Processing batch 491/1300\n",
      "2025-06-10 03:42:51 - INFO - Batch 491/1300 - Loss: 0.7909 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:52 - INFO - Processing batch 501/1300\n",
      "2025-06-10 03:42:53 - INFO - Batch 501/1300 - Loss: 0.5905 - Avg batch time: 0.16s\n",
      "2025-06-10 03:42:54 - INFO - Processing batch 511/1300\n",
      "2025-06-10 03:42:54 - INFO - Batch 511/1300 - Loss: 0.7865 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:55 - INFO - Processing batch 521/1300\n",
      "2025-06-10 03:42:56 - INFO - Batch 521/1300 - Loss: 0.3862 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:57 - INFO - Processing batch 531/1300\n",
      "2025-06-10 03:42:57 - INFO - Batch 531/1300 - Loss: 0.5448 - Avg batch time: 0.15s\n",
      "2025-06-10 03:42:59 - INFO - Processing batch 541/1300\n",
      "2025-06-10 03:42:59 - INFO - Batch 541/1300 - Loss: 0.2420 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:00 - INFO - Processing batch 551/1300\n",
      "2025-06-10 03:43:00 - INFO - Batch 551/1300 - Loss: 1.0612 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:02 - INFO - Processing batch 561/1300\n",
      "2025-06-10 03:43:02 - INFO - Batch 561/1300 - Loss: 0.4159 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:03 - INFO - Processing batch 571/1300\n",
      "2025-06-10 03:43:03 - INFO - Batch 571/1300 - Loss: 0.6171 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:05 - INFO - Processing batch 581/1300\n",
      "2025-06-10 03:43:05 - INFO - Batch 581/1300 - Loss: 0.4187 - Avg batch time: 0.16s\n",
      "2025-06-10 03:43:06 - INFO - Processing batch 591/1300\n",
      "2025-06-10 03:43:06 - INFO - Batch 591/1300 - Loss: 0.6058 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:08 - INFO - Processing batch 601/1300\n",
      "2025-06-10 03:43:08 - INFO - Batch 601/1300 - Loss: 0.3110 - Avg batch time: 0.16s\n",
      "2025-06-10 03:43:09 - INFO - Processing batch 611/1300\n",
      "2025-06-10 03:43:10 - INFO - Batch 611/1300 - Loss: 0.5525 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:11 - INFO - Processing batch 621/1300\n",
      "2025-06-10 03:43:11 - INFO - Batch 621/1300 - Loss: 0.4108 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:12 - INFO - Processing batch 631/1300\n",
      "2025-06-10 03:43:13 - INFO - Batch 631/1300 - Loss: 0.3795 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:14 - INFO - Processing batch 641/1300\n",
      "2025-06-10 03:43:14 - INFO - Batch 641/1300 - Loss: 0.3082 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:16 - INFO - Processing batch 651/1300\n",
      "2025-06-10 03:43:16 - INFO - Batch 651/1300 - Loss: 0.8364 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:17 - INFO - Processing batch 661/1300\n",
      "2025-06-10 03:43:17 - INFO - Batch 661/1300 - Loss: 0.5358 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:19 - INFO - Processing batch 671/1300\n",
      "2025-06-10 03:43:19 - INFO - Batch 671/1300 - Loss: 0.6094 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:20 - INFO - Processing batch 681/1300\n",
      "2025-06-10 03:43:20 - INFO - Batch 681/1300 - Loss: 0.9484 - Avg batch time: 0.16s\n",
      "2025-06-10 03:43:22 - INFO - Processing batch 691/1300\n",
      "2025-06-10 03:43:22 - INFO - Batch 691/1300 - Loss: 0.5933 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:23 - INFO - Processing batch 701/1300\n",
      "2025-06-10 03:43:23 - INFO - Batch 701/1300 - Loss: 0.2393 - Avg batch time: 0.16s\n",
      "2025-06-10 03:43:25 - INFO - Processing batch 711/1300\n",
      "2025-06-10 03:43:25 - INFO - Batch 711/1300 - Loss: 0.5868 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:26 - INFO - Processing batch 721/1300\n",
      "2025-06-10 03:43:26 - INFO - Batch 721/1300 - Loss: 0.6752 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:28 - INFO - Processing batch 731/1300\n",
      "2025-06-10 03:43:28 - INFO - Batch 731/1300 - Loss: 0.2538 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:29 - INFO - Processing batch 741/1300\n",
      "2025-06-10 03:43:30 - INFO - Batch 741/1300 - Loss: 0.7634 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:31 - INFO - Processing batch 751/1300\n",
      "2025-06-10 03:43:31 - INFO - Batch 751/1300 - Loss: 0.2437 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:32 - INFO - Processing batch 761/1300\n",
      "2025-06-10 03:43:33 - INFO - Batch 761/1300 - Loss: 0.8015 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:34 - INFO - Processing batch 771/1300\n",
      "2025-06-10 03:43:34 - INFO - Batch 771/1300 - Loss: 0.3891 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:36 - INFO - Processing batch 781/1300\n",
      "2025-06-10 03:43:36 - INFO - Batch 781/1300 - Loss: 0.4314 - Avg batch time: 0.16s\n",
      "2025-06-10 03:43:37 - INFO - Processing batch 791/1300\n",
      "2025-06-10 03:43:37 - INFO - Batch 791/1300 - Loss: 0.5442 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:39 - INFO - Processing batch 801/1300\n",
      "2025-06-10 03:43:39 - INFO - Batch 801/1300 - Loss: 0.3352 - Avg batch time: 0.16s\n",
      "2025-06-10 03:43:40 - INFO - Processing batch 811/1300\n",
      "2025-06-10 03:43:40 - INFO - Batch 811/1300 - Loss: 0.6079 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:42 - INFO - Processing batch 821/1300\n",
      "2025-06-10 03:43:42 - INFO - Batch 821/1300 - Loss: 0.7657 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:43 - INFO - Processing batch 831/1300\n",
      "2025-06-10 03:43:43 - INFO - Batch 831/1300 - Loss: 0.2410 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:45 - INFO - Processing batch 841/1300\n",
      "2025-06-10 03:43:45 - INFO - Batch 841/1300 - Loss: 0.1972 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:46 - INFO - Processing batch 851/1300\n",
      "2025-06-10 03:43:47 - INFO - Batch 851/1300 - Loss: 0.1227 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:48 - INFO - Processing batch 861/1300\n",
      "2025-06-10 03:43:48 - INFO - Batch 861/1300 - Loss: 0.7745 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:49 - INFO - Processing batch 871/1300\n",
      "2025-06-10 03:43:50 - INFO - Batch 871/1300 - Loss: 0.1236 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:51 - INFO - Processing batch 881/1300\n",
      "2025-06-10 03:43:51 - INFO - Batch 881/1300 - Loss: 0.0766 - Avg batch time: 0.16s\n",
      "2025-06-10 03:43:53 - INFO - Processing batch 891/1300\n",
      "2025-06-10 03:43:53 - INFO - Batch 891/1300 - Loss: 0.4251 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:54 - INFO - Processing batch 901/1300\n",
      "2025-06-10 03:43:54 - INFO - Batch 901/1300 - Loss: 0.1607 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:56 - INFO - Processing batch 911/1300\n",
      "2025-06-10 03:43:56 - INFO - Batch 911/1300 - Loss: 1.6163 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:57 - INFO - Processing batch 921/1300\n",
      "2025-06-10 03:43:57 - INFO - Batch 921/1300 - Loss: 0.2014 - Avg batch time: 0.15s\n",
      "2025-06-10 03:43:59 - INFO - Processing batch 931/1300\n",
      "2025-06-10 03:43:59 - INFO - Batch 931/1300 - Loss: 0.3591 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:00 - INFO - Processing batch 941/1300\n",
      "2025-06-10 03:44:00 - INFO - Batch 941/1300 - Loss: 0.5465 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:02 - INFO - Processing batch 951/1300\n",
      "2025-06-10 03:44:02 - INFO - Batch 951/1300 - Loss: 0.3107 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:03 - INFO - Processing batch 961/1300\n",
      "2025-06-10 03:44:03 - INFO - Batch 961/1300 - Loss: 0.9313 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:05 - INFO - Processing batch 971/1300\n",
      "2025-06-10 03:44:05 - INFO - Batch 971/1300 - Loss: 0.8700 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:06 - INFO - Processing batch 981/1300\n",
      "2025-06-10 03:44:07 - INFO - Batch 981/1300 - Loss: 0.1048 - Avg batch time: 0.16s\n",
      "2025-06-10 03:44:08 - INFO - Processing batch 991/1300\n",
      "2025-06-10 03:44:08 - INFO - Batch 991/1300 - Loss: 0.9930 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:09 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 03:44:10 - INFO - Batch 1001/1300 - Loss: 0.1419 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:11 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 03:44:11 - INFO - Batch 1011/1300 - Loss: 0.3950 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:13 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 03:44:13 - INFO - Batch 1021/1300 - Loss: 0.4515 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:14 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 03:44:14 - INFO - Batch 1031/1300 - Loss: 0.6194 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:16 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 03:44:16 - INFO - Batch 1041/1300 - Loss: 0.1053 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:17 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 03:44:17 - INFO - Batch 1051/1300 - Loss: 0.0945 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:19 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 03:44:19 - INFO - Batch 1061/1300 - Loss: 0.4542 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:20 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 03:44:20 - INFO - Batch 1071/1300 - Loss: 0.4548 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:22 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 03:44:22 - INFO - Batch 1081/1300 - Loss: 0.1771 - Avg batch time: 0.16s\n",
      "2025-06-10 03:44:23 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 03:44:23 - INFO - Batch 1091/1300 - Loss: 0.4352 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:25 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 03:44:25 - INFO - Batch 1101/1300 - Loss: 0.3233 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:26 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 03:44:27 - INFO - Batch 1111/1300 - Loss: 0.2853 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:28 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 03:44:28 - INFO - Batch 1121/1300 - Loss: 0.5328 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:29 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 03:44:30 - INFO - Batch 1131/1300 - Loss: 0.0950 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:31 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 03:44:31 - INFO - Batch 1141/1300 - Loss: 0.3069 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:32 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 03:44:33 - INFO - Batch 1151/1300 - Loss: 0.3198 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:34 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 03:44:34 - INFO - Batch 1161/1300 - Loss: 1.0555 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:36 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 03:44:36 - INFO - Batch 1171/1300 - Loss: 0.2920 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:37 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 03:44:37 - INFO - Batch 1181/1300 - Loss: 0.4640 - Avg batch time: 0.16s\n",
      "2025-06-10 03:44:39 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 03:44:39 - INFO - Batch 1191/1300 - Loss: 0.5423 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:40 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 03:44:40 - INFO - Batch 1201/1300 - Loss: 0.5712 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:42 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 03:44:42 - INFO - Batch 1211/1300 - Loss: 0.4666 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:43 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 03:44:43 - INFO - Batch 1221/1300 - Loss: 0.3120 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:45 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 03:44:45 - INFO - Batch 1231/1300 - Loss: 0.3722 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:46 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 03:44:46 - INFO - Batch 1241/1300 - Loss: 0.7000 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:48 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 03:44:48 - INFO - Batch 1251/1300 - Loss: 0.3554 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:49 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 03:44:50 - INFO - Batch 1261/1300 - Loss: 0.6247 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:51 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 03:44:51 - INFO - Batch 1271/1300 - Loss: 0.4423 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:52 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 03:44:53 - INFO - Batch 1281/1300 - Loss: 0.6493 - Avg batch time: 0.16s\n",
      "2025-06-10 03:44:54 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 03:44:54 - INFO - Batch 1291/1300 - Loss: 0.5218 - Avg batch time: 0.15s\n",
      "2025-06-10 03:44:56 - INFO - \n",
      "Epoch 2 training completed in 200.57s\n",
      "2025-06-10 03:44:56 - INFO - Average training loss: 0.4927\n",
      "2025-06-10 03:45:13 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:   3%| | 3/100 [07:15<5:52:09, 217.83s/it, train_loss=0.4927, val_loss=0.4826, best_val_f1=0.0000, lr=1.00e-04, b2025-06-10 03:45:13 - INFO - \n",
      "Epoch 3/100 - Training phase\n",
      "2025-06-10 03:45:13 - INFO - Processing batch 1/1300\n",
      "2025-06-10 03:45:13 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 03:45:14 - INFO - Batch 1/1300 - Loss: 0.5285 - Avg batch time: 0.16s\n",
      "2025-06-10 03:45:15 - INFO - Processing batch 11/1300\n",
      "2025-06-10 03:45:15 - INFO - Batch 11/1300 - Loss: 1.0901 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:17 - INFO - Processing batch 21/1300\n",
      "2025-06-10 03:45:17 - INFO - Batch 21/1300 - Loss: 0.4620 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:18 - INFO - Processing batch 31/1300\n",
      "2025-06-10 03:45:18 - INFO - Batch 31/1300 - Loss: 0.3651 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:20 - INFO - Processing batch 41/1300\n",
      "2025-06-10 03:45:20 - INFO - Batch 41/1300 - Loss: 0.3655 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:21 - INFO - Processing batch 51/1300\n",
      "2025-06-10 03:45:21 - INFO - Batch 51/1300 - Loss: 0.3842 - Avg batch time: 0.16s\n",
      "2025-06-10 03:45:23 - INFO - Processing batch 61/1300\n",
      "2025-06-10 03:45:23 - INFO - Batch 61/1300 - Loss: 0.5224 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:24 - INFO - Processing batch 71/1300\n",
      "2025-06-10 03:45:24 - INFO - Batch 71/1300 - Loss: 0.3156 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:26 - INFO - Processing batch 81/1300\n",
      "2025-06-10 03:45:26 - INFO - Batch 81/1300 - Loss: 0.3204 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:27 - INFO - Processing batch 91/1300\n",
      "2025-06-10 03:45:28 - INFO - Batch 91/1300 - Loss: 0.4508 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:29 - INFO - Processing batch 101/1300\n",
      "2025-06-10 03:45:29 - INFO - Batch 101/1300 - Loss: 0.1541 - Avg batch time: 0.16s\n",
      "2025-06-10 03:45:30 - INFO - Processing batch 111/1300\n",
      "2025-06-10 03:45:31 - INFO - Batch 111/1300 - Loss: 0.3405 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:32 - INFO - Processing batch 121/1300\n",
      "2025-06-10 03:45:32 - INFO - Batch 121/1300 - Loss: 0.7730 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:34 - INFO - Processing batch 131/1300\n",
      "2025-06-10 03:45:34 - INFO - Batch 131/1300 - Loss: 0.1481 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:35 - INFO - Processing batch 141/1300\n",
      "2025-06-10 03:45:35 - INFO - Batch 141/1300 - Loss: 0.7905 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:37 - INFO - Processing batch 151/1300\n",
      "2025-06-10 03:45:37 - INFO - Batch 151/1300 - Loss: 0.7820 - Avg batch time: 0.16s\n",
      "2025-06-10 03:45:38 - INFO - Processing batch 161/1300\n",
      "2025-06-10 03:45:38 - INFO - Batch 161/1300 - Loss: 1.0593 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:40 - INFO - Processing batch 171/1300\n",
      "2025-06-10 03:45:40 - INFO - Batch 171/1300 - Loss: 0.7191 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:41 - INFO - Processing batch 181/1300\n",
      "2025-06-10 03:45:41 - INFO - Batch 181/1300 - Loss: 0.4941 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:43 - INFO - Processing batch 191/1300\n",
      "2025-06-10 03:45:43 - INFO - Batch 191/1300 - Loss: 0.6590 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:44 - INFO - Processing batch 201/1300\n",
      "2025-06-10 03:45:44 - INFO - Batch 201/1300 - Loss: 0.3420 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:46 - INFO - Processing batch 211/1300\n",
      "2025-06-10 03:45:46 - INFO - Batch 211/1300 - Loss: 0.6234 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:47 - INFO - Processing batch 221/1300\n",
      "2025-06-10 03:45:48 - INFO - Batch 221/1300 - Loss: 0.5899 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:49 - INFO - Processing batch 231/1300\n",
      "2025-06-10 03:45:49 - INFO - Batch 231/1300 - Loss: 0.4607 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:50 - INFO - Processing batch 241/1300\n",
      "2025-06-10 03:45:51 - INFO - Batch 241/1300 - Loss: 0.8657 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:52 - INFO - Processing batch 251/1300\n",
      "2025-06-10 03:45:52 - INFO - Batch 251/1300 - Loss: 0.1819 - Avg batch time: 0.16s\n",
      "2025-06-10 03:45:54 - INFO - Processing batch 261/1300\n",
      "2025-06-10 03:45:54 - INFO - Batch 261/1300 - Loss: 0.4427 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:55 - INFO - Processing batch 271/1300\n",
      "2025-06-10 03:45:55 - INFO - Batch 271/1300 - Loss: 0.2256 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:57 - INFO - Processing batch 281/1300\n",
      "2025-06-10 03:45:57 - INFO - Batch 281/1300 - Loss: 0.6596 - Avg batch time: 0.15s\n",
      "2025-06-10 03:45:58 - INFO - Processing batch 291/1300\n",
      "2025-06-10 03:45:58 - INFO - Batch 291/1300 - Loss: 1.2157 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:00 - INFO - Processing batch 301/1300\n",
      "2025-06-10 03:46:00 - INFO - Batch 301/1300 - Loss: 0.3786 - Avg batch time: 0.16s\n",
      "2025-06-10 03:46:01 - INFO - Processing batch 311/1300\n",
      "2025-06-10 03:46:01 - INFO - Batch 311/1300 - Loss: 1.4069 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:03 - INFO - Processing batch 321/1300\n",
      "2025-06-10 03:46:03 - INFO - Batch 321/1300 - Loss: 0.3178 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:04 - INFO - Processing batch 331/1300\n",
      "2025-06-10 03:46:04 - INFO - Batch 331/1300 - Loss: 0.4717 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:06 - INFO - Processing batch 341/1300\n",
      "2025-06-10 03:46:06 - INFO - Batch 341/1300 - Loss: 0.1168 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:07 - INFO - Processing batch 351/1300\n",
      "2025-06-10 03:46:08 - INFO - Batch 351/1300 - Loss: 0.8266 - Avg batch time: 0.16s\n",
      "2025-06-10 03:46:09 - INFO - Processing batch 361/1300\n",
      "2025-06-10 03:46:09 - INFO - Batch 361/1300 - Loss: 0.4846 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:10 - INFO - Processing batch 371/1300\n",
      "2025-06-10 03:46:11 - INFO - Batch 371/1300 - Loss: 0.0952 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:12 - INFO - Processing batch 381/1300\n",
      "2025-06-10 03:46:12 - INFO - Batch 381/1300 - Loss: 0.5254 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:14 - INFO - Processing batch 391/1300\n",
      "2025-06-10 03:46:14 - INFO - Batch 391/1300 - Loss: 0.5652 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:15 - INFO - Processing batch 401/1300\n",
      "2025-06-10 03:46:15 - INFO - Batch 401/1300 - Loss: 0.5511 - Avg batch time: 0.16s\n",
      "2025-06-10 03:46:17 - INFO - Processing batch 411/1300\n",
      "2025-06-10 03:46:17 - INFO - Batch 411/1300 - Loss: 0.4889 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:18 - INFO - Processing batch 421/1300\n",
      "2025-06-10 03:46:18 - INFO - Batch 421/1300 - Loss: 0.3692 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:20 - INFO - Processing batch 431/1300\n",
      "2025-06-10 03:46:20 - INFO - Batch 431/1300 - Loss: 0.3515 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:21 - INFO - Processing batch 441/1300\n",
      "2025-06-10 03:46:21 - INFO - Batch 441/1300 - Loss: 0.7702 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:23 - INFO - Processing batch 451/1300\n",
      "2025-06-10 03:46:23 - INFO - Batch 451/1300 - Loss: 0.4897 - Avg batch time: 0.16s\n",
      "2025-06-10 03:46:24 - INFO - Processing batch 461/1300\n",
      "2025-06-10 03:46:24 - INFO - Batch 461/1300 - Loss: 0.7439 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:26 - INFO - Processing batch 471/1300\n",
      "2025-06-10 03:46:26 - INFO - Batch 471/1300 - Loss: 0.3271 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:27 - INFO - Processing batch 481/1300\n",
      "2025-06-10 03:46:28 - INFO - Batch 481/1300 - Loss: 0.7728 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:29 - INFO - Processing batch 491/1300\n",
      "2025-06-10 03:46:29 - INFO - Batch 491/1300 - Loss: 0.5592 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:30 - INFO - Processing batch 501/1300\n",
      "2025-06-10 03:46:31 - INFO - Batch 501/1300 - Loss: 0.0853 - Avg batch time: 0.16s\n",
      "2025-06-10 03:46:32 - INFO - Processing batch 511/1300\n",
      "2025-06-10 03:46:32 - INFO - Batch 511/1300 - Loss: 0.6356 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:34 - INFO - Processing batch 521/1300\n",
      "2025-06-10 03:46:34 - INFO - Batch 521/1300 - Loss: 0.2936 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:35 - INFO - Processing batch 531/1300\n",
      "2025-06-10 03:46:35 - INFO - Batch 531/1300 - Loss: 0.4538 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:37 - INFO - Processing batch 541/1300\n",
      "2025-06-10 03:46:37 - INFO - Batch 541/1300 - Loss: 0.6554 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:38 - INFO - Processing batch 551/1300\n",
      "2025-06-10 03:46:38 - INFO - Batch 551/1300 - Loss: 0.4458 - Avg batch time: 0.16s\n",
      "2025-06-10 03:46:40 - INFO - Processing batch 561/1300\n",
      "2025-06-10 03:46:40 - INFO - Batch 561/1300 - Loss: 0.9266 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:41 - INFO - Processing batch 571/1300\n",
      "2025-06-10 03:46:41 - INFO - Batch 571/1300 - Loss: 0.2231 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:43 - INFO - Processing batch 581/1300\n",
      "2025-06-10 03:46:43 - INFO - Batch 581/1300 - Loss: 0.3113 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:44 - INFO - Processing batch 591/1300\n",
      "2025-06-10 03:46:44 - INFO - Batch 591/1300 - Loss: 0.5140 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:46 - INFO - Processing batch 601/1300\n",
      "2025-06-10 03:46:46 - INFO - Batch 601/1300 - Loss: 0.5825 - Avg batch time: 0.16s\n",
      "2025-06-10 03:46:47 - INFO - Processing batch 611/1300\n",
      "2025-06-10 03:46:48 - INFO - Batch 611/1300 - Loss: 0.4523 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:49 - INFO - Processing batch 621/1300\n",
      "2025-06-10 03:46:49 - INFO - Batch 621/1300 - Loss: 0.4576 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:50 - INFO - Processing batch 631/1300\n",
      "2025-06-10 03:46:51 - INFO - Batch 631/1300 - Loss: 0.5420 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:52 - INFO - Processing batch 641/1300\n",
      "2025-06-10 03:46:52 - INFO - Batch 641/1300 - Loss: 0.2092 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:54 - INFO - Processing batch 651/1300\n",
      "2025-06-10 03:46:54 - INFO - Batch 651/1300 - Loss: 0.4696 - Avg batch time: 0.16s\n",
      "2025-06-10 03:46:55 - INFO - Processing batch 661/1300\n",
      "2025-06-10 03:46:55 - INFO - Batch 661/1300 - Loss: 0.4112 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:57 - INFO - Processing batch 671/1300\n",
      "2025-06-10 03:46:57 - INFO - Batch 671/1300 - Loss: 0.4493 - Avg batch time: 0.15s\n",
      "2025-06-10 03:46:58 - INFO - Processing batch 681/1300\n",
      "2025-06-10 03:46:58 - INFO - Batch 681/1300 - Loss: 0.8231 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:00 - INFO - Processing batch 691/1300\n",
      "2025-06-10 03:47:00 - INFO - Batch 691/1300 - Loss: 0.4181 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:01 - INFO - Processing batch 701/1300\n",
      "2025-06-10 03:47:01 - INFO - Batch 701/1300 - Loss: 0.8390 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:03 - INFO - Processing batch 711/1300\n",
      "2025-06-10 03:47:03 - INFO - Batch 711/1300 - Loss: 0.4475 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:04 - INFO - Processing batch 721/1300\n",
      "2025-06-10 03:47:04 - INFO - Batch 721/1300 - Loss: 0.3766 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:06 - INFO - Processing batch 731/1300\n",
      "2025-06-10 03:47:06 - INFO - Batch 731/1300 - Loss: 0.2821 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:07 - INFO - Processing batch 741/1300\n",
      "2025-06-10 03:47:08 - INFO - Batch 741/1300 - Loss: 0.2136 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:09 - INFO - Processing batch 751/1300\n",
      "2025-06-10 03:47:09 - INFO - Batch 751/1300 - Loss: 0.0622 - Avg batch time: 0.16s\n",
      "2025-06-10 03:47:10 - INFO - Processing batch 761/1300\n",
      "2025-06-10 03:47:11 - INFO - Batch 761/1300 - Loss: 0.2276 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:12 - INFO - Processing batch 771/1300\n",
      "2025-06-10 03:47:12 - INFO - Batch 771/1300 - Loss: 0.3138 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:14 - INFO - Processing batch 781/1300\n",
      "2025-06-10 03:47:14 - INFO - Batch 781/1300 - Loss: 1.0212 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:15 - INFO - Processing batch 791/1300\n",
      "2025-06-10 03:47:15 - INFO - Batch 791/1300 - Loss: 0.3071 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:17 - INFO - Processing batch 801/1300\n",
      "2025-06-10 03:47:17 - INFO - Batch 801/1300 - Loss: 0.2600 - Avg batch time: 0.16s\n",
      "2025-06-10 03:47:18 - INFO - Processing batch 811/1300\n",
      "2025-06-10 03:47:18 - INFO - Batch 811/1300 - Loss: 1.2481 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:20 - INFO - Processing batch 821/1300\n",
      "2025-06-10 03:47:20 - INFO - Batch 821/1300 - Loss: 1.0138 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:21 - INFO - Processing batch 831/1300\n",
      "2025-06-10 03:47:21 - INFO - Batch 831/1300 - Loss: 0.1716 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:23 - INFO - Processing batch 841/1300\n",
      "2025-06-10 03:47:23 - INFO - Batch 841/1300 - Loss: 0.6077 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:24 - INFO - Processing batch 851/1300\n",
      "2025-06-10 03:47:25 - INFO - Batch 851/1300 - Loss: 0.4701 - Avg batch time: 0.16s\n",
      "2025-06-10 03:47:26 - INFO - Processing batch 861/1300\n",
      "2025-06-10 03:47:26 - INFO - Batch 861/1300 - Loss: 0.7640 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:27 - INFO - Processing batch 871/1300\n",
      "2025-06-10 03:47:28 - INFO - Batch 871/1300 - Loss: 0.6725 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:29 - INFO - Processing batch 881/1300\n",
      "2025-06-10 03:47:29 - INFO - Batch 881/1300 - Loss: 0.2598 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:30 - INFO - Processing batch 891/1300\n",
      "2025-06-10 03:47:31 - INFO - Batch 891/1300 - Loss: 0.3066 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:32 - INFO - Processing batch 901/1300\n",
      "2025-06-10 03:47:32 - INFO - Batch 901/1300 - Loss: 0.6327 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:34 - INFO - Processing batch 911/1300\n",
      "2025-06-10 03:47:34 - INFO - Batch 911/1300 - Loss: 0.2197 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:35 - INFO - Processing batch 921/1300\n",
      "2025-06-10 03:47:35 - INFO - Batch 921/1300 - Loss: 0.2677 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:37 - INFO - Processing batch 931/1300\n",
      "2025-06-10 03:47:37 - INFO - Batch 931/1300 - Loss: 0.5167 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:38 - INFO - Processing batch 941/1300\n",
      "2025-06-10 03:47:38 - INFO - Batch 941/1300 - Loss: 0.4004 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:40 - INFO - Processing batch 951/1300\n",
      "2025-06-10 03:47:40 - INFO - Batch 951/1300 - Loss: 0.4749 - Avg batch time: 0.16s\n",
      "2025-06-10 03:47:41 - INFO - Processing batch 961/1300\n",
      "2025-06-10 03:47:41 - INFO - Batch 961/1300 - Loss: 0.7524 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:43 - INFO - Processing batch 971/1300\n",
      "2025-06-10 03:47:43 - INFO - Batch 971/1300 - Loss: 0.1569 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:44 - INFO - Processing batch 981/1300\n",
      "2025-06-10 03:47:44 - INFO - Batch 981/1300 - Loss: 0.3131 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:46 - INFO - Processing batch 991/1300\n",
      "2025-06-10 03:47:46 - INFO - Batch 991/1300 - Loss: 0.6810 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:47 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 03:47:48 - INFO - Batch 1001/1300 - Loss: 0.3674 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:49 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 03:47:49 - INFO - Batch 1011/1300 - Loss: 0.3562 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:50 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 03:47:51 - INFO - Batch 1021/1300 - Loss: 0.4265 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:52 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 03:47:52 - INFO - Batch 1031/1300 - Loss: 0.8251 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:53 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 03:47:54 - INFO - Batch 1041/1300 - Loss: 0.2060 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:55 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 03:47:55 - INFO - Batch 1051/1300 - Loss: 0.4839 - Avg batch time: 0.16s\n",
      "2025-06-10 03:47:57 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 03:47:57 - INFO - Batch 1061/1300 - Loss: 0.1989 - Avg batch time: 0.15s\n",
      "2025-06-10 03:47:58 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 03:47:58 - INFO - Batch 1071/1300 - Loss: 0.6790 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:00 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 03:48:00 - INFO - Batch 1081/1300 - Loss: 0.3996 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:01 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 03:48:01 - INFO - Batch 1091/1300 - Loss: 0.3488 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:03 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 03:48:03 - INFO - Batch 1101/1300 - Loss: 0.8901 - Avg batch time: 0.16s\n",
      "2025-06-10 03:48:04 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 03:48:04 - INFO - Batch 1111/1300 - Loss: 0.5901 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:06 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 03:48:06 - INFO - Batch 1121/1300 - Loss: 0.4485 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:07 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 03:48:07 - INFO - Batch 1131/1300 - Loss: 0.6299 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:09 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 03:48:09 - INFO - Batch 1141/1300 - Loss: 0.6739 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:10 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 03:48:11 - INFO - Batch 1151/1300 - Loss: 0.2581 - Avg batch time: 0.16s\n",
      "2025-06-10 03:48:12 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 03:48:12 - INFO - Batch 1161/1300 - Loss: 0.1810 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:14 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 03:48:14 - INFO - Batch 1171/1300 - Loss: 0.3001 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:15 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 03:48:15 - INFO - Batch 1181/1300 - Loss: 0.3287 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:17 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 03:48:17 - INFO - Batch 1191/1300 - Loss: 0.4994 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:18 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 03:48:18 - INFO - Batch 1201/1300 - Loss: 0.3063 - Avg batch time: 0.16s\n",
      "2025-06-10 03:48:20 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 03:48:20 - INFO - Batch 1211/1300 - Loss: 0.4419 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:21 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 03:48:21 - INFO - Batch 1221/1300 - Loss: 0.6531 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:23 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 03:48:23 - INFO - Batch 1231/1300 - Loss: 0.7391 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:24 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 03:48:24 - INFO - Batch 1241/1300 - Loss: 0.1564 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:26 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 03:48:26 - INFO - Batch 1251/1300 - Loss: 0.5451 - Avg batch time: 0.16s\n",
      "2025-06-10 03:48:27 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 03:48:28 - INFO - Batch 1261/1300 - Loss: 0.3847 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:29 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 03:48:29 - INFO - Batch 1271/1300 - Loss: 0.6151 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:30 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 03:48:31 - INFO - Batch 1281/1300 - Loss: 0.1282 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:32 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 03:48:32 - INFO - Batch 1291/1300 - Loss: 0.0636 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:33 - INFO - \n",
      "Epoch 3 training completed in 200.30s\n",
      "2025-06-10 03:48:33 - INFO - Average training loss: 0.4716\n",
      "2025-06-10 03:48:51 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:   4%| | 4/100 [10:53<5:48:40, 217.92s/it, train_loss=0.4716, val_loss=0.4781, best_val_f1=0.0000, lr=1.00e-04, b2025-06-10 03:48:51 - INFO - \n",
      "Epoch 4/100 - Training phase\n",
      "2025-06-10 03:48:52 - INFO - Processing batch 1/1300\n",
      "2025-06-10 03:48:52 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 03:48:52 - INFO - Batch 1/1300 - Loss: 0.1402 - Avg batch time: 0.16s\n",
      "2025-06-10 03:48:53 - INFO - Processing batch 11/1300\n",
      "2025-06-10 03:48:53 - INFO - Batch 11/1300 - Loss: 0.5599 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:55 - INFO - Processing batch 21/1300\n",
      "2025-06-10 03:48:55 - INFO - Batch 21/1300 - Loss: 0.1303 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:56 - INFO - Processing batch 31/1300\n",
      "2025-06-10 03:48:57 - INFO - Batch 31/1300 - Loss: 0.5231 - Avg batch time: 0.16s\n",
      "2025-06-10 03:48:58 - INFO - Processing batch 41/1300\n",
      "2025-06-10 03:48:58 - INFO - Batch 41/1300 - Loss: 0.4526 - Avg batch time: 0.15s\n",
      "2025-06-10 03:48:59 - INFO - Processing batch 51/1300\n",
      "2025-06-10 03:49:00 - INFO - Batch 51/1300 - Loss: 0.6327 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:01 - INFO - Processing batch 61/1300\n",
      "2025-06-10 03:49:01 - INFO - Batch 61/1300 - Loss: 0.1411 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:03 - INFO - Processing batch 71/1300\n",
      "2025-06-10 03:49:03 - INFO - Batch 71/1300 - Loss: 0.4758 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:04 - INFO - Processing batch 81/1300\n",
      "2025-06-10 03:49:04 - INFO - Batch 81/1300 - Loss: 0.3039 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:06 - INFO - Processing batch 91/1300\n",
      "2025-06-10 03:49:06 - INFO - Batch 91/1300 - Loss: 0.3835 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:07 - INFO - Processing batch 101/1300\n",
      "2025-06-10 03:49:07 - INFO - Batch 101/1300 - Loss: 0.2661 - Avg batch time: 0.16s\n",
      "2025-06-10 03:49:09 - INFO - Processing batch 111/1300\n",
      "2025-06-10 03:49:09 - INFO - Batch 111/1300 - Loss: 0.3090 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:10 - INFO - Processing batch 121/1300\n",
      "2025-06-10 03:49:10 - INFO - Batch 121/1300 - Loss: 1.2598 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:12 - INFO - Processing batch 131/1300\n",
      "2025-06-10 03:49:12 - INFO - Batch 131/1300 - Loss: 0.3360 - Avg batch time: 0.16s\n",
      "2025-06-10 03:49:13 - INFO - Processing batch 141/1300\n",
      "2025-06-10 03:49:13 - INFO - Batch 141/1300 - Loss: 0.2776 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:15 - INFO - Processing batch 151/1300\n",
      "2025-06-10 03:49:15 - INFO - Batch 151/1300 - Loss: 0.4456 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:16 - INFO - Processing batch 161/1300\n",
      "2025-06-10 03:49:17 - INFO - Batch 161/1300 - Loss: 0.1934 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:18 - INFO - Processing batch 171/1300\n",
      "2025-06-10 03:49:18 - INFO - Batch 171/1300 - Loss: 0.4805 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:19 - INFO - Processing batch 181/1300\n",
      "2025-06-10 03:49:20 - INFO - Batch 181/1300 - Loss: 0.7767 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:21 - INFO - Processing batch 191/1300\n",
      "2025-06-10 03:49:21 - INFO - Batch 191/1300 - Loss: 1.4077 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:23 - INFO - Processing batch 201/1300\n",
      "2025-06-10 03:49:23 - INFO - Batch 201/1300 - Loss: 0.4088 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:24 - INFO - Processing batch 211/1300\n",
      "2025-06-10 03:49:24 - INFO - Batch 211/1300 - Loss: 0.0937 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:26 - INFO - Processing batch 221/1300\n",
      "2025-06-10 03:49:26 - INFO - Batch 221/1300 - Loss: 0.4323 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:27 - INFO - Processing batch 231/1300\n",
      "2025-06-10 03:49:27 - INFO - Batch 231/1300 - Loss: 0.2793 - Avg batch time: 0.16s\n",
      "2025-06-10 03:49:29 - INFO - Processing batch 241/1300\n",
      "2025-06-10 03:49:29 - INFO - Batch 241/1300 - Loss: 0.4078 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:30 - INFO - Processing batch 251/1300\n",
      "2025-06-10 03:49:30 - INFO - Batch 251/1300 - Loss: 0.6589 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:32 - INFO - Processing batch 261/1300\n",
      "2025-06-10 03:49:32 - INFO - Batch 261/1300 - Loss: 0.7899 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:33 - INFO - Processing batch 271/1300\n",
      "2025-06-10 03:49:33 - INFO - Batch 271/1300 - Loss: 0.7037 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:35 - INFO - Processing batch 281/1300\n",
      "2025-06-10 03:49:35 - INFO - Batch 281/1300 - Loss: 0.5800 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:36 - INFO - Processing batch 291/1300\n",
      "2025-06-10 03:49:37 - INFO - Batch 291/1300 - Loss: 0.4035 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:38 - INFO - Processing batch 301/1300\n",
      "2025-06-10 03:49:38 - INFO - Batch 301/1300 - Loss: 0.4886 - Avg batch time: 0.16s\n",
      "2025-06-10 03:49:39 - INFO - Processing batch 311/1300\n",
      "2025-06-10 03:49:40 - INFO - Batch 311/1300 - Loss: 0.6079 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:41 - INFO - Processing batch 321/1300\n",
      "2025-06-10 03:49:41 - INFO - Batch 321/1300 - Loss: 0.3618 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:43 - INFO - Processing batch 331/1300\n",
      "2025-06-10 03:49:43 - INFO - Batch 331/1300 - Loss: 0.1334 - Avg batch time: 0.16s\n",
      "2025-06-10 03:49:44 - INFO - Processing batch 341/1300\n",
      "2025-06-10 03:49:44 - INFO - Batch 341/1300 - Loss: 0.6511 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:46 - INFO - Processing batch 351/1300\n",
      "2025-06-10 03:49:46 - INFO - Batch 351/1300 - Loss: 0.4209 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:47 - INFO - Processing batch 361/1300\n",
      "2025-06-10 03:49:47 - INFO - Batch 361/1300 - Loss: 0.2659 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:49 - INFO - Processing batch 371/1300\n",
      "2025-06-10 03:49:49 - INFO - Batch 371/1300 - Loss: 0.9449 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:50 - INFO - Processing batch 381/1300\n",
      "2025-06-10 03:49:50 - INFO - Batch 381/1300 - Loss: 0.1107 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:52 - INFO - Processing batch 391/1300\n",
      "2025-06-10 03:49:52 - INFO - Batch 391/1300 - Loss: 0.9096 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:53 - INFO - Processing batch 401/1300\n",
      "2025-06-10 03:49:53 - INFO - Batch 401/1300 - Loss: 0.7490 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:55 - INFO - Processing batch 411/1300\n",
      "2025-06-10 03:49:55 - INFO - Batch 411/1300 - Loss: 0.2625 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:56 - INFO - Processing batch 421/1300\n",
      "2025-06-10 03:49:56 - INFO - Batch 421/1300 - Loss: 0.7261 - Avg batch time: 0.15s\n",
      "2025-06-10 03:49:58 - INFO - Processing batch 431/1300\n",
      "2025-06-10 03:49:58 - INFO - Batch 431/1300 - Loss: 0.2585 - Avg batch time: 0.16s\n",
      "2025-06-10 03:49:59 - INFO - Processing batch 441/1300\n",
      "2025-06-10 03:50:00 - INFO - Batch 441/1300 - Loss: 0.4843 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:01 - INFO - Processing batch 451/1300\n",
      "2025-06-10 03:50:01 - INFO - Batch 451/1300 - Loss: 0.5845 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:03 - INFO - Processing batch 461/1300\n",
      "2025-06-10 03:50:03 - INFO - Batch 461/1300 - Loss: 0.5545 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:04 - INFO - Processing batch 471/1300\n",
      "2025-06-10 03:50:04 - INFO - Batch 471/1300 - Loss: 0.4173 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:06 - INFO - Processing batch 481/1300\n",
      "2025-06-10 03:50:06 - INFO - Batch 481/1300 - Loss: 0.2197 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:07 - INFO - Processing batch 491/1300\n",
      "2025-06-10 03:50:07 - INFO - Batch 491/1300 - Loss: 0.5368 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:09 - INFO - Processing batch 501/1300\n",
      "2025-06-10 03:50:09 - INFO - Batch 501/1300 - Loss: 0.1519 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:10 - INFO - Processing batch 511/1300\n",
      "2025-06-10 03:50:10 - INFO - Batch 511/1300 - Loss: 0.2831 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:12 - INFO - Processing batch 521/1300\n",
      "2025-06-10 03:50:12 - INFO - Batch 521/1300 - Loss: 0.5738 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:13 - INFO - Processing batch 531/1300\n",
      "2025-06-10 03:50:13 - INFO - Batch 531/1300 - Loss: 0.1661 - Avg batch time: 0.16s\n",
      "2025-06-10 03:50:15 - INFO - Processing batch 541/1300\n",
      "2025-06-10 03:50:15 - INFO - Batch 541/1300 - Loss: 0.4275 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:16 - INFO - Processing batch 551/1300\n",
      "2025-06-10 03:50:16 - INFO - Batch 551/1300 - Loss: 0.6950 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:18 - INFO - Processing batch 561/1300\n",
      "2025-06-10 03:50:18 - INFO - Batch 561/1300 - Loss: 0.3632 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:19 - INFO - Processing batch 571/1300\n",
      "2025-06-10 03:50:20 - INFO - Batch 571/1300 - Loss: 0.5248 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:21 - INFO - Processing batch 581/1300\n",
      "2025-06-10 03:50:21 - INFO - Batch 581/1300 - Loss: 0.3277 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:22 - INFO - Processing batch 591/1300\n",
      "2025-06-10 03:50:23 - INFO - Batch 591/1300 - Loss: 0.4973 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:24 - INFO - Processing batch 601/1300\n",
      "2025-06-10 03:50:24 - INFO - Batch 601/1300 - Loss: 0.7337 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:26 - INFO - Processing batch 611/1300\n",
      "2025-06-10 03:50:26 - INFO - Batch 611/1300 - Loss: 0.5286 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:27 - INFO - Processing batch 621/1300\n",
      "2025-06-10 03:50:27 - INFO - Batch 621/1300 - Loss: 0.4137 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:29 - INFO - Processing batch 631/1300\n",
      "2025-06-10 03:50:29 - INFO - Batch 631/1300 - Loss: 0.3379 - Avg batch time: 0.16s\n",
      "2025-06-10 03:50:30 - INFO - Processing batch 641/1300\n",
      "2025-06-10 03:50:30 - INFO - Batch 641/1300 - Loss: 0.4236 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:32 - INFO - Processing batch 651/1300\n",
      "2025-06-10 03:50:32 - INFO - Batch 651/1300 - Loss: 0.5567 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:33 - INFO - Processing batch 661/1300\n",
      "2025-06-10 03:50:33 - INFO - Batch 661/1300 - Loss: 0.4851 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:35 - INFO - Processing batch 671/1300\n",
      "2025-06-10 03:50:35 - INFO - Batch 671/1300 - Loss: 0.3631 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:36 - INFO - Processing batch 681/1300\n",
      "2025-06-10 03:50:36 - INFO - Batch 681/1300 - Loss: 0.4885 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:38 - INFO - Processing batch 691/1300\n",
      "2025-06-10 03:50:38 - INFO - Batch 691/1300 - Loss: 0.3542 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:39 - INFO - Processing batch 701/1300\n",
      "2025-06-10 03:50:40 - INFO - Batch 701/1300 - Loss: 0.4290 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:41 - INFO - Processing batch 711/1300\n",
      "2025-06-10 03:50:41 - INFO - Batch 711/1300 - Loss: 0.5599 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:42 - INFO - Processing batch 721/1300\n",
      "2025-06-10 03:50:43 - INFO - Batch 721/1300 - Loss: 0.4547 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:44 - INFO - Processing batch 731/1300\n",
      "2025-06-10 03:50:44 - INFO - Batch 731/1300 - Loss: 0.3621 - Avg batch time: 0.16s\n",
      "2025-06-10 03:50:46 - INFO - Processing batch 741/1300\n",
      "2025-06-10 03:50:46 - INFO - Batch 741/1300 - Loss: 0.3776 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:47 - INFO - Processing batch 751/1300\n",
      "2025-06-10 03:50:47 - INFO - Batch 751/1300 - Loss: 0.3668 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:49 - INFO - Processing batch 761/1300\n",
      "2025-06-10 03:50:49 - INFO - Batch 761/1300 - Loss: 0.1860 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:50 - INFO - Processing batch 771/1300\n",
      "2025-06-10 03:50:50 - INFO - Batch 771/1300 - Loss: 0.3378 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:52 - INFO - Processing batch 781/1300\n",
      "2025-06-10 03:50:52 - INFO - Batch 781/1300 - Loss: 0.1939 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:53 - INFO - Processing batch 791/1300\n",
      "2025-06-10 03:50:53 - INFO - Batch 791/1300 - Loss: 0.2536 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:55 - INFO - Processing batch 801/1300\n",
      "2025-06-10 03:50:55 - INFO - Batch 801/1300 - Loss: 0.2300 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:56 - INFO - Processing batch 811/1300\n",
      "2025-06-10 03:50:56 - INFO - Batch 811/1300 - Loss: 0.3976 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:58 - INFO - Processing batch 821/1300\n",
      "2025-06-10 03:50:58 - INFO - Batch 821/1300 - Loss: 0.4265 - Avg batch time: 0.15s\n",
      "2025-06-10 03:50:59 - INFO - Processing batch 831/1300\n",
      "2025-06-10 03:51:00 - INFO - Batch 831/1300 - Loss: 0.3976 - Avg batch time: 0.16s\n",
      "2025-06-10 03:51:01 - INFO - Processing batch 841/1300\n",
      "2025-06-10 03:51:01 - INFO - Batch 841/1300 - Loss: 0.5834 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:02 - INFO - Processing batch 851/1300\n",
      "2025-06-10 03:51:03 - INFO - Batch 851/1300 - Loss: 0.0925 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:04 - INFO - Processing batch 861/1300\n",
      "2025-06-10 03:51:04 - INFO - Batch 861/1300 - Loss: 0.2371 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:05 - INFO - Processing batch 871/1300\n",
      "2025-06-10 03:51:06 - INFO - Batch 871/1300 - Loss: 0.1405 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:07 - INFO - Processing batch 881/1300\n",
      "2025-06-10 03:51:07 - INFO - Batch 881/1300 - Loss: 0.2321 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:09 - INFO - Processing batch 891/1300\n",
      "2025-06-10 03:51:09 - INFO - Batch 891/1300 - Loss: 0.6736 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:10 - INFO - Processing batch 901/1300\n",
      "2025-06-10 03:51:10 - INFO - Batch 901/1300 - Loss: 0.5999 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:12 - INFO - Processing batch 911/1300\n",
      "2025-06-10 03:51:12 - INFO - Batch 911/1300 - Loss: 0.1972 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:13 - INFO - Processing batch 921/1300\n",
      "2025-06-10 03:51:13 - INFO - Batch 921/1300 - Loss: 0.5391 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:15 - INFO - Processing batch 931/1300\n",
      "2025-06-10 03:51:15 - INFO - Batch 931/1300 - Loss: 0.9318 - Avg batch time: 0.16s\n",
      "2025-06-10 03:51:16 - INFO - Processing batch 941/1300\n",
      "2025-06-10 03:51:16 - INFO - Batch 941/1300 - Loss: 0.3130 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:18 - INFO - Processing batch 951/1300\n",
      "2025-06-10 03:51:18 - INFO - Batch 951/1300 - Loss: 0.3288 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:19 - INFO - Processing batch 961/1300\n",
      "2025-06-10 03:51:19 - INFO - Batch 961/1300 - Loss: 0.5321 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:21 - INFO - Processing batch 971/1300\n",
      "2025-06-10 03:51:21 - INFO - Batch 971/1300 - Loss: 0.1133 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:22 - INFO - Processing batch 981/1300\n",
      "2025-06-10 03:51:22 - INFO - Batch 981/1300 - Loss: 1.0958 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:24 - INFO - Processing batch 991/1300\n",
      "2025-06-10 03:51:24 - INFO - Batch 991/1300 - Loss: 1.0092 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:25 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 03:51:26 - INFO - Batch 1001/1300 - Loss: 0.2208 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:27 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 03:51:27 - INFO - Batch 1011/1300 - Loss: 0.4110 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:28 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 03:51:29 - INFO - Batch 1021/1300 - Loss: 0.3190 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:30 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 03:51:30 - INFO - Batch 1031/1300 - Loss: 0.4389 - Avg batch time: 0.16s\n",
      "2025-06-10 03:51:32 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 03:51:32 - INFO - Batch 1041/1300 - Loss: 1.1785 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:33 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 03:51:33 - INFO - Batch 1051/1300 - Loss: 0.3459 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:35 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 03:51:35 - INFO - Batch 1061/1300 - Loss: 0.1741 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:36 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 03:51:36 - INFO - Batch 1071/1300 - Loss: 0.6781 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:38 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 03:51:38 - INFO - Batch 1081/1300 - Loss: 0.5052 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:39 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 03:51:39 - INFO - Batch 1091/1300 - Loss: 0.7682 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:41 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 03:51:41 - INFO - Batch 1101/1300 - Loss: 0.8074 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:42 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 03:51:42 - INFO - Batch 1111/1300 - Loss: 0.1761 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:44 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 03:51:44 - INFO - Batch 1121/1300 - Loss: 0.3382 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:45 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 03:51:46 - INFO - Batch 1131/1300 - Loss: 0.2888 - Avg batch time: 0.16s\n",
      "2025-06-10 03:51:47 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 03:51:47 - INFO - Batch 1141/1300 - Loss: 0.4957 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:48 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 03:51:49 - INFO - Batch 1151/1300 - Loss: 0.2537 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:50 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 03:51:50 - INFO - Batch 1161/1300 - Loss: 1.0662 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:51 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 03:51:52 - INFO - Batch 1171/1300 - Loss: 0.5055 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:53 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 03:51:53 - INFO - Batch 1181/1300 - Loss: 0.5267 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:55 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 03:51:55 - INFO - Batch 1191/1300 - Loss: 0.3257 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:56 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 03:51:56 - INFO - Batch 1201/1300 - Loss: 0.6914 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:58 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 03:51:58 - INFO - Batch 1211/1300 - Loss: 0.4163 - Avg batch time: 0.15s\n",
      "2025-06-10 03:51:59 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 03:51:59 - INFO - Batch 1221/1300 - Loss: 0.1433 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:01 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 03:52:01 - INFO - Batch 1231/1300 - Loss: 0.5168 - Avg batch time: 0.16s\n",
      "2025-06-10 03:52:02 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 03:52:02 - INFO - Batch 1241/1300 - Loss: 0.1891 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:04 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 03:52:04 - INFO - Batch 1251/1300 - Loss: 1.2254 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:05 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 03:52:05 - INFO - Batch 1261/1300 - Loss: 0.2729 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:07 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 03:52:07 - INFO - Batch 1271/1300 - Loss: 0.5937 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:08 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 03:52:09 - INFO - Batch 1281/1300 - Loss: 0.7056 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:10 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 03:52:10 - INFO - Batch 1291/1300 - Loss: 0.3330 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:11 - INFO - \n",
      "Epoch 4 training completed in 200.17s\n",
      "2025-06-10 03:52:11 - INFO - Average training loss: 0.4700\n",
      "2025-06-10 03:52:29 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:   5%| | 5/100 [14:31<5:45:01, 217.91s/it, train_loss=0.4700, val_loss=0.4461, best_val_f1=0.3361, lr=1.00e-04, b2025-06-10 03:52:29 - INFO - \n",
      "Epoch 5/100 - Training phase\n",
      "2025-06-10 03:52:29 - INFO - Processing batch 1/1300\n",
      "2025-06-10 03:52:29 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 03:52:30 - INFO - Batch 1/1300 - Loss: 0.1015 - Avg batch time: 0.16s\n",
      "2025-06-10 03:52:31 - INFO - Processing batch 11/1300\n",
      "2025-06-10 03:52:31 - INFO - Batch 11/1300 - Loss: 0.3097 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:32 - INFO - Processing batch 21/1300\n",
      "2025-06-10 03:52:33 - INFO - Batch 21/1300 - Loss: 0.0911 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:34 - INFO - Processing batch 31/1300\n",
      "2025-06-10 03:52:34 - INFO - Batch 31/1300 - Loss: 0.0589 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:36 - INFO - Processing batch 41/1300\n",
      "2025-06-10 03:52:36 - INFO - Batch 41/1300 - Loss: 0.3396 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:37 - INFO - Processing batch 51/1300\n",
      "2025-06-10 03:52:37 - INFO - Batch 51/1300 - Loss: 0.4064 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:39 - INFO - Processing batch 61/1300\n",
      "2025-06-10 03:52:39 - INFO - Batch 61/1300 - Loss: 0.8137 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:40 - INFO - Processing batch 71/1300\n",
      "2025-06-10 03:52:40 - INFO - Batch 71/1300 - Loss: 1.0352 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:42 - INFO - Processing batch 81/1300\n",
      "2025-06-10 03:52:42 - INFO - Batch 81/1300 - Loss: 0.3252 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:43 - INFO - Processing batch 91/1300\n",
      "2025-06-10 03:52:43 - INFO - Batch 91/1300 - Loss: 0.5304 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:45 - INFO - Processing batch 101/1300\n",
      "2025-06-10 03:52:45 - INFO - Batch 101/1300 - Loss: 0.5171 - Avg batch time: 0.16s\n",
      "2025-06-10 03:52:46 - INFO - Processing batch 111/1300\n",
      "2025-06-10 03:52:47 - INFO - Batch 111/1300 - Loss: 0.5137 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:48 - INFO - Processing batch 121/1300\n",
      "2025-06-10 03:52:48 - INFO - Batch 121/1300 - Loss: 0.9803 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:49 - INFO - Processing batch 131/1300\n",
      "2025-06-10 03:52:50 - INFO - Batch 131/1300 - Loss: 0.2372 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:51 - INFO - Processing batch 141/1300\n",
      "2025-06-10 03:52:51 - INFO - Batch 141/1300 - Loss: 0.2757 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:53 - INFO - Processing batch 151/1300\n",
      "2025-06-10 03:52:53 - INFO - Batch 151/1300 - Loss: 0.5746 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:54 - INFO - Processing batch 161/1300\n",
      "2025-06-10 03:52:54 - INFO - Batch 161/1300 - Loss: 0.7353 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:56 - INFO - Processing batch 171/1300\n",
      "2025-06-10 03:52:56 - INFO - Batch 171/1300 - Loss: 0.0752 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:57 - INFO - Processing batch 181/1300\n",
      "2025-06-10 03:52:57 - INFO - Batch 181/1300 - Loss: 1.1447 - Avg batch time: 0.15s\n",
      "2025-06-10 03:52:59 - INFO - Processing batch 191/1300\n",
      "2025-06-10 03:52:59 - INFO - Batch 191/1300 - Loss: 0.3251 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:00 - INFO - Processing batch 201/1300\n",
      "2025-06-10 03:53:00 - INFO - Batch 201/1300 - Loss: 0.8437 - Avg batch time: 0.16s\n",
      "2025-06-10 03:53:02 - INFO - Processing batch 211/1300\n",
      "2025-06-10 03:53:02 - INFO - Batch 211/1300 - Loss: 0.2494 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:03 - INFO - Processing batch 221/1300\n",
      "2025-06-10 03:53:03 - INFO - Batch 221/1300 - Loss: 0.4791 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:05 - INFO - Processing batch 231/1300\n",
      "2025-06-10 03:53:05 - INFO - Batch 231/1300 - Loss: 0.5358 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:06 - INFO - Processing batch 241/1300\n",
      "2025-06-10 03:53:07 - INFO - Batch 241/1300 - Loss: 0.3763 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:08 - INFO - Processing batch 251/1300\n",
      "2025-06-10 03:53:08 - INFO - Batch 251/1300 - Loss: 0.4032 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:09 - INFO - Processing batch 261/1300\n",
      "2025-06-10 03:53:10 - INFO - Batch 261/1300 - Loss: 0.5654 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:11 - INFO - Processing batch 271/1300\n",
      "2025-06-10 03:53:11 - INFO - Batch 271/1300 - Loss: 0.9413 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:12 - INFO - Processing batch 281/1300\n",
      "2025-06-10 03:53:13 - INFO - Batch 281/1300 - Loss: 0.3731 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:14 - INFO - Processing batch 291/1300\n",
      "2025-06-10 03:53:14 - INFO - Batch 291/1300 - Loss: 0.1447 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:16 - INFO - Processing batch 301/1300\n",
      "2025-06-10 03:53:16 - INFO - Batch 301/1300 - Loss: 1.2610 - Avg batch time: 0.16s\n",
      "2025-06-10 03:53:17 - INFO - Processing batch 311/1300\n",
      "2025-06-10 03:53:17 - INFO - Batch 311/1300 - Loss: 0.5886 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:19 - INFO - Processing batch 321/1300\n",
      "2025-06-10 03:53:19 - INFO - Batch 321/1300 - Loss: 0.3769 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:20 - INFO - Processing batch 331/1300\n",
      "2025-06-10 03:53:20 - INFO - Batch 331/1300 - Loss: 0.5067 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:22 - INFO - Processing batch 341/1300\n",
      "2025-06-10 03:53:22 - INFO - Batch 341/1300 - Loss: 0.5158 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:23 - INFO - Processing batch 351/1300\n",
      "2025-06-10 03:53:23 - INFO - Batch 351/1300 - Loss: 0.6314 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:25 - INFO - Processing batch 361/1300\n",
      "2025-06-10 03:53:25 - INFO - Batch 361/1300 - Loss: 1.0310 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:26 - INFO - Processing batch 371/1300\n",
      "2025-06-10 03:53:26 - INFO - Batch 371/1300 - Loss: 0.1625 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:28 - INFO - Processing batch 381/1300\n",
      "2025-06-10 03:53:28 - INFO - Batch 381/1300 - Loss: 0.2987 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:29 - INFO - Processing batch 391/1300\n",
      "2025-06-10 03:53:30 - INFO - Batch 391/1300 - Loss: 0.4435 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:31 - INFO - Processing batch 401/1300\n",
      "2025-06-10 03:53:31 - INFO - Batch 401/1300 - Loss: 0.2044 - Avg batch time: 0.16s\n",
      "2025-06-10 03:53:33 - INFO - Processing batch 411/1300\n",
      "2025-06-10 03:53:33 - INFO - Batch 411/1300 - Loss: 1.1208 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:34 - INFO - Processing batch 421/1300\n",
      "2025-06-10 03:53:34 - INFO - Batch 421/1300 - Loss: 0.2577 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:36 - INFO - Processing batch 431/1300\n",
      "2025-06-10 03:53:36 - INFO - Batch 431/1300 - Loss: 0.2543 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:37 - INFO - Processing batch 441/1300\n",
      "2025-06-10 03:53:37 - INFO - Batch 441/1300 - Loss: 0.3524 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:39 - INFO - Processing batch 451/1300\n",
      "2025-06-10 03:53:39 - INFO - Batch 451/1300 - Loss: 0.1084 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:40 - INFO - Processing batch 461/1300\n",
      "2025-06-10 03:53:40 - INFO - Batch 461/1300 - Loss: 0.3786 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:42 - INFO - Processing batch 471/1300\n",
      "2025-06-10 03:53:42 - INFO - Batch 471/1300 - Loss: 0.3955 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:43 - INFO - Processing batch 481/1300\n",
      "2025-06-10 03:53:43 - INFO - Batch 481/1300 - Loss: 0.4721 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:45 - INFO - Processing batch 491/1300\n",
      "2025-06-10 03:53:45 - INFO - Batch 491/1300 - Loss: 0.3541 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:46 - INFO - Processing batch 501/1300\n",
      "2025-06-10 03:53:46 - INFO - Batch 501/1300 - Loss: 0.8303 - Avg batch time: 0.16s\n",
      "2025-06-10 03:53:48 - INFO - Processing batch 511/1300\n",
      "2025-06-10 03:53:48 - INFO - Batch 511/1300 - Loss: 0.0870 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:49 - INFO - Processing batch 521/1300\n",
      "2025-06-10 03:53:50 - INFO - Batch 521/1300 - Loss: 0.2716 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:51 - INFO - Processing batch 531/1300\n",
      "2025-06-10 03:53:51 - INFO - Batch 531/1300 - Loss: 1.0600 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:52 - INFO - Processing batch 541/1300\n",
      "2025-06-10 03:53:53 - INFO - Batch 541/1300 - Loss: 0.4959 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:54 - INFO - Processing batch 551/1300\n",
      "2025-06-10 03:53:54 - INFO - Batch 551/1300 - Loss: 0.4582 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:56 - INFO - Processing batch 561/1300\n",
      "2025-06-10 03:53:56 - INFO - Batch 561/1300 - Loss: 0.4886 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:57 - INFO - Processing batch 571/1300\n",
      "2025-06-10 03:53:57 - INFO - Batch 571/1300 - Loss: 0.2541 - Avg batch time: 0.15s\n",
      "2025-06-10 03:53:59 - INFO - Processing batch 581/1300\n",
      "2025-06-10 03:53:59 - INFO - Batch 581/1300 - Loss: 0.5225 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:00 - INFO - Processing batch 591/1300\n",
      "2025-06-10 03:54:00 - INFO - Batch 591/1300 - Loss: 0.3420 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:02 - INFO - Processing batch 601/1300\n",
      "2025-06-10 03:54:02 - INFO - Batch 601/1300 - Loss: 0.1814 - Avg batch time: 0.16s\n",
      "2025-06-10 03:54:03 - INFO - Processing batch 611/1300\n",
      "2025-06-10 03:54:03 - INFO - Batch 611/1300 - Loss: 0.4175 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:05 - INFO - Processing batch 621/1300\n",
      "2025-06-10 03:54:05 - INFO - Batch 621/1300 - Loss: 0.2229 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:06 - INFO - Processing batch 631/1300\n",
      "2025-06-10 03:54:06 - INFO - Batch 631/1300 - Loss: 0.0716 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:08 - INFO - Processing batch 641/1300\n",
      "2025-06-10 03:54:08 - INFO - Batch 641/1300 - Loss: 1.8475 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:09 - INFO - Processing batch 651/1300\n",
      "2025-06-10 03:54:10 - INFO - Batch 651/1300 - Loss: 0.1461 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:11 - INFO - Processing batch 661/1300\n",
      "2025-06-10 03:54:11 - INFO - Batch 661/1300 - Loss: 0.4248 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:12 - INFO - Processing batch 671/1300\n",
      "2025-06-10 03:54:13 - INFO - Batch 671/1300 - Loss: 0.8145 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:14 - INFO - Processing batch 681/1300\n",
      "2025-06-10 03:54:14 - INFO - Batch 681/1300 - Loss: 0.3886 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:16 - INFO - Processing batch 691/1300\n",
      "2025-06-10 03:54:16 - INFO - Batch 691/1300 - Loss: 0.7723 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:17 - INFO - Processing batch 701/1300\n",
      "2025-06-10 03:54:17 - INFO - Batch 701/1300 - Loss: 1.0392 - Avg batch time: 0.16s\n",
      "2025-06-10 03:54:19 - INFO - Processing batch 711/1300\n",
      "2025-06-10 03:54:19 - INFO - Batch 711/1300 - Loss: 0.4595 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:20 - INFO - Processing batch 721/1300\n",
      "2025-06-10 03:54:20 - INFO - Batch 721/1300 - Loss: 0.6561 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:22 - INFO - Processing batch 731/1300\n",
      "2025-06-10 03:54:22 - INFO - Batch 731/1300 - Loss: 0.1747 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:23 - INFO - Processing batch 741/1300\n",
      "2025-06-10 03:54:23 - INFO - Batch 741/1300 - Loss: 0.8044 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:25 - INFO - Processing batch 751/1300\n",
      "2025-06-10 03:54:25 - INFO - Batch 751/1300 - Loss: 0.4425 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:26 - INFO - Processing batch 761/1300\n",
      "2025-06-10 03:54:26 - INFO - Batch 761/1300 - Loss: 0.0949 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:28 - INFO - Processing batch 771/1300\n",
      "2025-06-10 03:54:28 - INFO - Batch 771/1300 - Loss: 0.8899 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:29 - INFO - Processing batch 781/1300\n",
      "2025-06-10 03:54:30 - INFO - Batch 781/1300 - Loss: 0.4268 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:31 - INFO - Processing batch 791/1300\n",
      "2025-06-10 03:54:31 - INFO - Batch 791/1300 - Loss: 0.4336 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:33 - INFO - Processing batch 801/1300\n",
      "2025-06-10 03:54:33 - INFO - Batch 801/1300 - Loss: 0.3873 - Avg batch time: 0.16s\n",
      "2025-06-10 03:54:34 - INFO - Processing batch 811/1300\n",
      "2025-06-10 03:54:34 - INFO - Batch 811/1300 - Loss: 0.7018 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:36 - INFO - Processing batch 821/1300\n",
      "2025-06-10 03:54:36 - INFO - Batch 821/1300 - Loss: 0.7397 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:37 - INFO - Processing batch 831/1300\n",
      "2025-06-10 03:54:37 - INFO - Batch 831/1300 - Loss: 0.4120 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:39 - INFO - Processing batch 841/1300\n",
      "2025-06-10 03:54:39 - INFO - Batch 841/1300 - Loss: 0.4927 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:40 - INFO - Processing batch 851/1300\n",
      "2025-06-10 03:54:40 - INFO - Batch 851/1300 - Loss: 0.4029 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:42 - INFO - Processing batch 861/1300\n",
      "2025-06-10 03:54:42 - INFO - Batch 861/1300 - Loss: 0.3603 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:43 - INFO - Processing batch 871/1300\n",
      "2025-06-10 03:54:43 - INFO - Batch 871/1300 - Loss: 0.7089 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:45 - INFO - Processing batch 881/1300\n",
      "2025-06-10 03:54:45 - INFO - Batch 881/1300 - Loss: 0.3666 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:46 - INFO - Processing batch 891/1300\n",
      "2025-06-10 03:54:46 - INFO - Batch 891/1300 - Loss: 0.3950 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:48 - INFO - Processing batch 901/1300\n",
      "2025-06-10 03:54:48 - INFO - Batch 901/1300 - Loss: 0.4195 - Avg batch time: 0.16s\n",
      "2025-06-10 03:54:49 - INFO - Processing batch 911/1300\n",
      "2025-06-10 03:54:50 - INFO - Batch 911/1300 - Loss: 0.1599 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:51 - INFO - Processing batch 921/1300\n",
      "2025-06-10 03:54:51 - INFO - Batch 921/1300 - Loss: 0.4466 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:52 - INFO - Processing batch 931/1300\n",
      "2025-06-10 03:54:53 - INFO - Batch 931/1300 - Loss: 0.5241 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:54 - INFO - Processing batch 941/1300\n",
      "2025-06-10 03:54:54 - INFO - Batch 941/1300 - Loss: 0.2110 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:56 - INFO - Processing batch 951/1300\n",
      "2025-06-10 03:54:56 - INFO - Batch 951/1300 - Loss: 0.4886 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:57 - INFO - Processing batch 961/1300\n",
      "2025-06-10 03:54:57 - INFO - Batch 961/1300 - Loss: 0.3883 - Avg batch time: 0.15s\n",
      "2025-06-10 03:54:59 - INFO - Processing batch 971/1300\n",
      "2025-06-10 03:54:59 - INFO - Batch 971/1300 - Loss: 0.4048 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:00 - INFO - Processing batch 981/1300\n",
      "2025-06-10 03:55:00 - INFO - Batch 981/1300 - Loss: 0.5398 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:02 - INFO - Processing batch 991/1300\n",
      "2025-06-10 03:55:02 - INFO - Batch 991/1300 - Loss: 0.3717 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:03 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 03:55:03 - INFO - Batch 1001/1300 - Loss: 0.3976 - Avg batch time: 0.16s\n",
      "2025-06-10 03:55:05 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 03:55:05 - INFO - Batch 1011/1300 - Loss: 0.4352 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:06 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 03:55:06 - INFO - Batch 1021/1300 - Loss: 0.4239 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:08 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 03:55:08 - INFO - Batch 1031/1300 - Loss: 0.7857 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:09 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 03:55:10 - INFO - Batch 1041/1300 - Loss: 0.0612 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:11 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 03:55:11 - INFO - Batch 1051/1300 - Loss: 0.1042 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:12 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 03:55:13 - INFO - Batch 1061/1300 - Loss: 0.3220 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:14 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 03:55:14 - INFO - Batch 1071/1300 - Loss: 1.1968 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:16 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 03:55:16 - INFO - Batch 1081/1300 - Loss: 0.6644 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:17 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 03:55:17 - INFO - Batch 1091/1300 - Loss: 0.2980 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:19 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 03:55:19 - INFO - Batch 1101/1300 - Loss: 0.6935 - Avg batch time: 0.16s\n",
      "2025-06-10 03:55:20 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 03:55:20 - INFO - Batch 1111/1300 - Loss: 0.7666 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:22 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 03:55:22 - INFO - Batch 1121/1300 - Loss: 0.6329 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:23 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 03:55:23 - INFO - Batch 1131/1300 - Loss: 0.2835 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:25 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 03:55:25 - INFO - Batch 1141/1300 - Loss: 0.3103 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:26 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 03:55:26 - INFO - Batch 1151/1300 - Loss: 0.1701 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:28 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 03:55:28 - INFO - Batch 1161/1300 - Loss: 0.2644 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:29 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 03:55:30 - INFO - Batch 1171/1300 - Loss: 0.3860 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:31 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 03:55:31 - INFO - Batch 1181/1300 - Loss: 0.8298 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:32 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 03:55:33 - INFO - Batch 1191/1300 - Loss: 0.4112 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:34 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 03:55:34 - INFO - Batch 1201/1300 - Loss: 1.0570 - Avg batch time: 0.16s\n",
      "2025-06-10 03:55:36 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 03:55:36 - INFO - Batch 1211/1300 - Loss: 0.2589 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:37 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 03:55:37 - INFO - Batch 1221/1300 - Loss: 0.6775 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:39 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 03:55:39 - INFO - Batch 1231/1300 - Loss: 0.4275 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:40 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 03:55:40 - INFO - Batch 1241/1300 - Loss: 0.5072 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:42 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 03:55:42 - INFO - Batch 1251/1300 - Loss: 0.5497 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:43 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 03:55:43 - INFO - Batch 1261/1300 - Loss: 0.8286 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:45 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 03:55:45 - INFO - Batch 1271/1300 - Loss: 0.4283 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:46 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 03:55:46 - INFO - Batch 1281/1300 - Loss: 0.2048 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:48 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 03:55:48 - INFO - Batch 1291/1300 - Loss: 0.7922 - Avg batch time: 0.15s\n",
      "2025-06-10 03:55:49 - INFO - \n",
      "Epoch 5 training completed in 200.26s\n",
      "2025-06-10 03:55:49 - INFO - Average training loss: 0.4693\n",
      "2025-06-10 03:56:07 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:   6%| | 6/100 [18:09<5:41:26, 217.95s/it, train_loss=0.4693, val_loss=0.4916, best_val_f1=0.3361, lr=1.00e-04, b2025-06-10 03:56:07 - INFO - \n",
      "Epoch 6/100 - Training phase\n",
      "2025-06-10 03:56:07 - INFO - Processing batch 1/1300\n",
      "2025-06-10 03:56:07 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 03:56:08 - INFO - Batch 1/1300 - Loss: 0.1784 - Avg batch time: 0.16s\n",
      "2025-06-10 03:56:09 - INFO - Processing batch 11/1300\n",
      "2025-06-10 03:56:09 - INFO - Batch 11/1300 - Loss: 1.2568 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:10 - INFO - Processing batch 21/1300\n",
      "2025-06-10 03:56:11 - INFO - Batch 21/1300 - Loss: 0.0813 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:12 - INFO - Processing batch 31/1300\n",
      "2025-06-10 03:56:12 - INFO - Batch 31/1300 - Loss: 0.2695 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:14 - INFO - Processing batch 41/1300\n",
      "2025-06-10 03:56:14 - INFO - Batch 41/1300 - Loss: 0.5028 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:15 - INFO - Processing batch 51/1300\n",
      "2025-06-10 03:56:15 - INFO - Batch 51/1300 - Loss: 0.3298 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:17 - INFO - Processing batch 61/1300\n",
      "2025-06-10 03:56:17 - INFO - Batch 61/1300 - Loss: 0.5013 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:18 - INFO - Processing batch 71/1300\n",
      "2025-06-10 03:56:18 - INFO - Batch 71/1300 - Loss: 0.3707 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:20 - INFO - Processing batch 81/1300\n",
      "2025-06-10 03:56:20 - INFO - Batch 81/1300 - Loss: 0.1309 - Avg batch time: 0.16s\n",
      "2025-06-10 03:56:21 - INFO - Processing batch 91/1300\n",
      "2025-06-10 03:56:21 - INFO - Batch 91/1300 - Loss: 0.3254 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:23 - INFO - Processing batch 101/1300\n",
      "2025-06-10 03:56:23 - INFO - Batch 101/1300 - Loss: 0.5053 - Avg batch time: 0.16s\n",
      "2025-06-10 03:56:24 - INFO - Processing batch 111/1300\n",
      "2025-06-10 03:56:25 - INFO - Batch 111/1300 - Loss: 0.3295 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:26 - INFO - Processing batch 121/1300\n",
      "2025-06-10 03:56:26 - INFO - Batch 121/1300 - Loss: 0.6296 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:27 - INFO - Processing batch 131/1300\n",
      "2025-06-10 03:56:28 - INFO - Batch 131/1300 - Loss: 0.3882 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:29 - INFO - Processing batch 141/1300\n",
      "2025-06-10 03:56:29 - INFO - Batch 141/1300 - Loss: 0.2741 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:30 - INFO - Processing batch 151/1300\n",
      "2025-06-10 03:56:31 - INFO - Batch 151/1300 - Loss: 0.8740 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:32 - INFO - Processing batch 161/1300\n",
      "2025-06-10 03:56:32 - INFO - Batch 161/1300 - Loss: 0.6991 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:34 - INFO - Processing batch 171/1300\n",
      "2025-06-10 03:56:34 - INFO - Batch 171/1300 - Loss: 0.3591 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:35 - INFO - Processing batch 181/1300\n",
      "2025-06-10 03:56:35 - INFO - Batch 181/1300 - Loss: 0.4604 - Avg batch time: 0.16s\n",
      "2025-06-10 03:56:37 - INFO - Processing batch 191/1300\n",
      "2025-06-10 03:56:37 - INFO - Batch 191/1300 - Loss: 0.4852 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:38 - INFO - Processing batch 201/1300\n",
      "2025-06-10 03:56:38 - INFO - Batch 201/1300 - Loss: 0.7489 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:40 - INFO - Processing batch 211/1300\n",
      "2025-06-10 03:56:40 - INFO - Batch 211/1300 - Loss: 0.7584 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:41 - INFO - Processing batch 221/1300\n",
      "2025-06-10 03:56:41 - INFO - Batch 221/1300 - Loss: 1.3330 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:43 - INFO - Processing batch 231/1300\n",
      "2025-06-10 03:56:43 - INFO - Batch 231/1300 - Loss: 0.3072 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:44 - INFO - Processing batch 241/1300\n",
      "2025-06-10 03:56:44 - INFO - Batch 241/1300 - Loss: 0.4885 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:46 - INFO - Processing batch 251/1300\n",
      "2025-06-10 03:56:46 - INFO - Batch 251/1300 - Loss: 0.5483 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:47 - INFO - Processing batch 261/1300\n",
      "2025-06-10 03:56:48 - INFO - Batch 261/1300 - Loss: 0.7223 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:49 - INFO - Processing batch 271/1300\n",
      "2025-06-10 03:56:49 - INFO - Batch 271/1300 - Loss: 0.1979 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:51 - INFO - Processing batch 281/1300\n",
      "2025-06-10 03:56:51 - INFO - Batch 281/1300 - Loss: 0.1695 - Avg batch time: 0.16s\n",
      "2025-06-10 03:56:52 - INFO - Processing batch 291/1300\n",
      "2025-06-10 03:56:52 - INFO - Batch 291/1300 - Loss: 0.9016 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:54 - INFO - Processing batch 301/1300\n",
      "2025-06-10 03:56:54 - INFO - Batch 301/1300 - Loss: 0.9144 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:55 - INFO - Processing batch 311/1300\n",
      "2025-06-10 03:56:55 - INFO - Batch 311/1300 - Loss: 0.4518 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:57 - INFO - Processing batch 321/1300\n",
      "2025-06-10 03:56:57 - INFO - Batch 321/1300 - Loss: 0.1456 - Avg batch time: 0.15s\n",
      "2025-06-10 03:56:58 - INFO - Processing batch 331/1300\n",
      "2025-06-10 03:56:58 - INFO - Batch 331/1300 - Loss: 0.5001 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:00 - INFO - Processing batch 341/1300\n",
      "2025-06-10 03:57:00 - INFO - Batch 341/1300 - Loss: 0.8724 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:01 - INFO - Processing batch 351/1300\n",
      "2025-06-10 03:57:01 - INFO - Batch 351/1300 - Loss: 0.5714 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:03 - INFO - Processing batch 361/1300\n",
      "2025-06-10 03:57:03 - INFO - Batch 361/1300 - Loss: 0.1295 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:04 - INFO - Processing batch 371/1300\n",
      "2025-06-10 03:57:04 - INFO - Batch 371/1300 - Loss: 0.8250 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:06 - INFO - Processing batch 381/1300\n",
      "2025-06-10 03:57:06 - INFO - Batch 381/1300 - Loss: 0.4950 - Avg batch time: 0.16s\n",
      "2025-06-10 03:57:07 - INFO - Processing batch 391/1300\n",
      "2025-06-10 03:57:08 - INFO - Batch 391/1300 - Loss: 0.2060 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:09 - INFO - Processing batch 401/1300\n",
      "2025-06-10 03:57:09 - INFO - Batch 401/1300 - Loss: 0.6157 - Avg batch time: 0.16s\n",
      "2025-06-10 03:57:11 - INFO - Processing batch 411/1300\n",
      "2025-06-10 03:57:11 - INFO - Batch 411/1300 - Loss: 0.5688 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:12 - INFO - Processing batch 421/1300\n",
      "2025-06-10 03:57:12 - INFO - Batch 421/1300 - Loss: 0.2782 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:14 - INFO - Processing batch 431/1300\n",
      "2025-06-10 03:57:14 - INFO - Batch 431/1300 - Loss: 0.6647 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:15 - INFO - Processing batch 441/1300\n",
      "2025-06-10 03:57:15 - INFO - Batch 441/1300 - Loss: 0.2786 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:17 - INFO - Processing batch 451/1300\n",
      "2025-06-10 03:57:17 - INFO - Batch 451/1300 - Loss: 0.5410 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:18 - INFO - Processing batch 461/1300\n",
      "2025-06-10 03:57:18 - INFO - Batch 461/1300 - Loss: 0.3801 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:20 - INFO - Processing batch 471/1300\n",
      "2025-06-10 03:57:20 - INFO - Batch 471/1300 - Loss: 0.3824 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:21 - INFO - Processing batch 481/1300\n",
      "2025-06-10 03:57:21 - INFO - Batch 481/1300 - Loss: 0.4641 - Avg batch time: 0.16s\n",
      "2025-06-10 03:57:23 - INFO - Processing batch 491/1300\n",
      "2025-06-10 03:57:23 - INFO - Batch 491/1300 - Loss: 0.7400 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:24 - INFO - Processing batch 501/1300\n",
      "2025-06-10 03:57:25 - INFO - Batch 501/1300 - Loss: 0.7579 - Avg batch time: 0.16s\n",
      "2025-06-10 03:57:26 - INFO - Processing batch 511/1300\n",
      "2025-06-10 03:57:26 - INFO - Batch 511/1300 - Loss: 0.5444 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:27 - INFO - Processing batch 521/1300\n",
      "2025-06-10 03:57:28 - INFO - Batch 521/1300 - Loss: 0.2032 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:29 - INFO - Processing batch 531/1300\n",
      "2025-06-10 03:57:29 - INFO - Batch 531/1300 - Loss: 0.6520 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:30 - INFO - Processing batch 541/1300\n",
      "2025-06-10 03:57:31 - INFO - Batch 541/1300 - Loss: 0.5921 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:32 - INFO - Processing batch 551/1300\n",
      "2025-06-10 03:57:32 - INFO - Batch 551/1300 - Loss: 0.7467 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:34 - INFO - Processing batch 561/1300\n",
      "2025-06-10 03:57:34 - INFO - Batch 561/1300 - Loss: 0.5046 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:35 - INFO - Processing batch 571/1300\n",
      "2025-06-10 03:57:35 - INFO - Batch 571/1300 - Loss: 0.1936 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:37 - INFO - Processing batch 581/1300\n",
      "2025-06-10 03:57:37 - INFO - Batch 581/1300 - Loss: 0.6213 - Avg batch time: 0.16s\n",
      "2025-06-10 03:57:38 - INFO - Processing batch 591/1300\n",
      "2025-06-10 03:57:38 - INFO - Batch 591/1300 - Loss: 0.3932 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:40 - INFO - Processing batch 601/1300\n",
      "2025-06-10 03:57:40 - INFO - Batch 601/1300 - Loss: 0.5847 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:41 - INFO - Processing batch 611/1300\n",
      "2025-06-10 03:57:41 - INFO - Batch 611/1300 - Loss: 0.5223 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:43 - INFO - Processing batch 621/1300\n",
      "2025-06-10 03:57:43 - INFO - Batch 621/1300 - Loss: 0.9431 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:44 - INFO - Processing batch 631/1300\n",
      "2025-06-10 03:57:44 - INFO - Batch 631/1300 - Loss: 0.2588 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:46 - INFO - Processing batch 641/1300\n",
      "2025-06-10 03:57:46 - INFO - Batch 641/1300 - Loss: 1.2900 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:47 - INFO - Processing batch 651/1300\n",
      "2025-06-10 03:57:48 - INFO - Batch 651/1300 - Loss: 0.6363 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:49 - INFO - Processing batch 661/1300\n",
      "2025-06-10 03:57:49 - INFO - Batch 661/1300 - Loss: 0.3907 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:50 - INFO - Processing batch 671/1300\n",
      "2025-06-10 03:57:51 - INFO - Batch 671/1300 - Loss: 0.8747 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:52 - INFO - Processing batch 681/1300\n",
      "2025-06-10 03:57:52 - INFO - Batch 681/1300 - Loss: 0.3824 - Avg batch time: 0.16s\n",
      "2025-06-10 03:57:54 - INFO - Processing batch 691/1300\n",
      "2025-06-10 03:57:54 - INFO - Batch 691/1300 - Loss: 0.4923 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:55 - INFO - Processing batch 701/1300\n",
      "2025-06-10 03:57:55 - INFO - Batch 701/1300 - Loss: 0.6338 - Avg batch time: 0.16s\n",
      "2025-06-10 03:57:57 - INFO - Processing batch 711/1300\n",
      "2025-06-10 03:57:57 - INFO - Batch 711/1300 - Loss: 0.2742 - Avg batch time: 0.15s\n",
      "2025-06-10 03:57:58 - INFO - Processing batch 721/1300\n",
      "2025-06-10 03:57:58 - INFO - Batch 721/1300 - Loss: 0.5554 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:00 - INFO - Processing batch 731/1300\n",
      "2025-06-10 03:58:00 - INFO - Batch 731/1300 - Loss: 0.5607 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:01 - INFO - Processing batch 741/1300\n",
      "2025-06-10 03:58:01 - INFO - Batch 741/1300 - Loss: 0.2818 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:03 - INFO - Processing batch 751/1300\n",
      "2025-06-10 03:58:03 - INFO - Batch 751/1300 - Loss: 0.5498 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:04 - INFO - Processing batch 761/1300\n",
      "2025-06-10 03:58:04 - INFO - Batch 761/1300 - Loss: 0.3829 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:06 - INFO - Processing batch 771/1300\n",
      "2025-06-10 03:58:06 - INFO - Batch 771/1300 - Loss: 0.3621 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:07 - INFO - Processing batch 781/1300\n",
      "2025-06-10 03:58:08 - INFO - Batch 781/1300 - Loss: 0.2357 - Avg batch time: 0.16s\n",
      "2025-06-10 03:58:09 - INFO - Processing batch 791/1300\n",
      "2025-06-10 03:58:09 - INFO - Batch 791/1300 - Loss: 0.1244 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:10 - INFO - Processing batch 801/1300\n",
      "2025-06-10 03:58:11 - INFO - Batch 801/1300 - Loss: 0.3861 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:12 - INFO - Processing batch 811/1300\n",
      "2025-06-10 03:58:12 - INFO - Batch 811/1300 - Loss: 1.0218 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:14 - INFO - Processing batch 821/1300\n",
      "2025-06-10 03:58:14 - INFO - Batch 821/1300 - Loss: 0.7179 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:15 - INFO - Processing batch 831/1300\n",
      "2025-06-10 03:58:15 - INFO - Batch 831/1300 - Loss: 0.3765 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:17 - INFO - Processing batch 841/1300\n",
      "2025-06-10 03:58:17 - INFO - Batch 841/1300 - Loss: 0.5136 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:18 - INFO - Processing batch 851/1300\n",
      "2025-06-10 03:58:18 - INFO - Batch 851/1300 - Loss: 0.3751 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:20 - INFO - Processing batch 861/1300\n",
      "2025-06-10 03:58:20 - INFO - Batch 861/1300 - Loss: 0.3631 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:21 - INFO - Processing batch 871/1300\n",
      "2025-06-10 03:58:21 - INFO - Batch 871/1300 - Loss: 0.5364 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:23 - INFO - Processing batch 881/1300\n",
      "2025-06-10 03:58:23 - INFO - Batch 881/1300 - Loss: 0.6895 - Avg batch time: 0.16s\n",
      "2025-06-10 03:58:24 - INFO - Processing batch 891/1300\n",
      "2025-06-10 03:58:24 - INFO - Batch 891/1300 - Loss: 0.2611 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:26 - INFO - Processing batch 901/1300\n",
      "2025-06-10 03:58:26 - INFO - Batch 901/1300 - Loss: 0.2851 - Avg batch time: 0.16s\n",
      "2025-06-10 03:58:27 - INFO - Processing batch 911/1300\n",
      "2025-06-10 03:58:28 - INFO - Batch 911/1300 - Loss: 0.4119 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:29 - INFO - Processing batch 921/1300\n",
      "2025-06-10 03:58:29 - INFO - Batch 921/1300 - Loss: 0.1293 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:30 - INFO - Processing batch 931/1300\n",
      "2025-06-10 03:58:31 - INFO - Batch 931/1300 - Loss: 0.3287 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:32 - INFO - Processing batch 941/1300\n",
      "2025-06-10 03:58:32 - INFO - Batch 941/1300 - Loss: 0.2871 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:34 - INFO - Processing batch 951/1300\n",
      "2025-06-10 03:58:34 - INFO - Batch 951/1300 - Loss: 0.4782 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:35 - INFO - Processing batch 961/1300\n",
      "2025-06-10 03:58:35 - INFO - Batch 961/1300 - Loss: 0.6098 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:37 - INFO - Processing batch 971/1300\n",
      "2025-06-10 03:58:37 - INFO - Batch 971/1300 - Loss: 0.1356 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:38 - INFO - Processing batch 981/1300\n",
      "2025-06-10 03:58:38 - INFO - Batch 981/1300 - Loss: 0.5079 - Avg batch time: 0.16s\n",
      "2025-06-10 03:58:40 - INFO - Processing batch 991/1300\n",
      "2025-06-10 03:58:40 - INFO - Batch 991/1300 - Loss: 0.3605 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:41 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 03:58:41 - INFO - Batch 1001/1300 - Loss: 0.2953 - Avg batch time: 0.16s\n",
      "2025-06-10 03:58:43 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 03:58:43 - INFO - Batch 1011/1300 - Loss: 0.4613 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:44 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 03:58:44 - INFO - Batch 1021/1300 - Loss: 0.1787 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:46 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 03:58:46 - INFO - Batch 1031/1300 - Loss: 0.1381 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:47 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 03:58:48 - INFO - Batch 1041/1300 - Loss: 0.6205 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:49 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 03:58:49 - INFO - Batch 1051/1300 - Loss: 0.2368 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:50 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 03:58:51 - INFO - Batch 1061/1300 - Loss: 0.5351 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:52 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 03:58:52 - INFO - Batch 1071/1300 - Loss: 0.2767 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:54 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 03:58:54 - INFO - Batch 1081/1300 - Loss: 0.2183 - Avg batch time: 0.16s\n",
      "2025-06-10 03:58:55 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 03:58:55 - INFO - Batch 1091/1300 - Loss: 0.4659 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:57 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 03:58:57 - INFO - Batch 1101/1300 - Loss: 0.4129 - Avg batch time: 0.15s\n",
      "2025-06-10 03:58:58 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 03:58:58 - INFO - Batch 1111/1300 - Loss: 0.3117 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:00 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 03:59:00 - INFO - Batch 1121/1300 - Loss: 0.1658 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:01 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 03:59:01 - INFO - Batch 1131/1300 - Loss: 0.2430 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:03 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 03:59:03 - INFO - Batch 1141/1300 - Loss: 0.3765 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:04 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 03:59:04 - INFO - Batch 1151/1300 - Loss: 0.4589 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:06 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 03:59:06 - INFO - Batch 1161/1300 - Loss: 0.3196 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:07 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 03:59:08 - INFO - Batch 1171/1300 - Loss: 0.1368 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:09 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 03:59:09 - INFO - Batch 1181/1300 - Loss: 0.4120 - Avg batch time: 0.16s\n",
      "2025-06-10 03:59:10 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 03:59:11 - INFO - Batch 1191/1300 - Loss: 0.3531 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:12 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 03:59:12 - INFO - Batch 1201/1300 - Loss: 0.6087 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:14 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 03:59:14 - INFO - Batch 1211/1300 - Loss: 0.1827 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:15 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 03:59:15 - INFO - Batch 1221/1300 - Loss: 0.3295 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:17 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 03:59:17 - INFO - Batch 1231/1300 - Loss: 0.2213 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:18 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 03:59:18 - INFO - Batch 1241/1300 - Loss: 0.5651 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:20 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 03:59:20 - INFO - Batch 1251/1300 - Loss: 0.2334 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:21 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 03:59:21 - INFO - Batch 1261/1300 - Loss: 0.7291 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:23 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 03:59:23 - INFO - Batch 1271/1300 - Loss: 0.2199 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:24 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 03:59:25 - INFO - Batch 1281/1300 - Loss: 0.1606 - Avg batch time: 0.16s\n",
      "2025-06-10 03:59:26 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 03:59:26 - INFO - Batch 1291/1300 - Loss: 0.4097 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:27 - INFO - \n",
      "Epoch 6 training completed in 200.27s\n",
      "2025-06-10 03:59:27 - INFO - Average training loss: 0.4714\n",
      "2025-06-10 03:59:45 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:   7%| | 7/100 [21:47<5:37:49, 217.95s/it, train_loss=0.4714, val_loss=0.4516, best_val_f1=0.3361, lr=1.00e-04, b2025-06-10 03:59:45 - INFO - \n",
      "Epoch 7/100 - Training phase\n",
      "2025-06-10 03:59:45 - INFO - Processing batch 1/1300\n",
      "2025-06-10 03:59:45 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 03:59:46 - INFO - Batch 1/1300 - Loss: 0.1701 - Avg batch time: 0.16s\n",
      "2025-06-10 03:59:47 - INFO - Processing batch 11/1300\n",
      "2025-06-10 03:59:47 - INFO - Batch 11/1300 - Loss: 0.6449 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:48 - INFO - Processing batch 21/1300\n",
      "2025-06-10 03:59:49 - INFO - Batch 21/1300 - Loss: 0.2448 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:50 - INFO - Processing batch 31/1300\n",
      "2025-06-10 03:59:50 - INFO - Batch 31/1300 - Loss: 0.2935 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:52 - INFO - Processing batch 41/1300\n",
      "2025-06-10 03:59:52 - INFO - Batch 41/1300 - Loss: 0.5157 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:53 - INFO - Processing batch 51/1300\n",
      "2025-06-10 03:59:53 - INFO - Batch 51/1300 - Loss: 0.1928 - Avg batch time: 0.16s\n",
      "2025-06-10 03:59:55 - INFO - Processing batch 61/1300\n",
      "2025-06-10 03:59:55 - INFO - Batch 61/1300 - Loss: 0.5672 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:56 - INFO - Processing batch 71/1300\n",
      "2025-06-10 03:59:56 - INFO - Batch 71/1300 - Loss: 0.1264 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:58 - INFO - Processing batch 81/1300\n",
      "2025-06-10 03:59:58 - INFO - Batch 81/1300 - Loss: 0.4983 - Avg batch time: 0.15s\n",
      "2025-06-10 03:59:59 - INFO - Processing batch 91/1300\n",
      "2025-06-10 03:59:59 - INFO - Batch 91/1300 - Loss: 0.4237 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:01 - INFO - Processing batch 101/1300\n",
      "2025-06-10 04:00:01 - INFO - Batch 101/1300 - Loss: 1.3175 - Avg batch time: 0.16s\n",
      "2025-06-10 04:00:02 - INFO - Processing batch 111/1300\n",
      "2025-06-10 04:00:02 - INFO - Batch 111/1300 - Loss: 0.1020 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:04 - INFO - Processing batch 121/1300\n",
      "2025-06-10 04:00:04 - INFO - Batch 121/1300 - Loss: 0.3798 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:05 - INFO - Processing batch 131/1300\n",
      "2025-06-10 04:00:06 - INFO - Batch 131/1300 - Loss: 0.5512 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:07 - INFO - Processing batch 141/1300\n",
      "2025-06-10 04:00:07 - INFO - Batch 141/1300 - Loss: 0.7843 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:09 - INFO - Processing batch 151/1300\n",
      "2025-06-10 04:00:09 - INFO - Batch 151/1300 - Loss: 0.6083 - Avg batch time: 0.16s\n",
      "2025-06-10 04:00:10 - INFO - Processing batch 161/1300\n",
      "2025-06-10 04:00:10 - INFO - Batch 161/1300 - Loss: 0.4451 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:12 - INFO - Processing batch 171/1300\n",
      "2025-06-10 04:00:12 - INFO - Batch 171/1300 - Loss: 0.2072 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:13 - INFO - Processing batch 181/1300\n",
      "2025-06-10 04:00:13 - INFO - Batch 181/1300 - Loss: 0.2856 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:15 - INFO - Processing batch 191/1300\n",
      "2025-06-10 04:00:15 - INFO - Batch 191/1300 - Loss: 0.2213 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:16 - INFO - Processing batch 201/1300\n",
      "2025-06-10 04:00:16 - INFO - Batch 201/1300 - Loss: 0.2667 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:18 - INFO - Processing batch 211/1300\n",
      "2025-06-10 04:00:18 - INFO - Batch 211/1300 - Loss: 0.0626 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:19 - INFO - Processing batch 221/1300\n",
      "2025-06-10 04:00:19 - INFO - Batch 221/1300 - Loss: 0.1331 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:21 - INFO - Processing batch 231/1300\n",
      "2025-06-10 04:00:21 - INFO - Batch 231/1300 - Loss: 0.4994 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:22 - INFO - Processing batch 241/1300\n",
      "2025-06-10 04:00:22 - INFO - Batch 241/1300 - Loss: 0.4733 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:24 - INFO - Processing batch 251/1300\n",
      "2025-06-10 04:00:24 - INFO - Batch 251/1300 - Loss: 0.1547 - Avg batch time: 0.16s\n",
      "2025-06-10 04:00:25 - INFO - Processing batch 261/1300\n",
      "2025-06-10 04:00:26 - INFO - Batch 261/1300 - Loss: 0.3670 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:27 - INFO - Processing batch 271/1300\n",
      "2025-06-10 04:00:27 - INFO - Batch 271/1300 - Loss: 0.1773 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:29 - INFO - Processing batch 281/1300\n",
      "2025-06-10 04:00:29 - INFO - Batch 281/1300 - Loss: 0.2541 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:30 - INFO - Processing batch 291/1300\n",
      "2025-06-10 04:00:30 - INFO - Batch 291/1300 - Loss: 0.4596 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:32 - INFO - Processing batch 301/1300\n",
      "2025-06-10 04:00:32 - INFO - Batch 301/1300 - Loss: 0.2600 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:33 - INFO - Processing batch 311/1300\n",
      "2025-06-10 04:00:33 - INFO - Batch 311/1300 - Loss: 0.2816 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:35 - INFO - Processing batch 321/1300\n",
      "2025-06-10 04:00:35 - INFO - Batch 321/1300 - Loss: 0.5681 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:36 - INFO - Processing batch 331/1300\n",
      "2025-06-10 04:00:36 - INFO - Batch 331/1300 - Loss: 0.5168 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:38 - INFO - Processing batch 341/1300\n",
      "2025-06-10 04:00:38 - INFO - Batch 341/1300 - Loss: 0.5396 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:39 - INFO - Processing batch 351/1300\n",
      "2025-06-10 04:00:39 - INFO - Batch 351/1300 - Loss: 0.2802 - Avg batch time: 0.16s\n",
      "2025-06-10 04:00:41 - INFO - Processing batch 361/1300\n",
      "2025-06-10 04:00:41 - INFO - Batch 361/1300 - Loss: 0.4156 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:42 - INFO - Processing batch 371/1300\n",
      "2025-06-10 04:00:43 - INFO - Batch 371/1300 - Loss: 0.7066 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:44 - INFO - Processing batch 381/1300\n",
      "2025-06-10 04:00:44 - INFO - Batch 381/1300 - Loss: 0.3667 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:45 - INFO - Processing batch 391/1300\n",
      "2025-06-10 04:00:46 - INFO - Batch 391/1300 - Loss: 0.2350 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:47 - INFO - Processing batch 401/1300\n",
      "2025-06-10 04:00:47 - INFO - Batch 401/1300 - Loss: 0.1971 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:49 - INFO - Processing batch 411/1300\n",
      "2025-06-10 04:00:49 - INFO - Batch 411/1300 - Loss: 0.4101 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:50 - INFO - Processing batch 421/1300\n",
      "2025-06-10 04:00:50 - INFO - Batch 421/1300 - Loss: 1.4917 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:52 - INFO - Processing batch 431/1300\n",
      "2025-06-10 04:00:52 - INFO - Batch 431/1300 - Loss: 0.1692 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:53 - INFO - Processing batch 441/1300\n",
      "2025-06-10 04:00:53 - INFO - Batch 441/1300 - Loss: 0.2772 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:55 - INFO - Processing batch 451/1300\n",
      "2025-06-10 04:00:55 - INFO - Batch 451/1300 - Loss: 0.4044 - Avg batch time: 0.16s\n",
      "2025-06-10 04:00:56 - INFO - Processing batch 461/1300\n",
      "2025-06-10 04:00:56 - INFO - Batch 461/1300 - Loss: 0.2577 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:58 - INFO - Processing batch 471/1300\n",
      "2025-06-10 04:00:58 - INFO - Batch 471/1300 - Loss: 0.2622 - Avg batch time: 0.15s\n",
      "2025-06-10 04:00:59 - INFO - Processing batch 481/1300\n",
      "2025-06-10 04:00:59 - INFO - Batch 481/1300 - Loss: 0.5514 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:01 - INFO - Processing batch 491/1300\n",
      "2025-06-10 04:01:01 - INFO - Batch 491/1300 - Loss: 0.3438 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:02 - INFO - Processing batch 501/1300\n",
      "2025-06-10 04:01:03 - INFO - Batch 501/1300 - Loss: 0.5437 - Avg batch time: 0.16s\n",
      "2025-06-10 04:01:04 - INFO - Processing batch 511/1300\n",
      "2025-06-10 04:01:04 - INFO - Batch 511/1300 - Loss: 0.2540 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:05 - INFO - Processing batch 521/1300\n",
      "2025-06-10 04:01:06 - INFO - Batch 521/1300 - Loss: 0.2178 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:07 - INFO - Processing batch 531/1300\n",
      "2025-06-10 04:01:07 - INFO - Batch 531/1300 - Loss: 0.4575 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:09 - INFO - Processing batch 541/1300\n",
      "2025-06-10 04:01:09 - INFO - Batch 541/1300 - Loss: 0.9160 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:10 - INFO - Processing batch 551/1300\n",
      "2025-06-10 04:01:10 - INFO - Batch 551/1300 - Loss: 0.7455 - Avg batch time: 0.16s\n",
      "2025-06-10 04:01:12 - INFO - Processing batch 561/1300\n",
      "2025-06-10 04:01:12 - INFO - Batch 561/1300 - Loss: 0.5687 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:13 - INFO - Processing batch 571/1300\n",
      "2025-06-10 04:01:13 - INFO - Batch 571/1300 - Loss: 0.5084 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:15 - INFO - Processing batch 581/1300\n",
      "2025-06-10 04:01:15 - INFO - Batch 581/1300 - Loss: 0.6982 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:16 - INFO - Processing batch 591/1300\n",
      "2025-06-10 04:01:16 - INFO - Batch 591/1300 - Loss: 0.2793 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:18 - INFO - Processing batch 601/1300\n",
      "2025-06-10 04:01:18 - INFO - Batch 601/1300 - Loss: 0.4511 - Avg batch time: 0.16s\n",
      "2025-06-10 04:01:19 - INFO - Processing batch 611/1300\n",
      "2025-06-10 04:01:19 - INFO - Batch 611/1300 - Loss: 0.2498 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:21 - INFO - Processing batch 621/1300\n",
      "2025-06-10 04:01:21 - INFO - Batch 621/1300 - Loss: 0.5366 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:22 - INFO - Processing batch 631/1300\n",
      "2025-06-10 04:01:23 - INFO - Batch 631/1300 - Loss: 0.3804 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:24 - INFO - Processing batch 641/1300\n",
      "2025-06-10 04:01:24 - INFO - Batch 641/1300 - Loss: 0.1961 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:26 - INFO - Processing batch 651/1300\n",
      "2025-06-10 04:01:26 - INFO - Batch 651/1300 - Loss: 0.9555 - Avg batch time: 0.16s\n",
      "2025-06-10 04:01:27 - INFO - Processing batch 661/1300\n",
      "2025-06-10 04:01:27 - INFO - Batch 661/1300 - Loss: 0.2028 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:29 - INFO - Processing batch 671/1300\n",
      "2025-06-10 04:01:29 - INFO - Batch 671/1300 - Loss: 1.0829 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:30 - INFO - Processing batch 681/1300\n",
      "2025-06-10 04:01:30 - INFO - Batch 681/1300 - Loss: 0.3669 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:32 - INFO - Processing batch 691/1300\n",
      "2025-06-10 04:01:32 - INFO - Batch 691/1300 - Loss: 0.2742 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:33 - INFO - Processing batch 701/1300\n",
      "2025-06-10 04:01:33 - INFO - Batch 701/1300 - Loss: 0.4019 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:35 - INFO - Processing batch 711/1300\n",
      "2025-06-10 04:01:35 - INFO - Batch 711/1300 - Loss: 0.4977 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:36 - INFO - Processing batch 721/1300\n",
      "2025-06-10 04:01:36 - INFO - Batch 721/1300 - Loss: 0.3825 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:38 - INFO - Processing batch 731/1300\n",
      "2025-06-10 04:01:38 - INFO - Batch 731/1300 - Loss: 0.7121 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:39 - INFO - Processing batch 741/1300\n",
      "2025-06-10 04:01:39 - INFO - Batch 741/1300 - Loss: 0.3715 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:41 - INFO - Processing batch 751/1300\n",
      "2025-06-10 04:01:41 - INFO - Batch 751/1300 - Loss: 0.2472 - Avg batch time: 0.16s\n",
      "2025-06-10 04:01:42 - INFO - Processing batch 761/1300\n",
      "2025-06-10 04:01:43 - INFO - Batch 761/1300 - Loss: 0.4675 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:44 - INFO - Processing batch 771/1300\n",
      "2025-06-10 04:01:44 - INFO - Batch 771/1300 - Loss: 1.3543 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:46 - INFO - Processing batch 781/1300\n",
      "2025-06-10 04:01:46 - INFO - Batch 781/1300 - Loss: 0.4806 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:47 - INFO - Processing batch 791/1300\n",
      "2025-06-10 04:01:47 - INFO - Batch 791/1300 - Loss: 0.4752 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:49 - INFO - Processing batch 801/1300\n",
      "2025-06-10 04:01:49 - INFO - Batch 801/1300 - Loss: 0.1980 - Avg batch time: 0.16s\n",
      "2025-06-10 04:01:50 - INFO - Processing batch 811/1300\n",
      "2025-06-10 04:01:50 - INFO - Batch 811/1300 - Loss: 0.8258 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:52 - INFO - Processing batch 821/1300\n",
      "2025-06-10 04:01:52 - INFO - Batch 821/1300 - Loss: 0.4718 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:53 - INFO - Processing batch 831/1300\n",
      "2025-06-10 04:01:53 - INFO - Batch 831/1300 - Loss: 0.3742 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:55 - INFO - Processing batch 841/1300\n",
      "2025-06-10 04:01:55 - INFO - Batch 841/1300 - Loss: 0.4325 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:56 - INFO - Processing batch 851/1300\n",
      "2025-06-10 04:01:56 - INFO - Batch 851/1300 - Loss: 0.3484 - Avg batch time: 0.16s\n",
      "2025-06-10 04:01:58 - INFO - Processing batch 861/1300\n",
      "2025-06-10 04:01:58 - INFO - Batch 861/1300 - Loss: 1.1097 - Avg batch time: 0.15s\n",
      "2025-06-10 04:01:59 - INFO - Processing batch 871/1300\n",
      "2025-06-10 04:02:00 - INFO - Batch 871/1300 - Loss: 0.9406 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:01 - INFO - Processing batch 881/1300\n",
      "2025-06-10 04:02:01 - INFO - Batch 881/1300 - Loss: 0.5469 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:02 - INFO - Processing batch 891/1300\n",
      "2025-06-10 04:02:03 - INFO - Batch 891/1300 - Loss: 0.8801 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:04 - INFO - Processing batch 901/1300\n",
      "2025-06-10 04:02:04 - INFO - Batch 901/1300 - Loss: 0.7581 - Avg batch time: 0.16s\n",
      "2025-06-10 04:02:06 - INFO - Processing batch 911/1300\n",
      "2025-06-10 04:02:06 - INFO - Batch 911/1300 - Loss: 0.3661 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:07 - INFO - Processing batch 921/1300\n",
      "2025-06-10 04:02:07 - INFO - Batch 921/1300 - Loss: 0.4744 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:09 - INFO - Processing batch 931/1300\n",
      "2025-06-10 04:02:09 - INFO - Batch 931/1300 - Loss: 0.7763 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:10 - INFO - Processing batch 941/1300\n",
      "2025-06-10 04:02:10 - INFO - Batch 941/1300 - Loss: 0.4001 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:12 - INFO - Processing batch 951/1300\n",
      "2025-06-10 04:02:12 - INFO - Batch 951/1300 - Loss: 0.7730 - Avg batch time: 0.16s\n",
      "2025-06-10 04:02:13 - INFO - Processing batch 961/1300\n",
      "2025-06-10 04:02:13 - INFO - Batch 961/1300 - Loss: 0.2048 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:15 - INFO - Processing batch 971/1300\n",
      "2025-06-10 04:02:15 - INFO - Batch 971/1300 - Loss: 0.9249 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:16 - INFO - Processing batch 981/1300\n",
      "2025-06-10 04:02:16 - INFO - Batch 981/1300 - Loss: 0.4170 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:18 - INFO - Processing batch 991/1300\n",
      "2025-06-10 04:02:18 - INFO - Batch 991/1300 - Loss: 0.2767 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:19 - INFO - Processing batch 1001/1300\n",
      "2025-06-10 04:02:20 - INFO - Batch 1001/1300 - Loss: 0.3549 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:21 - INFO - Processing batch 1011/1300\n",
      "2025-06-10 04:02:21 - INFO - Batch 1011/1300 - Loss: 0.1027 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:22 - INFO - Processing batch 1021/1300\n",
      "2025-06-10 04:02:23 - INFO - Batch 1021/1300 - Loss: 0.1853 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:24 - INFO - Processing batch 1031/1300\n",
      "2025-06-10 04:02:24 - INFO - Batch 1031/1300 - Loss: 0.4869 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:25 - INFO - Processing batch 1041/1300\n",
      "2025-06-10 04:02:26 - INFO - Batch 1041/1300 - Loss: 0.3064 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:27 - INFO - Processing batch 1051/1300\n",
      "2025-06-10 04:02:27 - INFO - Batch 1051/1300 - Loss: 0.6684 - Avg batch time: 0.16s\n",
      "2025-06-10 04:02:29 - INFO - Processing batch 1061/1300\n",
      "2025-06-10 04:02:29 - INFO - Batch 1061/1300 - Loss: 0.2815 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:30 - INFO - Processing batch 1071/1300\n",
      "2025-06-10 04:02:30 - INFO - Batch 1071/1300 - Loss: 0.5025 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:32 - INFO - Processing batch 1081/1300\n",
      "2025-06-10 04:02:32 - INFO - Batch 1081/1300 - Loss: 0.8465 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:33 - INFO - Processing batch 1091/1300\n",
      "2025-06-10 04:02:33 - INFO - Batch 1091/1300 - Loss: 0.4296 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:35 - INFO - Processing batch 1101/1300\n",
      "2025-06-10 04:02:35 - INFO - Batch 1101/1300 - Loss: 0.4723 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:36 - INFO - Processing batch 1111/1300\n",
      "2025-06-10 04:02:36 - INFO - Batch 1111/1300 - Loss: 0.4288 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:38 - INFO - Processing batch 1121/1300\n",
      "2025-06-10 04:02:38 - INFO - Batch 1121/1300 - Loss: 0.6646 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:39 - INFO - Processing batch 1131/1300\n",
      "2025-06-10 04:02:40 - INFO - Batch 1131/1300 - Loss: 0.2049 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:41 - INFO - Processing batch 1141/1300\n",
      "2025-06-10 04:02:41 - INFO - Batch 1141/1300 - Loss: 0.1251 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:43 - INFO - Processing batch 1151/1300\n",
      "2025-06-10 04:02:43 - INFO - Batch 1151/1300 - Loss: 0.2316 - Avg batch time: 0.16s\n",
      "2025-06-10 04:02:44 - INFO - Processing batch 1161/1300\n",
      "2025-06-10 04:02:44 - INFO - Batch 1161/1300 - Loss: 0.5466 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:46 - INFO - Processing batch 1171/1300\n",
      "2025-06-10 04:02:46 - INFO - Batch 1171/1300 - Loss: 0.1556 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:47 - INFO - Processing batch 1181/1300\n",
      "2025-06-10 04:02:47 - INFO - Batch 1181/1300 - Loss: 0.5226 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:49 - INFO - Processing batch 1191/1300\n",
      "2025-06-10 04:02:49 - INFO - Batch 1191/1300 - Loss: 0.2833 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:50 - INFO - Processing batch 1201/1300\n",
      "2025-06-10 04:02:50 - INFO - Batch 1201/1300 - Loss: 0.3472 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:52 - INFO - Processing batch 1211/1300\n",
      "2025-06-10 04:02:52 - INFO - Batch 1211/1300 - Loss: 0.4267 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:53 - INFO - Processing batch 1221/1300\n",
      "2025-06-10 04:02:53 - INFO - Batch 1221/1300 - Loss: 0.6448 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:55 - INFO - Processing batch 1231/1300\n",
      "2025-06-10 04:02:55 - INFO - Batch 1231/1300 - Loss: 0.6454 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:56 - INFO - Processing batch 1241/1300\n",
      "2025-06-10 04:02:56 - INFO - Batch 1241/1300 - Loss: 0.2635 - Avg batch time: 0.15s\n",
      "2025-06-10 04:02:58 - INFO - Processing batch 1251/1300\n",
      "2025-06-10 04:02:58 - INFO - Batch 1251/1300 - Loss: 0.4203 - Avg batch time: 0.16s\n",
      "2025-06-10 04:02:59 - INFO - Processing batch 1261/1300\n",
      "2025-06-10 04:03:00 - INFO - Batch 1261/1300 - Loss: 0.2647 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:01 - INFO - Processing batch 1271/1300\n",
      "2025-06-10 04:03:01 - INFO - Batch 1271/1300 - Loss: 0.1272 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:02 - INFO - Processing batch 1281/1300\n",
      "2025-06-10 04:03:03 - INFO - Batch 1281/1300 - Loss: 0.1943 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:04 - INFO - Processing batch 1291/1300\n",
      "2025-06-10 04:03:04 - INFO - Batch 1291/1300 - Loss: 0.1572 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:05 - INFO - \n",
      "Epoch 7 training completed in 200.41s\n",
      "2025-06-10 04:03:05 - INFO - Average training loss: 0.4633\n",
      "2025-06-10 04:03:23 - INFO - Median patient F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epochs:   8%| | 8/100 [25:25<5:34:16, 218.01s/it, train_loss=0.4633, val_loss=0.4812, best_val_f1=0.3361, lr=5.00e-05, b2025-06-10 04:03:23 - INFO - \n",
      "Epoch 8/100 - Training phase\n",
      "2025-06-10 04:03:23 - INFO - Processing batch 1/1300\n",
      "2025-06-10 04:03:23 - INFO - Batch shapes - x: torch.Size([152, 3000]), edge_index: torch.Size([2, 2736]), y: torch.Size([8, 1])\n",
      "2025-06-10 04:03:24 - INFO - Batch 1/1300 - Loss: 0.9051 - Avg batch time: 0.16s\n",
      "2025-06-10 04:03:25 - INFO - Processing batch 11/1300\n",
      "2025-06-10 04:03:25 - INFO - Batch 11/1300 - Loss: 0.4480 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:27 - INFO - Processing batch 21/1300\n",
      "2025-06-10 04:03:27 - INFO - Batch 21/1300 - Loss: 0.8167 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:28 - INFO - Processing batch 31/1300\n",
      "2025-06-10 04:03:28 - INFO - Batch 31/1300 - Loss: 0.7017 - Avg batch time: 0.16s\n",
      "2025-06-10 04:03:30 - INFO - Processing batch 41/1300\n",
      "2025-06-10 04:03:30 - INFO - Batch 41/1300 - Loss: 0.5292 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:31 - INFO - Processing batch 51/1300\n",
      "2025-06-10 04:03:31 - INFO - Batch 51/1300 - Loss: 0.5495 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:33 - INFO - Processing batch 61/1300\n",
      "2025-06-10 04:03:33 - INFO - Batch 61/1300 - Loss: 0.3868 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:34 - INFO - Processing batch 71/1300\n",
      "2025-06-10 04:03:34 - INFO - Batch 71/1300 - Loss: 0.4543 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:36 - INFO - Processing batch 81/1300\n",
      "2025-06-10 04:03:36 - INFO - Batch 81/1300 - Loss: 0.2902 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:37 - INFO - Processing batch 91/1300\n",
      "2025-06-10 04:03:38 - INFO - Batch 91/1300 - Loss: 0.5193 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:39 - INFO - Processing batch 101/1300\n",
      "2025-06-10 04:03:39 - INFO - Batch 101/1300 - Loss: 0.6271 - Avg batch time: 0.16s\n",
      "2025-06-10 04:03:40 - INFO - Processing batch 111/1300\n",
      "2025-06-10 04:03:41 - INFO - Batch 111/1300 - Loss: 0.2847 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:42 - INFO - Processing batch 121/1300\n",
      "2025-06-10 04:03:42 - INFO - Batch 121/1300 - Loss: 0.1260 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:44 - INFO - Processing batch 131/1300\n",
      "2025-06-10 04:03:44 - INFO - Batch 131/1300 - Loss: 0.4000 - Avg batch time: 0.16s\n",
      "2025-06-10 04:03:45 - INFO - Processing batch 141/1300\n",
      "2025-06-10 04:03:45 - INFO - Batch 141/1300 - Loss: 0.9771 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:47 - INFO - Processing batch 151/1300\n",
      "2025-06-10 04:03:47 - INFO - Batch 151/1300 - Loss: 0.1566 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:48 - INFO - Processing batch 161/1300\n",
      "2025-06-10 04:03:48 - INFO - Batch 161/1300 - Loss: 0.3645 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:50 - INFO - Processing batch 171/1300\n",
      "2025-06-10 04:03:50 - INFO - Batch 171/1300 - Loss: 0.5737 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:51 - INFO - Processing batch 181/1300\n",
      "2025-06-10 04:03:51 - INFO - Batch 181/1300 - Loss: 0.2594 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:53 - INFO - Processing batch 191/1300\n",
      "2025-06-10 04:03:53 - INFO - Batch 191/1300 - Loss: 0.6142 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:54 - INFO - Processing batch 201/1300\n",
      "2025-06-10 04:03:55 - INFO - Batch 201/1300 - Loss: 0.9808 - Avg batch time: 0.16s\n",
      "2025-06-10 04:03:56 - INFO - Processing batch 211/1300\n",
      "2025-06-10 04:03:56 - INFO - Batch 211/1300 - Loss: 0.5233 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:57 - INFO - Processing batch 221/1300\n",
      "2025-06-10 04:03:58 - INFO - Batch 221/1300 - Loss: 0.6279 - Avg batch time: 0.15s\n",
      "2025-06-10 04:03:59 - INFO - Processing batch 231/1300\n",
      "2025-06-10 04:03:59 - INFO - Batch 231/1300 - Loss: 0.7109 - Avg batch time: 0.16s\n",
      "2025-06-10 04:04:01 - INFO - Processing batch 241/1300\n",
      "2025-06-10 04:04:01 - INFO - Batch 241/1300 - Loss: 0.6579 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:02 - INFO - Processing batch 251/1300\n",
      "2025-06-10 04:04:02 - INFO - Batch 251/1300 - Loss: 0.3448 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:04 - INFO - Processing batch 261/1300\n",
      "2025-06-10 04:04:04 - INFO - Batch 261/1300 - Loss: 0.5907 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:05 - INFO - Processing batch 271/1300\n",
      "2025-06-10 04:04:05 - INFO - Batch 271/1300 - Loss: 0.1636 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:07 - INFO - Processing batch 281/1300\n",
      "2025-06-10 04:04:07 - INFO - Batch 281/1300 - Loss: 0.6527 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:08 - INFO - Processing batch 291/1300\n",
      "2025-06-10 04:04:08 - INFO - Batch 291/1300 - Loss: 0.5559 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:10 - INFO - Processing batch 301/1300\n",
      "2025-06-10 04:04:10 - INFO - Batch 301/1300 - Loss: 0.4932 - Avg batch time: 0.16s\n",
      "2025-06-10 04:04:11 - INFO - Processing batch 311/1300\n",
      "2025-06-10 04:04:11 - INFO - Batch 311/1300 - Loss: 0.1736 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:13 - INFO - Processing batch 321/1300\n",
      "2025-06-10 04:04:13 - INFO - Batch 321/1300 - Loss: 0.5814 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:14 - INFO - Processing batch 331/1300\n",
      "2025-06-10 04:04:15 - INFO - Batch 331/1300 - Loss: 0.4176 - Avg batch time: 0.16s\n",
      "2025-06-10 04:04:16 - INFO - Processing batch 341/1300\n",
      "2025-06-10 04:04:16 - INFO - Batch 341/1300 - Loss: 0.1893 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:17 - INFO - Processing batch 351/1300\n",
      "2025-06-10 04:04:18 - INFO - Batch 351/1300 - Loss: 1.0692 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:19 - INFO - Processing batch 361/1300\n",
      "2025-06-10 04:04:19 - INFO - Batch 361/1300 - Loss: 0.5563 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:21 - INFO - Processing batch 371/1300\n",
      "2025-06-10 04:04:21 - INFO - Batch 371/1300 - Loss: 0.5851 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:22 - INFO - Processing batch 381/1300\n",
      "2025-06-10 04:04:22 - INFO - Batch 381/1300 - Loss: 0.1194 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:24 - INFO - Processing batch 391/1300\n",
      "2025-06-10 04:04:24 - INFO - Batch 391/1300 - Loss: 0.3835 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:25 - INFO - Processing batch 401/1300\n",
      "2025-06-10 04:04:25 - INFO - Batch 401/1300 - Loss: 0.3842 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:27 - INFO - Processing batch 411/1300\n",
      "2025-06-10 04:04:27 - INFO - Batch 411/1300 - Loss: 0.3759 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:28 - INFO - Processing batch 421/1300\n",
      "2025-06-10 04:04:28 - INFO - Batch 421/1300 - Loss: 0.4175 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:30 - INFO - Processing batch 431/1300\n",
      "2025-06-10 04:04:30 - INFO - Batch 431/1300 - Loss: 1.1082 - Avg batch time: 0.16s\n",
      "2025-06-10 04:04:31 - INFO - Processing batch 441/1300\n",
      "2025-06-10 04:04:32 - INFO - Batch 441/1300 - Loss: 0.3851 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:33 - INFO - Processing batch 451/1300\n",
      "2025-06-10 04:04:33 - INFO - Batch 451/1300 - Loss: 0.2891 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:34 - INFO - Processing batch 461/1300\n",
      "2025-06-10 04:04:35 - INFO - Batch 461/1300 - Loss: 0.1877 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:36 - INFO - Processing batch 471/1300\n",
      "2025-06-10 04:04:36 - INFO - Batch 471/1300 - Loss: 0.3679 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:37 - INFO - Processing batch 481/1300\n",
      "2025-06-10 04:04:38 - INFO - Batch 481/1300 - Loss: 0.8576 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:39 - INFO - Processing batch 491/1300\n",
      "2025-06-10 04:04:39 - INFO - Batch 491/1300 - Loss: 0.2493 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:41 - INFO - Processing batch 501/1300\n",
      "2025-06-10 04:04:41 - INFO - Batch 501/1300 - Loss: 0.3921 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:42 - INFO - Processing batch 511/1300\n",
      "2025-06-10 04:04:42 - INFO - Batch 511/1300 - Loss: 0.1195 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:44 - INFO - Processing batch 521/1300\n",
      "2025-06-10 04:04:44 - INFO - Batch 521/1300 - Loss: 0.4365 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:45 - INFO - Processing batch 531/1300\n",
      "2025-06-10 04:04:45 - INFO - Batch 531/1300 - Loss: 0.5448 - Avg batch time: 0.16s\n",
      "2025-06-10 04:04:47 - INFO - Processing batch 541/1300\n",
      "2025-06-10 04:04:47 - INFO - Batch 541/1300 - Loss: 0.9011 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:48 - INFO - Processing batch 551/1300\n",
      "2025-06-10 04:04:48 - INFO - Batch 551/1300 - Loss: 0.6343 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:50 - INFO - Processing batch 561/1300\n",
      "2025-06-10 04:04:50 - INFO - Batch 561/1300 - Loss: 0.3783 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:51 - INFO - Processing batch 571/1300\n",
      "2025-06-10 04:04:52 - INFO - Batch 571/1300 - Loss: 1.0141 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:53 - INFO - Processing batch 581/1300\n",
      "2025-06-10 04:04:53 - INFO - Batch 581/1300 - Loss: 0.3850 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:54 - INFO - Processing batch 591/1300\n",
      "2025-06-10 04:04:55 - INFO - Batch 591/1300 - Loss: 0.3404 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:56 - INFO - Processing batch 601/1300\n",
      "2025-06-10 04:04:56 - INFO - Batch 601/1300 - Loss: 0.7011 - Avg batch time: 0.16s\n",
      "2025-06-10 04:04:58 - INFO - Processing batch 611/1300\n",
      "2025-06-10 04:04:58 - INFO - Batch 611/1300 - Loss: 0.1769 - Avg batch time: 0.15s\n",
      "2025-06-10 04:04:59 - INFO - Processing batch 621/1300\n",
      "2025-06-10 04:04:59 - INFO - Batch 621/1300 - Loss: 0.1853 - Avg batch time: 0.15s\n",
      "2025-06-10 04:05:01 - INFO - Processing batch 631/1300\n",
      "2025-06-10 04:05:01 - INFO - Batch 631/1300 - Loss: 0.7860 - Avg batch time: 0.16s\n",
      "2025-06-10 04:05:02 - INFO - Processing batch 641/1300\n",
      "2025-06-10 04:05:02 - INFO - Batch 641/1300 - Loss: 0.1528 - Avg batch time: 0.15s\n",
      "2025-06-10 04:05:04 - INFO - Processing batch 651/1300\n",
      "2025-06-10 04:05:04 - INFO - Batch 651/1300 - Loss: 0.1598 - Avg batch time: 0.15s\n",
      "2025-06-10 04:05:05 - INFO - Processing batch 661/1300\n",
      "2025-06-10 04:05:05 - INFO - Batch 661/1300 - Loss: 0.3837 - Avg batch time: 0.15s\n",
      "2025-06-10 04:05:07 - INFO - Processing batch 671/1300\n",
      "2025-06-10 04:05:07 - INFO - Batch 671/1300 - Loss: 0.4927 - Avg batch time: 0.15s\n",
      "2025-06-10 04:05:08 - INFO - Processing batch 681/1300\n",
      "2025-06-10 04:05:08 - INFO - Batch 681/1300 - Loss: 0.5495 - Avg batch time: 0.15s\n",
      "2025-06-10 04:05:10 - INFO - Processing batch 691/1300\n",
      "2025-06-10 04:05:10 - INFO - Batch 691/1300 - Loss: 0.3976 - Avg batch time: 0.15s\n",
      "2025-06-10 04:05:11 - INFO - Processing batch 701/1300\n",
      "2025-06-10 04:05:11 - INFO - Batch 701/1300 - Loss: 0.6639 - Avg batch time: 0.15s\n",
      "2025-06-10 04:05:13 - INFO - Processing batch 711/1300\n",
      "2025-06-10 04:05:13 - INFO - Batch 711/1300 - Loss: 0.5048 - Avg batch time: 0.15s\n",
      "Epochs:   8%| | 8/100 [27:16<5:58:30, 233.81s/it, train_loss=0.4633, val_loss=0.4812, best_val_f1=0.3361, lr=5.00e-05, b\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhybrid\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcnn_bilstm_gcn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EEGCNNBiLSTMGCN\n\u001b[1;32m      3\u001b[0m train_context \u001b[38;5;241m=\u001b[39m training_context\u001b[38;5;241m.\u001b[39mswitch_to(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspatial\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mk_fold_train_shorthand\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEEGCNNBiLSTMGCN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcnn_dropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# slightly higher dropout to avoid overfitting\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlstm_hidden_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlstm_out_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlstm_dropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# slightly higher dropout to avoid overfitting\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder_use_batch_norm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder_use_layer_norm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Parameters for the EEGGCN (graph neural network)\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mout_channels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpooling_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgcn_use_batch_norm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_conv_layers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgcn_dropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# slightly higher dropout to avoid overfitting\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_channels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m19\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_graph_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHECKPOINT_ROOT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcnn_bilstm_gcn_signal_k_fold_.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# use training loop for GNN models\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# lower batch size (GPU poor)\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# log to wandb\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 55\u001b[0m, in \u001b[0;36mk_fold_train_shorthand\u001b[0;34m(model_class, model_parameters, save_path, use_gnn, log_wandb, batch_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;66;03m# force import to avoid bug\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m aggregated_train_history, aggregated_val_history, fold_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_k_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# dataset to use\u001b[39;49;00m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_tr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclips\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# train models\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# optimizer\u001b[39;49;00m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBCEWithLogitsLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbetas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# scheduler\u001b[39;49;00m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfactor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hidden attribute\u001b[39;49;00m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m plot_training_loss(aggregated_train_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], aggregated_val_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:1238\u001b[0m, in \u001b[0;36mtrain_k_fold\u001b[0;34m(dataset, labels, model_class, model_kwargs, criterion, optimizer_class, optimizer_kwargs, device, save_dir, k_folds, stratified, scheduler_class, scheduler_kwargs, monitor, patience, num_epochs, grad_clip, batch_size, use_gnn, wandb_config, wandb_project, log_wandb, random_state)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;66;03m# Train this fold\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1238\u001b[0m     train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_clip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Always overwrite for k-fold\u001b[39;49;00m\n\u001b[1;32m   1252\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_wandb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_run_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_wandb_run_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtry_load_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Don't load checkpoint for k-fold\u001b[39;49;00m\n\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;66;03m# Get best scores for this fold\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m     best_train_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(train_history\u001b[38;5;241m.\u001b[39mget(monitor\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m), [\u001b[38;5;241m0\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m monitor \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(train_history\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)]))\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:428\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, wandb_config, wandb_project, wandb_run_name, log_wandb, try_load_checkpoint, original_dataset)\u001b[0m\n\u001b[1;32m    425\u001b[0m         graph_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;66;03m# Use safe model call that handles graph_features parameter automatically\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_model_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcurr_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcurr_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcurr_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mgraph_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;66;03m# unsqueeze y_targets to match the shape of the logits. used later!\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:149\u001b[0m, in \u001b[0;36m_safe_model_call\u001b[0;34m(model, x, edge_index, batch, graph_features)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph_features\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params \u001b[38;5;129;01mand\u001b[39;00m graph_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m         call_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph_features\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m graph_features\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcall_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# Non-GNN models - just pass x\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     call_args \u001b[38;5;241m=\u001b[39m [x]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1798\u001b[0m     ):\n\u001b[1;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/layers/hybrid/cnn_bilstm_gcn.py:151\u001b[0m, in \u001b[0;36mEEGCNNBiLSTMGCN.forward\u001b[0;34m(self, x, edge_index, batch, graph_features)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03mForward pass of the CNN-BiLSTM-GCN model.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Class logits for each graph in the batch. Shape: [num_graphs_in_batch, num_classes].\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    150\u001b[0m node_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_encoder(x)\n\u001b[0;32m--> 151\u001b[0m gcn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [num_graphs_in_batch, out_channels]\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Combine with graph-level features if available\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_graph_features:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# check if graph features have been loaded\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/layers/gnn/gcn.py:129\u001b[0m, in \u001b[0;36mEEGGCN.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# x = self.project_to(x)  # [N, in_channels]\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# GCN layers\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_conv_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 129\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norms[i](x)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:99\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_self_loops:\n\u001b[0;32m---> 99\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43madd_remaining_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), ), dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    104\u001b[0m                              device\u001b[38;5;241m=\u001b[39medge_index\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch_geometric/utils/loop.py:652\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    648\u001b[0m     is_undirected \u001b[38;5;241m=\u001b[39m edge_index\u001b[38;5;241m.\u001b[39mis_undirected\n\u001b[1;32m    650\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m edge_index[:, mask]\n\u001b[0;32m--> 652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_scripting\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, EdgeIndex):\n\u001b[1;32m    653\u001b[0m     edge_index\u001b[38;5;241m.\u001b[39m_is_undirected \u001b[38;5;241m=\u001b[39m is_undirected\n\u001b[1;32m    655\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([edge_index, loop_index], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/_jit_internal.py:103\u001b[0m, in \u001b[0;36mis_scripting\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m7\u001b[39m):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcastingList\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m BroadcastingList1\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_scripting\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    Function that returns True when in compilation and False otherwise. This\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    is useful especially with the @unused decorator to leave code in your\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m                return unsupported_linear_op(x)\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%aimport src.layers.hybrid.cnn_bilstm_gcn\n",
    "# from src.layers.hybrid.cnn_bilstm_gcn import EEGCNNBiLSTMGCN\n",
    "from src.layers.hybrid.cnn_bilstm_gcn_old import EEGCNNBiLSTMGCN\n",
    "train_context = training_context.switch_to('spatial')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGCNNBiLSTMGCN,\n",
    "    model_parameters={\n",
    "        # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "        \"cnn_dropout_prob\": 0.25, # slightly higher dropout to avoid overfitting\n",
    "        \"lstm_hidden_dim\": 128,\n",
    "        \"lstm_out_dim\": 128,\n",
    "        \"lstm_dropout\": 0.25, # slightly higher dropout to avoid overfitting\n",
    "        \"encoder_use_batch_norm\": True,\n",
    "        \"encoder_use_layer_norm\": False,\n",
    "        # Parameters for the EEGGCN (graph neural network)\n",
    "        \"gcn_hidden_channels\": 128,\n",
    "        \"gcn_out_channels\": 96,\n",
    "        \"gcn_pooling_type\": \"max\",\n",
    "        \"gcn_use_batch_norm\": True,\n",
    "        \"gcn_num_layers\": 4,\n",
    "        \"gcn_dropout_prob\": 0.6, # slightly higher dropout to avoid overfitting\n",
    "        \"num_channels\": 19,\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_signal_k_fold_.pt\",\n",
    "    use_gnn=True, # use training loop for GNN models\n",
    "    batch_size=8, # lower batch size (GPU poor)\n",
    "    log_wandb=True, # log to wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "177fedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.utils.train\n",
    "def wrap_gnn_train(model, save_path):\n",
    "    global graph_training_context\n",
    "    if 'graph_training_context' not in globals():\n",
    "        raise ValueError(\"Graph training context is not initialized. Please initialize it before calling this function.\")\n",
    "    if not isinstance(graph_training_context, TrainingContext):\n",
    "        raise ValueError(\"graph_training_context must be an instance of TrainingContext.\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "    # optimizer = Lion(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5)\n",
    "    loss = nn.BCEWithLogitsLoss()  # Not weighted as we use a balanced sampler!\n",
    "\n",
    "    # train model\n",
    "    train_history, val_history = train_model(\n",
    "        wandb_config=None,\n",
    "        model=model,\n",
    "        train_loader=graph_training_context.train_loader,\n",
    "        val_loader=graph_training_context.val_loader,\n",
    "        criterion=loss,\n",
    "        scheduler=scheduler,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=config[\"epochs\"],\n",
    "        patience=config[\"patience\"],\n",
    "        save_path=save_path,\n",
    "        use_gnn=True,\n",
    "        # hidden attribute\n",
    "        try_load_checkpoint=True,\n",
    "    )\n",
    "    plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720925b0",
   "metadata": {},
   "source": [
    "### Test 3 - First breakthrough model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036835f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm .checkpoints/cnn_bilstm_gcn_test_3_correlation_test_mean_pooling.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a43eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_test_best_model_{DATASET_TYPE}_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm = True,\n",
    "    encoder_use_layer_norm = False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim = 192,\n",
    "    out_channels = 128,\n",
    "    pooling_type = \"max\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_conv_layers = 4,\n",
    "    gcn_dropout = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    "    use_graph_features=False\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a5bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "from src.layers.hybrid.cnn_bilstm_gat import EEGCNNBiLSTMGAT\n",
    "\n",
    "\n",
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gat_test_3_{DATASET_TYPE}_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGAT(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout=0.25,\n",
    "    lstm_hidden_dim=128,\n",
    "    lstm_out_dim=128,\n",
    "    lstm_dropout=0.25,\n",
    "    encoder_use_batch_norm=True,\n",
    "    encoder_use_layer_norm=False,\n",
    "    # Parameters for the GAT (graph attention network)\n",
    "    hidden_dim=128,\n",
    "    out_channels=96,\n",
    "    pooling_type=\"mean\",\n",
    "    gat_use_batch_norm=True,\n",
    "    num_conv_layers=3,\n",
    "    gat_dropout=0.5,\n",
    "    gat_heads=4,  # Number of attention heads\n",
    "    num_channels=19,\n",
    "    # Graph features configuration\n",
    "    graph_feature_dim=graph_feature_dim,\n",
    "    use_graph_features=False if DATASET_TYPE == 'spatial' else True,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_test_3_{DATASET_TYPE}_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim = 128,\n",
    "    out_channels = 96,\n",
    "    pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_conv_layers = 3,\n",
    "    gcn_dropout = 0.5,\n",
    "    num_channels = 19,\n",
    "    # enable graph features\n",
    "    # NOTE: using graph level features gives worse results with spatial dataset\n",
    "    use_graph_features=False if DATASET_TYPE == 'spatial' else True,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11afb6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_test_3_{DATASET_TYPE}_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout=0.25,\n",
    "    lstm_hidden_dim=128,\n",
    "    lstm_out_dim=128,\n",
    "    lstm_dropout=0.25,\n",
    "    encoder_use_batch_norm=True,\n",
    "    encoder_use_layer_norm=False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim=128,\n",
    "    out_channels=128,\n",
    "    pooling_type=\"mean\",\n",
    "    gcn_use_batch_norm=True,\n",
    "    num_conv_layers=3,\n",
    "    gcn_dropout=0.5,\n",
    "    num_channels=19,\n",
    "    use_graph_features=True\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424f5b8b",
   "metadata": {},
   "source": [
    "### Test 4 - Smaller CGN output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_4.pt\"\n",
    "\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 64,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 3,\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eaaf2d",
   "metadata": {},
   "source": [
    "### Test 5 - Smaller GCN output channels + increased embedding length + Deeper GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1335c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_5.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 64,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da60626",
   "metadata": {},
   "source": [
    "\n",
    "### Test 6: slighly bigger GCN output channels\n",
    ">[HIGHEST F1 SCORE EVER RECORDED]\n",
    "```\n",
    "âœ… Checkpoint loaded. Resuming from epoch 33. Best 'val_f1' score: 0.7346\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_6.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 96,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad429a",
   "metadata": {},
   "source": [
    "### Test 7B: Alternative architecture to improve generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f882bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_8.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.35, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.35, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 96,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b4f3c",
   "metadata": {},
   "source": [
    "### Test 7C: slightly bigger GCN layers\n",
    "\n",
    "BEST MODEL YET!\n",
    "\n",
    "(SPATIAL FEATURES!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_best_model_correlation_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm = True,\n",
    "    encoder_use_layer_norm = False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim = 192,\n",
    "    out_channels = 128,\n",
    "    pooling_type = \"max\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_conv_layers = 4,\n",
    "    gcn_dropout = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    "    graph_feature_dim=graph_feature_dim,\n",
    "    use_graph_features=False if DATASET_TYPE == 'spatial' else True,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6355457",
   "metadata": {},
   "source": [
    "### Test 7D: even bigger GCN layers\n",
    "\n",
    "Comparable performance to best model. We might need to increase the number of GCN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed9d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_even_bigger.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm = True,\n",
    "    encoder_use_layer_norm = False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 224,\n",
    "    gcn_out_channels = 192,\n",
    "    gcn_pooling_type = \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50956294",
   "metadata": {},
   "source": [
    "### Test 7E: increased number of GCN layers\n",
    "\n",
    "Assumption: the previous model was unable to learn enough, maybe the GCN was unable to capture\n",
    "\n",
    "```\n",
    "Epochs:   9%| | 9/100 [17:54<3:23:31, 134.20s/it, train_loss=0.4532, val_loss=0.3489, best_val_f1=0.6695, lr=5.00e-05, b2025-06-07 17:01:05 - INFO - \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_even_more_bigger.pt\"\n",
    "model_generalizable_even_more_bigger = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 224,\n",
    "    gcn_out_channels = 192,\n",
    "    gcn_num_layers = 5,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08442186",
   "metadata": {},
   "source": [
    "### Test 7F: Increased number of BiLSTM layers + Test 7E architecture\n",
    "\n",
    "Assumpion: we saw a drammatical increase in accuracy by increasing the number of GCN layers. This hints that the model was now able to learn the most from the embeddings. To improve the performance even further without having to increase the number of GCN layers even more (overall reduce complexity, improve generalization), we will try to increase the number of BiLSTM layers. \n",
    "\n",
    "Using multiple BiLSTM layers will allow embeddings to be processed in a more complex way, potentially capturing more intricate relationships in the data. The GCN layers will take care of the graph structure, while the BiLSTM layers will enhance the temporal dependencies and relationships in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b963a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_even_more_bigger.pt\"\n",
    "model_generalizable_even_more_bigger = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    lstm_num_layers = 2,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 224,\n",
    "    gcn_out_channels = 192,\n",
    "    gcn_num_layers = 5,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663332c",
   "metadata": {},
   "source": [
    "```\n",
    "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]2025-06-07 18:55:16 - INFO -\n",
    "Epochs:   2%| | 2/100 [04:35<7:29:19, 275.10s/it, train_loss=0.6212, val_loss=0.4619, best_val_f1=0.4055, lr=1.00e-04, b2025-06-07 18:59:51 - INFO -\n",
    "Epochs:   3%| | 3/100 [09:09<7:23:49, 274.53s/it, train_loss=0.5819, val_loss=0.4295, best_val_f1=0.4055, lr=1.00e-04, b2025-06-07 19:04:25 - INFO -\n",
    "Epochs:   4%| | 4/100 [13:42<7:18:31, 274.08s/it, train_loss=0.5628, val_loss=0.4437, best_val_f1=0.4055, lr=1.00e-04, b2025-06-07 19:08:59 - INFO -\n",
    "Epochs:   5%| | 5/100 [18:16<7:13:28, 273.78s/it, train_loss=0.5452, val_loss=0.3942, best_val_f1=0.4858, lr=1.00e-04, b2025-06-07 19:13:32 - INFO -\n",
    "Epochs:   6%| | 6/100 [22:49<7:08:41, 273.63s/it, train_loss=0.5334, val_loss=0.4563, best_val_f1=0.4858, lr=1.00e-04, b2025-06-07 19:18:05 - INFO -\n",
    "Epochs:   7%| | 7/100 [27:22<7:04:01, 273.57s/it, train_loss=0.5319, val_loss=0.3738, best_val_f1=0.5137, lr=1.00e-04, b2025-06-07 19:22:39 - INFO -\n",
    "Epochs:   8%| | 8/100 [31:56<6:59:20, 273.48s/it, train_loss=0.5181, val_loss=0.4369, best_val_f1=0.5695, lr=1.00e-04, b2025-06-07 19:27:12 - INFO -\n",
    "Epochs:   9%| | 9/100 [36:29<6:54:50, 273.52s/it, train_loss=0.5220, val_loss=0.4202, best_val_f1=0.5695, lr=1.00e-04, b2025-06-07 19:31:46 - INFO -\n",
    "Epochs:  10%| | 10/100 [41:03<6:50:17, 273.52s/it, train_loss=0.5286, val_loss=0.4167, best_val_f1=0.5695, lr=1.00e-04, 2025-06-07 19:36:19 - INFO -\n",
    "Epochs:  11%| | 11/100 [45:36<6:45:44, 273.53s/it, train_loss=0.5065, val_loss=0.3864, best_val_f1=0.5695, lr=1.00e-04, 2025-06-07 19:40:53 - INFO -\n",
    "Epochs:  12%| | 12/100 [50:10<6:41:03, 273.45s/it, train_loss=0.5158, val_loss=0.5175, best_val_f1=0.5695, lr=5.00e-05, 2025-06-07 19:45:26 - INFO -\n",
    "Epochs:  13%|â–| 13/100 [54:43<6:36:23, 273.37s/it, train_loss=0.5035, val_loss=0.3785, best_val_f1=0.5940, lr=5.00e-05, 2025-06-07 19:49:59 - INFO -\n",
    "Epochs:  14%|â–| 14/100 [59:16<6:31:50, 273.38s/it, train_loss=0.4842, val_loss=0.3838, best_val_f1=0.5981, lr=5.00e-05, 2025-06-07 19:54:33 - INFO -\n",
    "Epochs:  15%|â–| 15/100 [1:03:50<6:27:17, 273.38s/it, train_loss=0.4644, val_loss=0.3493, best_val_f1=0.6106, lr=5.00e-052025-06-07 19:59:06 - INFO -\n",
    "Epochs:  16%|â–| 16/100 [1:08:23<6:22:46, 273.41s/it, train_loss=0.4887, val_loss=0.3737, best_val_f1=0.6106, lr=5.00e-052025-06-07 20:03:39 - INFO -\n",
    "Epochs:  17%|â–| 17/100 [1:12:57<6:18:12, 273.41s/it, train_loss=0.4775, val_loss=0.3565, best_val_f1=0.6106, lr=5.00e-052025-06-07 20:08:13 - INFO -\n",
    "Epochs:  18%|â–| 18/100 [1:17:30<6:13:42, 273.44s/it, train_loss=0.4635, val_loss=0.3704, best_val_f1=0.6106, lr=2.50e-052025-06-07 20:12:46 - INFO -\n",
    "Epochs:  19%|â–| 19/100 [1:22:04<6:09:15, 273.53s/it, train_loss=0.4501, val_loss=0.3635, best_val_f1=0.6131, lr=2.50e-052025-06-07 20:17:20 - INFO -\n",
    "Epochs:  20%|â–| 20/100 [1:26:37<6:04:39, 273.49s/it, train_loss=0.4379, val_loss=0.3638, best_val_f1=0.6179, lr=2.50e-052025-06-07 20:21:53 - INFO -\n",
    "Epochs:  21%|â–| 21/100 [1:31:10<6:00:01, 273.43s/it, train_loss=0.4494, val_loss=0.3543, best_val_f1=0.6179, lr=2.50e-052025-06-07 20:26:27 - INFO -\n",
    "Epochs:  22%|â–| 22/100 [1:35:44<5:55:26, 273.42s/it, train_loss=0.4616, val_loss=0.3616, best_val_f1=0.6659, lr=2.50e-052025-06-07 20:31:00 - INFO -\n",
    "Epochs:  23%|â–| 23/100 [1:40:17<5:50:54, 273.44s/it, train_loss=0.4381, val_loss=0.3532, best_val_f1=0.6659, lr=2.50e-052025-06-07 20:35:34 - INFO -\n",
    "Epochs:  24%|â–| 24/100 [1:44:51<5:46:22, 273.45s/it, train_loss=0.4423, val_loss=0.3635, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:40:07 - INFO -\n",
    "Epochs:  25%|â–Ž| 25/100 [1:49:24<5:41:52, 273.49s/it, train_loss=0.4291, val_loss=0.3473, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:44:41 - INFO -\n",
    "Epochs:  26%|â–Ž| 26/100 [1:53:58<5:37:12, 273.42s/it, train_loss=0.4403, val_loss=0.3380, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:49:14 - INFO -\n",
    "Epochs:  27%|â–Ž| 27/100 [1:58:31<5:32:38, 273.40s/it, train_loss=0.4312, val_loss=0.3374, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:53:47 - INFO -\n",
    "Epochs:  28%|â–Ž| 28/100 [2:03:05<5:28:07, 273.44s/it, train_loss=0.4393, val_loss=0.3441, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:58:21 - INFO -\n",
    "Epochs:  29%|â–Ž| 29/100 [2:07:38<5:23:35, 273.46s/it, train_loss=0.4226, val_loss=0.3392, best_val_f1=0.6659, lr=1.25e-052025-06-07 21:02:54 - INFO -\n",
    "Epochs:  30%|â–Ž| 30/100 [2:12:11<5:19:02, 273.46s/it, train_loss=0.4240, val_loss=0.3525, best_val_f1=0.6659, lr=6.25e-062025-06-07 21:07:28 - INFO -\n",
    "Epochs:  31%|â–Ž| 31/100 [2:16:45<5:14:28, 273.46s/it, train_loss=0.4249, val_loss=0.3492, best_val_f1=0.6659, lr=6.25e-062025-06-07 21:12:01 - INFO -\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_optimized.pt\"\n",
    "model_generalizable_optimized = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 160,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    lstm_num_layers = 2,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 192,\n",
    "    gcn_out_channels = 128,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.5, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c4b3a",
   "metadata": {},
   "source": [
    "### Test 8: Narrow but Deep GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff01da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_narrow_deep_model.pt\"\n",
    "narrow_deep_model = EEGCNNBiLSTMGCN(\n",
    "    # --- Simplify the Temporal Encoder ---\n",
    "    cnn_dropout_prob = 0.2,\n",
    "    lstm_hidden_dim = 64,  # Reduced\n",
    "    lstm_out_dim = 64,     # Reduced\n",
    "    lstm_dropout_prob = 0.2,\n",
    "    # --- Focus on the GCN ---\n",
    "    gcn_hidden_channels = 128, # Keep GCN capacity high\n",
    "    gcn_out_channels = 64,\n",
    "    gcn_num_layers = 5,      # Try going even deeper\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ef8b3",
   "metadata": {},
   "source": [
    "### Test 9: First best model, with wider + deeper GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_new_old_best_model.pt\"\n",
    "new_old_best_model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 128, # from 64 to 128\n",
    "    gcn_num_layers = 4, # from 3 to 4\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351da028",
   "metadata": {},
   "source": [
    "### Best model + attention BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.hybrid.cnn_bilstm_attention_gcn\n",
    "from src.layers.hybrid.cnn_bilstm_attention_gcn import EEGCNNBiLSTMAttentionGNN\n",
    "\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_attention.pt\"\n",
    "model_first_attention = EEGCNNBiLSTMAttentionGNN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 192,\n",
    "    gcn_out_channels = 128,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a8a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from src.utils.train import train_model\n",
    "\n",
    "model = model_small_gcn_bigger_embedding\n",
    "model = model.to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "# optimizer = Lion(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "loss = nn.BCEWithLogitsLoss() # Not weighted as we use a balanced sampler!\n",
    "\n",
    "# empty cache in order to free up VRAM (if available)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# train model\n",
    "train_history, val_history = train_model(\n",
    "    wandb_config=None,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=loss,\n",
    "    scheduler=scheduler,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=config[\"epochs\"],\n",
    "    patience=config[\"patience\"],\n",
    "    save_path=SAVE_PATH,\n",
    "    use_gnn=True,\n",
    "    # hidden attribute\n",
    "    try_load_checkpoint=True,\n",
    ")\n",
    "\n",
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch cuda clear cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63329a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
