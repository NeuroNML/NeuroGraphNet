{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d97e92",
   "metadata": {},
   "source": [
    "# NeuroGraphNet\n",
    "\n",
    "*A graph-based deep learning framework for EEG seizure detection, designed to improve accuracy and interpretability by leveraging Graph Neural Networks (GNNs) to capture spatial and temporal brain dynamics.*\n",
    "\n",
    "<hr />\n",
    "\n",
    "This notebook presents different approaches to EEG seizure detection using traditional machine learning and deep learning methods as well as a novel approaches using Graph Neural Networks (GNNs). The dataset used a subset of the TUSZ EEG Seizure dataset, which contains EEG recordings from patients with epilepsy.\n",
    "\n",
    "**Authors**: Luca Di Bello, Guillaume André Bélissent, Abdessalem Ben Ali, Beatriz Izquierdo González"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5837c2",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b998b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from src.utils.seeder import seed_everything\n",
    "\n",
    "# set seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# create useful constants\n",
    "RANDOM_SEED = 42\n",
    "IS_SCITAS = True # set to True if running on SCITAS cluster\n",
    "LOCAL_DATA_ROOT = Path(\"./data\")\n",
    "DATA_ROOT = Path(\"/home/ogut/data\") if IS_SCITAS else LOCAL_DATA_ROOT\n",
    "CHECKPOINT_ROOT = Path(\"./.checkpoints\")\n",
    "SUBMISSION_ROOT = Path(\"./.submissions\")\n",
    "\n",
    "# create directories if they do not exist\n",
    "CHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# set dataset root\n",
    "seed_everything(RANDOM_SEED)\n",
    "\n",
    "# setup torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f74a9",
   "metadata": {},
   "source": [
    "## Run feature extraction script\n",
    "\n",
    "In order to run all the models in this notebook, is necessary to run the feature extraction script first. This script extracts features from the EEG signals for both the training and test dataset, saving three files: `X_train.npy`, `y_train.npy`, and `X_test.npy`. The features extracted are the same used in the original paper, which are based on the EEG signals.\n",
    "\n",
    "You can run the script by uncommenting and executing the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d28586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# # execute feature extraction script\n",
    "# process = None\n",
    "# try:\n",
    "#     process = subprocess.Popen([\"python3\", \"scripts/feature_extractor.py\"])\n",
    "#     process.wait()\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"Process interrupted, terminating...\")\n",
    "#     if process:\n",
    "#         process.terminate()\n",
    "#         process.wait()\n",
    "# except Exception as e:\n",
    "#     print(f\"Error occurred: {e}\")\n",
    "#     if process:\n",
    "#         process.terminate()\n",
    "#         process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527547f9",
   "metadata": {},
   "source": [
    "## Define paths\n",
    "\n",
    "The following paths are used to load the required data files and save the results of the models. Make sure to adjust them according to your local setup.\n",
    "\n",
    "The current configuration assumes that the data files are located in a folder named `data` within the current working directory. \n",
    "\n",
    "**NOTE:** to simplify the process on SCITAS cluster, we provide a toggle `IS_SCITAS` to set the paths accordingly (_refer to first cell of the notebook_). If you are running this notebook on your local machine, you can set `IS_SCITAS = False` and adjust the paths as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45999291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacial distance matrix between sensors\n",
    "spatial_distance_file = LOCAL_DATA_ROOT / \"distances_3d.csv\"\n",
    "\n",
    "# absdiff correlation matrix\n",
    "absdiff_correlation_file = LOCAL_DATA_ROOT / \"diff_corr_matrix.csv\"\n",
    "\n",
    "# training data\n",
    "train_dir = DATA_ROOT / \"train\"\n",
    "train_dir_metadata = train_dir / \"segments.parquet\"\n",
    "train_dataset_absdiff_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_absdiff_correlation_train\"\n",
    "train_dataset_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_correlation_train\"\n",
    "train_dataset_spatial_dir = LOCAL_DATA_ROOT / \"graph_dataset_spatial_train\"\n",
    "train_dataset_timeseries_feature_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_features\")\n",
    "train_dataset_timeseries_signal_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_signal\")\n",
    "\n",
    "# test data\n",
    "test_dir = DATA_ROOT / \"test\"\n",
    "test_dir_metadata = test_dir / \"segments.parquet\"\n",
    "test_dataset_absdiff_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_absdiff_correlation_test\"\n",
    "test_dataset_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_correlation_test\"\n",
    "test_dataset_spatial_dir = LOCAL_DATA_ROOT / \"graph_dataset_spatial_test\"\n",
    "test_dataset_timeseries_signal_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_signal\")\n",
    "test_dataset_timeseries_feature_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_features\")\n",
    "\n",
    "# additional features\n",
    "extracted_features_dir = LOCAL_DATA_ROOT / \"extracted_features\"\n",
    "embeddings_dir =  LOCAL_DATA_ROOT / \"embeddings\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f1780",
   "metadata": {},
   "source": [
    "## Loading Train and Test Clips from the Dataset\n",
    "\n",
    "To load patient clips from the dataset, we use the `load_clips` function. This function retrieves EEG signals and labels from the specified paths and returns them as NumPy arrays.\n",
    "\n",
    "Different versions of Pandas may return either a MultiIndex or a single index, even when called with the same parameters. To address this inconsistency, we use the `ensure_eeg_multiindex` function to ensure that the resulting DataFrame has a MultiIndex structure. This is essential for subsequent processing steps.\n",
    "\n",
    "If a MultiIndex is not present, it will be created using the following levels: `patient_id`, `clip_id`, and `channel`. This structure is crucial for organizing the dataset, as EEG signals are grouped by patient, clip, and channel. It also ensures compatibility with existing code that expects this format, such as the `EEGDataset` class from the [seizure-eeg](https://www.piwheels.org/project/seiz-eeg/) package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d93851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.index import ensure_eeg_multiindex \n",
    "\n",
    "# Load clips from datasets\n",
    "clips_tr = pd.read_parquet(train_dir_metadata)\n",
    "clips_tr = ensure_eeg_multiindex(clips_tr)\n",
    "clips_tr['id'] = clips_tr.index.map(lambda x: '_'.join(str(i) for i in x))\n",
    "assert clips_tr.id.nunique() == len(clips_tr), \"There are duplicate IDs\"\n",
    "clips_tr = clips_tr[~clips_tr.label.isna()].reset_index()\n",
    "\n",
    "# Load clips from datasets\n",
    "clips_te = pd.read_parquet(test_dir_metadata)\n",
    "clips_te = ensure_eeg_multiindex(clips_te)\n",
    "clips_te['id'] = clips_te.index.map(lambda x: '_'.join(str(i) for i in x))\n",
    "assert clips_te.id.nunique() == len(clips_te), \"There are duplicate IDs\"\n",
    "clips_te = clips_te.reset_index()\n",
    "\n",
    "# sort in order to maintain the same submission order\n",
    "clips_te = clips_te.sort_values(by=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f7bc0",
   "metadata": {},
   "source": [
    "## Loading Datasets\n",
    "\n",
    "This notebook explores various approaches to EEG seizure detection, requiring multiple dataset variants with distinct preprocessing strategies (e.g., raw EEG signals, extracted features, and diverse graph construction methods). The `GraphEEGDataset` class, a custom implementation of `torch.utils.data.Dataset`, is used to load these datasets based on specified parameters.\n",
    "\n",
    "The `GraphEEGDataset` class is designed to support all preprocessing strategies, including graph-based approaches. It preprocesses EEG data on-the-fly, offering flexibility in data handling and model input preparation. Additionally, it includes a caching mechanism to store preprocessed data on disk. This mechanism ensures that subsequent calls with identical parameters load precomputed data, significantly reducing dataset loading time during repeated runs. This feature has been instrumental in accelerating development and experimentation within this notebook.\n",
    "\n",
    "Specifically, we will load the following datasets:\n",
    "\n",
    "A) **EEG signals**: \n",
    "\n",
    "- **Raw EEG**:\n",
    "\n",
    "    - Feature-based\n",
    "\n",
    "        - Raw EEG signals + signal time-filtering/rereferencing/normalization preprocessing\n",
    "\n",
    "    - Graph-based\n",
    "\n",
    "        - Raw EEG signals + spatial graph construction strategy + signal time-filtering/rereferencing/normalization preprocessing + graph-features\n",
    "\n",
    "        - Raw EEG signals + correlation graph construction strategy (top-k policy with k=10) + signal time-filtering/rereferencing/normalization preprocessing + graph-features\n",
    "\n",
    "        - Raw EEG signals + absolute difference correlation graph construction strategy (top-k policy with k=10) + signal time-filtering/rereferencing/normalization preprocessing + graph-features\n",
    "\n",
    "- **Feature-based EEG**:\n",
    "\n",
    "    - Extracted features + spatial graph construction strategy + signal time-filtering/rereferencing/normalization preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e759ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generael settings\n",
    "\n",
    "# bandpass filter settings (signal time-filtering)\n",
    "low_bandpass_frequency = 0.5\n",
    "high_bandpass_frequency = 50\n",
    "\n",
    "# additional settings\n",
    "# NOTE: the training already fights class imbalance, so this is not used\n",
    "oversampling_power = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef132cd",
   "metadata": {},
   "source": [
    "### Raw-EEG signal datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8110e5",
   "metadata": {},
   "source": [
    "#### A) Spatial graph construction strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a686e9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "2025-06-10 17:14:01 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 17:14:01 - INFO - Dataset parameters:\n",
      "2025-06-10 17:14:01 - INFO -   - Root directory: data/graph_dataset_spatial_train\n",
      "2025-06-10 17:14:01 - INFO -   - Edge strategy: spatial\n",
      "2025-06-10 17:14:01 - INFO -   - Top-k neighbors: None\n",
      "2025-06-10 17:14:01 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 17:14:01 - INFO -   - Force reprocess: False\n",
      "2025-06-10 17:14:01 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 17:14:01 - INFO -   - Segment length: 3000\n",
      "2025-06-10 17:14:01 - INFO -   - Apply filtering: True\n",
      "2025-06-10 17:14:01 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 17:14:01 - INFO -   - Apply normalization: True\n",
      "2025-06-10 17:14:01 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 17:14:01 - INFO -   - Test mode: False\n",
      "2025-06-10 17:14:01 - INFO -   - Extract graph features: False\n",
      "2025-06-10 17:14:01 - INFO -   - Diff Corr Matrix Path: None\n",
      "2025-06-10 17:14:01 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 17:14:01 - INFO - Setting up signal filters...\n",
      "2025-06-10 17:14:01 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 17:14:01 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 17:14:01 - INFO - Loaded 361 spatial distances in 0.04s\n",
      "2025-06-10 17:14:01 - INFO - Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset_spatial_tr: 12993\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_spatial_tr = GraphEEGDataset(\n",
    "    root=train_dataset_spatial_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"spatial\",\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file\n",
    "    ),\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=False,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_spatial_tr: {len(dataset_spatial_tr)}\")\n",
    "print(f' Eliminated IDs: {dataset_spatial_tr.ids_to_eliminate}')\n",
    "clips_spatial_tr = clips_tr[~clips_tr.index.isin(dataset_spatial_tr.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b33fe510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 17:14:02 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 17:14:02 - INFO - Dataset parameters:\n",
      "2025-06-10 17:14:02 - INFO -   - Root directory: data/graph_dataset_spatial_test\n",
      "2025-06-10 17:14:02 - INFO -   - Edge strategy: spatial\n",
      "2025-06-10 17:14:02 - INFO -   - Top-k neighbors: None\n",
      "2025-06-10 17:14:02 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 17:14:02 - INFO -   - Force reprocess: False\n",
      "2025-06-10 17:14:02 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 17:14:02 - INFO -   - Segment length: 3000\n",
      "2025-06-10 17:14:02 - INFO -   - Apply filtering: True\n",
      "2025-06-10 17:14:02 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 17:14:02 - INFO -   - Apply normalization: True\n",
      "2025-06-10 17:14:02 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 17:14:02 - INFO -   - Test mode: True\n",
      "2025-06-10 17:14:02 - INFO -   - Extract graph features: False\n",
      "2025-06-10 17:14:02 - INFO -   - Diff Corr Matrix Path: None\n",
      "2025-06-10 17:14:02 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 17:14:02 - INFO - Setting up signal filters...\n",
      "2025-06-10 17:14:02 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 17:14:02 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 17:14:02 - INFO - Loaded 361 spatial distances in 0.01s\n",
      "2025-06-10 17:14:02 - INFO - Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset_spatial_te: 3614\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "# load training dataset\n",
    "dataset_spatial_te = GraphEEGDataset(\n",
    "    root=test_dataset_spatial_dir,\n",
    "    clips=clips_te,\n",
    "    signal_folder=test_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"spatial\",\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file\n",
    "    ),\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    is_test=True,\n",
    "    # extract graph features\n",
    "    extract_graph_features=False,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_spatial_te: {len(dataset_spatial_te)}\")\n",
    "print(f' Eliminated IDs: {dataset_spatial_te.ids_to_eliminate}')\n",
    "clips_spatial_te = clips_te[~clips_te.index.isin(dataset_spatial_te.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8993f7",
   "metadata": {},
   "source": [
    "#### B) Correlation graph construction strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1472121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "top_k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c3f93ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 17:14:02 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 17:14:02 - INFO - Dataset parameters:\n",
      "2025-06-10 17:14:02 - INFO -   - Root directory: data/graph_dataset_correlation_train\n",
      "2025-06-10 17:14:02 - INFO -   - Edge strategy: correlation\n",
      "2025-06-10 17:14:02 - INFO -   - Top-k neighbors: 5\n",
      "2025-06-10 17:14:02 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 17:14:02 - INFO -   - Force reprocess: False\n",
      "2025-06-10 17:14:02 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 17:14:02 - INFO -   - Segment length: 3000\n",
      "2025-06-10 17:14:02 - INFO -   - Apply filtering: True\n",
      "2025-06-10 17:14:02 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 17:14:02 - INFO -   - Apply normalization: True\n",
      "2025-06-10 17:14:02 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 17:14:02 - INFO -   - Test mode: False\n",
      "2025-06-10 17:14:02 - INFO -   - Extract graph features: True\n",
      "2025-06-10 17:14:02 - INFO -   - Diff Corr Matrix Path: None\n",
      "2025-06-10 17:14:02 - INFO - Initializing graph feature extractor...\n",
      "2025-06-10 17:14:02,460 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 17:14:02 - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 17:14:02 - INFO - Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 17:14:02 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 17:14:02 - INFO - Setting up signal filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "Length of dataset_corr_tr: 12986\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_corr_tr = GraphEEGDataset(\n",
    "    root=train_dataset_correlation_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"correlation\",\n",
    "    spatial_distance_file=None,\n",
    "    top_k=top_k,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None # collect all graph features\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_corr_tr: {len(dataset_corr_tr)}\")\n",
    "print(f' Eliminated IDs: {dataset_corr_tr.ids_to_eliminate}')\n",
    "clips_corr_tr = clips_tr[~clips_tr.index.isin(dataset_corr_tr.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "233887f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 17:14:02 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 17:14:02 - INFO - Dataset parameters:\n",
      "2025-06-10 17:14:02 - INFO -   - Root directory: data/graph_dataset_correlation_test\n",
      "2025-06-10 17:14:02 - INFO -   - Edge strategy: spatial\n",
      "2025-06-10 17:14:02 - INFO -   - Top-k neighbors: None\n",
      "2025-06-10 17:14:02 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 17:14:02 - INFO -   - Force reprocess: False\n",
      "2025-06-10 17:14:02 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 17:14:02 - INFO -   - Segment length: 3000\n",
      "2025-06-10 17:14:02 - INFO -   - Apply filtering: True\n",
      "2025-06-10 17:14:02 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 17:14:02 - INFO -   - Apply normalization: True\n",
      "2025-06-10 17:14:02 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 17:14:02 - INFO -   - Test mode: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 17:14:02 - INFO -   - Extract graph features: True\n",
      "2025-06-10 17:14:02 - INFO -   - Diff Corr Matrix Path: None\n",
      "2025-06-10 17:14:02 - INFO - Initializing graph feature extractor...\n",
      "2025-06-10 17:14:02,541 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 17:14:02 - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 17:14:02 - INFO - Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 17:14:02 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 17:14:02 - INFO - Setting up signal filters...\n",
      "2025-06-10 17:14:02 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 17:14:02 - INFO - Loading spatial distances from data/distances_3d.csv\n",
      "2025-06-10 17:14:02 - INFO - Loaded 361 spatial distances in 0.01s\n",
      "2025-06-10 17:14:02 - INFO - Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset_corr_te: 3614\n",
      " Eliminated IDs:[]\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_corr_te = GraphEEGDataset(\n",
    "    root=test_dataset_correlation_dir,\n",
    "    clips=clips_te,\n",
    "    signal_folder=test_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"spatial\",\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file\n",
    "    ),\n",
    "    top_k=None,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    is_test=True, # NOTE: needed to let the dataset know that is okay to now have labels!\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None # collect all graph features\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_corr_te: {len(dataset_corr_te)}\")\n",
    "print(f' Eliminated IDs:{dataset_corr_te.ids_to_eliminate}')\n",
    "clips_spatial_te = clips_te[~clips_te.index.isin(dataset_corr_te.ids_to_eliminate)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4461766e",
   "metadata": {},
   "source": [
    "#### C) Absolute difference correlation graph construction strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2328a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "top_k = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bf782ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 17:14:02 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 17:14:02 - INFO - Dataset parameters:\n",
      "2025-06-10 17:14:02 - INFO -   - Root directory: data/graph_dataset_absdiff_correlation_train\n",
      "2025-06-10 17:14:02 - INFO -   - Edge strategy: relevance_diff_correlation\n",
      "2025-06-10 17:14:02 - INFO -   - Top-k neighbors: 8\n",
      "2025-06-10 17:14:02 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 17:14:02 - INFO -   - Force reprocess: False\n",
      "2025-06-10 17:14:02 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 17:14:02 - INFO -   - Segment length: 3000\n",
      "2025-06-10 17:14:02 - INFO -   - Apply filtering: True\n",
      "2025-06-10 17:14:02 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 17:14:02 - INFO -   - Apply normalization: True\n",
      "2025-06-10 17:14:02 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 17:14:02 - INFO -   - Test mode: False\n",
      "2025-06-10 17:14:02 - INFO -   - Extract graph features: True\n",
      "2025-06-10 17:14:02 - INFO -   - Diff Corr Matrix Path: data/diff_corr_matrix.csv\n",
      "2025-06-10 17:14:02 - INFO - Edge strategy: relevance_diff_correlation. Loading average correlation matrices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 17:14:02 - INFO - Loaded absolute difference correlation matrix from data/diff_corr_matrix.csv, shape: (19, 19)\n",
      "2025-06-10 17:14:02 - INFO - Initializing graph feature extractor...\n",
      "2025-06-10 17:14:02,748 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 17:14:02 - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 17:14:02 - INFO - Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 17:14:02 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 17:14:02 - INFO - Setting up signal filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset_absdiff_corr_tr: 4646\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_absdiff_corr_tr = GraphEEGDataset(\n",
    "    root=train_dataset_absdiff_correlation_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"relevance_diff_correlation\",\n",
    "    spatial_distance_file=None,\n",
    "    top_k=top_k,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None, # collect all graph features\n",
    "    # settings for absolute difference correlation\n",
    "    diff_corr_matrix_path=absdiff_correlation_file,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_absdiff_corr_tr: {len(dataset_absdiff_corr_tr)}\")\n",
    "print(f' Eliminated IDs: {dataset_absdiff_corr_tr.ids_to_eliminate}')\n",
    "clips_absdiff_corr_tr = clips_tr[~clips_tr.index.isin(dataset_absdiff_corr_tr.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18240a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 17:14:02 - INFO - Initializing GraphEEGDataset...\n",
      "2025-06-10 17:14:02 - INFO - Dataset parameters:\n",
      "2025-06-10 17:14:02 - INFO -   - Root directory: data/graph_dataset_absdiff_correlation_test\n",
      "2025-06-10 17:14:02 - INFO -   - Edge strategy: relevance_diff_correlation\n",
      "2025-06-10 17:14:02 - INFO -   - Top-k neighbors: 8\n",
      "2025-06-10 17:14:02 - INFO -   - Correlation threshold: 0.7\n",
      "2025-06-10 17:14:02 - INFO -   - Force reprocess: False\n",
      "2025-06-10 17:14:02 - INFO -   - Bandpass frequencies: (0.5, 50)\n",
      "2025-06-10 17:14:02 - INFO -   - Segment length: 3000\n",
      "2025-06-10 17:14:02 - INFO -   - Apply filtering: True\n",
      "2025-06-10 17:14:02 - INFO -   - Apply rereferencing: True\n",
      "2025-06-10 17:14:02 - INFO -   - Apply normalization: True\n",
      "2025-06-10 17:14:02 - INFO -   - Sampling rate: 250\n",
      "2025-06-10 17:14:02 - INFO -   - Test mode: False\n",
      "2025-06-10 17:14:02 - INFO -   - Extract graph features: True\n",
      "2025-06-10 17:14:02 - INFO -   - Diff Corr Matrix Path: data/diff_corr_matrix.csv\n",
      "2025-06-10 17:14:02 - INFO - Edge strategy: relevance_diff_correlation. Loading average correlation matrices.\n",
      "2025-06-10 17:14:02 - INFO - Loaded absolute difference correlation matrix from data/diff_corr_matrix.csv, shape: (19, 19)\n",
      "2025-06-10 17:14:02 - INFO - Initializing graph feature extractor...\n",
      "2025-06-10 17:14:02,813 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 17:14:02 - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 17:14:02 - INFO - Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "2025-06-10 17:14:02 - INFO - Number of EEG channels: 19\n",
      "2025-06-10 17:14:02 - INFO - Setting up signal filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "Length of dataset_absdiff_corr_te: 0\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load test dataset\n",
    "dataset_absdiff_corr_te = GraphEEGDataset(\n",
    "    root=test_dataset_absdiff_correlation_dir,\n",
    "    clips=clips_te,\n",
    "    signal_folder=test_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"relevance_diff_correlation\",\n",
    "    spatial_distance_file=None,\n",
    "    top_k=top_k,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None, # collect all graph features\n",
    "    # settings for absolute difference correlation\n",
    "    diff_corr_matrix_path=absdiff_correlation_file,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_absdiff_corr_te: {len(dataset_absdiff_corr_te)}\")\n",
    "print(f' Eliminated IDs: {dataset_absdiff_corr_te.ids_to_eliminate}')\n",
    "clips_absdiff_corr_te = clips_te[~clips_te.index.isin(dataset_absdiff_corr_te.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa474f0b",
   "metadata": {},
   "source": [
    "### Timeseries datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4a44e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "🚀 Initializing TimeseriesEEGDataset in SIGNAL mode.\n",
      "   - Sampling rate: 250 Hz\n",
      "   - Derived segment length: 3000 timesteps.\n",
      "   - Segment length: 3000 timesteps.\n",
      "   ✅ Using existing cached data from data/timeseries_dataset_train_signal/processed\n",
      "🏁 TimeseriesEEGDataset initialization complete. Loaded 12993 samples.\n",
      "🚀 Initializing TimeseriesEEGDataset in SIGNAL mode.\n",
      "   - Sampling rate: 250 Hz\n",
      "   - Derived segment length: 3000 timesteps.\n",
      "   - Segment length: 3000 timesteps.\n",
      "   ⚠️ Info: Column 'label' not found in clips_df. Processing without labels (e.g., test set).\n",
      "   ✅ Using existing cached data from data/timeseries_dataset_test_signal/processed\n",
      "🏁 TimeseriesEEGDataset initialization complete. Loaded 3614 samples.\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.utils.timeseries_eeg_dataset import TimeseriesEEGDataset\n",
    "\n",
    "dataset_timeseries_signal_tr = TimeseriesEEGDataset(\n",
    "    root=str(train_dataset_timeseries_signal_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_tr,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    force_reprocess=False,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='signal', # Use raw EEG signal data\n",
    ")\n",
    "dataset_timeseries_signal_te = TimeseriesEEGDataset(\n",
    "    root=str(test_dataset_timeseries_signal_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_te,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='signal', # Use raw EEG signal data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fe1f9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "🚀 Initializing TimeseriesEEGDataset in FEATURE mode.\n",
      "   ✅ Using existing cached data from data/timeseries_dataset_train_features/processed\n",
      "🏁 TimeseriesEEGDataset initialization complete. Loaded 12993 samples.\n",
      "🚀 Initializing TimeseriesEEGDataset in FEATURE mode.\n",
      "   ⚠️ Info: Column 'label' not found in clips_df. Processing without labels (e.g., test set).\n",
      "   ✅ Using existing cached data from data/timeseries_dataset_test_features/processed\n",
      "🏁 TimeseriesEEGDataset initialization complete. Loaded 3614 samples.\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.utils.timeseries_eeg_dataset import TimeseriesEEGDataset\n",
    "\n",
    "dataset_timeseries_feature_tr = TimeseriesEEGDataset(\n",
    "    root=str(train_dataset_timeseries_feature_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_tr,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='feature',\n",
    "    feature_file_path=str(extracted_features_dir / \"X_train.npy\"),\n",
    ")\n",
    "dataset_timeseries_feature_te = TimeseriesEEGDataset(\n",
    "    root=str(test_dataset_timeseries_feature_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_te,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='feature',\n",
    "    feature_file_path=str(extracted_features_dir / \"X_test.npy\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f38f791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "✅ TrainingContext initialized. Use .switch_to('dataset_type') to begin.\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.utils.lazy import LazyDataLoaderManager, TrainingContext\n",
    "\n",
    "datasets = {\n",
    "    # timeseries datasets\n",
    "    \"signal\": {\n",
    "        \"dataset_tr\": dataset_timeseries_signal_tr,\n",
    "        \"dataset_te\": dataset_timeseries_signal_te,\n",
    "        \"clips_tr\": clips_tr\n",
    "    },\n",
    "    \"feature\": {\n",
    "        \"dataset_tr\": dataset_timeseries_feature_tr,\n",
    "        \"dataset_te\": dataset_timeseries_feature_te,\n",
    "        \"clips_tr\": clips_tr\n",
    "    },\n",
    "    # graph datasets\n",
    "    \"spatial\": {\n",
    "        \"dataset_tr\": dataset_spatial_tr,\n",
    "        \"dataset_te\": dataset_spatial_te,\n",
    "        \"clips_tr\": clips_spatial_tr,\n",
    "    },\n",
    "    \"correlation\": {\n",
    "        \"dataset_tr\": dataset_corr_tr,\n",
    "        \"dataset_te\": dataset_corr_te,\n",
    "        \"clips_tr\": clips_corr_tr,\n",
    "    },\n",
    "    \"absolute_difference\": {\n",
    "        \"dataset_tr\": dataset_absdiff_corr_tr,\n",
    "        \"dataset_te\": dataset_absdiff_corr_te,\n",
    "        \"clips_tr\": clips_absdiff_corr_tr,\n",
    "    }\n",
    "}\n",
    "\n",
    "# create loaders for both datasets\n",
    "loader_manager = LazyDataLoaderManager(\n",
    "    datasets,\n",
    "    oversampling_power=oversampling_power,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# create train context\n",
    "training_context = TrainingContext(loader_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef9bd7",
   "metadata": {},
   "source": [
    "## Dataset Selection and Training Configuration\n",
    "\n",
    "This section allows you to select which dataset type to use for training by modifying the `DATASET_TYPE` variable in the next cell. The notebook supports multiple dataset types, each with different preprocessing strategies and model architectures.\n",
    "\n",
    "### Available Dataset Types\n",
    "\n",
    "#### Graph-Based Datasets (for GNN models):\n",
    "- **`'spatial'`** - Uses spatial distance-based graph connections between EEG electrodes\n",
    "- **`'correlation'`** - Uses correlation-based graph connections (top-k=5)\n",
    "- **`'absdiff_correlation'`** - Uses absolute difference correlation graph connections (top-k=8)\n",
    "\n",
    "#### Timeseries Datasets (for traditional deep learning models):\n",
    "- **`'signal'`** - Raw EEG signal data with temporal processing\n",
    "- **`'features'`** - Pre-extracted feature representations\n",
    "\n",
    "### Data Loaders Structure\n",
    "\n",
    "All datasets are automatically split into train/validation/test sets with the following configuration:\n",
    "- **Train/Validation ratio**: 80/20\n",
    "- **Random seed**: 42 (for reproducibility)  \n",
    "- **Class balancing**: WeightedRandomSampler with oversampling power = 1.0\n",
    "- **Batch size**: 64\n",
    "\n",
    "The data loaders are organized in dictionaries for easy access:\n",
    "- `graph_loaders` - Contains loaders for all graph-based datasets\n",
    "- `timeseries_loaders` - Contains loaders for all timeseries datasets\n",
    "\n",
    "### Usage Instructions\n",
    "\n",
    "1. Set the `DATASET_TYPE` variable to your desired dataset type\n",
    "2. The notebook will automatically configure the appropriate data loaders\n",
    "3. Use the selected `train_loader`, `val_loader`, and `te_loader` for model training\n",
    "4. Choose the corresponding model architecture (GNN for graph datasets, traditional models for timeseries datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67042055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from src.layers.hybrid.cnn_bilstm_gcn import EEGCNNBiLSTMGCN\n",
    "from src.utils.train import train_model\n",
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"patience\": 10,\n",
    "    \"epochs\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5446a5",
   "metadata": {},
   "source": [
    "## Training and Evaluation - Timeseries Models\n",
    "\n",
    "In this section, we will train and evaluate traditional deep learning models on the selected timeseries dataset. The models will be trained using the `train_loader` and evaluated on the `val_loader` and `te_loader` (the latter being used for final evaluation after training. Labels are not available for the test set, so we will not compute metrics on it)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d491209e",
   "metadata": {},
   "source": [
    "### Training / k-Fold Cross-Validation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60bc6d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.utils.train\n",
    "from src.utils.train import train_k_fold\n",
    "\n",
    "def wrap_traditional_train(model, save_path):\n",
    "    global train_context\n",
    "    if 'train_context' not in globals():\n",
    "        raise ValueError(\"Timeseries training context is not initialized. Please initialize it before calling this function.\")\n",
    "    if not isinstance(train_context, TrainingContext):\n",
    "        raise ValueError(\"train_context must be an instance of TrainingContext.\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # if multiple GPUs are available, use DataParallel\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for training\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5)\n",
    "    loss = nn.BCEWithLogitsLoss()  # Not weighted as we use a balanced sampler!\n",
    "\n",
    "    # train model\n",
    "    train_history, val_history = train_model(\n",
    "        wandb_config=None,\n",
    "        model=model,\n",
    "        train_loader=train_context.train_loader,\n",
    "        val_loader=train_context.val_loader,\n",
    "        criterion=loss,\n",
    "        scheduler=scheduler,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=config[\"epochs\"],\n",
    "        patience=config[\"patience\"],\n",
    "        save_path=save_path,\n",
    "        use_gnn=False,\n",
    "        # hidden attribute\n",
    "        try_load_checkpoint=True,\n",
    "        # FIXME: remove this before submission\n",
    "        log_wandb=False,\n",
    "    )\n",
    "    plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])\n",
    "\n",
    "def k_fold_train_shorthand(model_class, model_parameters, save_path, use_gnn=False, log_wandb=False, batch_size=64):\n",
    "    global train_context\n",
    "\n",
    "    if 'train_context' not in globals():\n",
    "        raise ValueError(\"Training context is not initialized. Please initialize it before calling this function.\")\n",
    "    if not isinstance(train_context, TrainingContext):\n",
    "        raise ValueError(\"train_context must be an instance of TrainingContext.\")\n",
    "\n",
    "    import torch.nn as nn # force import to avoid bug\n",
    "\n",
    "    # train model\n",
    "    aggregated_train_history, aggregated_val_history, fold_results = train_k_fold(\n",
    "        # dataset to use\n",
    "        dataset=datasets[train_context.dataset_type][\"dataset_tr\"],\n",
    "        labels=train_context.clips[\"label\"].values,\n",
    "        # train models\n",
    "        model_class=model_class,\n",
    "        model_kwargs=model_parameters,\n",
    "        # optimizer\n",
    "        criterion=nn.BCEWithLogitsLoss(),\n",
    "        optimizer_class=torch.optim.AdamW,\n",
    "        optimizer_kwargs={\n",
    "            \"lr\": config[\"learning_rate\"],\n",
    "            \"weight_decay\": config[\"weight_decay\"],\n",
    "            \"betas\": (0.9, 0.999)\n",
    "        },\n",
    "        # scheduler\n",
    "        scheduler_class=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        scheduler_kwargs={\n",
    "            \"mode\": 'min',\n",
    "            \"factor\": 0.5,\n",
    "            \"patience\": 5\n",
    "        },\n",
    "        batch_size=batch_size,\n",
    "        wandb_config=None,\n",
    "        device=device,\n",
    "        num_epochs=config[\"epochs\"],\n",
    "        patience=config[\"patience\"],\n",
    "        use_gnn=use_gnn,\n",
    "        # hidden attribute\n",
    "        log_wandb=log_wandb,\n",
    "        save_dir=save_path,\n",
    "    )\n",
    "    plot_training_loss(aggregated_train_history[\"loss\"], aggregated_val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06a1e4",
   "metadata": {},
   "source": [
    "### LSTM (signal-based model)\n",
    "\n",
    "The LSTM model is a recurrent neural network (RNN) architecture designed to handle sequential data, making it suitable for time-series analysis like EEG signals. It captures temporal dependencies in the data, allowing it to learn patterns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.temporal.lstm\n",
    "from src.layers.temporal.lstm import EEGLSTMClassifier\n",
    "\n",
    "# create loader manager\n",
    "train_context = training_context.switch_to('signal')\n",
    "\n",
    "# build model with current parameters\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"timeseries_signal_lstm_baseline.pt\"\n",
    "\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGLSTMClassifier,\n",
    "    model_parameters={\n",
    "        \"input_dim\": 19,\n",
    "        \"hidden_dim\": 64,\n",
    "        \"num_layers\": 4,\n",
    "        \"dropout\": 0.3,\n",
    "        \"bidirectional\": False,\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_lstm_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89807a",
   "metadata": {},
   "source": [
    "### BiLSTM (signal-based model)\n",
    "\n",
    "The BiLSTM (Bidirectional Long Short-Term Memory) model extends the LSTM by processing the input sequence in both forward and backward directions. This bidirectional approach allows the model to capture context from both past and future time steps, enhancing its ability to understand complex temporal relationships in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ce139",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.temporal.lstm\n",
    "from src.layers.temporal.lstm import EEGLSTMClassifier\n",
    "\n",
    "# create loader manager\n",
    "train_context = training_context.switch_to('signal')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGLSTMClassifier,\n",
    "    model_parameters={\n",
    "        \"input_dim\": 19,\n",
    "        \"hidden_dim\": 64,\n",
    "        \"num_layers\": 4,\n",
    "        \"dropout\": 0.3,\n",
    "        \"bidirectional\": True, # use bidirectional LSTM\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_bilstm_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4a846",
   "metadata": {},
   "source": [
    "### MLP (feature-based model)\n",
    "\n",
    "The MLP (Multi-Layer Perceptron) is a feedforward neural network architecture consisting of multiple layers of neurons. It is designed to learn complex mappings from input features to output labels, making it suitable for tasks like EEG seizure detection when using pre-extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.mlp\n",
    "from src.layers.mlp import EEGMLPClassifier\n",
    "\n",
    "train_context = training_context.switch_to('feature')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGMLPClassifier,\n",
    "    model_parameters={\n",
    "        \"input_dim\": 228, # extracted features dimension\n",
    "        \"hidden_dims\": [1024, 512, 256],\n",
    "        \"output_dim\": 1,\n",
    "        \"dropout_prob\": 0.3,\n",
    "        \"use_batch_norm\": True,\n",
    "        \"use_residual\": False,\n",
    "        \"activation\": \"relu\"\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_feature_mlp_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8f8223",
   "metadata": {},
   "source": [
    "### MLP (signal-based model, flattened EEG signals)\n",
    "\n",
    "The same MLP architecture as above, but trained on raw EEG signals instead of pre-extracted features to evaluate the performance of the model on raw data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.mlp\n",
    "from src.layers.mlp import EEGMLPClassifier\n",
    "\n",
    "# switch to signal dataset\n",
    "train_context = training_context.switch_to('signal')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGMLPClassifier,\n",
    "    model_parameters={\n",
    "        \"input_channels\": 19,\n",
    "        \"input_time_steps\": 3000,\n",
    "        \"hidden_dims\": [4096, 2048, 1024, 512, 256],\n",
    "        \"output_dim\": 1,\n",
    "        \"dropout_prob\": 0.3,\n",
    "        \"use_batch_norm\": True,\n",
    "        \"use_residual\": False,\n",
    "        \"activation\": \"leaky_relu\"\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_mlp_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d45648",
   "metadata": {},
   "source": [
    "### CNN-MLP (signal-based model)\n",
    "\n",
    "We have proven that the MLP model alone is not sufficient to capture the temporal dependencies in the EEG signals. Therefore, we will use a CNN-MLP model that combines convolutional layers to extract spatial features from the EEG signals and MLP layers to learn the mapping from these features to the output labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.cnn.cnn\n",
    "from src.layers.cnn.cnn import EEGCNNClassifier\n",
    "\n",
    "train_context = training_context.switch_to('signal')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGCNNClassifier,\n",
    "    model_parameters={\n",
    "        \"input_channels\": 19,\n",
    "        \"cnn_out_dim\": 128,\n",
    "        \"mlp_hidden_dims\": [256, 128],\n",
    "        \"output_dim\": 1,\n",
    "        \"cnn_dropout_prob\": 0.3,\n",
    "        \"mlp_dropout_prob\": 0.3,\n",
    "        \"activation_mlp\": \"leaky_relu\",\n",
    "        \"activation_cnn\": \"leaky_relu\",\n",
    "        \"cnn_use_batch_norm\": True,\n",
    "        \"use_batch_norm_mlp\": True,\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_cnn_mlp_k_fold.pt\",\n",
    "    batch_size=512 # higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febab4cf",
   "metadata": {},
   "source": [
    "### CNN-BiLSTM-MLP (signal-based model)\n",
    "\n",
    "In this section we test the CNN-BiLSTM model, which combines convolutional layers to extract spatial features from the EEG signals and LSTM layers to learn the temporal dependencies in the data. This architecture is particularly effective for EEG seizure detection, as it captures both spatial and temporal patterns in the signals. A final fully connected layer is used to map the extracted features to the output labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322efc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.cuda import clean_cuda_memory_usage\n",
    "clean_cuda_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ffe62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.cnn.cnn_lstm\n",
    "from src.layers.cnn.cnn_lstm import EEGCNNBiLSTMClassifier \n",
    "\n",
    "train_context = training_context.switch_to('signal')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=EEGCNNBiLSTMClassifier,\n",
    "    model_parameters={\n",
    "        \"input_channels\": 19,\n",
    "        \"output_dim\": 1,\n",
    "        \"cnn_dropout_prob\": 0.3,\n",
    "        \"mlp_dropout_prob\": 0.3,\n",
    "        \"activation_mlp\": \"leaky_relu\",\n",
    "        \"cnn_use_batch_norm\": True,\n",
    "        \"use_batch_norm_mlp\": True,\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / \"timeseries_signal_cnn_bilstm_k_fold\",\n",
    "    batch_size=128 # slightly higher batch size for faster training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae02a362",
   "metadata": {},
   "source": [
    "## Training and Evaluation - Graph-Based Models\n",
    "\n",
    "This section focuses on training and evaluating Graph Neural Network (GNN) models on the selected graph-based datasets. These models leverage the spatial and functional relationships between EEG electrodes to improve seizure detection accuracy.\n",
    "\n",
    "### Available Graph-Based Architectures\n",
    "\n",
    "The notebook implements several hybrid architectures that combine temporal and graph processing:\n",
    "\n",
    "- **CNN-BiLSTM-GCN**: Combines Convolutional Neural Networks for feature extraction, Bidirectional LSTM for temporal modeling, and Graph Convolutional Networks for spatial relationships\n",
    "- **CNN-BiLSTM-GAT**: Similar to above but uses Graph Attention Networks instead of GCN for learning adaptive attention weights between electrodes\n",
    "- **CNN-BiLSTM-Attention-GNN**: Enhanced version with attention mechanisms in both temporal and graph components\n",
    "\n",
    "### Graph Construction Strategies\n",
    "\n",
    "The models can be trained on different graph construction approaches:\n",
    "- **Spatial graphs**: Based on physical electrode distances (19 channels)\n",
    "- **Correlation graphs**: Dynamic graphs based on signal correlations (top-k=5)\n",
    "- **Absolute difference correlation**: Advanced correlation-based graphs (top-k=8)\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "All graph models use:\n",
    "- **Optimizer**: AdamW with learning rate 1e-4 and weight decay 0.01\n",
    "- **Loss function**: BCEWithLogitsLoss (unweighted due to balanced sampling)\n",
    "- **Scheduler**: ReduceLROnPlateau with factor 0.5 and patience 5\n",
    "- **Early stopping**: Patience of 10 epochs based on validation F1 score\n",
    "- **Data handling**: GeoDataLoader for efficient graph batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8277022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure to clean up CUDA memory before this big model\n",
    "from src.utils.cuda import clean_cuda_memory_usage\n",
    "clean_cuda_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b336bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 04:23:55 - INFO - Starting 5-fold cross-validation\n",
      "2025-06-10 04:23:55 - INFO - Dataset size: 12993\n",
      "2025-06-10 04:23:55 - INFO - Stratified: True\n",
      "2025-06-10 04:23:55 - INFO - Batch size: 64\n",
      "2025-06-10 04:23:55 - INFO - Using stratified k-fold with label distribution\n",
      "2025-06-10 04:23:55 - INFO - Created 5 folds\n",
      "2025-06-10 04:23:55 - INFO - Folds: [(array([    0,     2,     3, ..., 12990, 12991, 12992]), array([    1,     5,     6, ..., 12979, 12983, 12985])), (array([    0,     1,     2, ..., 12988, 12989, 12990]), array([    3,    19,    27, ..., 12977, 12991, 12992])), (array([    0,     1,     3, ..., 12987, 12991, 12992]), array([    2,     7,    17, ..., 12988, 12989, 12990])), (array([    0,     1,     2, ..., 12990, 12991, 12992]), array([    4,     9,    12, ..., 12975, 12978, 12987])), (array([    1,     2,     3, ..., 12990, 12991, 12992]), array([    0,    10,    16, ..., 12976, 12981, 12986]))]\n",
      "2025-06-10 04:23:55 - INFO - \n",
      "============================================================\n",
      "2025-06-10 04:23:55 - INFO - FOLD 1/5\n",
      "2025-06-10 04:23:55 - INFO - ============================================================\n",
      "2025-06-10 04:23:55 - INFO - Train samples: 10394\n",
      "2025-06-10 04:23:55 - INFO - Val samples: 2599\n",
      "2025-06-10 04:23:55 - INFO - Train positive ratio: 0.194\n",
      "2025-06-10 04:23:55 - INFO - Val positive ratio: 0.194\n",
      "2025-06-10 04:23:55 - INFO - Graph-level features are not included in this loader.\n",
      "2025-06-10 04:23:55 - INFO - Graph-level features are not included in this loader.\n",
      "2025-06-10 04:23:55 - INFO - Initialized model: EEGCNNBiLSTMGCN\n",
      "2025-06-10 04:23:55 - INFO - Initialized optimizer: AdamW\n",
      "2025-06-10 04:23:55 - INFO - Initialized scheduler: ReduceLROnPlateau\n",
      "2025-06-10 04:23:55 - INFO - Starting training for fold 1\n",
      "2025-06-10 04:23:55 - INFO - Starting training setup...\n",
      "2025-06-10 04:23:55 - INFO - Model type: GNN\n",
      "2025-06-10 04:23:55 - INFO - Device: cuda\n",
      "2025-06-10 04:23:55 - INFO - Batch size: 64\n",
      "2025-06-10 04:23:55 - INFO - Number of epochs: 100\n",
      "2025-06-10 04:23:55 - INFO - Patience: 10\n",
      "2025-06-10 04:23:55 - INFO - Monitor metric: val_f1\n",
      "2025-06-10 04:23:55 - INFO - Initializing wandb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌐 Switching context to 'SPATIAL' dataset...\n",
      "🧹 Cleared CUDA memory. Previous usage: 6676.97 MB. Current usage: 6676.97 MB\n",
      "🚀 Context ready for 'spatial'.\n",
      "   Memory usage: 6676.97 MB\n",
      "   Train batches: 163, Val batches: 41\n",
      "   Type: spatial\n",
      "   Total Train Samples: 12993\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>bad_epochs</td><td>▁▁▅█</td></tr><tr><td>best_val_f1</td><td> ▁▁▁</td></tr><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁</td></tr><tr><td>train/accuracy</td><td>▁▆▇█</td></tr><tr><td>train/auroc</td><td>▁▅▆█</td></tr><tr><td>train/f1</td><td>█▁▃█</td></tr><tr><td>train/loss</td><td>█▃▂▁</td></tr><tr><td>train/macro_f1</td><td>█▁▃█</td></tr><tr><td>train/precision</td><td>▁▄▇█</td></tr><tr><td>train/recall</td><td>█▁▁▃</td></tr><tr><td>val/accuracy</td><td>▁▆▃█</td></tr><tr><td>val/auroc</td><td>▅▁█▃</td></tr><tr><td>val/f1</td><td>█▃▁▂</td></tr><tr><td>val/loss</td><td>▅▂▁█</td></tr><tr><td>val/macro_f1</td><td>█▃▁▂</td></tr><tr><td>val/median_patient_f1</td><td>▁▁▁▁</td></tr><tr><td>val/median_patient_precision</td><td>▁▁▁▁</td></tr><tr><td>val/median_patient_recall</td><td>▁▁▁▁</td></tr><tr><td>val/precision</td><td>▁▅▄█</td></tr><tr><td>val/recall</td><td>█▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>bad_epochs</td><td>2</td></tr><tr><td>best_val_f1</td><td>0.13793</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>train/accuracy</td><td>0.8223</td></tr><tr><td>train/auroc</td><td>0.7312</td></tr><tr><td>train/f1</td><td>0.32419</td></tr><tr><td>train/loss</td><td>0.43197</td></tr><tr><td>train/macro_f1</td><td>0.32419</td></tr><tr><td>train/precision</td><td>0.61528</td></tr><tr><td>train/recall</td><td>0.22007</td></tr><tr><td>val/accuracy</td><td>0.81108</td></tr><tr><td>val/auroc</td><td>0.62196</td></tr><tr><td>val/f1</td><td>0.06831</td></tr><tr><td>val/loss</td><td>0.49336</td></tr><tr><td>val/macro_f1</td><td>0.06831</td></tr><tr><td>val/median_patient_f1</td><td>0</td></tr><tr><td>val/median_patient_precision</td><td>0</td></tr><tr><td>val/median_patient_recall</td><td>0</td></tr><tr><td>val/precision</td><td>0.78261</td></tr><tr><td>val/recall</td><td>0.03571</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fold_1</strong> at: <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/3ixceo05' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/3ixceo05</a><br> View project at: <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250610_041932-3ixceo05/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ldibello/NeuroGraphNet/wandb/run-20250610_042355-708pa4n9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/708pa4n9' target=\"_blank\">fold_1</a></strong> to <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/708pa4n9' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/708pa4n9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 04:23:57 - INFO - 🔗 Wandb run initialized: fold_1\n",
      "2025-06-10 04:23:57 - INFO - Total training batches per epoch: 163\n",
      "2025-06-10 04:23:57 - INFO - Starting training from epoch 1 to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Wandb initialized: fold_1\n",
      " 🗑️ Overwrite enabled: Removed existing checkpoint at .checkpoints/cnn_bilstm_gcn_old_signal_k_fold_.pt/fold_1_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|▊                                                                                  | 1/100 [00:00<?, ?it/s]2025-06-10 04:23:57 - INFO - \n",
      "Epoch 1/100 - Training phase\n",
      "2025-06-10 04:23:59 - INFO - Processing batch 1/163\n",
      "2025-06-10 04:23:59 - INFO - Batch shapes - x: torch.Size([1216, 3000]), edge_index: torch.Size([2, 21888]), y: torch.Size([64, 1])\n",
      "/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "2025-06-10 04:23:59 - INFO - Batch 1/163 - Loss: 0.8012 - Avg batch time: 0.27s\n",
      "2025-06-10 04:24:01 - INFO - Processing batch 11/163\n",
      "2025-06-10 04:24:02 - INFO - Batch 11/163 - Loss: 0.7273 - Avg batch time: 0.26s\n",
      "2025-06-10 04:24:04 - INFO - Processing batch 21/163\n",
      "2025-06-10 04:24:04 - INFO - Batch 21/163 - Loss: 0.6782 - Avg batch time: 0.26s\n"
     ]
    }
   ],
   "source": [
    "%aimport src.layers.hybrid.cnn_bilstm_gcn_old\n",
    "# from src.layers.hybrid.cnn_bilstm_gcn import EEGCNNBiLSTMGCN\n",
    "from src.layers.hybrid.cnn_bilstm_gcn_old import LSTM_GNN_Model\n",
    "train_context = training_context.switch_to('spatial')\n",
    "k_fold_train_shorthand(\n",
    "    model_class=LSTM_GNN_Model,\n",
    "    model_parameters={\n",
    "        # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "        \"cnn_dropout\": 0.25, # slightly higher dropout to avoid overfitting\n",
    "        \"lstm_hidden_dim\": 64, #128,\n",
    "        \"lstm_out_dim\": 64, #128,\n",
    "        \"lstm_dropout\": 0.25, # slightly higher dropout to avoid overfitting\n",
    "        # Parameters for the EEGGCN (graph neural network)\n",
    "        \"gcn_hidden_channels\": 64, #192,\n",
    "        \"gcn_out_channels\": 32, #128,\n",
    "        \"num_gcn_layers\": 4,\n",
    "        \"gcn_dropout\": 0.6, # slightly higher dropout to avoid overfitting\n",
    "        \"num_classes\": 1,  # For binary classification (seizure/non-seizure)\n",
    "        \"num_channels\": 19,\n",
    "    },\n",
    "    save_path=CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_old_signal_k_fold_.pt\",\n",
    "    use_gnn=True, # use training loop for GNN models\n",
    "    batch_size=64, # lower batch size (GPU poor)\n",
    "    log_wandb=True, # log to wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177fedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.utils.train\n",
    "def wrap_gnn_train(model, save_path):\n",
    "    global graph_training_context\n",
    "    if 'graph_training_context' not in globals():\n",
    "        raise ValueError(\"Graph training context is not initialized. Please initialize it before calling this function.\")\n",
    "    if not isinstance(graph_training_context, TrainingContext):\n",
    "        raise ValueError(\"graph_training_context must be an instance of TrainingContext.\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "    # optimizer = Lion(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5)\n",
    "    loss = nn.BCEWithLogitsLoss()  # Not weighted as we use a balanced sampler!\n",
    "\n",
    "    # train model\n",
    "    train_history, val_history = train_model(\n",
    "        wandb_config=None,\n",
    "        model=model,\n",
    "        train_loader=graph_training_context.train_loader,\n",
    "        val_loader=graph_training_context.val_loader,\n",
    "        criterion=loss,\n",
    "        scheduler=scheduler,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=config[\"epochs\"],\n",
    "        patience=config[\"patience\"],\n",
    "        save_path=save_path,\n",
    "        use_gnn=True,\n",
    "        # hidden attribute\n",
    "        try_load_checkpoint=True,\n",
    "    )\n",
    "    plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720925b0",
   "metadata": {},
   "source": [
    "### Test 3 - First breakthrough model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036835f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm .checkpoints/cnn_bilstm_gcn_test_3_correlation_test_mean_pooling.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a43eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_test_best_model_{DATASET_TYPE}_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm = True,\n",
    "    encoder_use_layer_norm = False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim = 192,\n",
    "    out_channels = 128,\n",
    "    pooling_type = \"max\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_conv_layers = 4,\n",
    "    gcn_dropout = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    "    use_graph_features=False\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a5bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "from src.layers.hybrid.cnn_bilstm_gat import EEGCNNBiLSTMGAT\n",
    "\n",
    "\n",
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gat_test_3_{DATASET_TYPE}_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGAT(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout=0.25,\n",
    "    lstm_hidden_dim=128,\n",
    "    lstm_out_dim=128,\n",
    "    lstm_dropout=0.25,\n",
    "    encoder_use_batch_norm=True,\n",
    "    encoder_use_layer_norm=False,\n",
    "    # Parameters for the GAT (graph attention network)\n",
    "    hidden_dim=128,\n",
    "    out_channels=96,\n",
    "    pooling_type=\"mean\",\n",
    "    gat_use_batch_norm=True,\n",
    "    num_conv_layers=3,\n",
    "    gat_dropout=0.5,\n",
    "    gat_heads=4,  # Number of attention heads\n",
    "    num_channels=19,\n",
    "    # Graph features configuration\n",
    "    graph_feature_dim=graph_feature_dim,\n",
    "    use_graph_features=False if DATASET_TYPE == 'spatial' else True,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_test_3_{DATASET_TYPE}_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim = 128,\n",
    "    out_channels = 96,\n",
    "    pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_conv_layers = 3,\n",
    "    gcn_dropout = 0.5,\n",
    "    num_channels = 19,\n",
    "    # enable graph features\n",
    "    # NOTE: using graph level features gives worse results with spatial dataset\n",
    "    use_graph_features=False if DATASET_TYPE == 'spatial' else True,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11afb6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_test_3_{DATASET_TYPE}_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout=0.25,\n",
    "    lstm_hidden_dim=128,\n",
    "    lstm_out_dim=128,\n",
    "    lstm_dropout=0.25,\n",
    "    encoder_use_batch_norm=True,\n",
    "    encoder_use_layer_norm=False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim=128,\n",
    "    out_channels=128,\n",
    "    pooling_type=\"mean\",\n",
    "    gcn_use_batch_norm=True,\n",
    "    num_conv_layers=3,\n",
    "    gcn_dropout=0.5,\n",
    "    num_channels=19,\n",
    "    use_graph_features=True\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424f5b8b",
   "metadata": {},
   "source": [
    "### Test 4 - Smaller CGN output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_4.pt\"\n",
    "\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 64,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 3,\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eaaf2d",
   "metadata": {},
   "source": [
    "### Test 5 - Smaller GCN output channels + increased embedding length + Deeper GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1335c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_5.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 64,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da60626",
   "metadata": {},
   "source": [
    "\n",
    "### Test 6: slighly bigger GCN output channels\n",
    ">[HIGHEST F1 SCORE EVER RECORDED]\n",
    "```\n",
    "✅ Checkpoint loaded. Resuming from epoch 33. Best 'val_f1' score: 0.7346\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_6.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 96,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad429a",
   "metadata": {},
   "source": [
    "### Test 7B: Alternative architecture to improve generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f882bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_8.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.35, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.35, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 96,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b4f3c",
   "metadata": {},
   "source": [
    "### Test 7C: slightly bigger GCN layers\n",
    "\n",
    "BEST MODEL YET!\n",
    "\n",
    "(SPATIAL FEATURES!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_best_model_correlation_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm = True,\n",
    "    encoder_use_layer_norm = False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim = 192,\n",
    "    out_channels = 128,\n",
    "    pooling_type = \"max\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_conv_layers = 4,\n",
    "    gcn_dropout = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    "    graph_feature_dim=graph_feature_dim,\n",
    "    use_graph_features=False if DATASET_TYPE == 'spatial' else True,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6355457",
   "metadata": {},
   "source": [
    "### Test 7D: even bigger GCN layers\n",
    "\n",
    "Comparable performance to best model. We might need to increase the number of GCN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed9d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_even_bigger.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm = True,\n",
    "    encoder_use_layer_norm = False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 224,\n",
    "    gcn_out_channels = 192,\n",
    "    gcn_pooling_type = \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50956294",
   "metadata": {},
   "source": [
    "### Test 7E: increased number of GCN layers\n",
    "\n",
    "Assumption: the previous model was unable to learn enough, maybe the GCN was unable to capture\n",
    "\n",
    "```\n",
    "Epochs:   9%| | 9/100 [17:54<3:23:31, 134.20s/it, train_loss=0.4532, val_loss=0.3489, best_val_f1=0.6695, lr=5.00e-05, b2025-06-07 17:01:05 - INFO - \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_even_more_bigger.pt\"\n",
    "model_generalizable_even_more_bigger = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 224,\n",
    "    gcn_out_channels = 192,\n",
    "    gcn_num_layers = 5,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08442186",
   "metadata": {},
   "source": [
    "### Test 7F: Increased number of BiLSTM layers + Test 7E architecture\n",
    "\n",
    "Assumpion: we saw a drammatical increase in accuracy by increasing the number of GCN layers. This hints that the model was now able to learn the most from the embeddings. To improve the performance even further without having to increase the number of GCN layers even more (overall reduce complexity, improve generalization), we will try to increase the number of BiLSTM layers. \n",
    "\n",
    "Using multiple BiLSTM layers will allow embeddings to be processed in a more complex way, potentially capturing more intricate relationships in the data. The GCN layers will take care of the graph structure, while the BiLSTM layers will enhance the temporal dependencies and relationships in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b963a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_even_more_bigger.pt\"\n",
    "model_generalizable_even_more_bigger = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    lstm_num_layers = 2,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 224,\n",
    "    gcn_out_channels = 192,\n",
    "    gcn_num_layers = 5,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663332c",
   "metadata": {},
   "source": [
    "```\n",
    "Epochs:   1%|▊                                                                                  | 1/100 [00:00<?, ?it/s]2025-06-07 18:55:16 - INFO -\n",
    "Epochs:   2%| | 2/100 [04:35<7:29:19, 275.10s/it, train_loss=0.6212, val_loss=0.4619, best_val_f1=0.4055, lr=1.00e-04, b2025-06-07 18:59:51 - INFO -\n",
    "Epochs:   3%| | 3/100 [09:09<7:23:49, 274.53s/it, train_loss=0.5819, val_loss=0.4295, best_val_f1=0.4055, lr=1.00e-04, b2025-06-07 19:04:25 - INFO -\n",
    "Epochs:   4%| | 4/100 [13:42<7:18:31, 274.08s/it, train_loss=0.5628, val_loss=0.4437, best_val_f1=0.4055, lr=1.00e-04, b2025-06-07 19:08:59 - INFO -\n",
    "Epochs:   5%| | 5/100 [18:16<7:13:28, 273.78s/it, train_loss=0.5452, val_loss=0.3942, best_val_f1=0.4858, lr=1.00e-04, b2025-06-07 19:13:32 - INFO -\n",
    "Epochs:   6%| | 6/100 [22:49<7:08:41, 273.63s/it, train_loss=0.5334, val_loss=0.4563, best_val_f1=0.4858, lr=1.00e-04, b2025-06-07 19:18:05 - INFO -\n",
    "Epochs:   7%| | 7/100 [27:22<7:04:01, 273.57s/it, train_loss=0.5319, val_loss=0.3738, best_val_f1=0.5137, lr=1.00e-04, b2025-06-07 19:22:39 - INFO -\n",
    "Epochs:   8%| | 8/100 [31:56<6:59:20, 273.48s/it, train_loss=0.5181, val_loss=0.4369, best_val_f1=0.5695, lr=1.00e-04, b2025-06-07 19:27:12 - INFO -\n",
    "Epochs:   9%| | 9/100 [36:29<6:54:50, 273.52s/it, train_loss=0.5220, val_loss=0.4202, best_val_f1=0.5695, lr=1.00e-04, b2025-06-07 19:31:46 - INFO -\n",
    "Epochs:  10%| | 10/100 [41:03<6:50:17, 273.52s/it, train_loss=0.5286, val_loss=0.4167, best_val_f1=0.5695, lr=1.00e-04, 2025-06-07 19:36:19 - INFO -\n",
    "Epochs:  11%| | 11/100 [45:36<6:45:44, 273.53s/it, train_loss=0.5065, val_loss=0.3864, best_val_f1=0.5695, lr=1.00e-04, 2025-06-07 19:40:53 - INFO -\n",
    "Epochs:  12%| | 12/100 [50:10<6:41:03, 273.45s/it, train_loss=0.5158, val_loss=0.5175, best_val_f1=0.5695, lr=5.00e-05, 2025-06-07 19:45:26 - INFO -\n",
    "Epochs:  13%|▏| 13/100 [54:43<6:36:23, 273.37s/it, train_loss=0.5035, val_loss=0.3785, best_val_f1=0.5940, lr=5.00e-05, 2025-06-07 19:49:59 - INFO -\n",
    "Epochs:  14%|▏| 14/100 [59:16<6:31:50, 273.38s/it, train_loss=0.4842, val_loss=0.3838, best_val_f1=0.5981, lr=5.00e-05, 2025-06-07 19:54:33 - INFO -\n",
    "Epochs:  15%|▏| 15/100 [1:03:50<6:27:17, 273.38s/it, train_loss=0.4644, val_loss=0.3493, best_val_f1=0.6106, lr=5.00e-052025-06-07 19:59:06 - INFO -\n",
    "Epochs:  16%|▏| 16/100 [1:08:23<6:22:46, 273.41s/it, train_loss=0.4887, val_loss=0.3737, best_val_f1=0.6106, lr=5.00e-052025-06-07 20:03:39 - INFO -\n",
    "Epochs:  17%|▏| 17/100 [1:12:57<6:18:12, 273.41s/it, train_loss=0.4775, val_loss=0.3565, best_val_f1=0.6106, lr=5.00e-052025-06-07 20:08:13 - INFO -\n",
    "Epochs:  18%|▏| 18/100 [1:17:30<6:13:42, 273.44s/it, train_loss=0.4635, val_loss=0.3704, best_val_f1=0.6106, lr=2.50e-052025-06-07 20:12:46 - INFO -\n",
    "Epochs:  19%|▏| 19/100 [1:22:04<6:09:15, 273.53s/it, train_loss=0.4501, val_loss=0.3635, best_val_f1=0.6131, lr=2.50e-052025-06-07 20:17:20 - INFO -\n",
    "Epochs:  20%|▏| 20/100 [1:26:37<6:04:39, 273.49s/it, train_loss=0.4379, val_loss=0.3638, best_val_f1=0.6179, lr=2.50e-052025-06-07 20:21:53 - INFO -\n",
    "Epochs:  21%|▏| 21/100 [1:31:10<6:00:01, 273.43s/it, train_loss=0.4494, val_loss=0.3543, best_val_f1=0.6179, lr=2.50e-052025-06-07 20:26:27 - INFO -\n",
    "Epochs:  22%|▏| 22/100 [1:35:44<5:55:26, 273.42s/it, train_loss=0.4616, val_loss=0.3616, best_val_f1=0.6659, lr=2.50e-052025-06-07 20:31:00 - INFO -\n",
    "Epochs:  23%|▏| 23/100 [1:40:17<5:50:54, 273.44s/it, train_loss=0.4381, val_loss=0.3532, best_val_f1=0.6659, lr=2.50e-052025-06-07 20:35:34 - INFO -\n",
    "Epochs:  24%|▏| 24/100 [1:44:51<5:46:22, 273.45s/it, train_loss=0.4423, val_loss=0.3635, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:40:07 - INFO -\n",
    "Epochs:  25%|▎| 25/100 [1:49:24<5:41:52, 273.49s/it, train_loss=0.4291, val_loss=0.3473, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:44:41 - INFO -\n",
    "Epochs:  26%|▎| 26/100 [1:53:58<5:37:12, 273.42s/it, train_loss=0.4403, val_loss=0.3380, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:49:14 - INFO -\n",
    "Epochs:  27%|▎| 27/100 [1:58:31<5:32:38, 273.40s/it, train_loss=0.4312, val_loss=0.3374, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:53:47 - INFO -\n",
    "Epochs:  28%|▎| 28/100 [2:03:05<5:28:07, 273.44s/it, train_loss=0.4393, val_loss=0.3441, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:58:21 - INFO -\n",
    "Epochs:  29%|▎| 29/100 [2:07:38<5:23:35, 273.46s/it, train_loss=0.4226, val_loss=0.3392, best_val_f1=0.6659, lr=1.25e-052025-06-07 21:02:54 - INFO -\n",
    "Epochs:  30%|▎| 30/100 [2:12:11<5:19:02, 273.46s/it, train_loss=0.4240, val_loss=0.3525, best_val_f1=0.6659, lr=6.25e-062025-06-07 21:07:28 - INFO -\n",
    "Epochs:  31%|▎| 31/100 [2:16:45<5:14:28, 273.46s/it, train_loss=0.4249, val_loss=0.3492, best_val_f1=0.6659, lr=6.25e-062025-06-07 21:12:01 - INFO -\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_optimized.pt\"\n",
    "model_generalizable_optimized = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 160,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    lstm_num_layers = 2,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 192,\n",
    "    gcn_out_channels = 128,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.5, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c4b3a",
   "metadata": {},
   "source": [
    "### Test 8: Narrow but Deep GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff01da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_narrow_deep_model.pt\"\n",
    "narrow_deep_model = EEGCNNBiLSTMGCN(\n",
    "    # --- Simplify the Temporal Encoder ---\n",
    "    cnn_dropout_prob = 0.2,\n",
    "    lstm_hidden_dim = 64,  # Reduced\n",
    "    lstm_out_dim = 64,     # Reduced\n",
    "    lstm_dropout_prob = 0.2,\n",
    "    # --- Focus on the GCN ---\n",
    "    gcn_hidden_channels = 128, # Keep GCN capacity high\n",
    "    gcn_out_channels = 64,\n",
    "    gcn_num_layers = 5,      # Try going even deeper\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ef8b3",
   "metadata": {},
   "source": [
    "### Test 9: First best model, with wider + deeper GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_new_old_best_model.pt\"\n",
    "new_old_best_model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 128, # from 64 to 128\n",
    "    gcn_num_layers = 4, # from 3 to 4\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351da028",
   "metadata": {},
   "source": [
    "### Best model + attention BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.hybrid.cnn_bilstm_attention_gcn\n",
    "from src.layers.hybrid.cnn_bilstm_attention_gcn import EEGCNNBiLSTMAttentionGNN\n",
    "\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_attention.pt\"\n",
    "model_first_attention = EEGCNNBiLSTMAttentionGNN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 192,\n",
    "    gcn_out_channels = 128,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a8a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from src.utils.train import train_model\n",
    "\n",
    "model = model_small_gcn_bigger_embedding\n",
    "model = model.to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "# optimizer = Lion(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "loss = nn.BCEWithLogitsLoss() # Not weighted as we use a balanced sampler!\n",
    "\n",
    "# empty cache in order to free up VRAM (if available)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# train model\n",
    "train_history, val_history = train_model(\n",
    "    wandb_config=None,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=loss,\n",
    "    scheduler=scheduler,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=config[\"epochs\"],\n",
    "    patience=config[\"patience\"],\n",
    "    save_path=SAVE_PATH,\n",
    "    use_gnn=True,\n",
    "    # hidden attribute\n",
    "    try_load_checkpoint=True,\n",
    ")\n",
    "\n",
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch cuda clear cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63329a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
