{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d97e92",
   "metadata": {},
   "source": [
    "# NeuroGraphNet\n",
    "\n",
    "*A graph-based deep learning framework for EEG seizure detection, designed to improve accuracy and interpretability by leveraging Graph Neural Networks (GNNs) to capture spatial and temporal brain dynamics.*\n",
    "\n",
    "<hr />\n",
    "\n",
    "This notebook presents different approaches to EEG seizure detection using traditional machine learning and deep learning methods as well as a novel approaches using Graph Neural Networks (GNNs). The dataset used a subset of the TUSZ EEG Seizure dataset, which contains EEG recordings from patients with epilepsy.\n",
    "\n",
    "**Authors**: Luca Di Bello, Guillaume AndrÃ© BÃ©lissent, Abdessalem Ben Ali, Beatriz Izquierdo GonzÃ¡lez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b998b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/ldibello/venvs/neuro/lib/python3.10/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from src.data.geodataloader import GeoDataLoader\n",
    "from torch.utils.data import Subset, WeightedRandomSampler\n",
    "# from torch.utils.data import DataLoader\n",
    "from src.utils.seeder import seed_everything\n",
    "\n",
    "# set seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# create useful constants\n",
    "RANDOM_SEED = 42\n",
    "IS_SCITAS = True # set to True if running on SCITAS cluster\n",
    "LOCAL_DATA_ROOT = Path(\"./data\")\n",
    "DATA_ROOT = Path(\"/home/ogut/data\") if IS_SCITAS else LOCAL_DATA_ROOT\n",
    "CHECKPOINT_ROOT = Path(\"./.checkpoints\")\n",
    "SUBMISSION_ROOT = Path(\"./.submissions\")\n",
    "\n",
    "# create directories if they do not exist\n",
    "CHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# set dataset root\n",
    "seed_everything(RANDOM_SEED)\n",
    "\n",
    "# setup torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f74a9",
   "metadata": {},
   "source": [
    "## Run feature extraction script\n",
    "\n",
    "In order to run all the models in this notebook, is necessary to run the feature extraction script first. This script extracts features from the EEG signals for both the training and test dataset, saving three files: `X_train.npy`, `y_train.npy`, and `X_test.npy`. The features extracted are the same used in the original paper, which are based on the EEG signals.\n",
    "\n",
    "You can run the script by uncommenting and executing the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d28586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# # execute feature extraction script\n",
    "# process = None\n",
    "# try:\n",
    "#     process = subprocess.Popen([\"python3\", \"scripts/feature_extractor.py\"])\n",
    "#     process.wait()\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"Process interrupted, terminating...\")\n",
    "#     if process:\n",
    "#         process.terminate()\n",
    "#         process.wait()\n",
    "# except Exception as e:\n",
    "#     print(f\"Error occurred: {e}\")\n",
    "#     if process:\n",
    "#         process.terminate()\n",
    "#         process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527547f9",
   "metadata": {},
   "source": [
    "## Define paths\n",
    "\n",
    "The following paths are used to load the required data files and save the results of the models. Make sure to adjust them according to your local setup.\n",
    "\n",
    "The current configuration assumes that the data files are located in a folder named `data` within the current working directory. \n",
    "\n",
    "**NOTE:** to simplify the process on SCITAS cluster, we provide a toggle `IS_SCITAS` to set the paths accordingly (_refer to first cell of the notebook_). If you are running this notebook on your local machine, you can set `IS_SCITAS = False` and adjust the paths as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45999291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacial distance matrix between sensors\n",
    "spatial_distance_file = LOCAL_DATA_ROOT / \"distances_3d.csv\"\n",
    "\n",
    "# absdiff correlation matrix\n",
    "absdiff_correlation_file = LOCAL_DATA_ROOT / \"diff_corr_matrix.csv\"\n",
    "\n",
    "# training data\n",
    "train_dir = DATA_ROOT / \"train\"\n",
    "train_dir_metadata = train_dir / \"segments.parquet\"\n",
    "train_dataset_absdiff_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_absdiff_correlation_train\"\n",
    "train_dataset_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_correlation_train\"\n",
    "train_dataset_spatial_dir = LOCAL_DATA_ROOT / \"graph_dataset_spatial_train\"\n",
    "train_dataset_timeseries_feature_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_features\")\n",
    "train_dataset_timeseries_signal_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_train_signal\")\n",
    "\n",
    "# test data\n",
    "test_dir = DATA_ROOT / \"test\"\n",
    "test_dir_metadata = test_dir / \"segments.parquet\"\n",
    "test_dataset_absdiff_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_absdiff_correlation_test\"\n",
    "test_dataset_correlation_dir = LOCAL_DATA_ROOT / \"graph_dataset_correlation_test\"\n",
    "test_dataset_spatial_dir = LOCAL_DATA_ROOT / \"graph_dataset_spatial_test\"\n",
    "test_dataset_timeseries_signal_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_signal\")\n",
    "test_dataset_timeseries_feature_dir = str(LOCAL_DATA_ROOT / \"timeseries_dataset_test_features\")\n",
    "\n",
    "# additional features\n",
    "extracted_features_dir = LOCAL_DATA_ROOT / \"extracted_features\"\n",
    "embeddings_dir =  LOCAL_DATA_ROOT / \"embeddings\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f1780",
   "metadata": {},
   "source": [
    "## Loading Train and Test Clips from the Dataset\n",
    "\n",
    "To load patient clips from the dataset, we use the `load_clips` function. This function retrieves EEG signals and labels from the specified paths and returns them as NumPy arrays.\n",
    "\n",
    "Different versions of Pandas may return either a MultiIndex or a single index, even when called with the same parameters. To address this inconsistency, we use the `ensure_eeg_multiindex` function to ensure that the resulting DataFrame has a MultiIndex structure. This is essential for subsequent processing steps.\n",
    "\n",
    "If a MultiIndex is not present, it will be created using the following levels: `patient_id`, `clip_id`, and `channel`. This structure is crucial for organizing the dataset, as EEG signals are grouped by patient, clip, and channel. It also ensures compatibility with existing code that expects this format, such as the `EEGDataset` class from the [seizure-eeg](https://www.piwheels.org/project/seiz-eeg/) package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d93851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.index import ensure_eeg_multiindex \n",
    "\n",
    "# Load clips from datasets\n",
    "clips_tr = pd.read_parquet(train_dir_metadata)\n",
    "clips_tr = ensure_eeg_multiindex(clips_tr)\n",
    "clips_tr['id'] = clips_tr.index.map(lambda x: '_'.join(str(i) for i in x))\n",
    "assert clips_tr.id.nunique() == len(clips_tr), \"There are duplicate IDs\"\n",
    "clips_tr = clips_tr[~clips_tr.label.isna()].reset_index()\n",
    "\n",
    "# Load clips from datasets\n",
    "clips_te = pd.read_parquet(test_dir_metadata)\n",
    "clips_te = ensure_eeg_multiindex(clips_te)\n",
    "clips_te['id'] = clips_te.index.map(lambda x: '_'.join(str(i) for i in x))\n",
    "assert clips_te.id.nunique() == len(clips_te), \"There are duplicate IDs\"\n",
    "clips_te = clips_te.reset_index()\n",
    "\n",
    "# sort in order to maintain the same submission order\n",
    "clips_te = clips_te.sort_values(by=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f7bc0",
   "metadata": {},
   "source": [
    "## Loading Datasets\n",
    "\n",
    "This notebook explores various approaches to EEG seizure detection, requiring multiple dataset variants with distinct preprocessing strategies (e.g., raw EEG signals, extracted features, and diverse graph construction methods). The `GraphEEGDataset` class, a custom implementation of `torch.utils.data.Dataset`, is used to load these datasets based on specified parameters.\n",
    "\n",
    "The `GraphEEGDataset` class is designed to support all preprocessing strategies, including graph-based approaches. It preprocesses EEG data on-the-fly, offering flexibility in data handling and model input preparation. Additionally, it includes a caching mechanism to store preprocessed data on disk. This mechanism ensures that subsequent calls with identical parameters load precomputed data, significantly reducing dataset loading time during repeated runs. This feature has been instrumental in accelerating development and experimentation within this notebook.\n",
    "\n",
    "Specifically, we will load the following datasets:\n",
    "\n",
    "A) **EEG signals**: \n",
    "\n",
    "- **Raw EEG**:\n",
    "\n",
    "    - Feature-based\n",
    "\n",
    "        - Raw EEG signals + signal time-filtering/rereferencing/normalization preprocessing\n",
    "\n",
    "    - Graph-based\n",
    "\n",
    "        - Raw EEG signals + spatial graph construction strategy + signal time-filtering/rereferencing/normalization preprocessing + graph-features\n",
    "\n",
    "        - Raw EEG signals + correlation graph construction strategy (top-k policy with k=10) + signal time-filtering/rereferencing/normalization preprocessing + graph-features\n",
    "\n",
    "        - Raw EEG signals + absolute difference correlation graph construction strategy (top-k policy with k=10) + signal time-filtering/rereferencing/normalization preprocessing + graph-features\n",
    "\n",
    "- **Feature-based EEG**:\n",
    "\n",
    "    - Extracted features + spatial graph construction strategy + signal time-filtering/rereferencing/normalization preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e759ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generael settings\n",
    "\n",
    "# bandpass filter settings (signal time-filtering)\n",
    "low_bandpass_frequency = 0.5\n",
    "high_bandpass_frequency = 50\n",
    "\n",
    "# additional settings\n",
    "# NOTE: the training already fights class imbalance, so this is not used\n",
    "oversampling_power = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef132cd",
   "metadata": {},
   "source": [
    "### Raw-EEG signal datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8110e5",
   "metadata": {},
   "source": [
    "#### A) Spatial graph construction strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a686e9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.dataset_graph:Initializing GraphEEGDataset...\n",
      "INFO:src.data.dataset_graph:Dataset parameters:\n",
      "INFO:src.data.dataset_graph:  - Root directory: data/graph_dataset_spatial_train\n",
      "INFO:src.data.dataset_graph:  - Edge strategy: spatial\n",
      "INFO:src.data.dataset_graph:  - Top-k neighbors: None\n",
      "INFO:src.data.dataset_graph:  - Correlation threshold: 0.7\n",
      "INFO:src.data.dataset_graph:  - Force reprocess: False\n",
      "INFO:src.data.dataset_graph:  - Bandpass frequencies: (0.5, 50)\n",
      "INFO:src.data.dataset_graph:  - Segment length: 3000\n",
      "INFO:src.data.dataset_graph:  - Apply filtering: True\n",
      "INFO:src.data.dataset_graph:  - Apply rereferencing: True\n",
      "INFO:src.data.dataset_graph:  - Apply normalization: True\n",
      "INFO:src.data.dataset_graph:  - Sampling rate: 250\n",
      "INFO:src.data.dataset_graph:  - Test mode: False\n",
      "INFO:src.data.dataset_graph:  - Extract graph features: True\n",
      "INFO:src.data.dataset_graph:  - Diff Corr Matrix Path: None\n",
      "INFO:src.data.dataset_graph:Initializing graph feature extractor...\n",
      "2025-06-09 18:46:33,630 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.utils.graph_features:GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Number of EEG channels: 19\n",
      "INFO:src.data.dataset_graph:Setting up signal filters...\n",
      "INFO:src.data.dataset_graph:Loading spatial distances from data/distances_3d.csv\n",
      "INFO:src.data.dataset_graph:Loading spatial distances from data/distances_3d.csv\n",
      "INFO:src.data.dataset_graph:Loaded 361 spatial distances in 0.01s\n",
      "INFO:src.data.dataset_graph:Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset_spatial_tr: 12993\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_spatial_tr = GraphEEGDataset(\n",
    "    root=train_dataset_spatial_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"spatial\",\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file\n",
    "    ),\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None # collect all graph features\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_spatial_tr: {len(dataset_spatial_tr)}\")\n",
    "print(f' Eliminated IDs: {dataset_spatial_tr.ids_to_eliminate}')\n",
    "clips_spatial_tr = clips_tr[~clips_tr.index.isin(dataset_spatial_tr.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b33fe510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.dataset_graph:Initializing GraphEEGDataset...\n",
      "INFO:src.data.dataset_graph:Dataset parameters:\n",
      "INFO:src.data.dataset_graph:  - Root directory: data/graph_dataset_spatial_test\n",
      "INFO:src.data.dataset_graph:  - Edge strategy: spatial\n",
      "INFO:src.data.dataset_graph:  - Top-k neighbors: None\n",
      "INFO:src.data.dataset_graph:  - Correlation threshold: 0.7\n",
      "INFO:src.data.dataset_graph:  - Force reprocess: False\n",
      "INFO:src.data.dataset_graph:  - Bandpass frequencies: (0.5, 50)\n",
      "INFO:src.data.dataset_graph:  - Segment length: 3000\n",
      "INFO:src.data.dataset_graph:  - Apply filtering: True\n",
      "INFO:src.data.dataset_graph:  - Apply rereferencing: True\n",
      "INFO:src.data.dataset_graph:  - Apply normalization: True\n",
      "INFO:src.data.dataset_graph:  - Sampling rate: 250\n",
      "INFO:src.data.dataset_graph:  - Test mode: False\n",
      "INFO:src.data.dataset_graph:  - Extract graph features: True\n",
      "INFO:src.data.dataset_graph:  - Diff Corr Matrix Path: None\n",
      "INFO:src.data.dataset_graph:Initializing graph feature extractor...\n",
      "2025-06-09 18:46:33,707 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.utils.graph_features:GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Number of EEG channels: 19\n",
      "INFO:src.data.dataset_graph:Setting up signal filters...\n",
      "INFO:src.data.dataset_graph:Loading spatial distances from data/distances_3d.csv\n",
      "INFO:src.data.dataset_graph:Loading spatial distances from data/distances_3d.csv\n",
      "INFO:src.data.dataset_graph:Loaded 361 spatial distances in 0.02s\n",
      "INFO:src.data.dataset_graph:Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset_spatial_te: 0\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "# load training dataset\n",
    "dataset_spatial_te = GraphEEGDataset(\n",
    "    root=test_dataset_spatial_dir,\n",
    "    clips=clips_te,\n",
    "    signal_folder=test_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"spatial\",\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file\n",
    "    ),\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None # collect all graph features\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_spatial_te: {len(dataset_spatial_te)}\")\n",
    "print(f' Eliminated IDs: {dataset_spatial_te.ids_to_eliminate}')\n",
    "clips_spatial_te = clips_te[~clips_te.index.isin(dataset_spatial_te.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8993f7",
   "metadata": {},
   "source": [
    "#### B) Correlation graph construction strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1472121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "top_k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c3f93ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.dataset_graph:Initializing GraphEEGDataset...\n",
      "INFO:src.data.dataset_graph:Dataset parameters:\n",
      "INFO:src.data.dataset_graph:  - Root directory: data/graph_dataset_correlation_train\n",
      "INFO:src.data.dataset_graph:  - Edge strategy: correlation\n",
      "INFO:src.data.dataset_graph:  - Top-k neighbors: 5\n",
      "INFO:src.data.dataset_graph:  - Correlation threshold: 0.7\n",
      "INFO:src.data.dataset_graph:  - Force reprocess: False\n",
      "INFO:src.data.dataset_graph:  - Bandpass frequencies: (0.5, 50)\n",
      "INFO:src.data.dataset_graph:  - Segment length: 3000\n",
      "INFO:src.data.dataset_graph:  - Apply filtering: True\n",
      "INFO:src.data.dataset_graph:  - Apply rereferencing: True\n",
      "INFO:src.data.dataset_graph:  - Apply normalization: True\n",
      "INFO:src.data.dataset_graph:  - Sampling rate: 250\n",
      "INFO:src.data.dataset_graph:  - Test mode: False\n",
      "INFO:src.data.dataset_graph:  - Extract graph features: True\n",
      "INFO:src.data.dataset_graph:  - Diff Corr Matrix Path: None\n",
      "INFO:src.data.dataset_graph:Initializing graph feature extractor...\n",
      "2025-06-09 18:46:33,966 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.utils.graph_features:GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Number of EEG channels: 19\n",
      "INFO:src.data.dataset_graph:Setting up signal filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "Length of dataset_corr_tr: 12986\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_corr_tr = GraphEEGDataset(\n",
    "    root=train_dataset_correlation_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"correlation\",\n",
    "    spatial_distance_file=None,\n",
    "    top_k=top_k,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None # collect all graph features\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_corr_tr: {len(dataset_corr_tr)}\")\n",
    "print(f' Eliminated IDs: {dataset_corr_tr.ids_to_eliminate}')\n",
    "clips_corr_tr = clips_tr[~clips_tr.index.isin(dataset_corr_tr.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "233887f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.dataset_graph:Initializing GraphEEGDataset...\n",
      "INFO:src.data.dataset_graph:Dataset parameters:\n",
      "INFO:src.data.dataset_graph:  - Root directory: data/graph_dataset_correlation_test\n",
      "INFO:src.data.dataset_graph:  - Edge strategy: spatial\n",
      "INFO:src.data.dataset_graph:  - Top-k neighbors: None\n",
      "INFO:src.data.dataset_graph:  - Correlation threshold: 0.7\n",
      "INFO:src.data.dataset_graph:  - Force reprocess: False\n",
      "INFO:src.data.dataset_graph:  - Bandpass frequencies: (0.5, 50)\n",
      "INFO:src.data.dataset_graph:  - Segment length: 3000\n",
      "INFO:src.data.dataset_graph:  - Apply filtering: True\n",
      "INFO:src.data.dataset_graph:  - Apply rereferencing: True\n",
      "INFO:src.data.dataset_graph:  - Apply normalization: True\n",
      "INFO:src.data.dataset_graph:  - Sampling rate: 250\n",
      "INFO:src.data.dataset_graph:  - Test mode: True\n",
      "INFO:src.data.dataset_graph:  - Extract graph features: True\n",
      "INFO:src.data.dataset_graph:  - Diff Corr Matrix Path: None\n",
      "INFO:src.data.dataset_graph:Initializing graph feature extractor...\n",
      "2025-06-09 18:46:34,028 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.utils.graph_features:GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Number of EEG channels: 19\n",
      "INFO:src.data.dataset_graph:Setting up signal filters...\n",
      "INFO:src.data.dataset_graph:Loading spatial distances from data/distances_3d.csv\n",
      "INFO:src.data.dataset_graph:Loading spatial distances from data/distances_3d.csv\n",
      "INFO:src.data.dataset_graph:Loaded 361 spatial distances in 0.01s\n",
      "INFO:src.data.dataset_graph:Loaded 361 spatial distance pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "Length of dataset_corr_te: 3614\n",
      " Eliminated IDs:[]\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_corr_te = GraphEEGDataset(\n",
    "    root=test_dataset_correlation_dir,\n",
    "    clips=clips_te,\n",
    "    signal_folder=test_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"spatial\",\n",
    "    spatial_distance_file=(\n",
    "        spatial_distance_file\n",
    "    ),\n",
    "    top_k=None,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    is_test=True, # NOTE: needed to let the dataset know that is okay to now have labels!\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None # collect all graph features\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_corr_te: {len(dataset_corr_te)}\")\n",
    "print(f' Eliminated IDs:{dataset_corr_te.ids_to_eliminate}')\n",
    "clips_spatial_te = clips_te[~clips_te.index.isin(dataset_corr_te.ids_to_eliminate)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4461766e",
   "metadata": {},
   "source": [
    "#### C) Absolute difference correlation graph construction strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2328a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "top_k = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bf782ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.dataset_graph:Initializing GraphEEGDataset...\n",
      "INFO:src.data.dataset_graph:Dataset parameters:\n",
      "INFO:src.data.dataset_graph:  - Root directory: data/graph_dataset_absdiff_correlation_train\n",
      "INFO:src.data.dataset_graph:  - Edge strategy: relevance_diff_correlation\n",
      "INFO:src.data.dataset_graph:  - Top-k neighbors: 8\n",
      "INFO:src.data.dataset_graph:  - Correlation threshold: 0.7\n",
      "INFO:src.data.dataset_graph:  - Force reprocess: False\n",
      "INFO:src.data.dataset_graph:  - Bandpass frequencies: (0.5, 50)\n",
      "INFO:src.data.dataset_graph:  - Segment length: 3000\n",
      "INFO:src.data.dataset_graph:  - Apply filtering: True\n",
      "INFO:src.data.dataset_graph:  - Apply rereferencing: True\n",
      "INFO:src.data.dataset_graph:  - Apply normalization: True\n",
      "INFO:src.data.dataset_graph:  - Sampling rate: 250\n",
      "INFO:src.data.dataset_graph:  - Test mode: False\n",
      "INFO:src.data.dataset_graph:  - Extract graph features: True\n",
      "INFO:src.data.dataset_graph:  - Diff Corr Matrix Path: data/diff_corr_matrix.csv\n",
      "INFO:src.data.dataset_graph:Edge strategy: relevance_diff_correlation. Loading average correlation matrices.\n",
      "INFO:src.data.dataset_graph:Loaded absolute difference correlation matrix from data/diff_corr_matrix.csv, shape: (19, 19)\n",
      "INFO:src.data.dataset_graph:Initializing graph feature extractor...\n",
      "2025-06-09 18:46:34,133 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.utils.graph_features:GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Number of EEG channels: 19\n",
      "INFO:src.data.dataset_graph:Setting up signal filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "Length of dataset_absdiff_corr_tr: 4646\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load training dataset\n",
    "dataset_absdiff_corr_tr = GraphEEGDataset(\n",
    "    root=train_dataset_absdiff_correlation_dir,\n",
    "    clips=clips_tr,\n",
    "    signal_folder=train_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"relevance_diff_correlation\",\n",
    "    spatial_distance_file=None,\n",
    "    top_k=top_k,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None, # collect all graph features\n",
    "    # settings for absolute difference correlation\n",
    "    diff_corr_matrix_path=absdiff_correlation_file,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_absdiff_corr_tr: {len(dataset_absdiff_corr_tr)}\")\n",
    "print(f' Eliminated IDs: {dataset_absdiff_corr_tr.ids_to_eliminate}')\n",
    "clips_absdiff_corr_tr = clips_tr[~clips_tr.index.isin(dataset_absdiff_corr_tr.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18240a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.dataset_graph:Initializing GraphEEGDataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.dataset_graph:Dataset parameters:\n",
      "INFO:src.data.dataset_graph:  - Root directory: data/graph_dataset_absdiff_correlation_test\n",
      "INFO:src.data.dataset_graph:  - Edge strategy: relevance_diff_correlation\n",
      "INFO:src.data.dataset_graph:  - Top-k neighbors: 8\n",
      "INFO:src.data.dataset_graph:  - Correlation threshold: 0.7\n",
      "INFO:src.data.dataset_graph:  - Force reprocess: False\n",
      "INFO:src.data.dataset_graph:  - Bandpass frequencies: (0.5, 50)\n",
      "INFO:src.data.dataset_graph:  - Segment length: 3000\n",
      "INFO:src.data.dataset_graph:  - Apply filtering: True\n",
      "INFO:src.data.dataset_graph:  - Apply rereferencing: True\n",
      "INFO:src.data.dataset_graph:  - Apply normalization: True\n",
      "INFO:src.data.dataset_graph:  - Sampling rate: 250\n",
      "INFO:src.data.dataset_graph:  - Test mode: False\n",
      "INFO:src.data.dataset_graph:  - Extract graph features: True\n",
      "INFO:src.data.dataset_graph:  - Diff Corr Matrix Path: data/diff_corr_matrix.csv\n",
      "INFO:src.data.dataset_graph:Edge strategy: relevance_diff_correlation. Loading average correlation matrices.\n",
      "INFO:src.data.dataset_graph:Loaded absolute difference correlation matrix from data/diff_corr_matrix.csv, shape: (19, 19)\n",
      "INFO:src.data.dataset_graph:Initializing graph feature extractor...\n",
      "2025-06-09 18:46:34,172 - src.utils.graph_features - INFO - GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.utils.graph_features:GraphFeatureExtractor initialized with features: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Graph feature types: ['degree', 'clustering', 'centrality', 'connectivity', 'path_length', 'efficiency', 'assortativity', 'modularity', 'laplacian_spectrum', 'k_core']\n",
      "INFO:src.data.dataset_graph:Number of EEG channels: 19\n",
      "INFO:src.data.dataset_graph:Setting up signal filters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset_absdiff_corr_te: 0\n",
      " Eliminated IDs: []\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.data.dataset_graph import GraphEEGDataset\n",
    "\n",
    "# load test dataset\n",
    "dataset_absdiff_corr_te = GraphEEGDataset(\n",
    "    root=test_dataset_absdiff_correlation_dir,\n",
    "    clips=clips_te,\n",
    "    signal_folder=test_dir,\n",
    "    extracted_features_dir=extracted_features_dir,\n",
    "    use_selected_features=False,\n",
    "    embeddings_dir=embeddings_dir,\n",
    "    use_embeddings=False,\n",
    "    edge_strategy=\"relevance_diff_correlation\",\n",
    "    spatial_distance_file=None,\n",
    "    top_k=top_k,\n",
    "    force_reprocess=False,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    segment_length=3000,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    # extract graph features\n",
    "    extract_graph_features=True,\n",
    "    graph_feature_types=None, # collect all graph features\n",
    "    # settings for absolute difference correlation\n",
    "    diff_corr_matrix_path=absdiff_correlation_file,\n",
    ")\n",
    "\n",
    "# Check the length of the dataset\n",
    "print(f\"Length of dataset_absdiff_corr_te: {len(dataset_absdiff_corr_te)}\")\n",
    "print(f' Eliminated IDs: {dataset_absdiff_corr_te.ids_to_eliminate}')\n",
    "clips_absdiff_corr_te = clips_te[~clips_te.index.isin(dataset_absdiff_corr_te.ids_to_eliminate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3124939",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_datasets = {\n",
    "    \"spatial\": {\n",
    "        \"dataset_tr\": dataset_spatial_tr,\n",
    "        \"dataset_te\": dataset_spatial_te,\n",
    "        \"clips_tr\": clips_spatial_tr,\n",
    "    },\n",
    "    \"correlation\": {\n",
    "        \"dataset_tr\": dataset_corr_tr,\n",
    "        \"dataset_te\": dataset_corr_te,\n",
    "        \"clips_tr\": clips_corr_tr,\n",
    "    },\n",
    "    \"absolute_difference\": {\n",
    "        \"dataset_tr\": dataset_absdiff_corr_tr,\n",
    "        \"dataset_te\": dataset_absdiff_corr_te,\n",
    "        \"clips_tr\": clips_absdiff_corr_tr,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa474f0b",
   "metadata": {},
   "source": [
    "### Timeseries datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4a44e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "ðŸš€ Initializing TimeseriesEEGDataset in SIGNAL mode.\n",
      "   - Sampling rate: 250 Hz\n",
      "   - Derived segment length: 3000 timesteps.\n",
      "   - Segment length: 3000 timesteps.\n",
      "   âœ… Using existing cached data from data/timeseries_dataset_train_signal/processed\n",
      "ðŸ TimeseriesEEGDataset initialization complete. Loaded 12993 samples.\n",
      "ðŸš€ Initializing TimeseriesEEGDataset in SIGNAL mode.\n",
      "   - Sampling rate: 250 Hz\n",
      "   - Derived segment length: 3000 timesteps.\n",
      "   - Segment length: 3000 timesteps.\n",
      "   âš ï¸ Info: Column 'label' not found in clips_df. Processing without labels (e.g., test set).\n",
      "   âœ… Using existing cached data from data/timeseries_dataset_test_signal/processed\n",
      "ðŸ TimeseriesEEGDataset initialization complete. Loaded 3614 samples.\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.utils.timeseries_eeg_dataset import TimeseriesEEGDataset\n",
    "\n",
    "dataset_timeseries_signal_tr = TimeseriesEEGDataset(\n",
    "    root=str(train_dataset_timeseries_signal_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_tr,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    force_reprocess=False,\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='signal', # Use raw EEG signal data\n",
    ")\n",
    "dataset_timeseries_signal_te = TimeseriesEEGDataset(\n",
    "    root=str(test_dataset_timeseries_signal_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_te,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='signal', # Use raw EEG signal data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fe1f9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "ðŸš€ Initializing TimeseriesEEGDataset in FEATURE mode.\n",
      "   âœ… Using existing cached data from data/timeseries_dataset_train_features/processed\n",
      "ðŸ TimeseriesEEGDataset initialization complete. Loaded 12993 samples.\n",
      "ðŸš€ Initializing TimeseriesEEGDataset in FEATURE mode.\n",
      "   âš ï¸ Info: Column 'label' not found in clips_df. Processing without labels (e.g., test set).\n",
      "   âœ… Using existing cached data from data/timeseries_dataset_test_features/processed\n",
      "ðŸ TimeseriesEEGDataset initialization complete. Loaded 3614 samples.\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.utils.timeseries_eeg_dataset import TimeseriesEEGDataset\n",
    "\n",
    "dataset_timeseries_feature_tr = TimeseriesEEGDataset(\n",
    "    root=str(train_dataset_timeseries_feature_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_tr,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='feature',\n",
    "    feature_file_path=str(extracted_features_dir / \"X_train.npy\"),\n",
    ")\n",
    "dataset_timeseries_feature_te = TimeseriesEEGDataset(\n",
    "    root=str(test_dataset_timeseries_feature_dir),\n",
    "    signal_folder=str(train_dir),\n",
    "    clips_df=clips_te,\n",
    "    bandpass_frequencies=(\n",
    "        low_bandpass_frequency,\n",
    "        high_bandpass_frequency,\n",
    "    ),\n",
    "    apply_filtering=True,\n",
    "    apply_rereferencing=True,\n",
    "    apply_normalization=True,\n",
    "    sampling_rate=250,\n",
    "    mode='feature',\n",
    "    feature_file_path=str(extracted_features_dir / \"X_test.npy\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f90b2daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_datasets = {\n",
    "    \"signal\": {\n",
    "        \"dataset_tr\": dataset_timeseries_signal_tr,\n",
    "        \"dataset_te\": dataset_timeseries_signal_te,\n",
    "        \"clips_tr\": clips_tr\n",
    "    },\n",
    "    \"feature\": {\n",
    "        \"dataset_tr\": dataset_timeseries_feature_tr,\n",
    "        \"dataset_te\": dataset_timeseries_feature_te,\n",
    "        \"clips_tr\": clips_tr\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f38f791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n",
      "âœ… TrainingContext initialized. Use .switch_to('dataset_type') to begin.\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "from src.utils.lazy import LazyDataLoaderManager, TrainingContext\n",
    "\n",
    "# create loaders for each kind of dataset\n",
    "timeseries_loader_manager = LazyDataLoaderManager(\n",
    "    timeseries_datasets,\n",
    "    oversampling_power=oversampling_power,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# create train context\n",
    "timeseries_training_context = TrainingContext(timeseries_loader_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef9bd7",
   "metadata": {},
   "source": [
    "## Dataset Selection and Training Configuration\n",
    "\n",
    "This section allows you to select which dataset type to use for training by modifying the `DATASET_TYPE` variable in the next cell. The notebook supports multiple dataset types, each with different preprocessing strategies and model architectures.\n",
    "\n",
    "### Available Dataset Types\n",
    "\n",
    "#### Graph-Based Datasets (for GNN models):\n",
    "- **`'spatial'`** - Uses spatial distance-based graph connections between EEG electrodes\n",
    "- **`'correlation'`** - Uses correlation-based graph connections (top-k=5)\n",
    "- **`'absdiff_correlation'`** - Uses absolute difference correlation graph connections (top-k=8)\n",
    "\n",
    "#### Timeseries Datasets (for traditional deep learning models):\n",
    "- **`'signal'`** - Raw EEG signal data with temporal processing\n",
    "- **`'features'`** - Pre-extracted feature representations\n",
    "\n",
    "### Data Loaders Structure\n",
    "\n",
    "All datasets are automatically split into train/validation/test sets with the following configuration:\n",
    "- **Train/Validation ratio**: 80/20\n",
    "- **Random seed**: 42 (for reproducibility)  \n",
    "- **Class balancing**: WeightedRandomSampler with oversampling power = 1.0\n",
    "- **Batch size**: 64\n",
    "\n",
    "The data loaders are organized in dictionaries for easy access:\n",
    "- `graph_loaders` - Contains loaders for all graph-based datasets\n",
    "- `timeseries_loaders` - Contains loaders for all timeseries datasets\n",
    "\n",
    "### Usage Instructions\n",
    "\n",
    "1. Set the `DATASET_TYPE` variable to your desired dataset type\n",
    "2. The notebook will automatically configure the appropriate data loaders\n",
    "3. Use the selected `train_loader`, `val_loader`, and `te_loader` for model training\n",
    "4. Choose the corresponding model architecture (GNN for graph datasets, traditional models for timeseries datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67042055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%aimport\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from src.layers.hybrid.cnn_bilstm_gcn import EEGCNNBiLSTMGCN\n",
    "from src.utils.train import train_model\n",
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"patience\": 10,\n",
    "    \"epochs\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5446a5",
   "metadata": {},
   "source": [
    "## Training and Evaluation - Timeseries Models\n",
    "\n",
    "In this section, we will train and evaluate traditional deep learning models on the selected timeseries dataset. The models will be trained using the `train_loader` and evaluated on the `val_loader` and `te_loader` (the latter being used for final evaluation after training. Labels are not available for the test set, so we will not compute metrics on it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60bc6d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_traditional_train(model, save_path):\n",
    "    global train_context\n",
    "    global train_context\n",
    "    if 'train_context' not in globals():\n",
    "        raise ValueError(\"Timeseries training context is not initialized. Please initialize it before calling this function.\")\n",
    "    if not isinstance(train_context, TrainingContext):\n",
    "        raise ValueError(\"train_context must be an instance of TrainingContext.\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # if multiple GPUs are available, use DataParallel\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for training\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5)\n",
    "    loss = nn.BCEWithLogitsLoss()  # Not weighted as we use a balanced sampler!\n",
    "\n",
    "    # train model\n",
    "    train_history, val_history = train_model(\n",
    "        wandb_config=None,\n",
    "        model=model,\n",
    "        train_loader=train_context.train_loader,\n",
    "        val_loader=train_context.val_loader,\n",
    "        criterion=loss,\n",
    "        scheduler=scheduler,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=config[\"epochs\"],\n",
    "        patience=config[\"patience\"],\n",
    "        save_path=save_path,\n",
    "        use_gnn=False,\n",
    "        # hidden attribute\n",
    "        try_load_checkpoint=True,\n",
    "        # FIXME: remove this before submission\n",
    "        log_wandb=False,\n",
    "    )\n",
    "    plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb500fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.00 MB\n",
      "Cached: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "from src.utils.cuda import print_cuda_memory_usage, clean_cuda_memory_usage\n",
    "clean_cuda_memory_usage()\n",
    "print_cuda_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06a1e4",
   "metadata": {},
   "source": [
    "#### LSTM (signal-based model)\n",
    "\n",
    "The LSTM model is a recurrent neural network (RNN) architecture designed to handle sequential data, making it suitable for time-series analysis like EEG signals. It captures temporal dependencies in the data, allowing it to learn patterns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3bc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Starting training setup...\n",
      "INFO:src.utils.train:Model type: Standard\n",
      "INFO:src.utils.train:Device: cuda\n",
      "INFO:src.utils.train:Batch size: 64\n",
      "INFO:src.utils.train:Number of epochs: 100\n",
      "INFO:src.utils.train:Patience: 10\n",
      "INFO:src.utils.train:Monitor metric: val_f1\n",
      "INFO:src.utils.train:Initializing wandb...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">timeseries_signal_lstm_baseline</strong> at: <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/8306egm0' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/8306egm0</a><br> View project at: <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250609_164136-8306egm0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ldibello/NeuroGraphNet/wandb/run-20250609_164223-g86foj17</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/g86foj17' target=\"_blank\">timeseries_signal_lstm_baseline</a></strong> to <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/g86foj17' target=\"_blank\">https://wandb.ai/lucadibello-epfl/neuro-graph-net/runs/g86foj17</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:ðŸ”— Wandb run initialized: timeseries_signal_lstm_baseline\n",
      "INFO:src.utils.train:Total training batches per epoch: 162\n",
      "INFO:src.utils.train:Starting training from epoch 1 to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Wandb initialized: timeseries_signal_lstm_baseline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]INFO:src.utils.train:\n",
      "Epoch 1/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: torch.Size([64, 19, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.6928 - Avg batch time: 4.20s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.6939 - Avg batch time: 0.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.6907 - Avg batch time: 0.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.6919 - Avg batch time: 0.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.6956 - Avg batch time: 0.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.6914 - Avg batch time: 0.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.6906 - Avg batch time: 0.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.6939 - Avg batch time: 0.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.6918 - Avg batch time: 0.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.6932 - Avg batch time: 0.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.6954 - Avg batch time: 0.20s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.6923 - Avg batch time: 0.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.6925 - Avg batch time: 0.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n",
      "BATCH: torch.Size([64, 19, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Processing batch 131/162\n",
      "Epochs:   1%|â–Š                                                                                  | 1/100 [02:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: torch.Size([64, 19, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m], weight_decay\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     14\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mwrap_traditional_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAVE_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 22\u001b[0m, in \u001b[0;36mwrap_traditional_train\u001b[0;34m(model, save_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()  \u001b[38;5;66;03m# Not weighted as we use a balanced sampler!\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hidden attribute\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_load_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m plot_training_loss(train_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], val_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:438\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, wandb_config, wandb_project, wandb_run_name, log_wandb, try_load_checkpoint, original_dataset)\u001b[0m\n\u001b[1;32m    436\u001b[0m     y_targets \u001b[38;5;241m=\u001b[39m y_targets\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;66;03m# Use safe model call for non-GNN models\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_model_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data_batch_item, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data_batch_item, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m to_dense_batch: \u001b[38;5;66;03m# Should be caught earlier if PyG not installed but good check\u001b[39;00m\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:152\u001b[0m, in \u001b[0;36m_safe_model_call\u001b[0;34m(model, x, edge_index, batch, graph_features)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph_features\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params \u001b[38;5;129;01mand\u001b[39;00m graph_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m     call_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph_features\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m graph_features\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcall_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1798\u001b[0m     ):\n\u001b[1;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/layers/temporal/lstm.py:64\u001b[0m, in \u001b[0;36mEEGLSTMClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (batch_size, time_steps, n_sensors)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Process the multi-sensor signal through LSTM\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m lstm_out, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Extract final hidden state\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# For bidirectional LSTM, concatenate forward and backward hidden states\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# hn shape: (num_layers * 2, batch_size, hidden_dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1086\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1084\u001b[0m unsorted_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1086\u001b[0m     h_zeros \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_directions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreal_hidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1093\u001b[0m     c_zeros \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m   1094\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m*\u001b[39m num_directions,\n\u001b[1;32m   1095\u001b[0m         max_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1098\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m   1099\u001b[0m     )\n\u001b[1;32m   1100\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (h_zeros, c_zeros)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%aimport src.layers.temporal.lstm\n",
    "from src.layers.temporal.lstm import EEGLSTMClassifier\n",
    "\n",
    "# create loader manager\n",
    "train_context = timeseries_training_context.switch_to('signal')\n",
    "\n",
    "# build model with current parameters\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"timeseries_signal_lstm_baseline.pt\"\n",
    "model = EEGLSTMClassifier(\n",
    "    input_dim=19,\n",
    "    hidden_dim=64,\n",
    "    num_layers=4,\n",
    "    dropout=0.3,\n",
    "    bidirectional=False,\n",
    "    input_type=\"signal\"\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "wrap_traditional_train(model, save_path=SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89807a",
   "metadata": {},
   "source": [
    "#### BiLSTM (signal-based model)\n",
    "\n",
    "The BiLSTM (Bidirectional Long Short-Term Memory) model extends the LSTM by processing the input sequence in both forward and backward directions. This bidirectional approach allows the model to capture context from both past and future time steps, enhancing its ability to understand complex temporal relationships in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d84ce139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒ Switching context to 'SIGNAL' dataset...\n",
      "\n",
      "--- Lazily creating loaders for 'signal' ---\n",
      "\n",
      "--- Creating train/val subsets for 'signal' dataset ---\n",
      "Splitting 12993 samples -> Train: 10394, Val: 2599\n",
      "[18:46:59] Train labels: 0 -> 8375, 1 -> 2019\n",
      "[18:46:59] Val labels:   0 -> 2101, 1 -> 498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Starting training setup...\n",
      "INFO:src.utils.train:Model type: Standard\n",
      "INFO:src.utils.train:Device: cuda\n",
      "INFO:src.utils.train:Batch size: 64\n",
      "INFO:src.utils.train:Number of epochs: 100\n",
      "INFO:src.utils.train:Patience: 10\n",
      "INFO:src.utils.train:Monitor metric: val_f1\n",
      "INFO:src.utils.train:Total training batches per epoch: 162\n",
      "INFO:src.utils.train:Starting training from epoch 1 to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [0.0001194  0.00049529]\n",
      "Train set class distribution: [8375 2019]\n",
      "ðŸš€ Context ready for 'signal'.\n",
      "   Train batches: 162, Val batches: 41\n",
      "   Type: signal\n",
      "   Total Train Samples: 12993\n",
      "   Channels: 19\n",
      "   Sequence Length: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]INFO:src.utils.train:\n",
      "Epoch 1/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.6973 - Avg batch time: 0.35s\n",
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m], weight_decay\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     23\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mwrap_traditional_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAVE_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m, in \u001b[0;36mwrap_traditional_train\u001b[0;34m(model, save_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()  \u001b[38;5;66;03m# Not weighted as we use a balanced sampler!\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hidden attribute\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_load_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# FIXME: remove this before submission\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m plot_training_loss(train_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], val_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:391\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, wandb_config, wandb_project, wandb_run_name, log_wandb, try_load_checkpoint, original_dataset)\u001b[0m\n\u001b[1;32m    388\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training phase\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    389\u001b[0m batch_times \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, data_batch_item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader_to_iterate):\n\u001b[1;32m    392\u001b[0m     batch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Log every 10 batches\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/timeseries_eeg_dataset.py:421\u001b[0m, in \u001b[0;36mTimeseriesEEGDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(idx) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processed_items_count:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of range [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processed_items_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cached_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[idx]\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/timeseries_eeg_dataset.py:449\u001b[0m, in \u001b[0;36mTimeseriesEEGDataset._get_cached_item\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    447\u001b[0m file_path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_file_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:1426\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m-> 1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_is_zipfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   1431\u001b[0m         overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:433\u001b[0m, in \u001b[0;36m_is_zipfile\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Read the first few bytes and match against the ZIP file signature\u001b[39;00m\n\u001b[1;32m    432\u001b[0m local_header_magic_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 433\u001b[0m read_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlocal_header_magic_number\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m f\u001b[38;5;241m.\u001b[39mseek(start)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m read_bytes \u001b[38;5;241m==\u001b[39m local_header_magic_number\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%aimport src.layers.temporal.lstm\n",
    "from src.layers.temporal.lstm import EEGLSTMClassifier\n",
    "\n",
    "# create loader manager\n",
    "train_context = timeseries_training_context.switch_to('signal')\n",
    "\n",
    "# build model with current parameters\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"timeseries_signal_bilstm.pt\"\n",
    "model = EEGLSTMClassifier(\n",
    "    input_dim=19,\n",
    "    hidden_dim=64,\n",
    "    num_layers=4,\n",
    "    dropout=0.3,\n",
    "    bidirectional=True\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "wrap_traditional_train(model, save_path=SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4a846",
   "metadata": {},
   "source": [
    "#### MLP (feature-based model)\n",
    "\n",
    "The MLP (Multi-Layer Perceptron) is a feedforward neural network architecture consisting of multiple layers of neurons. It is designed to learn complex mappings from input features to output labels, making it suitable for tasks like EEG seizure detection when using pre-extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22d2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Starting training setup...\n",
      "INFO:src.utils.train:Model type: Standard\n",
      "INFO:src.utils.train:Device: cuda\n",
      "INFO:src.utils.train:Batch size: 64\n",
      "INFO:src.utils.train:Number of epochs: 100\n",
      "INFO:src.utils.train:Patience: 10\n",
      "INFO:src.utils.train:Monitor metric: val_f1\n",
      "INFO:src.utils.train:Total training batches per epoch: 162\n",
      "INFO:src.utils.train:Starting training from epoch 2 to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Attempting to load checkpoint from .checkpoints/timeseries_feature_mlp.pt...\n",
      "   - Loading checkpoint from: .checkpoints/timeseries_feature_mlp.pt\n",
      "   - Detected full checkpoint dictionary.\n",
      "   - Optimizer state loaded from checkpoint.\n",
      "   - Model state successfully loaded.\n",
      " âœ… Checkpoint loaded. Resuming from epoch 2. Best 'val_f1' score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|â–ˆâ–‹                                                                                 | 2/100 [00:00<?, ?it/s]INFO:src.utils.train:\n",
      "Epoch 2/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.7236 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.7583 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.7549 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.6828 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.7190 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.7481 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.7640 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.7232 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.6890 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.7008 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.7646 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.7144 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.7104 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.7293 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.7058 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.7415 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.7059 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:\n",
      "Epoch 2 training completed in 4.21s\n",
      "INFO:src.utils.train:Average training loss: 0.7213\n",
      "Epochs:   3%| | 3/100 [00:05<08:15,  5.11s/it, train_loss=0.7213, val_loss=0.5967, best_val_f1=0.0000, lr=1.00e-04, bad_INFO:src.utils.train:\n",
      "Epoch 3/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.7168 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.7655 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.7088 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.7394 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.7153 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.7176 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.7241 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.7426 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.7421 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.7040 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.6930 - Avg batch time: 0.00s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.7460 - Avg batch time: 0.00s\n",
      "Epochs:   3%| | 3/100 [00:08<13:09,  8.14s/it, train_loss=0.7213, val_loss=0.5967, best_val_f1=0.0000, lr=1.00e-04, bad_\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m], weight_decay\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     22\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mwrap_traditional_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAVE_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 22\u001b[0m, in \u001b[0;36mwrap_traditional_train\u001b[0;34m(model, save_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()  \u001b[38;5;66;03m# Not weighted as we use a balanced sampler!\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hidden attribute\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_load_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# FIXME: remove this before submission\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m plot_training_loss(train_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], val_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:391\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, wandb_config, wandb_project, wandb_run_name, log_wandb, try_load_checkpoint, original_dataset)\u001b[0m\n\u001b[1;32m    388\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training phase\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    389\u001b[0m batch_times \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, data_batch_item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader_to_iterate):\n\u001b[1;32m    392\u001b[0m     batch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Log every 10 batches\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/timeseries_eeg_dataset.py:421\u001b[0m, in \u001b[0;36mTimeseriesEEGDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(idx) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processed_items_count:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of range [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processed_items_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cached_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[idx]\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/timeseries_eeg_dataset.py:449\u001b[0m, in \u001b[0;36mTimeseriesEEGDataset._get_cached_item\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    447\u001b[0m file_path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_file_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:1462\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1469\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1470\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:1964\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   1963\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1964\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1965\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1967\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/_weights_only_unpickler.py:512\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    506\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_maybe_decode_ascii(pid[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m     ):\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[1;32m    510\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m         )\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[38;5;241m0\u001b[39m], LONG_BINGET[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[1;32m    514\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (read(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m BINGET[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, read(\u001b[38;5;241m4\u001b[39m)))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:1927\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1925\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m loaded_storages[key]\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1927\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_element_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1928\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m load_tensor(\n\u001b[1;32m   1929\u001b[0m         dtype, nbytes, key, _maybe_decode_ascii(location)\n\u001b[1;32m   1930\u001b[0m     )\n\u001b[1;32m   1932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/_utils.py:859\u001b[0m, in \u001b[0;36m_element_size\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_element_size\u001b[39m(dtype):\n\u001b[1;32m    856\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;124;03m    Returns the element size for a dtype, in bytes\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    860\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected torch.dtype, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dtype)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mis_complex:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%aimport src.layers.mlp\n",
    "from src.layers.mlp import EEGMLPClassifier\n",
    "\n",
    "# switch to feature dataset\n",
    "train_context = timeseries_training_context.switch_to('feature')\n",
    "\n",
    "# build model with current parameters\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"timeseries_feature_mlp.pt\"\n",
    "model = EEGMLPClassifier(\n",
    "    input_dim=228, # extracted features dimension\n",
    "    hidden_dims=[1024, 512, 256],\n",
    "    output_dim=1,\n",
    "    dropout_prob=0.3,\n",
    "    use_batch_norm=True,\n",
    "    use_residual=False,\n",
    "    activation=\"relu\"\n",
    ")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "wrap_traditional_train(model, save_path=SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8f8223",
   "metadata": {},
   "source": [
    "#### MLP (signal-based model, flattened EEG signals)\n",
    "\n",
    "The same MLP architecture as above, but trained on raw EEG signals instead of pre-extracted features to evaluate the performance of the model on raw data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307a8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Starting training setup...\n",
      "INFO:src.utils.train:Model type: Standard\n",
      "INFO:src.utils.train:Device: cuda\n",
      "INFO:src.utils.train:Batch size: 64\n",
      "INFO:src.utils.train:Number of epochs: 100\n",
      "INFO:src.utils.train:Patience: 10\n",
      "INFO:src.utils.train:Monitor metric: val_f1\n",
      "INFO:src.utils.train:Total training batches per epoch: 162\n",
      "INFO:src.utils.train:Starting training from epoch 1 to 100\n",
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]INFO:src.utils.train:\n",
      "Epoch 1/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.6557 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.7758 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.6898 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.6811 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.6099 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.6682 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.5708 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.5833 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.6560 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.6404 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.5109 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.4890 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.5547 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.5594 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.4951 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.4099 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.3763 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:\n",
      "Epoch 1 training completed in 19.38s\n",
      "INFO:src.utils.train:Average training loss: 0.5984\n",
      "Epochs:   2%| | 2/100 [00:30<50:03, 30.64s/it, train_loss=0.5984, val_loss=0.4979, best_val_f1=0.2500, lr=1.00e-04, bad_INFO:src.utils.train:\n",
      "Epoch 2/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.4322 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.3271 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.5097 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.3626 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.4072 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.4805 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.3614 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.2735 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.2348 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.4421 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.3489 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.2458 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.4730 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.3169 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.5019 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.3223 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.1773 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:\n",
      "Epoch 2 training completed in 20.86s\n",
      "INFO:src.utils.train:Average training loss: 0.3498\n",
      "Epochs:   3%| | 3/100 [00:54<43:26, 26.88s/it, train_loss=0.3498, val_loss=0.6073, best_val_f1=0.2500, lr=1.00e-04, bad_INFO:src.utils.train:\n",
      "Epoch 3/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.3703 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.2056 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.3031 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.2305 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.1975 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.2344 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.1778 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.4540 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.1942 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.2038 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.2775 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.1384 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.1902 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.2692 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.2234 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.0918 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.1351 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:\n",
      "Epoch 3 training completed in 20.43s\n",
      "INFO:src.utils.train:Average training loss: 0.2169\n",
      "Epochs:   4%| | 4/100 [01:18<40:53, 25.56s/it, train_loss=0.2169, val_loss=0.7287, best_val_f1=0.2500, lr=1.00e-04, bad_INFO:src.utils.train:\n",
      "Epoch 4/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.1194 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.1642 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.2061 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.1583 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.1665 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.1715 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.1158 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.1587 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.1903 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.1902 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.1256 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.0845 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.1135 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.1800 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.2041 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.1592 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.0487 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:\n",
      "Epoch 4 training completed in 22.90s\n",
      "INFO:src.utils.train:Average training loss: 0.1564\n",
      "Epochs:   5%| | 5/100 [01:54<46:27, 29.34s/it, train_loss=0.1564, val_loss=0.7770, best_val_f1=0.2571, lr=1.00e-04, bad_INFO:src.utils.train:\n",
      "Epoch 5/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.1604 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.0680 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.2426 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.1668 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.0741 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.1475 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.0802 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.0851 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.2446 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.0945 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.1303 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.0705 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.0868 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.1705 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.0531 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.1085 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.0979 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:\n",
      "Epoch 5 training completed in 26.29s\n",
      "INFO:src.utils.train:Average training loss: 0.1231\n",
      "Epochs:   6%| | 6/100 [02:23<45:57, 29.33s/it, train_loss=0.1231, val_loss=0.8877, best_val_f1=0.2571, lr=1.00e-04, bad_INFO:src.utils.train:\n",
      "Epoch 6/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.0820 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.0959 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.2188 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.0611 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.1037 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.0384 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.1719 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.1919 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.1584 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.0913 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.2087 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.0628 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.1163 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.0469 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.0257 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.1319 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.0453 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:\n",
      "Epoch 6 training completed in 17.32s\n",
      "INFO:src.utils.train:Average training loss: 0.0986\n",
      "Epochs:   7%| | 7/100 [02:44<40:57, 26.42s/it, train_loss=0.0986, val_loss=0.9548, best_val_f1=0.2571, lr=1.00e-04, bad_INFO:src.utils.train:\n",
      "Epoch 7/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.1716 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.0563 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.1107 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.0631 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.0357 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.0738 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.1154 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.0401 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.0391 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.0381 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.0711 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.0960 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.0272 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.0450 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.1155 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.0677 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.0754 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:\n",
      "Epoch 7 training completed in 17.74s\n",
      "INFO:src.utils.train:Average training loss: 0.0925\n",
      "Epochs:   8%| | 8/100 [03:15<42:55, 28.00s/it, train_loss=0.0925, val_loss=0.9504, best_val_f1=0.2698, lr=1.00e-04, bad_INFO:src.utils.train:\n",
      "Epoch 8/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.0257 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.1389 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.0319 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.0414 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.1107 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.0476 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.0799 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.0420 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.0341 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.1263 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.0211 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.0755 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.0246 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.0059 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.0187 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.1011 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.0136 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:\n",
      "Epoch 8 training completed in 22.16s\n",
      "INFO:src.utils.train:Average training loss: 0.0728\n",
      "Epochs:   9%| | 9/100 [03:41<41:30, 27.37s/it, train_loss=0.0728, val_loss=0.9963, best_val_f1=0.2698, lr=1.00e-04, bad_INFO:src.utils.train:\n",
      "Epoch 9/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.0646 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.0772 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.0865 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.0105 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.0534 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.0172 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.1974 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.0175 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.1708 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.0348 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.0496 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.0605 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.1863 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.1042 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.0114 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.0052 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.0274 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:\n",
      "Epoch 9 training completed in 17.91s\n",
      "INFO:src.utils.train:Average training loss: 0.0630\n",
      "Epochs:  10%| | 10/100 [04:02<38:09, 25.44s/it, train_loss=0.0630, val_loss=1.0075, best_val_f1=0.2698, lr=1.00e-04, badINFO:src.utils.train:\n",
      "Epoch 10/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.0110 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.0732 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.1453 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.0430 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.0644 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.0412 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.0558 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.0276 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.0315 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.0098 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.0389 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.0294 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.0157 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.1455 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.0264 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.0572 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.1377 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:\n",
      "Epoch 10 training completed in 17.13s\n",
      "INFO:src.utils.train:Average training loss: 0.0564\n",
      "Epochs:  11%| | 11/100 [04:24<35:58, 24.26s/it, train_loss=0.0564, val_loss=1.0850, best_val_f1=0.2698, lr=1.00e-04, badINFO:src.utils.train:\n",
      "Epoch 11/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.0338 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.0264 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.0108 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.0412 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.1724 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.0132 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.0287 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.0814 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.0326 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.0120 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.1001 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.0131 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.0346 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.0471 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.0690 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.0122 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.1599 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:\n",
      "Epoch 11 training completed in 18.09s\n",
      "INFO:src.utils.train:Average training loss: 0.0598\n",
      "Epochs:  12%| | 12/100 [04:45<34:27, 23.49s/it, train_loss=0.0598, val_loss=1.1035, best_val_f1=0.2698, lr=5.00e-05, badINFO:src.utils.train:\n",
      "Epoch 12/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.1037 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.0113 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.0181 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.0082 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.0023 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.0337 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.0074 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.0046 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.0437 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.0409 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.0086 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.0182 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.0030 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.1052 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.0043 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.0030 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.1426 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:\n",
      "Epoch 12 training completed in 17.59s\n",
      "INFO:src.utils.train:Average training loss: 0.0322\n",
      "Epochs:  13%|â–| 13/100 [05:06<32:51, 22.66s/it, train_loss=0.0322, val_loss=1.1645, best_val_f1=0.2698, lr=5.00e-05, badINFO:src.utils.train:\n",
      "Epoch 13/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.0282 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.0052 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.0035 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.0108 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.0136 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.0017 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.0038 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.0501 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.0043 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.0093 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.0052 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.0021 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.0084 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.0017 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.0022 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.0011 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.0064 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:\n",
      "Epoch 13 training completed in 18.42s\n",
      "INFO:src.utils.train:Average training loss: 0.0168\n",
      "Epochs:  14%|â–| 14/100 [05:30<32:45, 22.86s/it, train_loss=0.0168, val_loss=1.1193, best_val_f1=0.2698, lr=5.00e-05, badINFO:src.utils.train:\n",
      "Epoch 14/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.0286 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.0075 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.0104 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.0019 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.0019 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.0017 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.0053 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.0539 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.0021 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.0612 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.0107 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.0345 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.0042 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.0037 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.0047 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.0019 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.0019 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:\n",
      "Epoch 14 training completed in 18.42s\n",
      "INFO:src.utils.train:Average training loss: 0.0170\n",
      "Epochs:  15%|â–| 15/100 [05:52<32:05, 22.65s/it, train_loss=0.0170, val_loss=1.2421, best_val_f1=0.2698, lr=5.00e-05, badINFO:src.utils.train:\n",
      "Epoch 15/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.0130 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.0038 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.0100 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.0018 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.0331 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.0628 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.0011 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.0143 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.0011 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.0057 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.0047 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.0012 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.0041 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.0025 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.0272 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.0047 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.0537 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:\n",
      "Epoch 15 training completed in 18.28s\n",
      "INFO:src.utils.train:Average training loss: 0.0179\n",
      "Epochs:  16%|â–| 16/100 [06:13<31:07, 22.23s/it, train_loss=0.0179, val_loss=1.2850, best_val_f1=0.2698, lr=5.00e-05, badINFO:src.utils.train:\n",
      "Epoch 16/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.0114 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.0014 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.0011 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.1324 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.0015 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.0137 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.0011 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.0030 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.0242 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.0350 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.0275 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.0035 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.0015 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.0007 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.0009 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.0037 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.0202 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:\n",
      "Epoch 16 training completed in 17.78s\n",
      "INFO:src.utils.train:Average training loss: 0.0163\n",
      "Epochs:  17%|â–| 17/100 [06:37<31:19, 22.65s/it, train_loss=0.0163, val_loss=1.3014, best_val_f1=0.2698, lr=5.00e-05, badINFO:src.utils.train:\n",
      "Epoch 17/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.1398 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.0018 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.0034 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.0007 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.0206 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.0016 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.0086 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.0044 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.0006 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.0009 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.0571 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 111/162\n",
      "INFO:src.utils.train:Batch 111/162 - Loss: 0.0014 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 121/162\n",
      "INFO:src.utils.train:Batch 121/162 - Loss: 0.0018 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 131/162\n",
      "INFO:src.utils.train:Batch 131/162 - Loss: 0.0005 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 141/162\n",
      "INFO:src.utils.train:Batch 141/162 - Loss: 0.0723 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 151/162\n",
      "INFO:src.utils.train:Batch 151/162 - Loss: 0.0104 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 161/162\n",
      "INFO:src.utils.train:Batch 161/162 - Loss: 0.0057 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:\n",
      "Epoch 17 training completed in 18.29s\n",
      "INFO:src.utils.train:Average training loss: 0.0143\n",
      "Epochs:  17%|â–| 17/100 [06:59<31:19, 22.65s/it, train_loss=0.0143, val_loss=1.2603, best_val_f1=0.2698, lr=5.00e-05, badINFO:src.utils.train:Early stopping triggered: no 'val_f1' improvement in 10 epochs\n",
      "Epochs:  17%|â–| 17/100 [06:59<36:13, 26.19s/it, train_loss=0.0143, val_loss=1.2603, best_val_f1=0.2698, lr=5.00e-05, bad\n",
      "INFO:src.utils.train:Training completed successfully!\n",
      "INFO:src.utils.train:Final results: {'final_best_score': 0.2697795033454895, 'final_epoch': 17, 'total_bad_epochs': 10}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%aimport src.layers.mlp\n",
    "from src.layers.mlp import EEGMLPClassifier\n",
    "\n",
    "# switch to signal dataset\n",
    "train_context = timeseries_training_context.switch_to('signal')\n",
    "\n",
    "# build model with current parameters\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"timeseries_signal_mlp.pt\"\n",
    "model = EEGMLPClassifier(\n",
    "    # 19 channels * 3000 time steps = 57,000\n",
    "    input_channels=19,\n",
    "    input_time_steps=3000,\n",
    "    hidden_dims=[4096, 2048, 1024, 512, 256],\n",
    "    output_dim=1,\n",
    "    dropout_prob=0.3,\n",
    "    use_batch_norm=True,\n",
    "    use_residual=False,\n",
    "    activation=\"leaky_relu\"\n",
    ")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "wrap_traditional_train(model, save_path=SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d45648",
   "metadata": {},
   "source": [
    "#### CNN-MLP (signal-based model)\n",
    "\n",
    "We have proven that the MLP model alone is not sufficient to capture the temporal dependencies in the EEG signals. Therefore, we will use a CNN-MLP model that combines convolutional layers to extract spatial features from the EEG signals and MLP layers to learn the mapping from these features to the output labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0dc832a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Starting training setup...\n",
      "INFO:src.utils.train:Model type: Standard\n",
      "INFO:src.utils.train:Device: cuda\n",
      "INFO:src.utils.train:Batch size: 64\n",
      "INFO:src.utils.train:Number of epochs: 100\n",
      "INFO:src.utils.train:Patience: 10\n",
      "INFO:src.utils.train:Monitor metric: val_f1\n",
      "INFO:src.utils.train:Total training batches per epoch: 162\n",
      "INFO:src.utils.train:Starting training from epoch 1 to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒ Switching context to 'SIGNAL' dataset...\n",
      "ðŸš€ Context ready for 'signal'.\n",
      "   Train batches: 162, Val batches: 41\n",
      "   Type: signal\n",
      "   Total Train Samples: 12993\n",
      "   Channels: 19\n",
      "   Sequence Length: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]INFO:src.utils.train:\n",
      "Epoch 1/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.7661 - Avg batch time: 0.22s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.6958 - Avg batch time: 0.02s\n",
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m], weight_decay\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     27\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mwrap_traditional_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAVE_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m, in \u001b[0;36mwrap_traditional_train\u001b[0;34m(model, save_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()  \u001b[38;5;66;03m# Not weighted as we use a balanced sampler!\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hidden attribute\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_load_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# FIXME: remove this before submission\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m plot_training_loss(train_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], val_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:391\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, wandb_config, wandb_project, wandb_run_name, log_wandb, try_load_checkpoint, original_dataset)\u001b[0m\n\u001b[1;32m    388\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training phase\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    389\u001b[0m batch_times \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, data_batch_item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader_to_iterate):\n\u001b[1;32m    392\u001b[0m     batch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Log every 10 batches\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/timeseries_eeg_dataset.py:421\u001b[0m, in \u001b[0;36mTimeseriesEEGDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(idx) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processed_items_count:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of range [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processed_items_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cached_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[idx]\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/timeseries_eeg_dataset.py:449\u001b[0m, in \u001b[0;36mTimeseriesEEGDataset._get_cached_item\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    447\u001b[0m file_path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_file_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:1426\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m-> 1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_is_zipfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   1431\u001b[0m         overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:433\u001b[0m, in \u001b[0;36m_is_zipfile\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Read the first few bytes and match against the ZIP file signature\u001b[39;00m\n\u001b[1;32m    432\u001b[0m local_header_magic_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 433\u001b[0m read_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlocal_header_magic_number\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m f\u001b[38;5;241m.\u001b[39mseek(start)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m read_bytes \u001b[38;5;241m==\u001b[39m local_header_magic_number\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%aimport src.layers.cnn.cnn\n",
    "from src.layers.cnn.cnn import EEGCNNClassifier\n",
    "\n",
    "train_context = timeseries_training_context.switch_to('signal')\n",
    "# build model with current parameters\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"timeseries_signal_cnn_mlp.pt\"\n",
    "model = EEGCNNClassifier(\n",
    "    input_channels=19,\n",
    "    cnn_out_dim=128,\n",
    "    mlp_hidden_dims=[256, 128],\n",
    "    output_dim=1,\n",
    "    cnn_dropout_prob=0.3,\n",
    "    mlp_dropout_prob=0.3,\n",
    "    activation_mlp=\"leaky_relu\",\n",
    "    activation_cnn=\"leaky_relu\",\n",
    "    cnn_use_batch_norm=True,\n",
    "    use_batch_norm_mlp=True,\n",
    ")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "wrap_traditional_train(model, save_path=SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febab4cf",
   "metadata": {},
   "source": [
    "#### CNN-LSTM-MLP (signal-based model)\n",
    "\n",
    "In this section we test the CNN-LSTM model, which combines convolutional layers to extract spatial features from the EEG signals and LSTM layers to learn the temporal dependencies in the data. This architecture is particularly effective for EEG seizure detection, as it captures both spatial and temporal patterns in the signals. A final fully connected layer is used to map the extracted features to the output labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcae7288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.train:Starting training setup...\n",
      "INFO:src.utils.train:Model type: Standard\n",
      "INFO:src.utils.train:Device: cuda\n",
      "INFO:src.utils.train:Batch size: 64\n",
      "INFO:src.utils.train:Number of epochs: 100\n",
      "INFO:src.utils.train:Patience: 10\n",
      "INFO:src.utils.train:Monitor metric: val_f1\n",
      "INFO:src.utils.train:Total training batches per epoch: 162\n",
      "INFO:src.utils.train:Starting training from epoch 1 to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒ Switching context to 'SIGNAL' dataset...\n",
      "ðŸš€ Context ready for 'signal'.\n",
      "   Train batches: 162, Val batches: 41\n",
      "   Type: signal\n",
      "   Total Train Samples: 12993\n",
      "   Channels: 19\n",
      "   Sequence Length: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]INFO:src.utils.train:\n",
      "Epoch 1/100 - Training phase\n",
      "INFO:src.utils.train:Processing batch 1/162\n",
      "INFO:src.utils.train:Batch 1/162 - Loss: 0.6675 - Avg batch time: 0.49s\n",
      "INFO:src.utils.train:Processing batch 11/162\n",
      "INFO:src.utils.train:Batch 11/162 - Loss: 0.7106 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 21/162\n",
      "INFO:src.utils.train:Batch 21/162 - Loss: 0.6888 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 31/162\n",
      "INFO:src.utils.train:Batch 31/162 - Loss: 0.7558 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 41/162\n",
      "INFO:src.utils.train:Batch 41/162 - Loss: 0.7465 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 51/162\n",
      "INFO:src.utils.train:Batch 51/162 - Loss: 0.7816 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 61/162\n",
      "INFO:src.utils.train:Batch 61/162 - Loss: 0.7152 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 71/162\n",
      "INFO:src.utils.train:Batch 71/162 - Loss: 0.7065 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 81/162\n",
      "INFO:src.utils.train:Batch 81/162 - Loss: 0.7525 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 91/162\n",
      "INFO:src.utils.train:Batch 91/162 - Loss: 0.7814 - Avg batch time: 0.04s\n",
      "INFO:src.utils.train:Processing batch 101/162\n",
      "INFO:src.utils.train:Batch 101/162 - Loss: 0.7061 - Avg batch time: 0.04s\n",
      "Epochs:   1%|â–Š                                                                                  | 1/100 [00:58<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m], weight_decay\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     26\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mwrap_traditional_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAVE_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m, in \u001b[0;36mwrap_traditional_train\u001b[0;34m(model, save_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()  \u001b[38;5;66;03m# Not weighted as we use a balanced sampler!\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m train_history, val_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hidden attribute\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_load_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# FIXME: remove this before submission\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m plot_training_loss(train_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], val_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/train.py:391\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, scheduler, monitor, patience, num_epochs, grad_clip, overwrite, use_gnn, wandb_config, wandb_project, wandb_run_name, log_wandb, try_load_checkpoint, original_dataset)\u001b[0m\n\u001b[1;32m    388\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training phase\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    389\u001b[0m batch_times \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, data_batch_item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader_to_iterate):\n\u001b[1;32m    392\u001b[0m     batch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Log every 10 batches\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/timeseries_eeg_dataset.py:421\u001b[0m, in \u001b[0;36mTimeseriesEEGDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(idx) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processed_items_count:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of range [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processed_items_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cached_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[idx]\n",
      "File \u001b[0;32m~/NeuroGraphNet/src/utils/timeseries_eeg_dataset.py:449\u001b[0m, in \u001b[0;36mTimeseriesEEGDataset._get_cached_item\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    447\u001b[0m file_path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_file_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:1462\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1469\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1470\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/serialization.py:1964\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   1963\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1964\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1965\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1967\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/venvs/neuro/lib/python3.10/site-packages/torch/_weights_only_unpickler.py:460\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m TUPLE2[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])]\n\u001b[0;32m--> 460\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m TUPLE3[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:] \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])]\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Basic types construction\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%aimport src.layers.cnn.cnn_lstm\n",
    "from src.layers.cnn.cnn_lstm import EEGCNNBiLSTMClassifier \n",
    "\n",
    "train_context = timeseries_training_context.switch_to('signal')\n",
    "# build model with current parameters\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"timeseries_signal_cnn_lstm_mlp.pt\"\n",
    "model = EEGCNNBiLSTMClassifier(\n",
    "    input_channels=19,\n",
    "    cnn_out_dim=128,\n",
    "    mlp_hidden_dims=[256, 128],\n",
    "    output_dim=1,\n",
    "    cnn_dropout_prob=0.3,\n",
    "    mlp_dropout_prob=0.3,\n",
    "    activation_mlp=\"leaky_relu\",\n",
    "    cnn_use_batch_norm=True,\n",
    "    use_batch_norm_mlp=True,\n",
    ")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for this combination.\")\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming this remains constant\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "wrap_traditional_train(model, save_path=SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae02a362",
   "metadata": {},
   "source": [
    "## Training and Evaluation - Graph-Based Models\n",
    "\n",
    "This section focuses on training and evaluating Graph Neural Network (GNN) models on the selected graph-based datasets. These models leverage the spatial and functional relationships between EEG electrodes to improve seizure detection accuracy.\n",
    "\n",
    "### Available Graph-Based Architectures\n",
    "\n",
    "The notebook implements several hybrid architectures that combine temporal and graph processing:\n",
    "\n",
    "- **CNN-BiLSTM-GCN**: Combines Convolutional Neural Networks for feature extraction, Bidirectional LSTM for temporal modeling, and Graph Convolutional Networks for spatial relationships\n",
    "- **CNN-BiLSTM-GAT**: Similar to above but uses Graph Attention Networks instead of GCN for learning adaptive attention weights between electrodes\n",
    "- **CNN-BiLSTM-Attention-GNN**: Enhanced version with attention mechanisms in both temporal and graph components\n",
    "\n",
    "### Graph Construction Strategies\n",
    "\n",
    "The models can be trained on different graph construction approaches:\n",
    "- **Spatial graphs**: Based on physical electrode distances (19 channels)\n",
    "- **Correlation graphs**: Dynamic graphs based on signal correlations (top-k=5)\n",
    "- **Absolute difference correlation**: Advanced correlation-based graphs (top-k=8)\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "All graph models use:\n",
    "- **Optimizer**: AdamW with learning rate 1e-4 and weight decay 0.01\n",
    "- **Loss function**: BCEWithLogitsLoss (unweighted due to balanced sampling)\n",
    "- **Scheduler**: ReduceLROnPlateau with factor 0.5 and patience 5\n",
    "- **Early stopping**: Patience of 10 epochs based on validation F1 score\n",
    "- **Data handling**: GeoDataLoader for efficient graph batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72841db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_loader_manager = LazyDataLoaderManager(\n",
    "    graph_datasets,\n",
    "    oversampling_power=oversampling_power,\n",
    "    batch_size=64\n",
    ")\n",
    "graph_training_context = TrainingContext(graph_loader_manager)\n",
    "train_context = graph_training_context.switch_to('spatial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177fedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.utils.train\n",
    "def wrap_gnn_train(model, save_path):\n",
    "    global graph_training_context\n",
    "    if 'graph_training_context' not in globals():\n",
    "        raise ValueError(\"Graph training context is not initialized. Please initialize it before calling this function.\")\n",
    "    if not isinstance(graph_training_context, TrainingContext):\n",
    "        raise ValueError(\"graph_training_context must be an instance of TrainingContext.\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "    # optimizer = Lion(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5)\n",
    "    loss = nn.BCEWithLogitsLoss()  # Not weighted as we use a balanced sampler!\n",
    "\n",
    "    # train model\n",
    "    train_history, val_history = train_model(\n",
    "        wandb_config=None,\n",
    "        model=model,\n",
    "        train_loader=graph_training_context.train_loader,\n",
    "        val_loader=graph_training_context.val_loader,\n",
    "        criterion=loss,\n",
    "        scheduler=scheduler,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=config[\"epochs\"],\n",
    "        patience=config[\"patience\"],\n",
    "        save_path=save_path,\n",
    "        use_gnn=True,\n",
    "        # hidden attribute\n",
    "        try_load_checkpoint=True,\n",
    "    )\n",
    "    plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720925b0",
   "metadata": {},
   "source": [
    "### Test 3 - First breakthrough model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036835f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm .checkpoints/cnn_bilstm_gcn_test_3_correlation_test_mean_pooling.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a43eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_test_best_model_{DATASET_TYPE}_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm = True,\n",
    "    encoder_use_layer_norm = False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim = 192,\n",
    "    out_channels = 128,\n",
    "    pooling_type = \"max\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_conv_layers = 4,\n",
    "    gcn_dropout = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    "    graph_feature_dim=graph_feature_dim,\n",
    "    use_graph_features=False\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a5bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "from src.layers.hybrid.cnn_bilstm_gat import EEGCNNBiLSTMGAT\n",
    "\n",
    "\n",
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gat_test_3_{DATASET_TYPE}_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGAT(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout=0.25,\n",
    "    lstm_hidden_dim=128,\n",
    "    lstm_out_dim=128,\n",
    "    lstm_dropout=0.25,\n",
    "    encoder_use_batch_norm=True,\n",
    "    encoder_use_layer_norm=False,\n",
    "    # Parameters for the GAT (graph attention network)\n",
    "    hidden_dim=128,\n",
    "    out_channels=96,\n",
    "    pooling_type=\"mean\",\n",
    "    gat_use_batch_norm=True,\n",
    "    num_conv_layers=3,\n",
    "    gat_dropout=0.5,\n",
    "    gat_heads=4,  # Number of attention heads\n",
    "    num_channels=19,\n",
    "    # Graph features configuration\n",
    "    graph_feature_dim=graph_feature_dim,\n",
    "    use_graph_features=False if DATASET_TYPE == 'spatial' else True,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_test_3_{DATASET_TYPE}_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim = 128,\n",
    "    out_channels = 96,\n",
    "    pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_conv_layers = 3,\n",
    "    gcn_dropout = 0.5,\n",
    "    num_channels = 19,\n",
    "    # enable graph features\n",
    "    # NOTE: using graph level features gives worse results with spatial dataset\n",
    "    use_graph_features=False if DATASET_TYPE == 'spatial' else True,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11afb6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / f\"cnn_bilstm_gcn_test_3_{DATASET_TYPE}_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout=0.25,\n",
    "    lstm_hidden_dim=128,\n",
    "    lstm_out_dim=128,\n",
    "    lstm_dropout=0.25,\n",
    "    encoder_use_batch_norm=True,\n",
    "    encoder_use_layer_norm=False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim=128,\n",
    "    out_channels=128,\n",
    "    pooling_type=\"mean\",\n",
    "    gcn_use_batch_norm=True,\n",
    "    num_conv_layers=3,\n",
    "    gcn_dropout=0.5,\n",
    "    num_channels=19,\n",
    "    use_graph_features=True\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424f5b8b",
   "metadata": {},
   "source": [
    "### Test 4 - Smaller CGN output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_4.pt\"\n",
    "\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 64,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 3,\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eaaf2d",
   "metadata": {},
   "source": [
    "### Test 5 - Smaller GCN output channels + increased embedding length + Deeper GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1335c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_5.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 64,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da60626",
   "metadata": {},
   "source": [
    "\n",
    "### Test 6: slighly bigger GCN output channels\n",
    ">[HIGHEST F1 SCORE EVER RECORDED]\n",
    "```\n",
    "âœ… Checkpoint loaded. Resuming from epoch 33. Best 'val_f1' score: 0.7346\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_6.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 96,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad429a",
   "metadata": {},
   "source": [
    "### Test 7B: Alternative architecture to improve generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f882bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_8.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.35, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.35, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 96,\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b4f3c",
   "metadata": {},
   "source": [
    "### Test 7C: slightly bigger GCN layers\n",
    "\n",
    "BEST MODEL YET!\n",
    "\n",
    "(SPATIAL FEATURES!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"cnn_bilstm_gcn_test_best_model_correlation_test_mean_pooling.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm = True,\n",
    "    encoder_use_layer_norm = False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    hidden_dim = 192,\n",
    "    out_channels = 128,\n",
    "    pooling_type = \"max\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_conv_layers = 4,\n",
    "    gcn_dropout = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    "    graph_feature_dim=graph_feature_dim,\n",
    "    use_graph_features=False if DATASET_TYPE == 'spatial' else True,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6355457",
   "metadata": {},
   "source": [
    "### Test 7D: even bigger GCN layers\n",
    "\n",
    "Comparable performance to best model. We might need to increase the number of GCN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed9d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_even_bigger.pt\"\n",
    "model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm = True,\n",
    "    encoder_use_layer_norm = False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 224,\n",
    "    gcn_out_channels = 192,\n",
    "    gcn_pooling_type = \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    ")\n",
    "wrap_gnn_train(model, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50956294",
   "metadata": {},
   "source": [
    "### Test 7E: increased number of GCN layers\n",
    "\n",
    "Assumption: the previous model was unable to learn enough, maybe the GCN was unable to capture\n",
    "\n",
    "```\n",
    "Epochs:   9%| | 9/100 [17:54<3:23:31, 134.20s/it, train_loss=0.4532, val_loss=0.3489, best_val_f1=0.6695, lr=5.00e-05, b2025-06-07 17:01:05 - INFO - \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_even_more_bigger.pt\"\n",
    "model_generalizable_even_more_bigger = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 224,\n",
    "    gcn_out_channels = 192,\n",
    "    gcn_num_layers = 5,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08442186",
   "metadata": {},
   "source": [
    "### Test 7F: Increased number of BiLSTM layers + Test 7E architecture\n",
    "\n",
    "Assumpion: we saw a drammatical increase in accuracy by increasing the number of GCN layers. This hints that the model was now able to learn the most from the embeddings. To improve the performance even further without having to increase the number of GCN layers even more (overall reduce complexity, improve generalization), we will try to increase the number of BiLSTM layers. \n",
    "\n",
    "Using multiple BiLSTM layers will allow embeddings to be processed in a more complex way, potentially capturing more intricate relationships in the data. The GCN layers will take care of the graph structure, while the BiLSTM layers will enhance the temporal dependencies and relationships in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b963a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_even_more_bigger.pt\"\n",
    "model_generalizable_even_more_bigger = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    lstm_num_layers = 2,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 224,\n",
    "    gcn_out_channels = 192,\n",
    "    gcn_num_layers = 5,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663332c",
   "metadata": {},
   "source": [
    "```\n",
    "Epochs:   1%|â–Š                                                                                  | 1/100 [00:00<?, ?it/s]2025-06-07 18:55:16 - INFO -\n",
    "Epochs:   2%| | 2/100 [04:35<7:29:19, 275.10s/it, train_loss=0.6212, val_loss=0.4619, best_val_f1=0.4055, lr=1.00e-04, b2025-06-07 18:59:51 - INFO -\n",
    "Epochs:   3%| | 3/100 [09:09<7:23:49, 274.53s/it, train_loss=0.5819, val_loss=0.4295, best_val_f1=0.4055, lr=1.00e-04, b2025-06-07 19:04:25 - INFO -\n",
    "Epochs:   4%| | 4/100 [13:42<7:18:31, 274.08s/it, train_loss=0.5628, val_loss=0.4437, best_val_f1=0.4055, lr=1.00e-04, b2025-06-07 19:08:59 - INFO -\n",
    "Epochs:   5%| | 5/100 [18:16<7:13:28, 273.78s/it, train_loss=0.5452, val_loss=0.3942, best_val_f1=0.4858, lr=1.00e-04, b2025-06-07 19:13:32 - INFO -\n",
    "Epochs:   6%| | 6/100 [22:49<7:08:41, 273.63s/it, train_loss=0.5334, val_loss=0.4563, best_val_f1=0.4858, lr=1.00e-04, b2025-06-07 19:18:05 - INFO -\n",
    "Epochs:   7%| | 7/100 [27:22<7:04:01, 273.57s/it, train_loss=0.5319, val_loss=0.3738, best_val_f1=0.5137, lr=1.00e-04, b2025-06-07 19:22:39 - INFO -\n",
    "Epochs:   8%| | 8/100 [31:56<6:59:20, 273.48s/it, train_loss=0.5181, val_loss=0.4369, best_val_f1=0.5695, lr=1.00e-04, b2025-06-07 19:27:12 - INFO -\n",
    "Epochs:   9%| | 9/100 [36:29<6:54:50, 273.52s/it, train_loss=0.5220, val_loss=0.4202, best_val_f1=0.5695, lr=1.00e-04, b2025-06-07 19:31:46 - INFO -\n",
    "Epochs:  10%| | 10/100 [41:03<6:50:17, 273.52s/it, train_loss=0.5286, val_loss=0.4167, best_val_f1=0.5695, lr=1.00e-04, 2025-06-07 19:36:19 - INFO -\n",
    "Epochs:  11%| | 11/100 [45:36<6:45:44, 273.53s/it, train_loss=0.5065, val_loss=0.3864, best_val_f1=0.5695, lr=1.00e-04, 2025-06-07 19:40:53 - INFO -\n",
    "Epochs:  12%| | 12/100 [50:10<6:41:03, 273.45s/it, train_loss=0.5158, val_loss=0.5175, best_val_f1=0.5695, lr=5.00e-05, 2025-06-07 19:45:26 - INFO -\n",
    "Epochs:  13%|â–| 13/100 [54:43<6:36:23, 273.37s/it, train_loss=0.5035, val_loss=0.3785, best_val_f1=0.5940, lr=5.00e-05, 2025-06-07 19:49:59 - INFO -\n",
    "Epochs:  14%|â–| 14/100 [59:16<6:31:50, 273.38s/it, train_loss=0.4842, val_loss=0.3838, best_val_f1=0.5981, lr=5.00e-05, 2025-06-07 19:54:33 - INFO -\n",
    "Epochs:  15%|â–| 15/100 [1:03:50<6:27:17, 273.38s/it, train_loss=0.4644, val_loss=0.3493, best_val_f1=0.6106, lr=5.00e-052025-06-07 19:59:06 - INFO -\n",
    "Epochs:  16%|â–| 16/100 [1:08:23<6:22:46, 273.41s/it, train_loss=0.4887, val_loss=0.3737, best_val_f1=0.6106, lr=5.00e-052025-06-07 20:03:39 - INFO -\n",
    "Epochs:  17%|â–| 17/100 [1:12:57<6:18:12, 273.41s/it, train_loss=0.4775, val_loss=0.3565, best_val_f1=0.6106, lr=5.00e-052025-06-07 20:08:13 - INFO -\n",
    "Epochs:  18%|â–| 18/100 [1:17:30<6:13:42, 273.44s/it, train_loss=0.4635, val_loss=0.3704, best_val_f1=0.6106, lr=2.50e-052025-06-07 20:12:46 - INFO -\n",
    "Epochs:  19%|â–| 19/100 [1:22:04<6:09:15, 273.53s/it, train_loss=0.4501, val_loss=0.3635, best_val_f1=0.6131, lr=2.50e-052025-06-07 20:17:20 - INFO -\n",
    "Epochs:  20%|â–| 20/100 [1:26:37<6:04:39, 273.49s/it, train_loss=0.4379, val_loss=0.3638, best_val_f1=0.6179, lr=2.50e-052025-06-07 20:21:53 - INFO -\n",
    "Epochs:  21%|â–| 21/100 [1:31:10<6:00:01, 273.43s/it, train_loss=0.4494, val_loss=0.3543, best_val_f1=0.6179, lr=2.50e-052025-06-07 20:26:27 - INFO -\n",
    "Epochs:  22%|â–| 22/100 [1:35:44<5:55:26, 273.42s/it, train_loss=0.4616, val_loss=0.3616, best_val_f1=0.6659, lr=2.50e-052025-06-07 20:31:00 - INFO -\n",
    "Epochs:  23%|â–| 23/100 [1:40:17<5:50:54, 273.44s/it, train_loss=0.4381, val_loss=0.3532, best_val_f1=0.6659, lr=2.50e-052025-06-07 20:35:34 - INFO -\n",
    "Epochs:  24%|â–| 24/100 [1:44:51<5:46:22, 273.45s/it, train_loss=0.4423, val_loss=0.3635, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:40:07 - INFO -\n",
    "Epochs:  25%|â–Ž| 25/100 [1:49:24<5:41:52, 273.49s/it, train_loss=0.4291, val_loss=0.3473, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:44:41 - INFO -\n",
    "Epochs:  26%|â–Ž| 26/100 [1:53:58<5:37:12, 273.42s/it, train_loss=0.4403, val_loss=0.3380, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:49:14 - INFO -\n",
    "Epochs:  27%|â–Ž| 27/100 [1:58:31<5:32:38, 273.40s/it, train_loss=0.4312, val_loss=0.3374, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:53:47 - INFO -\n",
    "Epochs:  28%|â–Ž| 28/100 [2:03:05<5:28:07, 273.44s/it, train_loss=0.4393, val_loss=0.3441, best_val_f1=0.6659, lr=1.25e-052025-06-07 20:58:21 - INFO -\n",
    "Epochs:  29%|â–Ž| 29/100 [2:07:38<5:23:35, 273.46s/it, train_loss=0.4226, val_loss=0.3392, best_val_f1=0.6659, lr=1.25e-052025-06-07 21:02:54 - INFO -\n",
    "Epochs:  30%|â–Ž| 30/100 [2:12:11<5:19:02, 273.46s/it, train_loss=0.4240, val_loss=0.3525, best_val_f1=0.6659, lr=6.25e-062025-06-07 21:07:28 - INFO -\n",
    "Epochs:  31%|â–Ž| 31/100 [2:16:45<5:14:28, 273.46s/it, train_loss=0.4249, val_loss=0.3492, best_val_f1=0.6659, lr=6.25e-062025-06-07 21:12:01 - INFO -\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_generalizable_optimized.pt\"\n",
    "model_generalizable_optimized = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 160,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    lstm_num_layers = 2,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 192,\n",
    "    gcn_out_channels = 128,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.5, # slightly higher dropout to avoid overfitting\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c4b3a",
   "metadata": {},
   "source": [
    "### Test 8: Narrow but Deep GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff01da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_narrow_deep_model.pt\"\n",
    "narrow_deep_model = EEGCNNBiLSTMGCN(\n",
    "    # --- Simplify the Temporal Encoder ---\n",
    "    cnn_dropout_prob = 0.2,\n",
    "    lstm_hidden_dim = 64,  # Reduced\n",
    "    lstm_out_dim = 64,     # Reduced\n",
    "    lstm_dropout_prob = 0.2,\n",
    "    # --- Focus on the GCN ---\n",
    "    gcn_hidden_channels = 128, # Keep GCN capacity high\n",
    "    gcn_out_channels = 64,\n",
    "    gcn_num_layers = 5,      # Try going even deeper\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ef8b3",
   "metadata": {},
   "source": [
    "### Test 9: First best model, with wider + deeper GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_new_old_best_model.pt\"\n",
    "new_old_best_model = EEGCNNBiLSTMGCN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25,\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 128,\n",
    "    gcn_out_channels = 128, # from 64 to 128\n",
    "    gcn_num_layers = 4, # from 3 to 4\n",
    "    gcn_dropout_prob = 0.5,\n",
    "    num_classes = 1,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351da028",
   "metadata": {},
   "source": [
    "### Best model + attention BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.layers.hybrid.cnn_bilstm_attention_gcn\n",
    "from src.layers.hybrid.cnn_bilstm_attention_gcn import EEGCNNBiLSTMAttentionGNN\n",
    "\n",
    "SAVE_PATH = CHECKPOINT_ROOT / \"lstm_gnn_attention.pt\"\n",
    "model_first_attention = EEGCNNBiLSTMAttentionGNN(\n",
    "    # Parameters for the CNN_BiLSTM_Encoder (temporal encoder)\n",
    "    cnn_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    lstm_hidden_dim = 128,\n",
    "    lstm_out_dim = 128,\n",
    "    lstm_dropout_prob = 0.25, # slightly higher dropout to avoid overfitting\n",
    "    encoder_use_batch_norm= True,\n",
    "    encoder_use_layer_norm= False,\n",
    "    # Parameters for the EEGGCN (graph neural network)\n",
    "    gcn_hidden_channels = 192,\n",
    "    gcn_out_channels = 128,\n",
    "    gcn_num_layers = 4,\n",
    "    gcn_dropout_prob = 0.6, # slightly higher dropout to avoid overfitting\n",
    "    gcn_pooling_type= \"mean\",\n",
    "    gcn_use_batch_norm = True,\n",
    "    num_channels = 19,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a8a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from src.utils.train import train_model\n",
    "\n",
    "model = model_small_gcn_bigger_embedding\n",
    "model = model.to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "# optimizer = Lion(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "loss = nn.BCEWithLogitsLoss() # Not weighted as we use a balanced sampler!\n",
    "\n",
    "# empty cache in order to free up VRAM (if available)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# train model\n",
    "train_history, val_history = train_model(\n",
    "    wandb_config=None,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=loss,\n",
    "    scheduler=scheduler,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=config[\"epochs\"],\n",
    "    patience=config[\"patience\"],\n",
    "    save_path=SAVE_PATH,\n",
    "    use_gnn=True,\n",
    "    # hidden attribute\n",
    "    try_load_checkpoint=True,\n",
    ")\n",
    "\n",
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch cuda clear cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63329a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.plot import plot_training_loss\n",
    "\n",
    "plot_training_loss(train_history[\"loss\"], val_history[\"loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
